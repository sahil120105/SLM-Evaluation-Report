[
  {
    "id": "gen_nat_000",
    "category": "feature_request",
    "source_file": "microsoft_vscode_issue.json",
    "context": "Repository: microsoft/vscode\nTitle: NES + notebooks: Suggestions rendered below viewport\n\nA user reported the following issue titled 'NES + notebooks: Suggestions rendered below viewport' in the 'microsoft/vscode' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! feature_request_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\nWhen I got suggestions from NES that were outside my current viewport (i.e. some number of lines below in my current cell), I wasn't shown a downward arrow in the gutter that signaled to me there was an NES below. I just stumbled upon these edits by deciding to scroll myself.\n\nI'd expect a downward arrow, like we use in other text documents.\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'DonJayamanne' said:\n---\nI can repro this.\n---\n\nUser 'DonJayamanne' said:\n---\nUnable to repro any more\nWorks now, will keep trying to repro the issue\nGiven this is standard functionality and no special casing for notebooks I am assuming there must have been some changes unrelated to this.\n---\n\nUser 'bamurtaugh' said:\n---\nLooks like I am still able to reproduce. I changed the name of a variable in an early notebook cell, and the NES in notebooks does direct me to 1) later spots in the same cell, and 2) later cells across the notebook. \n\nBut in the final case shown at the end of this gif, I ran into the same issue of a suggestion being rendered outside the viewport: \n- I waited to see if I'd get NES\n- I didn't get a downward arrow, so I thought maybe no NES\n- When I scrolled down the notebook myself, I saw there was actually an NES that I wasn't notified about in my previous viewport\n\n![Image](https://github.com/user-attachments/assets/3458cfad-0945-4e73-8b54-b118ac350b27)\n---\n\nUser 'bamurtaugh' said:\n---\nThis is the kind of downward arrow I get in other file types when there's an NES below my current viewport:\n\n<img width=\"1368\" height=\"348\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8e38a191-1819-4add-9ff6-d80c941db521\" />\n---\n\nUser 'DonJayamanne' said:\n---\n@bamurtaugh Just an update, this is an existing issue with NES but specific to Notebooks. I'm looking into this, unlikely I'll land a fix in the release. Very interesting bug, caused by a long standing bug/debt item in Notebooks.\n\n@rebornix I am looking into this today, not sure I can get this fixed in time.\nGiven this issue has existed for a while (ie. its not specific to the new NES work in notebook).\nIts caused by the fact that we render lines in the Monaco Editor outside the view port (i.e. in previous example the editor is the size of viewport, however editor is scroll out of view, thus editor thinks lines are still visible hence NES is rendered below viewport).\nFix is to ensure editor doesn't scroll beyond viewport, by resizing it as we scroll (this requires careful thought and implementation)\n---",
    "question": "Has anyone seen 'NES + notebooks: Suggestions rendered below viewport' in microsoft/vscode? I can't figure out the cause.",
    "ideal_answer": "I can repro this.\n\nUnable to repro any more\nWorks now, will keep trying to repro the issue\nGiven this is standard functionality and no special casing for notebooks I am assuming there must have been some changes unrelated to this.\n\nLooks like I am still able to reproduce. I changed the name of a variable in an early notebook cell, and the NES in notebooks does direct me to 1) later spots in the same cell, and 2) later cells across the notebook. \n\nBut in the final case shown at the end of this gif, I ran into the same issue of a suggestion being rendered outside the viewport: \n- I waited to see if I'd get NES\n- I didn't get a downward arrow, so I thought maybe no NES\n- When I scrolled down the notebook myself, I saw there was actually an NES that I wasn't notified about in my previous viewport\n\n![Image](https://github.com/user-attachments/assets/3458cfad-0945-4e73-8b54-b118ac350b27)\n\nThis is the kind of downward arrow I get in other file types when there's an NES below my current viewport:\n\n<img width=\"1368\" height=\"348\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8e38a191-1819-4add-9ff6-d80c941db521\" />\n\n@bamurtaugh Just an update, this is an existing issue with NES but specific to Notebooks. I'm looking into this, unlikely I'll land a fix in the release. Very interesting bug, caused by a long standing bug/debt item in Notebooks.\n\n@rebornix I am looking into this today, not sure I can get this fixed in time.\nGiven this issue has existed for a while (ie. its not specific to the new NES work in notebook).\nIts caused by the fact that we render lines in the Monaco Editor outside the view port (i.e. in previous example the editor is the size of viewport, however editor is scroll out of view, thus editor thinks lines are still visible hence NES is rendered below viewport).\nFix is to ensure editor doesn't scroll beyond viewport, by resizing it as we scroll (this requires careful thought and implementation)"
  },
  {
    "id": "gen_nat_001",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: Sometime Liveness/Readiness Probes fail because of net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\n\nA user reported the following issue titled 'Sometime Liveness/Readiness Probes fail because of net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n**What happened**:\r\nIn my cluster sometimes readiness the probes are failing. But the application works fine.\r\n```\r\nApr 06 18:15:14 kubenode** kubelet[34236]: I0406 18:15:14.056915   34236 prober.go:111] Readiness probe for \"default-nginx-daemonset-4g6b5_default(a3734646-77fd-11ea-ad94-509a4c9f2810):nginx\" failed (failure): Get http://172.18.123.127:80/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\r\n```\r\n\r\n**What you expected to happen**:\r\nSuccessful Readiness Probe.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\nWe have few clusters with different workloads.\r\nOnly in cluster with big number of short living pods we have this issue.\r\nBut not on all nodes.\r\nWe can't reproduce this error on other our clusters (that have same configuration but different workload).\r\nHow i found the issue?\r\nI deployed a daemonset:\r\n```\r\napiVersion: extensions/v1beta1\r\nkind: DaemonSet\r\nmetadata:\r\n  name: default-nginx-daemonset\r\n  namespace: default\r\n  labels:\r\n    k8s-app: default-nginx\r\nspec:\r\n  selector:\r\n    matchLabels:\r\n      name: default-nginx\r\n  template:\r\n    metadata:\r\n      labels:\r\n        name: default-nginx\r\n    spec:\r\n      tolerations:\r\n      - operator: Exists\r\n      containers:\r\n      - name: nginx\r\n        image: nginx:latest\r\n        resources:\r\n          limits:\r\n            cpu: \"1\"\r\n            memory: \"1Gi\"\r\n          requests:\r\n            cpu: \"1\"\r\n            memory: \"1Gi\"\r\n        readinessProbe:\r\n          httpGet:\r\n            path: /\r\n            port: 80\r\n```\r\nThen i started to listen events on all pods of this daemonset.\r\nAfter a couple of time i received next events:\r\n```\r\nWarning  Unhealthy  110s (x5 over 44m)  kubelet, kubenode20  Readiness probe failed: Get http://172.18.122.143:80/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\r\nWarning  Unhealthy  11m (x3 over 32m)  kubelet, kubenode10  Readiness probe failed: Get http://172.18.65.57:80/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\r\n....\r\n```\r\nThose events where on ~50% of pods of this daemonset.\r\n\r\nOn the nodes where the pods with failed probes was placed, I collected the logs of kubelet.\r\nAnd there was errors like:\r\n```\r\nApr 06 14:08:35 kubenode20 kubelet[10653]: I0406 14:08:35.464223   10653 prober.go:111] Readiness probe for \"default-nginx-daemonset-nkwkf_default(90a3883b-77f3-11ea-ad94-509a4c9f2810):nginx\" failed (failure): Get http://172.18.122.143:80/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\r\n```\r\n\r\nI was thinking that sometimes the nginx in pod really response slowly.\r\nFor excluding this theory, I created a short script that curl the application in pod and store response time in a file:\r\n```\r\nwhile true; do curl http://172.18.122.143:80/ -s -o /dev/null -w  \"%{time_starttransfer}\\n\" >> /tmp/measurment.txt; don\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'yuripastushenko' said:\n---\n/sig network\n---\n\nUser 'athenabot' said:\n---\n/triage unresolved\n\nComment `/remove-triage unresolved` when the issue is assessed and confirmed.\n\n\ud83e\udd16 I am a bot run by vllry. \ud83d\udc69\u200d\ud83d\udd2c\n---\n\nUser 'Nittarab' said:\n---\nAre you sure the application don't hit the resources limits? \r\n\r\nIn my case, the application starting fine, then the container start using more resources until he hit the limit. After that the readiness probes fail\n---\n\nUser 'yuripastushenko' said:\n---\nYes, I am sure. I collected the metrics.\r\nCpu usage on pod was around 0.001\r\nRam - around 4mb.\r\nThis is a test application (pure nginx image) that did not do anything (no traffic is sent to it).\n---\n\nUser 'yuripastushenko' said:\n---\n/area kubelet\n---\n\nUser 'manikanta-kondeti' said:\n---\nany update on this? we have been facing similar issues since few weeks\n---\n\nUser 'thockin' said:\n---\nThis is the first such report I have seen.  There's nothing obvious about why this would happen.\r\n\r\nIt's possible kubelet is too busy and starved for CPU and the probe happened to be thing that got thrashed.  How many pods are on this machine?  How busy is it?\r\n\r\nIt's possible the node itself is thrashing or something and OOM behavior is weird.  Does dmesg show any OOMs?\r\n\r\nIt's possible some other failure down deep in kubelet is being translated into this?  You could try running kubelet at a higher log level to get more details on what is happening.\r\n\r\nA lot of bugs have been fixed since 1.12, so we'd need to try to reproduce this and then try again in a more recent version.  Is there any way you can help boil down a simpler reproduction?\n---\n\nUser 'athenabot' said:\n---\n@thockin\nIf this issue has been triaged, please comment `/remove-triage unresolved`.\n\nIf you aren't able to handle this issue, consider unassigning yourself and/or adding the `help-wanted` label.\n\n\ud83e\udd16 I am a bot run by vllry. \ud83d\udc69\u200d\ud83d\udd2c\n---\n\nUser 'manikanta-kondeti' said:\n---\n@thockin  \r\n`returns-console-558995dd78-b6bjm                                  0/1     Running            0          23h`\r\n\r\nFor example, if you see the above get pods, `readiness probe` of one pod is failing almost every day. A restart would fix this. However, we're unable to find the root cause. But I don't see any abnormal numbers on CPU or memory or thread count. Doing a `describe pod` would give me that `readiness` has failed. How do we debug in such scenario? \r\n\r\nAlso, this is happening for this particular deployment only. The other pods are running fine.\r\n\r\n\"returns-console-558995dd78-pbjf8                                  1/1     Running            0          23h\r\nreturns-console-558995dd78-plqdr                                  1/1     Running            0          23h\r\nreturns-console-558995dd78-tlb5k                                  1/1     Running            0          23h\r\nreturns-console-558995dd78-tr2kd                                  1/1     Running            0          23h\r\nreturns-console-558995dd78-vfj2n                                  1/1     Running            0          23h\"\n---\n\nUser 'tarunwadhwa13' said:\n---\nWe too are facing the same issue in our cluster. \r\n\r\nKubernetes Version - 1.16.8\r\nKernel - 4.4.218-1\r\nOS - Centos 7\r\n\r\nInstalled using Kubespray\r\n\r\nIn our case, timeouts were not related to application but related to specific nodes in cluster. In our 13 node cluster, node 1 to 4 had some kind of issue wherein the pods running on these nodes had random failures due to timeouts. \r\n\r\nChecked that there weren't any cpu aur memory usage spikes also.\r\n\r\nP.S We are using NodePort for production use case. Is it possible that the node port setup cannot handle too many socket connections?\n---\n\nUser 'thockin' said:\n---\nI have no idea what might cause spurious probe failues.  @derekwaynecarr have you heard any other reports of this?\r\n\r\n@tarunwadhwa13 are you saying PROBES failed (always same node) or user traffic failed?  If you have any other information about what was going on with those nodes when the failures happen, it would help.  Check for OOMs, high CPU usage, conntrack failures?\n---\n\nUser 'tarunwadhwa13' said:\n---\n@thockin  Conntrack shows hardly 2 or 3 errors. Memory consumption is 60-65% per node.\r\n\r\nJust found that the timeouts were for almost all request and not just probe. We added istio lately to check connection stats and understand if the behaviour was due to application. But the findings were weird. Istio itself is now failing readiness probe quite frequently\r\n\r\n![image](https://user-images.githubusercontent.com/20948402/82115058-1c053c00-977e-11ea-828e-bde30286faed.png)\r\n\r\n157 failures in ~3 hours \r\n\r\nWould like to add that kubernetes is running in our Datacenter. And since iptables version is 1.4.21, --random-fully is not being implemented. But since all machines have same configuration, we ruled out this possibility\n---\n\nUser 'casey-robertson' said:\n---\nI apologize for not having a lot of details to share but I'd add my 2 cents.  We recently upgraded from Istio 1.4.4 to 1.5.4 and started seeing the issues described by OP.  Lots of liveness / readiness issues there were not happening before.  It SEEMS like adding let's say a 20sec initial delay had helped in most cases.  At this point we are still seeing it and not sure what the root cause is. \r\n\r\nrunning on EKS 1.15 (control plane) / 1.14 managed nodeGroups\n---\n\nUser 'athenabot' said:\n---\n@thockin\nIf this issue has been triaged, please comment `/remove-triage unresolved`.\n\nIf you aren't able to handle this issue, consider unassigning yourself and/or adding the `help-wanted` label.\n\n\ud83e\udd16 I am a bot run by vllry. \ud83d\udc69\u200d\ud83d\udd2c\n---\n\nUser 'thockin' said:\n---\nI'm afraid the only way to know more is to get something like a tcpdump from both inside and outside the pod, which captures one or more failing requests.\r\n\r\nPossible?\n---\n\nUser 'den-is' said:\n---\nI'm getting same issue.\r\nCouple Nginx+PHP Pods running on huge instances in parallel with couple other small applications. \r\nThese are staging apps+nodes without constant traffic.\r\nI constantly receive notification that these Nginx+PHP app has restarted... specifically Nginx container of these Pods.\r\nAt the same time other apps running in the same namespace, nodes never restart.\r\n```\r\nk -n staging get events\r\n...\r\n22m         Warning   Unhealthy                  pod/myapp-staging3-59f75b5d49-5tscw    Liveness probe failed: Get http://100.100.161.197:80/list/en/health/ping: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\r\n23m         Warning   Unhealthy                  pod/myapp-staging4-7589945cc6-2nf4s    Liveness probe failed: Get http://100.100.161.198:80/list/en/health/ping: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\r\n11m         Warning   Unhealthy                  pod/myapp-staging5-84fb4494db-5dvph    Liveness probe failed: Get http://100.104.124.220:80/list/en/health/ping: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\r\n...\r\n```\r\n\r\nLiveness on an Nginx container looks like this:\r\n```yaml\r\n  liveness:\r\n    initialDelaySeconds: 10\r\n    periodSeconds: 10\r\n    failureThreshold: 3\r\n    httpGet:\r\n      path: /list/en/health/ping\r\n      port: 80\r\n```\r\nUPD: Strange thing is that completely distinct deployments you can see above staging4 staging5 stagingN - above 10 deployments fail at once.\r\n\r\nMy possible problem might be in missing `timeoutSeconds` which is default `1s`\n---\n\nUser 'rudolfdobias' said:\n---\nHaving the same problem here. \r\n```yaml\r\nlivenessProbe:\r\n        httpGet:\r\n          path: /status\r\n          port: 80\r\n        initialDelaySeconds: 30\r\n        periodSeconds: 10\r\n        timeoutSeconds: 10\r\n        failureThreshold: 3\r\n```\r\n\r\nError cca 3 - 10 times a day:\r\n> Liveness probe failed: Get http://10.244.0.154:8002/status: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\r\n\r\nThe service operates normally and responds to /status in 5ms, though.\r\n\r\n\u26a0\ufe0f  Also, and that is a bigger problem, at similar random times some pods refuse to connect to each other. \r\n\r\nRunning on Azure Kubernetes Service\r\nV 1.14.8\n---\n\nUser 'arjun921' said:\n---\nI'm facing the same issue as well, increasing timeoutSeconds didn't help.\r\n```yaml\r\n          livenessProbe:\r\n            httpGet:\r\n              path: /ping\r\n              port: http\r\n            failureThreshold: 5\r\n            initialDelaySeconds: 5\r\n            periodSeconds: 20\r\n            timeoutSeconds: 10\r\n          readinessProbe:\r\n            httpGet:\r\n              path: /ping\r\n              port: http\r\n            initialDelaySeconds: 5\r\n            periodSeconds: 20\r\n            timeoutSeconds: 10\r\n```\r\nRuning on Kubernetes v1.16.7 on AWS deployed via KOPS\n---\n\nUser 'thockin' said:\n---\nI appreciate the extra reports.  It sounds like something is really weird.  I'll repeat myself from above:\r\n\r\nI'm afraid the only way to know more is to get something like a tcpdump from both inside and outside the pod, which captures one or more failing requests.\r\n\r\nIs that possible?  Without that I am flying very blind.  I don't see this experimentally and I'm not flooded with reports of this, so it's going to be difficult to pin down.  If you say you see it repeatedly, please try to capture a pcap?\n---\n\nUser 'arjun921' said:\n---\n@thockin I'll try to get a dump if I'm able to replicate this issue consistently, since it tends to happen randomly.\r\nJust to clarify, what exactly did you mean by tcpdump outside the pod?\r\nDid you mean tcpdump of the node where the pod resides?\r\n\r\nSorry, I'm relatively new to this :sweat_smile:\n---\n\nUser 'athenabot' said:\n---\n@thockin\nIf this issue has been triaged, please comment `/remove-triage unresolved`.\n\nIf you aren't able to handle this issue, consider unassigning yourself and/or adding the `help-wanted` label.\n\n\ud83e\udd16 I am a bot run by vllry. \ud83d\udc69\u200d\ud83d\udd2c\n---\n\nUser 'fscz' said:\n---\n/remove-triage unresolved\n---\n\nUser 'bigbitbus' said:\n---\nWe are also seeing this issue - trying to run a Django+WSGI+nginx server that works on a lower version of K8s, when we try this on Linode's managed Kubernetes service - LKE we see this\r\n\r\n```Events:\r\n  Type     Reason     Age    From                                Message\r\n  ----     ------     ----   ----                                -------\r\n  Normal   Scheduled  2m45s  default-scheduler                   Successfully assigned be/bigbitbus-stack-64c7b65b97-c2cth to lke6438-8072-5eecc7cc4d98\r\n  Normal   Pulled     2m45s  kubelet, lke6438-8072-5eecc7cc4d98  Container image \"##############/bigbitbus/bigbitbus-$$$$$$-server:###########\" already present on machine\r\n  Normal   Created    2m44s  kubelet, lke6438-8072-5eecc7cc4d98  Created container bbbchart\r\n  Normal   Started    2m44s  kubelet, lke6438-8072-5eecc7cc4d98  Started container bbbchart\r\n  Warning  Unhealthy  9s     kubelet, lke6438-8072-5eecc7cc4d98  Liveness probe failed: Get http://10.2.1.32:80/api/jindahoon/: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\r\n```\n---\n\nUser 'den-is' said:\n---\nUPD: So since my last comment, I've updated timeouts to something more realistic. e.g.:\r\n```yaml\r\n  livenessProbe:\r\n    initialDelaySeconds: 30\r\n    periodSeconds: 10\r\n    timeoutSeconds: 5\r\n    failureThreshold: 3\r\n    successThreshold: 1\r\n    httpGet:\r\n      path: /health/ping\r\n      port: 80\r\n```\r\nEverything was quiet since then. But the day before yesterday, and yesterday: My prometheus-operator> alertmanger and slack have have exploded. And today by magic it is silent again not a single alert.\r\n\r\nMy apps is a usual PHP+Nginx pod. Staging environments deployed on 3 huge boxes. Just a dozen preview deploys which are deployed only once, and then opened by devs only once. No load at all.\r\nFor example, the app had 10 staging deploys. so app-staging1..10\r\nAnd all containers of exactly that one app were failing all at once. i repeat virtually not related deployments spread on 3 nodes.\r\nIn pod, only Nginx containers were restarting.\r\nIn logs, I was able to catch that actual Nginx was answering 499 to the liveness probe. Nothing else.\r\nStill investigating.\r\n\r\nWhile ~5 other PHP apps deploys on the same staging nodes never restart.\n---\n\nUser 'Siddharthk' said:\n---\nI am also facing the same issue. Running on Kubernetes v1.16.8 on AWS EKS. Any workarounds?\n---\n\nUser 'zaabskr' said:\n---\nI am also facing the same issue with AKS 1.16.9. This is occuring consistently. Is there any way to investigate the issue?\n---\n\nUser 'AndriiNeverov' said:\n---\n@zaabskr \r\n\r\nLooks like this error message is a symptom of a more general problem of node-to-pod connectivity issues.\r\n\r\nIn our case, we confirmed that by using `traceroute` from the node to the pod's internal IP as suggested in https://discuss.kubernetes.io/t/who-where-actually-work-liveness-probe-in-kubernetes/4771/6. That IP was perfectly routable from the other pods and even from many other nodes, but not from that particular node it ended up on. The root cause turned out to be that the br-int (bridge) interface that connects the pod networks and the NSX overlay logical switch was failing to come up because of missing static routes. Fixing that magically solved the problem.\r\n\r\nIn other cases, the root cause may be different, but the first step would be to confirm whether the node can actually access that pod, e.g. ssh and traceroute or curl into the readiness probe (`curl -v http://172.70.3.3:15020/healthz/ready` when Istio sidecar containers fail to get ready), etc.\n---\n\nUser 'chasebolt' said:\n---\nexperiencing this issue in multiple GKE clusters. just started about 5 days ago. right now i'm having to drain the bad node and remove it from the pool. running `1.16.9-gke.6`\n---\n\nUser 'colotiline' said:\n---\nMy issue was in a REST API dependency that was checked by readinessProbe. Doubles readinessTimeout helped.\n---\n\nUser '23ewrdtf' said:\n---\n**What happened**: After updating EKS from v1.14.9-eks-f459c0 to v1.15.11-eks-065dce some of our ReplicaSets and DaemonSets Liveness/Readiness probes started failing with `Liveness probe failed: Get http://xxx.xxx.xxx.xxx:80/: net/http: request canceled (Client.Timeout exceeded while awaiting headers` \r\n\r\nIt seems that random pods are affected. I can successfully curl to them from other pods.\r\n\r\nAll nodes and pods are fine, not CPU/Memory/Storage issues.\r\n\r\nTwo nodes are running 1.15 and the rest 1.14. There doesn't seem to be a correlation between node version and the issue.\r\n\r\nNothing obvious in `kubectl cluster-info dump` or `journalctl -r` or `kube-proxy.log`\r\n\r\nProbes are mostly configured like this (truncated):\r\n```\r\n    livenessProbe:\r\n      failureThreshold: 3\r\n      periodSeconds: 10\r\n      successThreshold: 1\r\n      timeoutSeconds: 1\r\n\r\n    readinessProbe:\r\n      failureThreshold: 3\r\n      periodSeconds: 10\r\n      successThreshold: 1\r\n      timeoutSeconds: 1\r\n```\r\n\r\nI will increase the timeouts but it would be good to solve this. What changed in 1.15?\r\n\r\n**What you expected to happen**: Liveness/Readiness probes not to fail.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**: Upgrade EKS 1.14 to 1.15\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): \r\n```\r\nClient Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.6\", GitCommit:\"dff82dc0de47299ab66c83c626e08b245ab19037\", GitTreeState:\"clean\", BuildDate:\"2020-07-16T00:04:31Z\", GoVersion:\"go1.14.4\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"15+\", GitVersion:\"v1.15.11-eks-065dce\", GitCommit:\"065dcecfcd2a91bd68a17ee0b5e895088430bd05\", GitTreeState:\"clean\", BuildDate:\"2020-07-16T01:44:47Z\", GoVersion:\"go1.12.17\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\r\n- Cloud provider or hardware configuration: EKS\r\n- OS (e.g: `cat /etc/os-release`):\r\n1.14:\r\n```\r\nami-048d37e92ce89022e amazon-eks-node-1.14-v20200507\r\n$ uname -a\r\nLinux ip-xxxxxxx.xxxxxx.compute.internal 4.14.177-139.253.amzn2.x86_64 #1 SMP Wed Apr 29 09:56:20 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n$ cat /etc/os-release\r\nNAME=\"Amazon Linux\"\r\nVERSION=\"2\"\r\nID=\"amzn\"\r\nID_LIKE=\"centos rhel fedora\"\r\nVERSION_ID=\"2\"\r\nPRETTY_NAME=\"Amazon Linux 2\"\r\nANSI_COLOR=\"0;33\"\r\nCPE_NAME=\"cpe:2.3:o:amazon:amazon_linux:2\"\r\nHOME_URL=\"https://amazonlinux.com/\"\r\n```\r\n\r\n1.15\r\n```\r\n$ ami-0c42d7bd0e31ee2fe amazon-eks-node-1.15-v20200814\r\n$ uname -a\r\nLinux ip-xxxxxx.xxxxxx.compute.internal 4.14.186-146.268.amzn2.x86_64 #1 SMP Tue Jul 14 18:16:52 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n$ cat /etc/os-release\r\nNAME=\"Amazon Linux\"\r\nVERSION=\"2\"\r\nID=\"amzn\"\r\nID_LIKE=\"centos rhel fedora\"\r\nVERSION_ID=\"2\"\r\nPRETTY_NAME=\"Amazon Linux 2\"\r\nANSI_COLOR=\"0;33\"\r\nCPE_NAME=\"cpe:2.3:o:amazon:amazon_linux:2\"\r\nHOME_URL=\"https://amazonlinux.com/\"\r\n```\r\n- Install tools: AWS Console\r\n- Network plugin and version (if this is a network-related bug): amazon-k8s-cni:v1.6.3-eksbuild.1\n---",
    "question": "Has anyone seen 'Sometime Liveness/Readiness Probes fail because of net/http: request canceled while waiting for connection' in kubernetes/kubernetes? I can't figure out the cause.",
    "ideal_answer": "/triage unresolved\n\nComment `/remove-triage unresolved` when the issue is assessed and confirmed.\n\n\ud83e\udd16 I am a bot run by vllry. \ud83d\udc69\u200d\ud83d\udd2c\n\nAre you sure the application don't hit the resources limits? \r\n\nIn my case, the application starting fine, then the container start using more resources until he hit the limit. After that the readiness probes fail\n\nYes, I am sure. I collected the metrics.\r\nCpu usage on pod was around 0.001\r\nRam - around 4mb.\r\nThis is a test application (pure nginx image) that did not do anything (no traffic is sent to it).\n\nany update on this? we have been facing similar issues since few weeks\n\nThis is the first such report I have seen.  There's nothing obvious about why this would happen.\r\n\nIt's possible kubelet is too busy and starved for CPU and the probe happened to be thing that got thrashed.  How many pods are on this machine?  How busy is it?\r\n\nIt's possible the node itself is thrashing or something and OOM behavior is weird.  Does dmesg show any OOMs?\r\n\nIt's possible some other failure down deep in kubelet is being translated into this?  You could try running kubelet at a higher log level to get more details on what is happening.\r\n\nA lot of bugs have been fixed since 1.12, so we'd need to try to reproduce this and then try again in a more recent version.  Is there any way you can help boil down a simpler reproduction?\n\n@thockin\nIf this issue has been triaged, please comment `/remove-triage unresolved`.\n\nIf you aren't able to handle this issue, consider unassigning yourself and/or adding the `help-wanted` label.\n\n\ud83e\udd16 I am a bot run by vllry. \ud83d\udc69\u200d\ud83d\udd2c\n\n@thockin  \r\n`returns-console-558995dd78-b6bjm                                  0/1     Running            0          23h`\r\n\nFor example, if you see the above get pods, `readiness probe` of one pod is failing almost every day. A restart would fix this. However, we're unable to find the root cause. But I don't see any abnormal numbers on CPU or memory or thread count. Doing a `describe pod` would give me that `readiness` has failed. How do we debug in such scenario? \r\n\nAlso, this is happening for this particular deployment only. The other pods are running fine.\r\n\n\"returns-console-558995dd78-pbjf8                                  1/1     Running            0          23h\r\nreturns-console-558995dd78-plqdr                                  1/1     Running            0          23h\r\nreturns-console-558995dd78-tlb5k                                  1/1     Running            0          23h\r\nreturns-console-558995dd78-tr2kd                                  1/1     Running            0          23h\r\nreturns-console-558995dd78-vfj2n                                  1/1     Running            0          23h\"\n\nWe too are facing the same issue in our cluster. \r\n\nKubernetes Version - 1.16.8\r\nKernel - 4.4.218-1\r\nOS - Centos 7\r\n\nInstalled using Kubespray\r\n\nIn our case, timeouts were not related to application but related to specific nodes in cluster. In our 13 node cluster, node 1 to 4 had some kind of issue wherein the pods running on these nodes had random failures due to timeouts. \r\n\nChecked that there weren't any cpu aur memory usage spikes also.\r\n\nP.S We are using NodePort for production use case. Is it possible that the node port setup cannot handle too many socket connections?\n\nI have no idea what might cause spurious probe failues.  @derekwaynecarr have you heard any other reports of this?\r\n\n@tarunwadhwa13 are you saying PROBES failed (always same node) or user traffic failed?  If you have any other information about what was going on with those nodes when the failures happen, it would help.  Check for OOMs, high CPU usage, conntrack failures?\n\n@thockin  Conntrack shows hardly 2 or 3 errors. Memory consumption is 60-65% per node.\r\n\nJust found that the timeouts were for almost all request and not just probe. We added istio lately to check connection stats and understand if the behaviour was due to application. But the findings were weird. Istio itself is now failing readiness probe quite frequently\r\n\n![image](https://user-images.githubusercontent.com/20948402/82115058-1c053c00-977e-11ea-828e-bde30286faed.png)\r\n\n157 failures in ~3 hours \r\n\nWould like to add that kubernetes is running in our Datacenter. And since iptables version is 1.4.21, --random-fully is not being implemented. But since all machines have same configuration, we ruled out this possibility\n\nI apologize for not having a lot of details to share but I'd add my 2 cents.  We recently upgraded from Istio 1.4.4 to 1.5.4 and started seeing the issues described by OP.  Lots of liveness / readiness issues there were not happening before.  It SEEMS like adding let's say a 20sec initial delay had helped in most cases.  At this point we are still seeing it and not sure what the root cause is. \r\n\nrunning on EKS 1.15 (control plane) / 1.14 managed nodeGroups\n\n@thockin\nIf this issue has been triaged, please comment `/remove-triage unresolved`.\n\nIf you aren't able to handle this issue, consider unassigning yourself and/or adding the `help-wanted` label.\n\n\ud83e\udd16 I am a bot run by vllry. \ud83d\udc69\u200d\ud83d\udd2c\n\nI'm afraid the only way to know more is to get something like a tcpdump from both inside and outside the pod, which captures one or more failing requests.\r\n\nPossible?\n\nI'm getting same issue.\r\nCouple Nginx+PHP Pods running on huge instances in parallel with couple other small applications. \r\nThese are staging apps+nodes without constant traffic.\r\nI constantly receive notification that these Nginx+PHP app has restarted... specifically Nginx container of these Pods.\r\nAt the same time other apps running in the same namespace, nodes never restart.\r\n```\r\nk -n staging get events\r\n...\r\n22m         Warning   Unhealthy                  pod/myapp-staging3-59f75b5d49-5tscw    Liveness probe failed: Get http://100.100.161.197:80/list/en/health/ping: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\r\n23m         Warning   Unhealthy                  pod/myapp-staging4-7589945cc6-2nf4s    Liveness probe failed: Get http://100.100.161.198:80/list/en/health/ping: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\r\n11m         Warning   Unhealthy                  pod/myapp-staging5-84fb4494db-5dvph    Liveness probe failed: Get http://100.104.124.220:80/list/en/health/ping: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\r\n...\r\n```\r\n\nLiveness on an Nginx container looks like this:\r\n```yaml\r\n  liveness:\r\n    initialDelaySeconds: 10\r\n    periodSeconds: 10\r\n    failureThreshold: 3\r\n    httpGet:\r\n      path: /list/en/health/ping\r\n      port: 80\r\n```\r\nUPD: Strange thing is that completely distinct deployments you can see above staging4 staging5 stagingN - above 10 deployments fail at once.\r\n\nMy possible problem might be in missing `timeoutSeconds` which is default `1s`\n\nHaving the same problem here. \r\n```yaml\r\nlivenessProbe:\r\n        httpGet:\r\n          path: /status\r\n          port: 80\r\n        initialDelaySeconds: 30\r\n        periodSeconds: 10\r\n        timeoutSeconds: 10\r\n        failureThreshold: 3\r\n```\r\n\nError cca 3 - 10 times a day:\r\n> Liveness probe failed: Get http://10.244.0.154:8002/status: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\r\n\nThe service operates normally and responds to /status in 5ms, though.\r\n\n\u26a0\ufe0f  Also, and that is a bigger problem, at similar random times some pods refuse to connect to each other. \r\n\nRunning on Azure Kubernetes Service\r\nV 1.14.8\n\nI'm facing the same issue as well, increasing timeoutSeconds didn't help.\r\n```yaml\r\n          livenessProbe:\r\n            httpGet:\r\n              path: /ping\r\n              port: http\r\n            failureThreshold: 5\r\n            initialDelaySeconds: 5\r\n            periodSeconds: 20\r\n            timeoutSeconds: 10\r\n          readinessProbe:\r\n            httpGet:\r\n              path: /ping\r\n              port: http\r\n            initialDelaySeconds: 5\r\n            periodSeconds: 20\r\n            timeoutSeconds: 10\r\n```\r\nRuning on Kubernetes v1.16.7 on AWS deployed via KOPS\n\nI appreciate the extra reports.  It sounds like something is really weird.  I'll repeat myself from above:\r\n\nI'm afraid the only way to know more is to get something like a tcpdump from both inside and outside the pod, which captures one or more failing requests.\r\n\nIs that possible?  Without that I am flying very blind.  I don't see this experimentally and I'm not flooded with reports of this, so it's going to be difficult to pin down.  If you say you see it repeatedly, please try to capture a pcap?\n\n@thockin I'll try to get a dump if I'm able to replicate this issue consistently, since it tends to happen randomly.\r\nJust to clarify, what exactly did you mean by tcpdump outside the pod?\r\nDid you mean tcpdump of the node where the pod resides?\r\n\nSorry, I'm relatively new to this :sweat_smile:\n\n@thockin\nIf this issue has been triaged, please comment `/remove-triage unresolved`.\n\nIf you aren't able to handle this issue, consider unassigning yourself and/or adding the `help-wanted` label.\n\n\ud83e\udd16 I am a bot run by vllry. \ud83d\udc69\u200d\ud83d\udd2c\n\n/remove-triage unresolved\n\nWe are also seeing this issue - trying to run a Django+WSGI+nginx server that works on a lower version of K8s, when we try this on Linode's managed Kubernetes service - LKE we see this\r\n\n```Events:\r\n  Type     Reason     Age    From                                Message\r\n  -          -   -                                -\r\n  Normal   Scheduled  2m45s  default-scheduler                   Successfully assigned be/bigbitbus-stack-64c7b65b97-c2cth to lke6438-8072-5eecc7cc4d98\r\n  Normal   Pulled     2m45s  kubelet, lke6438-8072-5eecc7cc4d98  Container image \"##############/bigbitbus/bigbitbus-$$$$$$-server:###########\" already present on machine\r\n  Normal   Created    2m44s  kubelet, lke6438-8072-5eecc7cc4d98  Created container bbbchart\r\n  Normal   Started    2m44s  kubelet, lke6438-8072-5eecc7cc4d98  Started container bbbchart\r\n  Warning  Unhealthy  9s     kubelet, lke6438-8072-5eecc7cc4d98  Liveness probe failed: Get http://10.2.1.32:80/api/jindahoon/: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\r\n```\n\nUPD: So since my last comment, I've updated timeouts to something more realistic. e.g.:\r\n```yaml\r\n  livenessProbe:\r\n    initialDelaySeconds: 30\r\n    periodSeconds: 10\r\n    timeoutSeconds: 5\r\n    failureThreshold: 3\r\n    successThreshold: 1\r\n    httpGet:\r\n      path: /health/ping\r\n      port: 80\r\n```\r\nEverything was quiet since then. But the day before yesterday, and yesterday: My prometheus-operator> alertmanger and slack have have exploded. And today by magic it is silent again not a single alert.\r\n\nMy apps is a usual PHP+Nginx pod. Staging environments deployed on 3 huge boxes. Just a dozen preview deploys which are deployed only once, and then opened by devs only once. No load at all.\r\nFor example, the app had 10 staging deploys. so app-staging1..10\r\nAnd all containers of exactly that one app were failing all at once. i repeat virtually not related deployments spread on 3 nodes.\r\nIn pod, only Nginx containers were restarting.\r\nIn logs, I was able to catch that actual Nginx was answering 499 to the liveness probe. Nothing else.\r\nStill investigating.\r\n\nWhile ~5 other PHP apps deploys on the same staging nodes never restart.\n\nI am also facing the same issue. Running on Kubernetes v1.16.8 on AWS EKS. Any workarounds?\n\nI am also facing the same issue with AKS 1.16.9. This is occuring consistently. Is there any way to investigate the issue?\n\n@zaabskr \r\n\nLooks like this error message is a symptom of a more general problem of node-to-pod connectivity issues.\r\n\nIn our case, we confirmed that by using `traceroute` from the node to the pod's internal IP as suggested in https://discuss.kubernetes.io/t/who-where-actually-work-liveness-probe-in-kubernetes/4771/6. That IP was perfectly routable from the other pods and even from many other nodes, but not from that particular node it ended up on. The root cause turned out to be that the br-int (bridge) interface that connects the pod networks and the NSX overlay logical switch was failing to come up because of missing static routes. Fixing that magically solved the problem.\r\n\nIn other cases, the root cause may be different, but the first step would be to confirm whether the node can actually access that pod, e.g. ssh and traceroute or curl into the readiness probe (`curl -v http://172.70.3.3:15020/healthz/ready` when Istio sidecar containers fail to get ready), etc.\n\nexperiencing this issue in multiple GKE clusters. just started about 5 days ago. right now i'm having to drain the bad node and remove it from the pool. running `1.16.9-gke.6`\n\nMy issue was in a REST API dependency that was checked by readinessProbe. Doubles readinessTimeout helped.\n\n**What happened**: After updating EKS from v1.14.9-eks-f459c0 to v1.15.11-eks-065dce some of our ReplicaSets and DaemonSets Liveness/Readiness probes started failing with `Liveness probe failed: Get http://xxx.xxx.xxx.xxx:80/: net/http: request canceled (Client.Timeout exceeded while awaiting headers` \r\n\nIt seems that random pods are affected. I can successfully curl to them from other pods.\r\n\nAll nodes and pods are fine, not CPU/Memory/Storage issues.\r\n\nTwo nodes are running 1.15 and the rest 1.14. There doesn't seem to be a correlation between node version and the issue.\r\n\nNothing obvious in `kubectl cluster-info dump` or `journalctl -r` or `kube-proxy.log`\r\n\nProbes are mostly configured like this (truncated):\r\n```\r\n    livenessProbe:\r\n      failureThreshold: 3\r\n      periodSeconds: 10\r\n      successThreshold: 1\r\n      timeoutSeconds: 1\r\n\n    readinessProbe:\r\n      failureThreshold: 3\r\n      periodSeconds: 10\r\n      successThreshold: 1\r\n      timeoutSeconds: 1\r\n```\r\n\nI will increase the timeouts but it would be good to solve this. What changed in 1.15?\r\n\n**What you expected to happen**: Liveness/Readiness probes not to fail.\r\n\n**How to reproduce it (as minimally and precisely as possible)**: Upgrade EKS 1.14 to 1.15\r\n\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): \r\n```\r\nClient Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.6\", GitCommit:\"dff82dc0de47299ab66c83c626e08b245ab19037\", GitTreeState:\"clean\", BuildDate:\"2020-07-16T00:04:31Z\", GoVersion:\"go1.14.4\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"15+\", GitVersion:\"v1.15.11-eks-065dce\", GitCommit:\"065dcecfcd2a91bd68a17ee0b5e895088430bd05\", GitTreeState:\"clean\", BuildDate:\"2020-07-16T01:44:47Z\", GoVersion:\"go1.12.17\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n\n- Cloud provider or hardware configuration: EKS\r\n- OS (e.g: `cat /etc/os-release`):\r\n1.14:\r\n```\r\nami-048d37e92ce89022e amazon-eks-node-1.14-v20200507\r\n$ uname -a\r\nLinux ip-xxxxxxx.xxxxxx.compute.internal 4.14.177-139.253.amzn2.x86_64 #1 SMP Wed Apr 29 09:56:20 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n$ cat /etc/os-release\r\nNAME=\"Amazon Linux\"\r\nVERSION=\"2\"\r\nID=\"amzn\"\r\nID_LIKE=\"centos rhel fedora\"\r\nVERSION_ID=\"2\"\r\nPRETTY_NAME=\"Amazon Linux 2\"\r\nANSI_COLOR=\"0;33\"\r\nCPE_NAME=\"cpe:2.3:o:amazon:amazon_linux:2\"\r\nHOME_URL=\"https://amazonlinux.com/\"\r\n```\r\n\n1.15\r\n```\r\n$ ami-0c42d7bd0e31ee2fe amazon-eks-node-1.15-v20200814\r\n$ uname -a\r\nLinux ip-xxxxxx.xxxxxx.compute.internal 4.14.186-146.268.amzn2.x86_64 #1 SMP Tue Jul 14 18:16:52 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n$ cat /etc/os-release\r\nNAME=\"Amazon Linux\"\r\nVERSION=\"2\"\r\nID=\"amzn\"\r\nID_LIKE=\"centos rhel fedora\"\r\nVERSION_ID=\"2\"\r\nPRETTY_NAME=\"Amazon Linux 2\"\r\nANSI_COLOR=\"0;33\"\r\nCPE_NAME=\"cpe:2.3:o:amazon:amazon_linux:2\"\r\nHOME_URL=\"https://amazonlinux.com/\"\r\n```\r\n- Install tools: AWS Console\r\n- Network plugin and version (if this is a network-related bug): amazon-k8s-cni:v1.6.3-eksbuild.1"
  },
  {
    "id": "gen_nat_002",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: CRD conversion webhooks should not be called for unused apiVersions\n\nA user reported the following issue titled 'CRD conversion webhooks should not be called for unused apiVersions' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nWe have CRDs with multiple apiVersions. Even if the old (non-storage) apiVersions are not used at all we regularly receive conversion requests for them.\n\nWe roughly get 1 conversion request for each non-storage apiVersion per kube-apiserver instance for every CR create/update (actually a little bit less than that, but not sure why).\n\nSo if we have a CRD with 5 old apiVersions and a cluster with 3 kube-apiservers\n\n=> we get roughly 15 conversion requests for every create/update on a CR (it's slightly less than that - not sure why though)\n\n\n\n### What did you expect to happen?\n\nI would expect to only get conversion requests when conversion is required, e.g. if a client requests a CR in a different apiVersion than the one in which the object is stored in etcd.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nCreate a Kubernetes cluster via kind\n```\nkind create cluster\n```\n\nDeploy the Cluster CRD\n```\n$ kubectl apply -f ./crd_cluster.yaml\n```\n\n<details>\n\n<summary>crd_cluster.yaml</summary>\n\n```yaml\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: v0.17.0\n  name: clusters.cluster.x-k8s.io\nspec:\n  conversion:\n    strategy: Webhook\n    webhook:\n      conversionReviewVersions: [\"v1\", \"v1beta1\"]\n      clientConfig:\n        service:\n          namespace: system\n          name: webhook-service\n          path: /convert\n  group: cluster.x-k8s.io\n  names:\n    kind: Cluster\n    listKind: ClusterList\n    plural: clusters\n    singular: cluster\n  scope: Namespaced\n  versions:\n  - deprecated: true\n    name: v1alpha3\n    schema:\n      openAPIV3Schema:\n        properties:\n          spec:\n            properties:\n              paused:\n                type: boolean\n            type: object\n        type: object\n    served: false\n    storage: false\n  - deprecated: true\n    name: v1alpha4\n    schema:\n      openAPIV3Schema:\n        properties:\n          spec:\n            properties:\n              paused:\n                type: boolean\n            type: object\n        type: object\n    served: true\n    storage: false\n  - name: v1beta1\n    schema:\n      openAPIV3Schema:\n        properties:\n          spec:\n            properties:\n              paused:\n                type: boolean\n            type: object\n        type: object\n    served: true\n    storage: true\n```\n\n</details>\n\nDeploy the Cluster CR\n```\n$ kubectl apply -f ./cr_cluster.yaml\n```\n\n<details>\n\n<summary>cr_cluster.yaml</summary>\n\n```yaml\nkind: Cluster\napiVersion: cluster.x-k8s.io/v1beta1\nmetadata:\n  name: cluster-1\n  namespace: default\n```\n\n</details>\n\nObserve kube-apiserver logs\n\n```\n$ kubectl -n kube-system logs -f kube-apiserver-kind-control-plane\n...\nE0204 13:21:54.207206       1 watcher.go:567] failed to prepare current and previous objects: conversion webhook for cluster.x-k8s.io/v1beta1, Kind=Cluster failed: Post \"https://webhook-service.system.svc:443/convert?timeout=30s\": service \"webho\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'sbueringer' said:\n---\n/sig api-machinery\n---\n\nUser 'sbueringer' said:\n---\nJust fyi, this issue is made more relevant because today it's pretty hard / very hacky to remove apiVersions. More details in: https://github.com/kubernetes/kubernetes/issues/111937\n---\n\nUser 'BenTheElder' said:\n---\n/triage accepted\n/help\n\nThis may be a tricky one, explicitly not marking it \"good first issue\".\n---\n\nUser 'k8s-ci-robot' said:\n---\n@BenTheElder: \n\tThis request has been marked as needing help from a contributor.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- Does this issue have zero to low barrier of entry?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://git.k8s.io/community/contributors/guide/help-wanted.md) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-help` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/129979):\n\n>/triage accepted\n>/help\n>\n>This may be a tricky one, explicitly not marking it \"good first issue\".\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'GrigoriyMikhalkin' said:\n---\n/assign\n---",
    "question": "I'm running into an issue with kubernetes/kubernetes: CRD conversion webhooks should not be called for unused apiVersions. How can I fix this?",
    "ideal_answer": "Just fyi, this issue is made more relevant because today it's pretty hard / very hacky to remove apiVersions. More details in: https://github.com/kubernetes/kubernetes/issues/111937\n\n/triage accepted\n/help\n\nThis may be a tricky one, explicitly not marking it \"good first issue\".\n\n@BenTheElder: \n\tThis request has been marked as needing help from a contributor.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- Does this issue have zero to low barrier of entry?\n- How can the assignee reach out to you for help?\n\nFor more details on the requirements of such an issue, please see [here](https://git.k8s.io/community/contributors/guide/help-wanted.md) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-help` command."
  },
  {
    "id": "gen_nat_003",
    "category": "general",
    "source_file": "microsoft_vscode_issue.json",
    "context": "Repository: microsoft/vscode\nTitle: Quicktree textbox should announce selected amount\n\nA user reported the following issue titled 'Quicktree textbox should announce selected amount' in the 'microsoft/vscode' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\nTesting #258361\n\n\n<img width=\"610\" height=\"297\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5e8076f1-247b-4c83-a3a2-26b29f374d00\" />\n\nIt doesn't announce 64 selected\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'TylerLeonhardt' said:\n---\nI'm not sure when I fixed this, but it does seem to announce now.\n---\n\nUser 'lramos15' said:\n---\n<img width=\"685\" height=\"398\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/942edd01-0a44-4f43-9aa2-e4f89731cd35\" />\n\nI'm still not getting announcements here.\n---",
    "question": "I'm seeing 'Quicktree textbox should announce selected amount'. Is this a known bug in microsoft/vscode?",
    "ideal_answer": "I'm not sure when I fixed this, but it does seem to announce now.\n\n<img width=\"685\" height=\"398\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/942edd01-0a44-4f43-9aa2-e4f89731cd35\" />\n\nI'm still not getting announcements here."
  },
  {
    "id": "gen_nat_004",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: [sig-network] Conntrack proxy implementation should not be vulnerable to the invalid conntrack state bug [Privileged] failing due to bug in test\n\nA user reported the following issue titled '[sig-network] Conntrack proxy implementation should not be vulnerable to the invalid conntrack state bug [Privileged] failing due to bug in test' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### Description\n\n`[sig-network] Conntrack proxy implementation should not be vulnerable to the invalid conntrack state bug [Privileged]` has been historically flaking in downstream OpenShift CI jobs. However, the flakes were very random and couldn't be easily reproduced for debugging.\n\n### Details\n\nAs per the test [details](https://github.com/kubernetes/kubernetes/blob/f9a55d78c773fb4fa4a586919d4eff6b4a59c4f9/test/e2e/network/conntrack.go#L544-L668), the test should pass if `nf_conntrack_tcp_be_liberal` is enforced. The test expects that when the server sends an out-of-window TCP packet, it should either be dropped or should be correctly NAT-ed and forwarded to the client. The test fails when the server receives an RST packet from the client which is expected happen when the out-of-window packet is not dropped and also not correctly NAT-ed.\n\nOpenShift uses OVN-Kubernetes CNI and `nf_conntrack_tcp_be_liberal` is always [enforced](https://github.com/torvalds/linux/blob/5aca7966d2a7255ba92fd5e63268dd767b223aa5/net/openvswitch/conntrack.c#L832). Thus, the issue of incorrect NAT should not arise. However, the test still randomly flaked because the server had received an RST packet from the client.\n\nRecently, we took an effort to understand the reason why the test was flaking. For reproducing the issue, the test was run for longer duration of time and packets were captured. The test eventually did fail, however looking at the packet captures it became evident the test failed due to an incorrect logic in the server code, instead of incorrect packet NAT.\n\nPacket capture at the client side:\n```\n9715    2025-09-09 16:58:37.217654    10.129.0.42    172.30.47.170    TCP    74    36955 \u2192 9000 [SYN] Seq=0 Win=62027 Len=0 MSS=8861 SACK_PERM TSval=3989781034 TSecr=0 WS=128\n9716    2025-09-09 16:58:37.217933    172.30.47.170    10.129.0.42    TCP    74    9000 \u2192 36955 [SYN, ACK] Seq=0 Ack=1 Win=61943 Len=0 MSS=8861 SACK_PERM TSval=1962462568 TSecr=3989781034 WS=128\n9717    2025-09-09 16:58:37.217952    10.129.0.42    172.30.47.170    TCP    66    36955 \u2192 9000 [ACK] Seq=1 Ack=1 Win=62080 Len=0 TSval=3989781034 TSecr=1962462568\n9718    2025-09-09 16:58:37.217965    10.129.0.42    172.30.47.170    TCP    66    36955 \u2192 9000 [FIN, ACK] Seq=1 Ack=1 Win=62080 Len=0 TSval=3989781034 TSecr=1962462568\n9719    2025-09-09 16:58:37.218255    172.30.47.170    10.129.0.42    TCP    61    [TCP Spurious Retransmission] 9000 \u2192 36955 [PSH, ACK] Seq=4294867297 Ack=1 Win=62080 Len=7\n9720    2025-09-09 16:58:37.218262    10.129.0.42    172.30.47.170    TCP    78    [TCP Dup ACK 9717#1] 36955 \u2192 9000 [ACK] Seq=2 Ack=1 Win=62080 Len=0 TSval=3989781035 TSecr=1962462568 SLE=4294867297 SRE=4294867304\n9721    2025-09-09 16:58:37.218435    172.30.47.170    10.129.0.42    TCP    66    9000 \u2192 36955 [ACK] Seq=1 Ack=2 Win=61952 Len=0 TSval=1962462569 TSecr=3989781034\n9802    2025-09-09 16:58:47.218027    10.129.0.42    172.30.47.170    TCP    74    38949 \u2192 9000 [SYN] Seq=0 Win=62027 Len=0 MSS=886\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'aojea' said:\n---\n/triage accepted\n\ngreat analysis. please send the fix so we can release a new image of agnhost without this problem\n---\n\nUser 'arkadeepsen' said:\n---\n/assign\n---\n\nUser 'tssurya' said:\n---\n@arkadeepsen : Thanks for driving this forward!\nPlease add  @kyrtapz as a co-author to the commit.\nThanks both for taking care of this flake\n---",
    "question": "I'm getting an error: Conntrack proxy implementation should not be vulnerable to the invalid conntrack state bug  failing due to bug in test. Is there a known workaround?",
    "ideal_answer": "/triage accepted\n\ngreat analysis. please send the fix so we can release a new image of agnhost without this problem\n\n@arkadeepsen : Thanks for driving this forward!\nPlease add  @kyrtapz as a co-author to the commit.\nThanks both for taking care of this flake"
  },
  {
    "id": "gen_nat_005",
    "category": "troubleshooting",
    "source_file": "microsoft_vscode_issue.json",
    "context": "Repository: microsoft/vscode\nTitle: Native context menu hijacks custom one in webview editor\n\nA user reported the following issue titled 'Native context menu hijacks custom one in webview editor' in the 'microsoft/vscode' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\nType: <b>Bug</b>\n\nVS code january... \n\nOpen Git Graph and right-click an item. \n\nThe context menu shows only text operations.\n\n\nVS Code version: Code 1.97.0 (Universal) (33fc5a94a3f99ebe7087e8fe79fbe1d37a251016, 2025-02-04T22:41:26.688Z)\nOS version: Darwin arm64 23.6.0\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|Apple M2 Pro (10 x 2400)|\n|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|3, 3, 3|\n|Memory (System)|16.00GB (0.11GB free)|\n|Process Argv|--crash-reporter-id 0e8e2e6e-1b7c-420f-80bc-68c11cd9ef00|\n|Screen Reader|no|\n|VM|0%|\n</details><details><summary>Extensions (44)</summary>\n\nExtension|Author (truncated)|Version\n---|---|---\nbetter-comments|aar|3.0.2\nBookmarks|ale|13.5.0\nproject-manager|ale|12.8.0\naws-toolkit-vscode|ama|3.45.0\nbrowse-lite|ant|0.3.9\ngoto-alias|ant|0.2.1\nvscode-apollo|apo|2.5.4\nlit-html|bie|1.11.1\nvscode-intelephense-client|bme|1.12.6\nvscode-tailwindcss|bra|0.14.3\npath-intellisense|chr|2.10.0\ncodestream|Cod|15.20.0\nvscode-eslint|dba|3.0.10\ngithistory|don|0.6.20\ngitlens|eam|16.2.2\nvscode-diff|fab|2.1.2\nmacros|ged|1.2.1\ngitlab-workflow|Git|5.38.0\ntodo-tree|Gru|0.0.226\nvscode-peacock|joh|4.2.2\nstring-manipulation|mar|0.7.25\ngit-graph|mhu|1.30.0\ndebugpy|ms-|2025.0.0\npython|ms-|2025.0.0\nvscode-pylance|ms-|2025.2.1\natom-keybindings|ms-|3.3.0\nvsliveshare|ms-|1.0.5948\nvscode-twoslash-queries|Ort|1.5.0\nadvanced-new-file|pat|1.2.2\nlaravel-jump-controller|pgl|0.0.33\npostman-for-vscode|Pos|1.7.0\nvscode-thunder-client|ran|2.34.0\nvscode-yaml|red|1.15.0\nLiveServer|rit|5.7.9\nopen-in-browser|tec|2.0.0\nes6-string-html|Tob|2.17.0\npdf|tom|1.2.2\nsort-lines|Tyr|1.12.0\nexplorer|vit|1.10.7\nvscode-icons|vsc|12.11.0\nvolar|Vue|2.2.0\nsnippet-generator|wen|0.3.8\npretty-ts-errors|Yoa|0.6.1\ntype-challenges|YRM|1.15.0\n\n\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368:30146709\nvspor879:30202332\nvspor708:30202333\nvspor363:30204092\nvscod805cf:30301675\nbinariesv615:30325510\nvsaa593:30376534\npy29gd2263:31024239\nc4g48928:30535728\nazure-dev_surveyone:30548225\n2i9eh265:30646982\n962ge761:30959799\npythonnoceb:30805159\npythonmypyd1:30879173\nh48ei257:31000450\npythontbext0:30879054\ncppperfnew:31000557\ndwnewjupyter:31046869\nnativerepl1:31139838\npythonrstrctxt:31112756\nnativeloc2:31192216\niacca1:31171482\n5fd0e150:31155592\ndwcopilot:31170013\nstablechunks:31184530\n6074i472:31201624\ncustomenabled:31232589\n8did9651:31230678\n9064b325:31222308\ncopilot_t_ci:31222730\n\n```\n\n</details>\n\n<!-- generated by issue reporter -->\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'dminhhoang26' said:\n---\nyes, im in the same issue\n---\n\nUser 'gjsjohnmurray' said:\n---\nHave you reported it to the extension author yet?\n---\n\nUser 'barneyzhao' said:\n---\nSame issue here.\n\nIt's [reported](https://github.com/mhutchie/vscode-git-graph/issues/869) to the extension author, but looks like they're not maintaining the project anymore.\n\nAccording to [@ammarsdc](https://github.com/mhutchie/vscode-git-graph/issues/869#issuecomment-2641775797), the extension context menu works in version 1.96.4\n---\n\nUser 'cdpark0530' said:\n---\nSame, thank you for the report\n---\n\nUser 'bpasero' said:\n---\nBisect points to: https://github.com/microsoft/vscode/compare/151ef3514e76629f4e7bf3951439b1e0dae0a6e5...fca210cd103a496f25c23786b861a67f4d1ee16b\n\n@deepak1556 looks like the minor Electron update caused this.\n\nInstall https://marketplace.visualstudio.com/items?itemName=mhutchie.git-graph and click on the status bar entry.\n\n**Before:**\n\n<img width=\"502\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/040af248-651c-4eda-a0f7-a5f44739e0c7\" />\n\n**After:**\n\n<img width=\"399\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7bdadd55-5894-4684-99bb-519bf5d7a27d\" />\n\nIt seems to me that \"before\" a custom menu from the webview opened and now a native menu. You can see the visual difference.\n---\n\nUser 'deepak1556' said:\n---\nIssue seems to be from `contextmenu` event being fired twice on an element that messes up the state managed by this extension. It got regressed with https://github.com/electron/electron/pull/44954 due to missing return after the first dispatch.\n\nEdit: It has been addressed in https://github.com/electron/electron/pull/44978 which is available with `34.x.y`\n---\n\nUser 'hoangnq3004' said:\n---\nThere is a temporary solution is right click on the left margin of the git-graph window, the context menu will appear\n\n<img width=\"690\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1322a745-52e7-4a99-81e6-e767d88a416d\" />\n---\n\nUser 'deepak1556' said:\n---\nFix backported in https://devdiv.visualstudio.com/DevDiv/_git/electron-build/commit/5ab0ea02f205badc9322c907bba193b9b7ae4751?refName=refs/heads/robo/hotfix/release_32_x_y\n---\n\nUser 'reidsneo' said:\n---\nNo new vscode push update?\nthis is really annoying bug\n---\n\nUser 'git-hub-tig' said:\n---\nSolved with upgraded extension [Git Graph 3](https://marketplace.visualstudio.com/items?itemName=Gxl.git-graph-3&ssr=false#overview)\n---\n\nUser 'mitjakukovec' said:\n---\n> Solved with upgraded extension [Git Graph 3](https://marketplace.visualstudio.com/items?itemName=Gxl.git-graph-3&ssr=false#overview)\n\nIt's probably nothing serious, but I get this when trying to install\n\n<img width=\"516\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e043de46-d96d-46e2-b67b-2a001fc35898\" />\n---\n\nUser 'captain-corgi' said:\n---\nExperienced the same\n---\n\nUser 'git-hub-tig' said:\n---\n> > Solved with upgraded extension [Git Graph 3](https://marketplace.visualstudio.com/items?itemName=Gxl.git-graph-3&ssr=false#overview)\n> \n> It's probably nothing serious, but I get this when trying to install\n> \n> <img alt=\"Image\" width=\"516\" src=\"https://private-user-images.githubusercontent.com/1738630/411648455-e043de46-d96d-46e2-b67b-2a001fc35898.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzkyNTEzMDcsIm5iZiI6MTczOTI1MTAwNywicGF0aCI6Ii8xNzM4NjMwLzQxMTY0ODQ1NS1lMDQzZGU0Ni1kOTZkLTQ2ZTItYjY3Yi0yYTAwMWZjMzU4OTgucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDIxMSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTAyMTFUMDUxNjQ3WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9Y2M3YmM1NTE3YTRjMzMyZjE3NTFhNDIxZDE4MjUwZmUyNWE0MjI0ZDZlNTk2ODNjNzdjYWRiM2U0MTczNWM3ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.h086OX-KonnL-zdtBVraC6DqNjkLpDuVsPeAXt89iKg\">\n\nThanks for your feedback, publisher verification setting is undergoing.\n---\n\nUser 'PierreFaibrik' said:\n---\nIMHO, I would be a bit careful installing a non-verified extension that has only approx 100 downloads, with a logo that looks pretty much GitGraph's one and that can access my code and all my files on my machine\n---\n\nUser 'deepak1556' said:\n---\nThe issue has been addressed in today's insider https://code.visualstudio.com/insiders/ and will also be backported to the candidate release `1.97.2` later this week. I will lock this issue to avoid off-topic discussions\n---\n\nUser 'deepak1556' said:\n---\nClosing for verification.\n\n/closedWith https://github.com/microsoft/vscode/commit/39e41616659a1bba9d73114ea95072b2449729ea\n---",
    "question": "In microsoft/vscode, Native context menu hijacks custom one in webview editor. Any ideas why?",
    "ideal_answer": "yes, im in the same issue\n\nHave you reported it to the extension author yet?\n\nSame issue here.\n\nIt's [reported](https://github.com/mhutchie/vscode-git-graph/issues/869) to the extension author, but looks like they're not maintaining the project anymore.\n\nAccording to [@ammarsdc](https://github.com/mhutchie/vscode-git-graph/issues/869#issuecomment-2641775797), the extension context menu works in version 1.96.4\n\nSame, thank you for the report\n\nBisect points to: https://github.com/microsoft/vscode/compare/151ef3514e76629f4e7bf3951439b1e0dae0a6e5...fca210cd103a496f25c23786b861a67f4d1ee16b\n\n@deepak1556 looks like the minor Electron update caused this.\n\nInstall https://marketplace.visualstudio.com/items?itemName=mhutchie.git-graph and click on the status bar entry.\n\n**Before:**\n\n<img width=\"502\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/040af248-651c-4eda-a0f7-a5f44739e0c7\" />\n\n**After:**\n\n<img width=\"399\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7bdadd55-5894-4684-99bb-519bf5d7a27d\" />\n\nIt seems to me that \"before\" a custom menu from the webview opened and now a native menu. You can see the visual difference.\n\nIssue seems to be from `contextmenu` event being fired twice on an element that messes up the state managed by this extension. It got regressed with https://github.com/electron/electron/pull/44954 due to missing return after the first dispatch.\n\nEdit: It has been addressed in https://github.com/electron/electron/pull/44978 which is available with `34.x.y`\n\nThere is a temporary solution is right click on the left margin of the git-graph window, the context menu will appear\n\n<img width=\"690\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1322a745-52e7-4a99-81e6-e767d88a416d\" />\n\nFix backported in https://devdiv.visualstudio.com/DevDiv/_git/electron-build/commit/5ab0ea02f205badc9322c907bba193b9b7ae4751?refName=refs/heads/robo/hotfix/release_32_x_y\n\nNo new vscode push update?\nthis is really annoying bug\n\nSolved with upgraded extension [Git Graph 3](https://marketplace.visualstudio.com/items?itemName=Gxl.git-graph-3&ssr=false#overview)\n\n> Solved with upgraded extension [Git Graph 3](https://marketplace.visualstudio.com/items?itemName=Gxl.git-graph-3&ssr=false#overview)\n\nIt's probably nothing serious, but I get this when trying to install\n\n<img width=\"516\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e043de46-d96d-46e2-b67b-2a001fc35898\" />\n\nExperienced the same\n\n> > Solved with upgraded extension [Git Graph 3](https://marketplace.visualstudio.com/items?itemName=Gxl.git-graph-3&ssr=false#overview)\n> \n> It's probably nothing serious, but I get this when trying to install\n> \n> <img alt=\"Image\" width=\"516\" src=\"https://private-user-images.githubusercontent.com/1738630/411648455-e043de46-d96d-46e2-b67b-2a001fc35898.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzkyNTEzMDcsIm5iZiI6MTczOTI1MTAwNywicGF0aCI6Ii8xNzM4NjMwLzQxMTY0ODQ1NS1lMDQzZGU0Ni1kOTZkLTQ2ZTItYjY3Yi0yYTAwMWZjMzU4OTgucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDIxMSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTAyMTFUMDUxNjQ3WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9Y2M3YmM1NTE3YTRjMzMyZjE3NTFhNDIxZDE4MjUwZmUyNWE0MjI0ZDZlNTk2ODNjNzdjYWRiM2U0MTczNWM3ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.h086OX-KonnL-zdtBVraC6DqNjkLpDuVsPeAXt89iKg\">\n\nThanks for your feedback, publisher verification setting is undergoing.\n\nIMHO, I would be a bit careful installing a non-verified extension that has only approx 100 downloads, with a logo that looks pretty much GitGraph's one and that can access my code and all my files on my machine\n\nThe issue has been addressed in today's insider https://code.visualstudio.com/insiders/ and will also be backported to the candidate release `1.97.2` later this week. I will lock this issue to avoid off-topic discussions\n\nClosing for verification."
  },
  {
    "id": "gen_nat_006",
    "category": "feature_request",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: pod(restartPolicy == Never) stuck pending when vm reboot\n\nA user reported the following issue titled 'pod(restartPolicy == Never) stuck pending when vm reboot' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\n1. create a job, set job restartPolicy to Never;\r\n``` yaml\r\napiVersion: batch/v1\r\nkind: Job\r\nmetadata:\r\n  annotations:\r\n    description: ''\r\n  enable: true\r\n  name: test-created\r\n  namespace: default\r\n  lables: {}\r\nspec:\r\n  completions: 1\r\n  parallelism: 1\r\n  activeDeadlineSeconds: null\r\n  template:\r\n    metadata:\r\n      enable: true\r\n      name: test-created\r\n      labels:\r\n        app: test-created\r\n        version: v1\r\n    spec:\r\n      containers:\r\n        - name: container-1\r\n          image: ubuntu:xenial-20190610\r\n          imagePullPolicy: IfNotPresent\r\n          command:\r\n            - /bin/bash\r\n            - '-c'\r\n            - sleep 60\r\n          resources: {}\r\n      imagePullSecrets:\r\n        - name: default-secret\r\n      restartPolicy: Never\r\n      volumes: []\r\n      dnsConfig:\r\n        options:\r\n          - name: single-request-reopen\r\n      initContainers: []\r\n  completionMode: NonIndexed\r\n```\r\n2. pod scheduled to Node A, and Node A reboot\uff0cafter reboot, pod stuck pending state\r\n\r\nsee kubelet log, only see \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"default/test-created-w2bk5\"\r\nafter code review, if reboot occur when ```the container created success, but it is not started```, may lead this problem;\r\n\r\n2.1 after reboot, pod's sandbox exit, so PodSandboxChanged return true, and new attemp count set to one, and kubelet log \"o ready sandbox for pod can be found. Need to start a new one\" can confirm this.\r\n``` go\r\n// PodSandboxChanged checks whether the spec of the pod is changed and returns\r\n// (changed, new attempt, original sandboxID if exist).\r\nfunc PodSandboxChanged(pod *v1.Pod, podStatus *kubecontainer.PodStatus) (bool, uint32, string) {\r\n\tif len(podStatus.SandboxStatuses) == 0 {\r\n\t\tklog.V(2).InfoS(\"No sandbox for pod can be found. Need to start a new one\", \"pod\", klog.KObj(pod))\r\n\t\treturn true, 0, \"\"\r\n\t}\r\n\r\n\treadySandboxCount := 0\r\n\tfor _, s := range podStatus.SandboxStatuses {\r\n\t\tif s.State == runtimeapi.PodSandboxState_SANDBOX_READY {\r\n\t\t\treadySandboxCount++\r\n\t\t}\r\n\t}\r\n\r\n\t// Needs to create a new sandbox when readySandboxCount > 1 or the ready sandbox is not the latest one.\r\n\tsandboxStatus := podStatus.SandboxStatuses[0]\r\n\tif readySandboxCount > 1 {\r\n\t\tklog.V(2).InfoS(\"Multiple sandboxes are ready for Pod. Need to reconcile them\", \"pod\", klog.KObj(pod))\r\n\t\treturn true, sandboxStatus.Metadata.Attempt + 1, sandboxStatus.Id\r\n\t}\r\n\tif sandboxStatus.State != runtimeapi.PodSandboxState_SANDBOX_READY {\r\n\t\tklog.V(2).InfoS(\"No ready sandbox for pod can be found. Need to start a new one\", \"pod\", klog.KObj(pod))\r\n\t\treturn true, sandboxStatus.Metadata.Attempt + 1, sandboxStatus.Id\r\n\t}\r\n\r\n\t// Needs to create a new sandbox when network namespace changed.\r\n\tif sandboxStatus.GetLinux().GetNamespaces().GetOptions().GetNetwork() != NetworkNamespaceForPod(pod) {\r\n\t\tklog.V(2).InfoS(\"Sandbox for pod has changed. Need to start a new one\", \"pod\", klog.KObj(pod))\r\n\t\treturn true, sandboxStatus.Metadata.Attempt\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'Dingshujie' said:\n---\n/sig node\n---\n\nUser 'Dingshujie' said:\n---\n@ehashman @SergeyKanzhelev\n---\n\nUser 'SergeyKanzhelev' said:\n---\n/cc @ffromani\n---\n\nUser 'SergeyKanzhelev' said:\n---\n/cc @mimowo @xmcqueen \r\n\r\n/triage accepted\r\n\r\nLooks like the issue has enough details for a repro.\n---\n\nUser 'SergeyKanzhelev' said:\n---\n/priority important-longerm\r\n\r\nDoesn't sound like a regression.\n---\n\nUser 'k8s-ci-robot' said:\n---\n@SergeyKanzhelev: The label(s) `priority/important-longerm` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/118321#issuecomment-1570621606):\n\n>/priority important-longerm\r\n>\r\n>Doesn't sound like a regression.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'mimowo' said:\n---\n@Dingshujie can you reproduce this issue on 1.27? Not sure this would help, but I'm asking cause there have been a couple of fixes to pod lifecycle in 1.27, including https://github.com/kubernetes/kubernetes/pull/115331.\n---\n\nUser 'tuibeovince' said:\n---\n@Dingshujie  I tried reproducing the problem in `v1.29.0` and the pod doesn't seem to get stuck in `Pending` anymore; but does this resolve your current problem?\r\n\r\n1. Using the job config yaml in the issue description; I created a job in a single worker node cluster \r\n```\r\n# kubectl get no,po -o wide\r\nNAME                             STATUS     ROLES           AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE          KERNEL-VERSION          CONTAINER-RUNTIME\r\nnode/localhost.localdomain       Ready      control-plane   47m   v1.29.0   XXX.XXX.XXX.XXX   <none>        CentOS Stream 8   4.18.0-544.el8.x86_64   containerd://1.6.28\r\nnode/localmachine2.localdomain   Ready      <none>          45m   v1.29.0   XXX.XXX.XXX.XXX   <none>        CentOS Stream 8   4.18.0-544.el8.x86_64   containerd://1.6.28\r\n\r\nNAME                     READY   STATUS    RESTARTS   AGE   IP            NODE                        NOMINATED NODE   READINESS GATES\r\npod/test-created-2mfq7   1/1     Running   0          6s    10.244.1.33   localmachine2.localdomain   <none>           <none>\r\n```\r\n\r\n2.a [With Graceful Node Shutdown enabled] After rebooting the worker node; and waiting for a while, the old pod ends up in an `Error` and starts a new pod which ends up `Completed`. Zero Restarts.\r\n\r\n```\r\nNAME                     READY   STATUS      RESTARTS   AGE     IP            NODE                        NOMINATED NODE   READINESS GATES\r\npod/test-created-2mfq7   0/1     Error       0          10m     <none>        localmachine2.localdomain   <none>           <none>\r\npod/test-created-5g5cc   0/1     Completed   0          9m22s   10.244.1.34   localmachine2.localdomain   <none>           <none>\r\n```\r\n\r\n2.b [Without Graceful Node Shutdown] After rebooting the worker node and waiting for a while, the old pod ends up with `Unknown` and starts a new pod which ends up `Completed`. Zero Restarts.\r\n```\r\nNAME                     READY   STATUS      RESTARTS   AGE     IP            NODE                        NOMINATED NODE   READINESS GATES\r\npod/test-created-7lx7h   0/1     Unknown     0          3m12s   <none>        localmachine2.localdomain   <none>           <none>\r\npod/test-created-bfh98   0/1     Completed   0          102s    10.244.1.39   localmachine2.localdomain   <none>           <none>\r\n```\r\n\r\n---\r\n\r\nAlso a fix from the job config file since `enable` is not a valid `metadata` field:\r\n> ### What happened?\r\n> 1. create a job, set job restartPolicy to Never;\r\n> \r\n> ```yaml\r\n> apiVersion: batch/v1\r\n> kind: Job\r\n> metadata:\r\n>   annotations:\r\n>     description: ''\r\n>   # enable: true\r\n>    ...\r\n>   labels: {}  # lables: {}\r\n> spec:\r\n>   ...\r\n>   template:\r\n>     metadata:\r\n>       # enable: true\r\n>       ...\r\n> ```\r\n\r\nI\n---\n\nUser 'wxx213' said:\n---\nWe also encountered this issue when kubelet experience lots of cri function call time out in the memory shortage  situation.\r\n\r\nHere I give the cause detail from the kubelet log:\r\n1. kubelet created the sandbox and the pod status was not synced to kubelet by pleg because of the cri function call time out\r\n2. kubelet start pod container time out, then the pod container was enter in Created state\r\n3. in the next syncLoop\uff0ckubelet still cannot get the pod ip\uff0cso the pod needs to be recreated, but the following codes in computePodActions function make the pod recreation action was canceled.\r\n```go\r\nif !shouldRestartOnFailure(pod) && attempt != 0 && len(podStatus.ContainerStatuses) != 0 {\r\n\t\t\t// Should not restart the pod, just return.\r\n\t\t\t// we should not create a sandbox for a pod if it is already done.\r\n\t\t\t// if all containers are done and should not be started, there is no need to create a new sandbox.\r\n\t\t\t// this stops confusing logs on pods whose containers all have exit codes, but we recreate a sandbox before terminating it.\r\n\t\t\t//\r\n\t\t\t// If ContainerStatuses is empty, we assume that we've never\r\n\t\t\t// successfully created any containers. In this case, we should\r\n\t\t\t// retry creating the sandbox.\r\n\t\t\tchanges.CreateSandbox = false\r\n\t\t\treturn changes\r\n\t\t}\r\n```\r\n4. then the sandbox would never be recreated and the pod was always  stuck in Pending state\r\n\r\nThe completely reproduce is not easy,  adding some fake codes could help:\r\n1. add the fake codes in kubelet\uff0calways make the test pod start time out\r\n```go\r\nfunc (ds *dockerService) StartContainer(_ context.Context, r *runtimeapi.StartContainerRequest) (*runtimeapi.StartContainerResponse, error) {\r\n\tinfo, err := ds.client.InspectContainer(r.ContainerId)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"inspect error %v\", err)\r\n\t}\r\n\tpodName, ok := info.Config.Labels[\"io.kubernetes.pod.name\"]\r\n\tif ok {\r\n\t\tif strings.Compare(podName, \"centos7-pod\") == 0 {\r\n\t\t\ttime.Sleep(1 * time.Hour)\r\n\t\t}\r\n\t}\r\n     .......\r\n}\r\n```\r\n2. create a pod with \"restartPolicy: Never\" config\r\n```yaml\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: centos7-pod\r\n  labels:\r\n    name: centos7-pod\r\n    scheduling.sankuai.com/system-service: 'true'\r\nspec:\r\n  containers:\r\n  - name: centos7-pod\r\n    image: centos:7\r\n    env:\r\n    - name: GET_HOST_FROM\r\n      value: env\r\n    ports:\r\n    - containerPort: 80\r\n    command:\r\n    - sleep\r\n    args:\r\n    - infinity\r\n  restartPolicy: Never\r\n```\r\n3. when the pod container start time out, kill the sandbox container to trigger the pod rebuild\r\n```\r\ndocker kill $cid\r\n```\r\n\r\n4, check the kubelet log, the CreateSandbox field  always be false\r\n```\r\n\"computePodActions got for pod\" podActions={KillPod:true CreateSandbox:false...}\r\n```\r\n\r\nWe use kubelet 1.21 and it's seems that the latest kubelet have the same issue.\n---\n\nUser 'wxx213' said:\n---\n@tuibeovince could you help to check this reproduce?\n---\n\nUser 'k8s-triage-robot' said:\n---\nThis issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted\n---\n\nUser 'haircommander' said:\n---\nwe haven't heard about this in a while, can folks try on a newer version please?\n\n/triage needs-information\n/cc @esotsal\n---\n\nUser 'esotsal' said:\n---\n> we haven't heard about this in a while, can folks try on a newer version please?\n\nTried with minikube with v1.34.0, on a multi node cluster with cri-o, with GracefullNodeShutdown enabled by default. I modified the test-job.yaml to use the multinode-demo-m02 worker node.\n```\nminikube start --nodes 2 -p multinode-demo --vm --container-runtime crio\nminikube -p multinode-demo kubectl --apply -f ~/test-job.yaml \n```\nVM reboot emulated using \n```\nminikube node stop 'multinode-demo-m02' -p multinode-demo\nminikube node start 'multinode-demo-m02' -p multinode-demo\n```\n\nDid also additional test sshing in the VM, to kill kubelet and to stop and start the service.\n```\nminikube ssh -n 'multinode-demo-m02' -p multinode-demo\n```\n\nI confirm same behaviour as https://github.com/kubernetes/kubernetes/issues/118321#issuecomment-2058400503 , unable to reproduce issue with v1.34.0\n\nFor the second comment  https://github.com/kubernetes/kubernetes/issues/118321#issuecomment-2283258025 , there are two problems: \n\n1) it is unclear how to reproduce the use case\n2) it is mentioned docker. Dockershim has been removed from the [Kubernetes project as of release 1.24,](https://kubernetes.io/docs/setup/production-environment/container-runtimes/) \n\nI propose the author of the second comment to create a separate issue , include reproduction guide with cri-o or containerd, \n\nBased on above analysis, i propose @haircommander  to close this issue.\n---\n\nUser 'haircommander' said:\n---\nworks for me\n/close\n---\n\nUser 'k8s-ci-robot' said:\n---\n@haircommander: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/118321#issuecomment-3339680496):\n\n>works for me\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---",
    "question": "What is the resolution for 'pod stuck pending when vm reboot'?",
    "ideal_answer": "@ehashman @SergeyKanzhelev\n\n/cc @ffromani\n\n/cc @mimowo @xmcqueen \r\n\n/triage accepted\r\n\nLooks like the issue has enough details for a repro.\n\n/priority important-longerm\r\n\nDoesn't sound like a regression.\n\n@SergeyKanzhelev: The label(s) `priority/important-longerm` cannot be applied, because the repository doesn't have them.\n\n@Dingshujie can you reproduce this issue on 1.27? Not sure this would help, but I'm asking cause there have been a couple of fixes to pod lifecycle in 1.27, including https://github.com/kubernetes/kubernetes/pull/115331.\n\n@Dingshujie  I tried reproducing the problem in `v1.29.0` and the pod doesn't seem to get stuck in `Pending` anymore; but does this resolve your current problem?\r\n\n1. Using the job config yaml in the issue description; I created a job in a single worker node cluster \r\n```\r\n# kubectl get no,po -o wide\r\nNAME                             STATUS     ROLES           AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE          KERNEL-VERSION          CONTAINER-RUNTIME\r\nnode/localhost.localdomain       Ready      control-plane   47m   v1.29.0   XXX.XXX.XXX.XXX   <none>        CentOS Stream 8   4.18.0-544.el8.x86_64   containerd://1.6.28\r\nnode/localmachine2.localdomain   Ready      <none>          45m   v1.29.0   XXX.XXX.XXX.XXX   <none>        CentOS Stream 8   4.18.0-544.el8.x86_64   containerd://1.6.28\r\n\nNAME                     READY   STATUS    RESTARTS   AGE   IP            NODE                        NOMINATED NODE   READINESS GATES\r\npod/test-created-2mfq7   1/1     Running   0          6s    10.244.1.33   localmachine2.localdomain   <none>           <none>\r\n```\r\n\n2.a [With Graceful Node Shutdown enabled] After rebooting the worker node; and waiting for a while, the old pod ends up in an `Error` and starts a new pod which ends up `Completed`. Zero Restarts.\r\n\n```\r\nNAME                     READY   STATUS      RESTARTS   AGE     IP            NODE                        NOMINATED NODE   READINESS GATES\r\npod/test-created-2mfq7   0/1     Error       0          10m     <none>        localmachine2.localdomain   <none>           <none>\r\npod/test-created-5g5cc   0/1     Completed   0          9m22s   10.244.1.34   localmachine2.localdomain   <none>           <none>\r\n```\r\n\n2.b [Without Graceful Node Shutdown] After rebooting the worker node and waiting for a while, the old pod ends up with `Unknown` and starts a new pod which ends up `Completed`. Zero Restarts.\r\n```\r\nNAME                     READY   STATUS      RESTARTS   AGE     IP            NODE                        NOMINATED NODE   READINESS GATES\r\npod/test-created-7lx7h   0/1     Unknown     0          3m12s   <none>        localmachine2.localdomain   <none>           <none>\r\npod/test-created-bfh98   0/1     Completed   0          102s    10.244.1.39   localmachine2.localdomain   <none>           <none>\r\n```\r\n\nAlso a fix from the job config file since `enable` is not a valid `metadata` field:\r\n> ### What happened?\r\n> 1. create a job, set job restartPolicy to Never;\r\n> \r\n> ```yaml\r\n> apiVersion: batch/v1\r\n> kind: Job\r\n> metadata:\r\n>   annotations:\r\n>     description: ''\r\n>   # enable: true\r\n>    ...\r\n>   labels: {}  # lables: {}\r\n> spec:\r\n>   ...\r\n>   template:\r\n>     metadata:\r\n>       # enable: true\r\n>       ...\r\n> ```\r\n\nI\n\nWe also encountered this issue when kubelet experience lots of cri function call time out in the memory shortage  situation.\r\n\nHere I give the cause detail from the kubelet log:\r\n1. kubelet created the sandbox and the pod status was not synced to kubelet by pleg because of the cri function call time out\r\n2. kubelet start pod container time out, then the pod container was enter in Created state\r\n3. in the next syncLoop\uff0ckubelet still cannot get the pod ip\uff0cso the pod needs to be recreated, but the following codes in computePodActions function make the pod recreation action was canceled.\r\n```go\r\nif !shouldRestartOnFailure(pod) && attempt != 0 && len(podStatus.ContainerStatuses) != 0 {\r\n\t\t\t// Should not restart the pod, just return.\r\n\t\t\t// we should not create a sandbox for a pod if it is already done.\r\n\t\t\t// if all containers are done and should not be started, there is no need to create a new sandbox.\r\n\t\t\t// this stops confusing logs on pods whose containers all have exit codes, but we recreate a sandbox before terminating it.\r\n\t\t\t//\r\n\t\t\t// If ContainerStatuses is empty, we assume that we've never\r\n\t\t\t// successfully created any containers. In this case, we should\r\n\t\t\t// retry creating the sandbox.\r\n\t\t\tchanges.CreateSandbox = false\r\n\t\t\treturn changes\r\n\t\t}\r\n```\r\n4. then the sandbox would never be recreated and the pod was always  stuck in Pending state\r\n\nThe completely reproduce is not easy,  adding some fake codes could help:\r\n1. add the fake codes in kubelet\uff0calways make the test pod start time out\r\n```go\r\nfunc (ds *dockerService) StartContainer(_ context.Context, r *runtimeapi.StartContainerRequest) (*runtimeapi.StartContainerResponse, error) {\r\n\tinfo, err := ds.client.InspectContainer(r.ContainerId)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"inspect error %v\", err)\r\n\t}\r\n\tpodName, ok := info.Config.Labels[\"io.kubernetes.pod.name\"]\r\n\tif ok {\r\n\t\tif strings.Compare(podName, \"centos7-pod\") == 0 {\r\n\t\t\ttime.Sleep(1 * time.Hour)\r\n\t\t}\r\n\t}\r\n     .......\r\n}\r\n```\r\n2. create a pod with \"restartPolicy: Never\" config\r\n```yaml\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: centos7-pod\r\n  labels:\r\n    name: centos7-pod\r\n    scheduling.sankuai.com/system-service: 'true'\r\nspec:\r\n  containers:\r\n  - name: centos7-pod\r\n    image: centos:7\r\n    env:\r\n    - name: GET_HOST_FROM\r\n      value: env\r\n    ports:\r\n    - containerPort: 80\r\n    command:\r\n    - sleep\r\n    args:\r\n    - infinity\r\n  restartPolicy: Never\r\n```\r\n3. when the pod container start time out, kill the sandbox container to trigger the pod rebuild\r\n```\r\ndocker kill $cid\r\n```\r\n\n4, check the kubelet log, the CreateSandbox field  always be false\r\n```\r\n\"computePodActions got for pod\" podActions={KillPod:true CreateSandbox:false...}\r\n```\r\n\nWe use kubelet 1.21 and it's seems that the latest kubelet have the same issue.\n\n@tuibeovince could you help to check this reproduce?\n\nThis issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted\n\nwe haven't heard about this in a while, can folks try on a newer version please?\n\n/triage needs-information\n/cc @esotsal\n\n> we haven't heard about this in a while, can folks try on a newer version please?\n\nTried with minikube with v1.34.0, on a multi node cluster with cri-o, with GracefullNodeShutdown enabled by default. I modified the test-job.yaml to use the multinode-demo-m02 worker node.\n```\nminikube start --nodes 2 -p multinode-demo --vm --container-runtime crio\nminikube -p multinode-demo kubectl --apply -f ~/test-job.yaml \n```\nVM reboot emulated using \n```\nminikube node stop 'multinode-demo-m02' -p multinode-demo\nminikube node start 'multinode-demo-m02' -p multinode-demo\n```\n\nDid also additional test sshing in the VM, to kill kubelet and to stop and start the service.\n```\nminikube ssh -n 'multinode-demo-m02' -p multinode-demo\n```\n\nI confirm same behaviour as https://github.com/kubernetes/kubernetes/issues/118321#issuecomment-2058400503 , unable to reproduce issue with v1.34.0\n\nFor the second comment  https://github.com/kubernetes/kubernetes/issues/118321#issuecomment-2283258025 , there are two problems: \n\n1) it is unclear how to reproduce the use case\n2) it is mentioned docker. Dockershim has been removed from the [Kubernetes project as of release 1.24,](https://kubernetes.io/docs/setup/production-environment/container-runtimes/) \n\nI propose the author of the second comment to create a separate issue , include reproduction guide with cri-o or containerd, \n\nBased on above analysis, i propose @haircommander  to close this issue.\n\nworks for me\n\n@haircommander: Closing this issue."
  },
  {
    "id": "gen_nat_007",
    "category": "feature_request",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: ClusterIP load balancers intermittently deleted in HNS by Windows KubeProxy when internalTrafficPolicy is set to Local\n\nA user reported the following issue titled 'ClusterIP load balancers intermittently deleted in HNS by Windows KubeProxy when internalTrafficPolicy is set to Local' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nWhen internalTrafficPolicy was set to Local, Windows KubeProxy intermittently deleted the corresponding ClusterIP load balancers in HNS. This caused service connectivity disruptions since the load balancer rules were unexpectedly removed.\n\n### What did you expect to happen?\n\nClusterIP load balancers in HNS should remain consistently present and stable, even when internalTrafficPolicy is set to Local. The configuration should not trigger deletion of the load balancer or service disruptions.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n- Create a ClusterIP service in a multi node cluster with internalTrafficPolicy tag set to Local.\n- Scale up the pods to some good number so that few of the pods came up in remote node (Node B).\n- Keep monitoring the traffic initiated from a different client pod to the clusterip after the scale.\n- Scale up or down, but make sure the new pod coming up or new pod getting deleted happens in node b (neighbor node). \n- Make a connection to the clusterip from a client pod in the node A.\n- The connection won't succeed.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# 1.31+\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nAKS\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\nWindows Server 2022\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'princepereira' said:\n---\n/assign @princepereira\n---\n\nUser 'princepereira' said:\n---\n/sig windows network\n---\n\nUser 'MikeZappa87' said:\n---\n/triage accepted\n---",
    "question": "In kubernetes/kubernetes, ClusterIP load balancers intermittently deleted in HNS by Windows KubeProxy when internalTrafficPolicy is set to Local. Any ideas why?",
    "ideal_answer": "/triage accepted"
  },
  {
    "id": "gen_nat_008",
    "category": "general",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: Node swap metric uses memory timestamp instead of swap timestamp\n\nA user reported the following issue titled 'Node swap metric uses memory timestamp instead of swap timestamp' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nThe `collectNodeSwapMetrics` function emits node swap usage metrics with the timestamp from node memory stats rather than swap stats. This issue was caught in https://github.com/kubernetes/kubernetes/pull/132945\n\n### What did you expect to happen?\n\nNode swap usage metrics should use the swap stat timestamp to accurately reflect the time of stat collection, matching the handling for pod and container swap metrics.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n- Review `collectNodeSwapMetrics` in `pkg/kubelet/metrics/collectors/resource_metrics.go`.\n\n### Anything else we need to know?\n\nPod and container swap metrics already use the correct swap stat timestamp.\n\n### Kubernetes version\n\nN/A\n\n### Cloud provider\n\nN/A\n\n### OS version\n\n_No response_\n\n### Install tools\n\n_No response_\n\n### Container runtime (CRI) and version (if applicable)\n\n_No response_\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n_No response_\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'ajaysundark' said:\n---\n/assign @ajaysundark\n---\n\nUser 'ajaysundark' said:\n---\n/triage accepted\n/sig node\n---",
    "question": "I'm running into an issue with kubernetes/kubernetes: Node swap metric uses memory timestamp instead of swap timestamp. How can I fix this?",
    "ideal_answer": "/triage accepted"
  },
  {
    "id": "gen_nat_009",
    "category": "general",
    "source_file": "microsoft_vscode_issue.json",
    "context": "Repository: microsoft/vscode\nTitle: Icon are clipped in terminal suggest when using lineHeight\n\nA user reported the following issue titled 'Icon are clipped in terminal suggest when using lineHeight' in the 'microsoft/vscode' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\nRepro\n\n```\n\"terminal.integrated.lineHeight\": 1.1,\n```\n\n<img width=\"771\" height=\"470\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/55842803-81fb-4494-bfff-2b20cd9c241f\" />\n\nWithout that it's fine:\n\n<img width=\"386\" height=\"96\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b9ba7131-854b-4b43-aebf-66af271ce5e6\" />\n\n1.1 is meant to multiply the default line height by 1.1, and it looks like it's making it smaller. The line height settings in terminal and editor work a little differently from each other.\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'Tyriar' said:\n---\n<img width=\"618\" height=\"306\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7a5afc4c-0508-4735-8fd8-aa12701a7c8c\" />\n---\n\nUser 'pwang347' said:\n---\nNote: it's technically possible to get clipped lines in the editor as well by setting editor.suggestLineHeight to 8.\n\n<img width=\"459\" height=\"57\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e0cc96d4-2158-47f5-8869-b4349f4bba83\" />\n\nThough in this case since Linux users are set to 1.1 by default we should probably make sure that the default isn't a clipped experience. In the linked PR, I set the new baseline min value to be the non-clipped height.\n---",
    "question": "Has anyone seen 'Icon are clipped in terminal suggest when using lineHeight' in microsoft/vscode? I can't figure out the cause.",
    "ideal_answer": "<img width=\"618\" height=\"306\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7a5afc4c-0508-4735-8fd8-aa12701a7c8c\" />\n\nNote: it's technically possible to get clipped lines in the editor as well by setting editor.suggestLineHeight to 8.\n\n<img width=\"459\" height=\"57\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e0cc96d4-2158-47f5-8869-b4349f4bba83\" />\n\nThough in this case since Linux users are set to 1.1 by default we should probably make sure that the default isn't a clipped experience. In the linked PR, I set the new baseline min value to be the non-clipped height."
  },
  {
    "id": "gen_nat_010",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: WebSocket streaming upgrade failures create excessive log noise during container lifecycle transitions\n\nA user reported the following issue titled 'WebSocket streaming upgrade failures create excessive log noise during container lifecycle transitions' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nDuring normal Kubernetes operations, kubelet's streaming server logs frequent `unable to upgrade websocket connection: websocket server finished before becoming ready` errors at ERROR level. These errors occur during container lifecycle transitions (creation, termination, cleanup) but do not impact functionality since client-side fallback to SPDY handles them gracefully.\n\n```sh\n  Log Pattern:\n  time=\"2025-07-08T20:37:33.311285139Z\" level=error msg=\"Unhandled Error: unable to upgrade websocket connection: websocket server\n  finished before becoming ready (logger=\\\"UnhandledError\\\")\"\n```\n\n  Root Cause:\n  Race condition between WebSocket streaming setup and rapid container state changes. When kubectl/operators initiate streaming\n  operations (exec, attach, logs) during container lifecycle transitions, the WebSocket server setup fails before becoming ready, but\n  SPDY fallback ensures the operation completes successfully.\n\nxref: https://issues.redhat.com/browse/OCPBUGS-60036\n\n### What did you expect to happen?\n\n  WebSocket upgrade failures that are gracefully handled by fallback mechanisms should not generate ERROR-level log noise.\n\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nWe are seeing a bunch of errors in Openshift CI. You can look into https://issues.redhat.com/browse/OCPBUGS-60036 to get more details.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n1.33\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nAny cloud\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n$ cat /etc/os-release\n  NAME=\"Red Hat Enterprise Linux CoreOS\"\n  VERSION=\"9.6.20250707-1 (Plow)\"\n  ID=\"rhcos\"\n  ID_LIKE=\"rhel fedora\"\n  VERSION_ID=\"9.6\"\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\nCRI-O\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'sohankunkerkar' said:\n---\n/sig api-machinery\n---\n\nUser 'ardaguclu' said:\n---\nAs opposed to Kubernetes (`TranslateStreamCloseWebsocketRequests` feature is enabled by default), it is disabled by default in OpenShift. So that might be the issue. I'll have a look at this. However, from the Kubernetes point of view, this is not an issue. Because it shouldn't fall back to SPDY.\n---\n\nUser 'ardaguclu' said:\n---\nI tried to explain the issue in [OpenShift](https://issues.redhat.com/browse/OCPBUGS-60036?focusedId=28036512&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-28036512). In my opinion, there is no issue in upstream and we can close this.\n---\n\nUser 'Jefftree' said:\n---\n/cc @seans3 \n/triage accepted\n---\n\nUser 'aojea' said:\n---\n> I tried to explain the issue in [OpenShift](https://issues.redhat.com/browse/OCPBUGS-60036?focusedId=28036512&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-28036512). In my opinion, there is no issue in upstream and we can close this.\n\n/close\n---\n\nUser 'k8s-ci-robot' said:\n---\n@aojea: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/134000#issuecomment-3282752714):\n\n>> I tried to explain the issue in [OpenShift](https://issues.redhat.com/browse/OCPBUGS-60036?focusedId=28036512&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-28036512). In my opinion, there is no issue in upstream and we can close this.\n>\n>/close\n>\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---",
    "question": "In kubernetes/kubernetes, WebSocket streaming upgrade failures create excessive log noise during container lifecycle transitions. Any ideas why?",
    "ideal_answer": "As opposed to Kubernetes (`TranslateStreamCloseWebsocketRequests` feature is enabled by default), it is disabled by default in OpenShift. So that might be the issue. I'll have a look at this. However, from the Kubernetes point of view, this is not an issue. Because it shouldn't fall back to SPDY.\n\nI tried to explain the issue in [OpenShift](https://issues.redhat.com/browse/OCPBUGS-60036?focusedId=28036512&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-28036512). In my opinion, there is no issue in upstream and we can close this.\n\n/cc @seans3 \n/triage accepted\n\n> I tried to explain the issue in [OpenShift](https://issues.redhat.com/browse/OCPBUGS-60036?focusedId=28036512&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-28036512). In my opinion, there is no issue in upstream and we can close this.\n\n@aojea: Closing this issue."
  },
  {
    "id": "gen_nat_011",
    "category": "troubleshooting",
    "source_file": "microsoft_vscode_issue.json",
    "context": "Repository: microsoft/vscode\nTitle: Rerun task not working in monorepos\n\nA user reported the following issue titled 'Rerun task not working in monorepos' in the 'microsoft/vscode' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\nType: <b>Bug</b>\n\n## Issue\n\n`Rerun task` not working properly in monorepo.\nThis error seems to occur with tasks that are left running in the background, such as a --watch.\n\n## Steps to reproduce\n\n1- Create a simple monorepo with npm workspaces. \n2- Create any kind of repo inside the monorepo ( library, app or what you want).\n3- Launch any script you want from the NPM script window.\n4- Try to `rerun task` from the terminal side bar. It will fail and show an error in the bottom right corner: `Task dev - packages/cb-password-manager-library no longer exists or has been modified. Cannot restart.`\n\nhttps://github.com/user-attachments/assets/74553bd9-01e7-44ed-95f1-b69ce37b3829\n\nVS Code version: Code 1.104.3 (385651c938df8a906869babee516bffd0ddb9829, 2025-10-02T12:30:51.747Z)\nOS version: Windows_NT x64 10.0.26100\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz (16 x 2304)|\n|GPU Status|2d_canvas: enabled<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>trees_in_viz: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|undefined|\n|Memory (System)|31.79GB (20.50GB free)|\n|Process Argv|--crash-reporter-id a4868ab4-718b-4469-820a-1c09a518091d|\n|Screen Reader|no|\n|VM|0%|\n</details><details><summary>Extensions (53)</summary>\n\nExtension|Author (truncated)|Version\n---|---|---\ncodesnap|adp|1.3.4\ntsl-problem-matcher|amo|0.6.2\nng-template|Ang|20.2.2\ncssrem|cip|4.1.1\nesbuild-problem-matchers|con|0.0.3\nangular-schematics|cyr|6.23.0\nvscode-eslint|dba|3.0.16\nproxy-toggle|Dom|1.0.1\nes7-react-js-snippets|dsz|4.4.3\ngitlens|eam|17.5.1\nprettier-vscode|esb|11.0.0\ncode-runner|for|0.12.2\nfetch-client|Gan|1.8.0\ncopilot|Git|1.372.0\ncopilot-chat|Git|0.31.4\nvscode-github-actions|git|0.28.0\ntodo-tree|Gru|0.0.226\nvscode-drawio|hed|1.9.0\nAngular2|joh|18.0.2\nvscode-peacock|joh|4.2.2\nrainbow-csv|mec|3.22.0\ngit-graph|mhu|1.30.0\nremote-ssh|ms-|0.120.0\nremote-ssh-edit|ms-|0.87.0\nextension-test-runner|ms-|0.0.12\nremote-explorer|ms-|0.5.0\nvsliveshare|ms-|1.0.5959\nvscode-versionlens|pfl|1.22.4\nmaterial-icon-theme|PKi|5.27.0\npostman-for-vscode|Pos|1.16.2\nquicktype|qui|23.0.170\nvscode-services|rap|1.0.2\njava|red|1.46.0\nvscode-xml|red|0.29.0\nvscode-yaml|red|1.19.0\nLiveServer|rit|5.7.9\npartial-diff|ryu|1.4.3\ncode-spell-checker|str|4.2.6\ncode-spell-checker-spanish|str|2.3.9\nvscode-stylelint|sty|1.5.3\nerrorlens|use|3.26.0\nintellicode-api-usage-examples|Vis|0.2.9\nvscodeintellicode|Vis|1.3.2\nmigrate-java-to-azure|vsc|1.5.2\nvscode-java-debug|vsc|0.58.2\nvscode-java-dependency|vsc|0.25.2025092303\nvscode-java-pack|vsc|0.30.2\nvscode-java-test|vsc|0.43.1\nvscode-java-upgrade|vsc|1.6.1\nvscode-maven|vsc|0.44.0\nvscode-icons|vs\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'meganrogge' said:\n---\nI cannot repro, thus closing\n\nhttps://github.com/user-attachments/assets/8a794a1c-96cf-46f7-ab0a-edd260f53d33\n---\n\nUser 'Celtic-Bytes' said:\n---\nMaybe if you take time to read the issue you will notice i said the issue happens with tasks that KEEPS RUNNING IN THE BACKGROUND like a 'watch',a dev server something similar.. In your video, you are rerunning a normal task.\n---\n\nUser 'meganrogge' said:\n---\nAh, I see. `Task watch - build no longer exists or has been modified. Cannot restart.` happens in that case\n---",
    "question": "Has anyone seen 'Rerun task not working in monorepos' in microsoft/vscode? I can't figure out the cause.",
    "ideal_answer": "I cannot repro, thus closing\n\nhttps://github.com/user-attachments/assets/8a794a1c-96cf-46f7-ab0a-edd260f53d33\n\nMaybe if you take time to read the issue you will notice i said the issue happens with tasks that KEEPS RUNNING IN THE BACKGROUND like a 'watch',a dev server something similar.. In your video, you are rerunning a normal task.\n\nAh, I see. `Task watch - build no longer exists or has been modified. Cannot restart.` happens in that case"
  },
  {
    "id": "gen_nat_012",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: kubelet fails with `UnmountVolume.NewUnmounter failed for volume` and `vol_data.json: no such file or directory` for CSI volumes and floods the logs\n\nA user reported the following issue titled 'kubelet fails with `UnmountVolume.NewUnmounter failed for volume` and `vol_data.json: no such file or directory` for CSI volumes and floods the logs' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nI think kubelet sometimes is not able to properly identify unmounted volume and clean up its `actualstateofworld` cache. It thinks the volume is not unmounted and keeps on trying unmounting it. This outputs a lot of logs which flood journalctl -> fluentd/bit -> third party logging solution. The flooding causes 2 problems:\r\n1. Consumes node disk which reduces the amount of disk available for pods\r\n2. Increases ingestion rate for paid third party logging solution thereby costing extra money1 and 2 can be worked around by rotating the logs more frequently, dropping the logs or filtering them out from ingestion into the third party logging solution but it is not a permanent fix to the problem. Also, note that re-starting kubelet fixes the issue. Here's what the error log looks like:\r\n```\r\n\"Feb 13 22:05:35 ip-10-57-71-57 kubelet: E0213 22:05:35.753487   28558 reconciler.go:193] \\\"operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \\\\\\\"drupal-code\\\\\\\" (UniqueName: \\\\\\\"kubernetes.io/csi/48033fa8-05f1-4c32-b109-4e25f23107ef-drupal-code\\\\\\\") pod \\\\\\\"48033fa8-05f1-4c32-b109-4e25f23107ef\\\\\\\" (UID: \\\\\\\"48033fa8-05f1-4c32-b109-4e25f23107ef\\\\\\\") : UnmountVolume.NewUnmounter failed for volume \\\\\\\"drupal-code\\\\\\\" (UniqueName: \\\\\\\"kubernetes.io/csi/48033fa8-05f1-4c32-b109-4e25f23107ef-drupal-code\\\\\\\") pod \\\\\\\"48033fa8-05f1-4c32-b109-4e25f23107ef\\\\\\\" (UID: \\\\\\\"48033fa8-05f1-4c32-b109-4e25f23107ef\\\\\\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/48033fa8-05f1-4c32-b109-4e25f23107ef/volumes/kubernetes.io~csi/drupal-code/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/48033fa8-05f1-4c32-b109-4e25f23107ef/volumes/kubernetes.io~csi/drupal-code/vol_data.json]: open /var/lib/kubelet/pods/48033fa8-05f1-4c32-b109-4e25f23107ef/volumes/kubernetes.io~csi/drupal-code/vol_data.json: no such file or directory\\\" err=\\\"UnmountVolume.NewUnmounter failed for volume \\\\\\\"drupal-code\\\\\\\" (UniqueName: \\\\\\\"kubernetes.io/csi/48033fa8-05f1-4c32-b109-4e25f23107ef-drupal-code\\\\\\\") pod \\\\\\\"48033fa8-05f1-4c32-b109-4e25f23107ef\\\\\\\" (UID: \\\\\\\"48033fa8-05f1-4c32-b109-4e25f23107ef\\\\\\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/48033fa8-05f1-4c32-b109-4e25f23107ef/volumes/kubernetes.io~csi/drupal-code/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/48033fa8-05f1-4c32-b109-4e25f23107ef/volumes/kubernetes.io~csi/drupal-code/vol_data.json]: open /var/lib/kubelet/pods/48033fa8-05f1-4c32-b109-4e25f23107ef/volumes/kubernetes.io~csi/drupal-code/vol_data.json: no such file or directory\\\"\r\n```\r\nIf you look at the code for 1.22, you can see the the log is output [here](https://github.com/kubernetes/kubernetes/blob/bc4763cbf8b9e6adfbc07bb3194509b187b0b901/pkg/kubelet/volumemanager/reconciler/reconciler.go#L191).\r\n\r\nCouple of things to note here: \r\n1. We have seen this happen a lot for short-lived pods. \r\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'k8s-ci-robot' said:\n---\nThis issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'vadasambar' said:\n---\nPlease let me know if you need more details \ud83d\ude4f\n---\n\nUser 'kundan2707' said:\n---\n/triage not-reproducible\n---\n\nUser 'vadasambar' said:\n---\nNote that when I say it's hard to reproduce manually, I mean it's hard to get into a state where `vol_data.json` is deleted automatically. You can easily reproduce the error message above using the following steps:\r\n1. Create a pod with CSI volume\r\n2. Log into the node and go to the pod directory e.g., `cd /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<vol-name>`\r\n3. Remove vol_data.json using `rm vol_data.json`\r\n4. You should start seeing the above error in kubelet logs\n---\n\nUser 'HirazawaUi' said:\n---\n/assign \r\ni'll have a look\n---\n\nUser 'HirazawaUi' said:\n---\n> Note that when I say it's hard to reproduce manually, I mean it's hard to get into a state where `vol_data.json` is deleted automatically. You can easily reproduce the error message above using the following steps:\r\n> \r\n> 1. Create a pod with CSI volume\r\n> 2. Log into the node and go to the pod directory e.g., `cd /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<vol-name>`\r\n> 3. Remove vol_data.json using `rm vol_data.json`\r\n> 4. You should start seeing the above error in kubelet logs\r\n\r\nI have reproduced this error in a local cluster running version 1.26.3 following the steps mentioned above. However, I am uncertain if this error will be triggered when kubelet is functioning normally. I will continue to monitor it. Starting from a particular issue may help to better understand the source code of kubelet :)\n---\n\nUser 'vadasambar' said:\n---\nThank you for looking into this @HirazawaUi!\n---\n\nUser 'HirazawaUi' said:\n---\n@vadasambar It seems to have disappeared on v1.26.3. I used the CSI-Drive like you and implemented short code to create PV, PVC, and POD every two minutes, and then delete them after one minute when they are created (simulating a short-lived pod). Everything was fine after running continuously for two days. Could you test if this issue occurs on the new version? (if possible)\n---\n\nUser 'vadasambar' said:\n---\n@HirazawaUi thank you for looking deeper into this!\r\n\r\n>  I used the CSI-Drive like you and implemented short code to create PV, PVC, and POD every two minutes, and then delete them after one minute when they are created (simulating a short-lived pod). Everything was fine after running continuously for two days.\r\n\r\nI have tried something similar (CronJob which is set to run every minute with PV/PVC) on older Kubernetes version (1.21/1.22) but I wasn't able to reproduce this issue. It makes me think, something else might be involved here (perhaps a race condition?).\n---\n\nUser 'HirazawaUi' said:\n---\n> @HirazawaUi thank you for looking deeper into this!\r\n> \r\n> > I used the CSI-Drive like you and implemented short code to create PV, PVC, and POD every two minutes, and then delete them after one minute when they are created (simulating a short-lived pod). Everything was fine after running continuously for two days.\r\n> \r\n> I have tried something similar (CronJob which is set to run every minute with PV/PVC) on older Kubernetes version (1.21/1.22) but I wasn't able to reproduce this issue. It makes me think, something else might be involved here (perhaps a race condition?).\r\n\r\nI'm sorry, I wasn't able to solve your issue. Perhaps need to wait for someone who is more familiar with storage\r\n/unassign\r\n/sig storage\n---\n\nUser 'vadasambar' said:\n---\n@HirazawaUi not at all. Really appreciate you looking into this!\n---\n\nUser 'qiliRedHat' said:\n---\n@HirazawaUi @vadasambar I saw the similar issue too on Openshift OCP cluster https://issues.redhat.com/browse/OCPBUGS-11470\r\nVersion\r\n```\r\nServer Version: 4.13.0-rc.2\r\nKubernetes Version: v1.26.2+dc93b13\r\n\r\nand\r\n\r\nServer Version: 4.12.0\r\nKubernetes Version: v1.25.4+77bec7a\r\n```\r\nAWS with ebs.csi.aws.com csidriver\r\n```\r\n % oc get csidriver\r\nNAME              ATTACHREQUIRED   PODINFOONMOUNT   STORAGECAPACITY   TOKENREQUESTS   REQUIRESREPUBLISH   MODES        AGE\r\nebs.csi.aws.com   true             false            false             <unset>         false               Persistent   5d3h\r\n```\r\nHow can we reproduce it (as minimally and precisely as possible)?\r\nI encountered this in a long run test. The test have 15 concurrent users who create namespace, create pod and pvc under the namespace and then delete the namespace, continuously. The issue happened on about 4-5 days after I started the long run test. It happened very rarely, once or twice in a 5 days long run, which had totally 10k+ namespaces creation/deletion.\r\n\r\nThis bug is similar to a previous bug:\r\nhttps://bugzilla.redhat.com/show_bug.cgi?id=2038780\r\nhttps://github.com/kubernetes/kubernetes/pull/110670\n---\n\nUser 'jsafrane' said:\n---\n@cvvz, @Chaunceyctx, does it look like a duplicate of https://github.com/kubernetes/kubernetes/issues/114207 ? Or something very similar, now in SetUp / TearDown instead of MountDevice / UnmountDevice?\n---\n\nUser 'Chaunceyctx' said:\n---\n> @cvvz, @Chaunceyctx, does it look like a duplicate of #114207 ? Or something very similar, now in SetUp / TearDown instead of MountDevice / UnmountDevice?\r\n\r\nThese two problems look like similar. TearDown Operation may be performed multiple times. I will take a look if I have time LOL.\n---\n\nUser 'vadasambar' said:\n---\nWe checked some pods with unmount errors and Interestingly we are seeing `Orphaned pod found, but volumes are not cleaned up\" podUID=XXXX` log such pods. This might be due to parent getting removed abruptly. I wonder if the parent controller is removing the pods in a way it shouldn't or if kubelet is not responding well to such deletions. We have seen it happen with Job and ReplicaSets.\n---\n\nUser 'dims' said:\n---\ncc @ConnorJC3 @torredil see references to `ebs.csi.aws.com csidriver` above\n---\n\nUser 'cvvz' said:\n---\nI think these two issues are the same, there is a TOCTOU problem and `vol.data` maybe removed multiple times which trigger this issue.\r\nWe have fixed this issue by skipping `UnmountDevice` when `vol_data.json` file does not exist,  since in this situation, `UnmountDevice` must has been executed successfully before, no need to do `UnmountDevice` multiple times.\n---\n\nUser 'jingxu97' said:\n---\n/cc @jingxu97\n---\n\nUser 'vadasambar' said:\n---\n@cvvz which release is this fixed in? (and thank you!)\n---\n\nUser 'jingxu97' said:\n---\nhttps://github.com/kubernetes/kubernetes/issues/114207#issuecomment-1535623925\r\n\r\nif it is the correct fix\n---\n\nUser 'vadasambar' said:\n---\n@jingxu97 thank you for the link. Looking [at the PR](https://github.com/kubernetes/kubernetes/pull/116138/files#diff-227f84916ffb93ece42ccaec840af8ea265714440c15c45f42b08d6a427a57bfR598-R600), I think the new change might resolve our issue. Thanks a bunch!\n---\n\nUser 'vadasambar' said:\n---\n>  FYI. this issue is fixed in 1.24.14, 1.25.10, 1.26.5, and has been fixed in v1.27.0\r\n\r\nhttps://github.com/kubernetes/kubernetes/issues/114207#issuecomment-1535623925\n---\n\nUser 'Venkat639' said:\n---\nStill getting same issue with k8s version 1.24.15. \r\nScenario I tried --\r\nCreate prometheus Statefulset pod but VPA updated resources for that pod and pod got immediately deleted within one minute of creation time.\r\nPod got stuck with in terminating state forever.\r\n\r\nFollowing are the logs --\r\n```\r\nJul 25 23:07:56 ip-x-x-x-x kubelet[6181]: E0725 23:07:56.957366    6181 nestedpendingoperations.go:348] Operation for \"{volumeName:kubernetes.io/csi/ebs.csi.aws.com^<vol-name> podName:<pod-name> nodeName:}\" failed. No retries permitted until 2023-07-25 23:07:57.457346569 +0000 UTC m=+153351.396529157 (durationBeforeRetry 500ms). Error: UnmountVolume.TearDown failed for volume \"storage-volume\" (UniqueName: \"kubernetes.io/csi/ebs.csi.aws.com^vol-xxxx\u201d) pod \"<pod-uid>\" (UID: \"<pod-uid>\") : kubernetes.io/csi: Unmounter.TearDownAt failed: rpc error: code = Internal desc = Could not unmount \"/var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/mount\": unmount failed: exit status 32\r\n\r\nJul 25 23:07:56 ip-x-x-x-x kubelet[6181]: Unmounting arguments: /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/mount\r\n\r\nJul 25 23:07:56 ip-x-x-x-x kubelet[6181]: Output: umount: /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/mount: no mount point specified. \r\n\r\nJul 25 23:07:57 ip-x-x-x-x kubelet[6181]: E0725 23:07:57.055425    6181 reconciler.go:208] \"operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \\\"storage-volume\\\" (UniqueName: \\\"[kubernetes.io/csi/ebs.csi.aws.com^<vol-name>](http://kubernetes.io/csi/ebs.csi.aws.com%5Evol-xxxxx%5C)\u201d) pod \\\"<pod-uid>\\\" (UID: \\\"<pod-uid>\\\") : UnmountVolume.NewUnmounter failed for volume \\\"storage-volume\\\" (UniqueName: \\\"kubernetes.io/csi/ebs.csi.aws.com^<vol-name>\\\u201d) pod \\\"<pod-uid>\\\" (UID: \\\"<pod-uid>\\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/vol_data.json]: open /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/vol_data.json: no such file or directory\" err=\"UnmountVolume.NewUnmounter failed for volume \\\"storage-volume\\\" (UniqueName: \\\"kubernetes.io/csi/ebs.csi.aws.com^<vol-name>\\\") pod \\\"<pod-uid>\\\" (UID: \\\"<pod-uid>\\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/<pod-uid>volumes/kubernetes.io~csi/<pvc-id>/vol_data.json]: open /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/vol_data.json: no such file or directory\"\r\n```\n---\n\nUser 'cvvz' said:\n---\n@Venkat639 This is not the same problem as what we fixed in https://github.com/kubernetes/kubernetes/pull/116138, we ~~fixed~~ workaround the race condition in `UnmountDevice`. However, this is another bug caused by the same race condition in `UnmountVolume` which is almost the same problem as what we have ~~fixed~~ workaround in `UnmountDevice` process and we need to fix this one as well.\n---\n\nUser 'cvvz' said:\n---\nI just filed a PR https://github.com/kubernetes/kubernetes/pull/120086 to resolve the race condition in reconciler, could you please take a look? @jingxu97 @jsafrane \r\n\r\nI think this is the real solution to this kind of problem. https://github.com/kubernetes/kubernetes/pull/116138 is just like some kind of workaround and not really solved the race condition problem.\n---\n\nUser 'Dunge' said:\n---\nGot the same issue today randomly out of nowhere on a microk8s 1.27 cluster and lonhorn 1.5.1, so it's not fixed yet. The journalctl keep flooding this error every ms:\r\n\r\n> Sep 06 20:56:56 microk8s.daemon-kubelite[4155471]: E0906 20:56:56.044356 4155471 reconciler_common.go:169] \"operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \\\"pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261\\\" (UniqueName: \\\"kubernetes.io/csi/driver.longhorn.io^pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261\\\") pod \\\"5fcddea2-77a4-475f-9a4c-a62f5243b353\\\" (UID: \\\"5fcddea2-77a4-475f-9a4c-a62f5243b353\\\") : UnmountVolume.NewUnmounter failed for volume \\\"pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261\\\" (UniqueName: \\\"kubernetes.io/csi/driver.longhorn.io^pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261\\\") pod \\\"5fcddea2-77a4-475f-9a4c-a62f5243b353\\\" (UID: \\\"5fcddea2-77a4-475f-9a4c-a62f5243b353\\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/snap/microk8s/common/var/lib/kubelet/pods/5fcddea2-77a4-475f-9a4c-a62f5243b353/volumes/kubernetes.io\\~csi/pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261/mount]: kubernetes.io/csi: failed to open volume data file [/var/snap/microk8s/common/var/lib/kubelet/pods/5fcddea2-77a4-475f-9a4c-a62f5243b353/volumes/kubernetes.io\\~csi/pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261/vol_data.json]: open /var/snap/microk8s/common/var/lib/kubelet/pods/5fcddea2-77a4-475f-9a4c-a62f5243b353/volumes/kubernetes.io~csi/pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261/vol_data.json: no such file or directory\"\r\n\r\nI had a promotheus pod stuck in \"terminating\". I managed to force delete it but the errors continue. How to stop it?\n---\n\nUser 'Dunge' said:\n---\nLast week I deleted the pod folder manually and rebooted the node and it seemed to have stopped.\r\nThis morning it started again, this time on a different pod.\r\n\r\nRestarting just the kubelet process (microk8s stop/start in my case) seems to have fixed it, but I wouldn't want to error to break my cluster every few days?\r\n\r\nPlease advise.\n---\n\nUser 'gnufied' said:\n---\n@Dunge thank you for reporting. Does the pod is stuck forever in `Terminating` state or did it clear soon?\n---\n\nUser 'Dunge' said:\n---\nIt was stuck terminating (over 12h during the night) with the errors about `vol_data.json` spamming the journalctl logs and with the new replacement pod who couldn't mount the volume so the service was down.\r\nAfter issuing `microk8s stop` to shut down the node and the errors stopped in the console. I waited 5minutes for k8s to transfer the workload on the other pods. then it did clear up. I then restarted the node.\r\n\r\nMy interrogation is finding out what caused this in the first place, because two occurrences in less than a week is less than desirable. People are using the services on this cluster and their work was impacted.\n---\n\nUser 'gnufied' said:\n---\nWhat is the kubelet version and can you capture kubelet log when this happens? I have found a way to reproduce this error - https://github.com/kubernetes/kubernetes/pull/120086#issuecomment-1718276990 but it does not prevent pod from being deleted and event spam eventually stops on its own.\n---",
    "question": "Has anyone seen 'kubelet fails with `UnmountVolume.NewUnmounter failed for volume` and `vol_data.json: no such file or directory` for CSI volumes and floods the logs' in kubernetes/kubernetes? I can't figure out the cause.",
    "ideal_answer": "If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\nPlease let me know if you need more details \ud83d\ude4f\n\n/triage not-reproducible\n\nNote that when I say it's hard to reproduce manually, I mean it's hard to get into a state where `vol_data.json` is deleted automatically. You can easily reproduce the error message above using the following steps:\r\n1. Create a pod with CSI volume\r\n2. Log into the node and go to the pod directory e.g., `cd /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<vol-name>`\r\n3. Remove vol_data.json using `rm vol_data.json`\r\n4. You should start seeing the above error in kubelet logs\n\ni'll have a look\n\n> Note that when I say it's hard to reproduce manually, I mean it's hard to get into a state where `vol_data.json` is deleted automatically. You can easily reproduce the error message above using the following steps:\r\n> \r\n> 1. Create a pod with CSI volume\r\n> 2. Log into the node and go to the pod directory e.g., `cd /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<vol-name>`\r\n> 3. Remove vol_data.json using `rm vol_data.json`\r\n> 4. You should start seeing the above error in kubelet logs\r\n\nI have reproduced this error in a local cluster running version 1.26.3 following the steps mentioned above. However, I am uncertain if this error will be triggered when kubelet is functioning normally. I will continue to monitor it. Starting from a particular issue may help to better understand the source code of kubelet :)\n\nThank you for looking into this @HirazawaUi!\n\n@vadasambar It seems to have disappeared on v1.26.3. I used the CSI-Drive like you and implemented short code to create PV, PVC, and POD every two minutes, and then delete them after one minute when they are created (simulating a short-lived pod). Everything was fine after running continuously for two days. Could you test if this issue occurs on the new version? (if possible)\n\n@HirazawaUi thank you for looking deeper into this!\r\n\n>  I used the CSI-Drive like you and implemented short code to create PV, PVC, and POD every two minutes, and then delete them after one minute when they are created (simulating a short-lived pod). Everything was fine after running continuously for two days.\r\n\nI have tried something similar (CronJob which is set to run every minute with PV/PVC) on older Kubernetes version (1.21/1.22) but I wasn't able to reproduce this issue. It makes me think, something else might be involved here (perhaps a race condition?).\n\n> @HirazawaUi thank you for looking deeper into this!\r\n> \r\n> > I used the CSI-Drive like you and implemented short code to create PV, PVC, and POD every two minutes, and then delete them after one minute when they are created (simulating a short-lived pod). Everything was fine after running continuously for two days.\r\n> \r\n> I have tried something similar (CronJob which is set to run every minute with PV/PVC) on older Kubernetes version (1.21/1.22) but I wasn't able to reproduce this issue. It makes me think, something else might be involved here (perhaps a race condition?).\r\n\nI'm sorry, I wasn't able to solve your issue. Perhaps need to wait for someone who is more familiar with storage\r\n/unassign\r\n\n@HirazawaUi not at all. Really appreciate you looking into this!\n\n@HirazawaUi @vadasambar I saw the similar issue too on Openshift OCP cluster https://issues.redhat.com/browse/OCPBUGS-11470\r\nVersion\r\n```\r\nServer Version: 4.13.0-rc.2\r\nKubernetes Version: v1.26.2+dc93b13\r\n\nand\r\n\nServer Version: 4.12.0\r\nKubernetes Version: v1.25.4+77bec7a\r\n```\r\nAWS with ebs.csi.aws.com csidriver\r\n```\r\n % oc get csidriver\r\nNAME              ATTACHREQUIRED   PODINFOONMOUNT   STORAGECAPACITY   TOKENREQUESTS   REQUIRESREPUBLISH   MODES        AGE\r\nebs.csi.aws.com   true             false            false             <unset>         false               Persistent   5d3h\r\n```\r\nHow can we reproduce it (as minimally and precisely as possible)?\r\nI encountered this in a long run test. The test have 15 concurrent users who create namespace, create pod and pvc under the namespace and then delete the namespace, continuously. The issue happened on about 4-5 days after I started the long run test. It happened very rarely, once or twice in a 5 days long run, which had totally 10k+ namespaces creation/deletion.\r\n\nThis bug is similar to a previous bug:\r\nhttps://bugzilla.redhat.com/show_bug.cgi?id=2038780\r\nhttps://github.com/kubernetes/kubernetes/pull/110670\n\n@cvvz, @Chaunceyctx, does it look like a duplicate of https://github.com/kubernetes/kubernetes/issues/114207 ? Or something very similar, now in SetUp / TearDown instead of MountDevice / UnmountDevice?\n\n> @cvvz, @Chaunceyctx, does it look like a duplicate of #114207 ? Or something very similar, now in SetUp / TearDown instead of MountDevice / UnmountDevice?\r\n\nThese two problems look like similar. TearDown Operation may be performed multiple times. I will take a look if I have time LOL.\n\nWe checked some pods with unmount errors and Interestingly we are seeing `Orphaned pod found, but volumes are not cleaned up\" podUID=XXXX` log such pods. This might be due to parent getting removed abruptly. I wonder if the parent controller is removing the pods in a way it shouldn't or if kubelet is not responding well to such deletions. We have seen it happen with Job and ReplicaSets.\n\ncc @ConnorJC3 @torredil see references to `ebs.csi.aws.com csidriver` above\n\nI think these two issues are the same, there is a TOCTOU problem and `vol.data` maybe removed multiple times which trigger this issue.\r\nWe have fixed this issue by skipping `UnmountDevice` when `vol_data.json` file does not exist,  since in this situation, `UnmountDevice` must has been executed successfully before, no need to do `UnmountDevice` multiple times.\n\n/cc @jingxu97\n\n@cvvz which release is this fixed in? (and thank you!)\n\nhttps://github.com/kubernetes/kubernetes/issues/114207#issuecomment-1535623925\r\n\nif it is the correct fix\n\n@jingxu97 thank you for the link. Looking [at the PR](https://github.com/kubernetes/kubernetes/pull/116138/files#diff-227f84916ffb93ece42ccaec840af8ea265714440c15c45f42b08d6a427a57bfR598-R600), I think the new change might resolve our issue. Thanks a bunch!\n\n>  FYI. this issue is fixed in 1.24.14, 1.25.10, 1.26.5, and has been fixed in v1.27.0\r\n\nhttps://github.com/kubernetes/kubernetes/issues/114207#issuecomment-1535623925\n\nStill getting same issue with k8s version 1.24.15. \r\nScenario I tried --\r\nCreate prometheus Statefulset pod but VPA updated resources for that pod and pod got immediately deleted within one minute of creation time.\r\nPod got stuck with in terminating state forever.\r\n\nFollowing are the logs --\r\n```\r\nJul 25 23:07:56 ip-x-x-x-x kubelet[6181]: E0725 23:07:56.957366    6181 nestedpendingoperations.go:348] Operation for \"{volumeName:kubernetes.io/csi/ebs.csi.aws.com^<vol-name> podName:<pod-name> nodeName:}\" failed. No retries permitted until 2023-07-25 23:07:57.457346569 +0000 UTC m=+153351.396529157 (durationBeforeRetry 500ms). Error: UnmountVolume.TearDown failed for volume \"storage-volume\" (UniqueName: \"kubernetes.io/csi/ebs.csi.aws.com^vol-xxxx\u201d) pod \"<pod-uid>\" (UID: \"<pod-uid>\") : kubernetes.io/csi: Unmounter.TearDownAt failed: rpc error: code = Internal desc = Could not unmount \"/var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/mount\": unmount failed: exit status 32\r\n\nJul 25 23:07:56 ip-x-x-x-x kubelet[6181]: Unmounting arguments: /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/mount\r\n\nJul 25 23:07:56 ip-x-x-x-x kubelet[6181]: Output: umount: /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/mount: no mount point specified. \r\n\nJul 25 23:07:57 ip-x-x-x-x kubelet[6181]: E0725 23:07:57.055425    6181 reconciler.go:208] \"operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \\\"storage-volume\\\" (UniqueName: \\\"[kubernetes.io/csi/ebs.csi.aws.com^<vol-name>](http://kubernetes.io/csi/ebs.csi.aws.com%5Evol-xxxxx%5C)\u201d) pod \\\"<pod-uid>\\\" (UID: \\\"<pod-uid>\\\") : UnmountVolume.NewUnmounter failed for volume \\\"storage-volume\\\" (UniqueName: \\\"kubernetes.io/csi/ebs.csi.aws.com^<vol-name>\\\u201d) pod \\\"<pod-uid>\\\" (UID: \\\"<pod-uid>\\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/vol_data.json]: open /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/vol_data.json: no such file or directory\" err=\"UnmountVolume.NewUnmounter failed for volume \\\"storage-volume\\\" (UniqueName: \\\"kubernetes.io/csi/ebs.csi.aws.com^<vol-name>\\\") pod \\\"<pod-uid>\\\" (UID: \\\"<pod-uid>\\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/<pod-uid>volumes/kubernetes.io~csi/<pvc-id>/vol_data.json]: open /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-id>/vol_data.json: no such file or directory\"\r\n```\n\n@Venkat639 This is not the same problem as what we fixed in https://github.com/kubernetes/kubernetes/pull/116138, we ~~fixed~~ workaround the race condition in `UnmountDevice`. However, this is another bug caused by the same race condition in `UnmountVolume` which is almost the same problem as what we have ~~fixed~~ workaround in `UnmountDevice` process and we need to fix this one as well.\n\nI just filed a PR https://github.com/kubernetes/kubernetes/pull/120086 to resolve the race condition in reconciler, could you please take a look? @jingxu97 @jsafrane \r\n\nI think this is the real solution to this kind of problem. https://github.com/kubernetes/kubernetes/pull/116138 is just like some kind of workaround and not really solved the race condition problem.\n\nGot the same issue today randomly out of nowhere on a microk8s 1.27 cluster and lonhorn 1.5.1, so it's not fixed yet. The journalctl keep flooding this error every ms:\r\n\n> Sep 06 20:56:56 microk8s.daemon-kubelite[4155471]: E0906 20:56:56.044356 4155471 reconciler_common.go:169] \"operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \\\"pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261\\\" (UniqueName: \\\"kubernetes.io/csi/driver.longhorn.io^pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261\\\") pod \\\"5fcddea2-77a4-475f-9a4c-a62f5243b353\\\" (UID: \\\"5fcddea2-77a4-475f-9a4c-a62f5243b353\\\") : UnmountVolume.NewUnmounter failed for volume \\\"pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261\\\" (UniqueName: \\\"kubernetes.io/csi/driver.longhorn.io^pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261\\\") pod \\\"5fcddea2-77a4-475f-9a4c-a62f5243b353\\\" (UID: \\\"5fcddea2-77a4-475f-9a4c-a62f5243b353\\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/snap/microk8s/common/var/lib/kubelet/pods/5fcddea2-77a4-475f-9a4c-a62f5243b353/volumes/kubernetes.io\\~csi/pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261/mount]: kubernetes.io/csi: failed to open volume data file [/var/snap/microk8s/common/var/lib/kubelet/pods/5fcddea2-77a4-475f-9a4c-a62f5243b353/volumes/kubernetes.io\\~csi/pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261/vol_data.json]: open /var/snap/microk8s/common/var/lib/kubelet/pods/5fcddea2-77a4-475f-9a4c-a62f5243b353/volumes/kubernetes.io~csi/pvc-ce65fb13-5ebf-4bd3-a202-b698de05c261/vol_data.json: no such file or directory\"\r\n\nI had a promotheus pod stuck in \"terminating\". I managed to force delete it but the errors continue. How to stop it?\n\nLast week I deleted the pod folder manually and rebooted the node and it seemed to have stopped.\r\nThis morning it started again, this time on a different pod.\r\n\nRestarting just the kubelet process (microk8s stop/start in my case) seems to have fixed it, but I wouldn't want to error to break my cluster every few days?\r\n\nPlease advise.\n\n@Dunge thank you for reporting. Does the pod is stuck forever in `Terminating` state or did it clear soon?\n\nIt was stuck terminating (over 12h during the night) with the errors about `vol_data.json` spamming the journalctl logs and with the new replacement pod who couldn't mount the volume so the service was down.\r\nAfter issuing `microk8s stop` to shut down the node and the errors stopped in the console. I waited 5minutes for k8s to transfer the workload on the other pods. then it did clear up. I then restarted the node.\r\n\nMy interrogation is finding out what caused this in the first place, because two occurrences in less than a week is less than desirable. People are using the services on this cluster and their work was impacted.\n\nWhat is the kubelet version and can you capture kubelet log when this happens? I have found a way to reproduce this error - https://github.com/kubernetes/kubernetes/pull/120086#issuecomment-1718276990 but it does not prevent pod from being deleted and event spam eventually stops on its own."
  },
  {
    "id": "gen_nat_013",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: WatchList failed when Accept content-type is Table\n\nA user reported the following issue titled 'WatchList failed when Accept content-type is Table' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\r\n\r\nWhen perform a List or WatchList with `Accept: application/json;as=Table;v=v1;g=meta.k8s.io`, api will convert every objects of response into `metav1.Table`. And as per the specification of the WatchList, a Bookmark event with `k8s.io/initial-events-end` should be sent after all initial events have been dispatched. Upon receiving this event, the client should close the watch and return the result.\r\n\r\nHowever, when response in Table format is returned, the annotations are placed on the `metav1.Table.Rows` instead of the top level of the object. This causes misbehavior in `client-go`.\r\n\r\n```\r\n# curl 'http://localhost:8888/api/v1/pods?allowWatchBookmarks=true&watch=true&sendInitialEvents=true&resourceVersionMatch=NotOlderThan' -H 'Accept: application/json;as=Table;v=v1;g=meta.k8s.io'\r\n...\r\n\r\n{\r\n  \"type\": \"BOOKMARK\",\r\n  \"object\": {\r\n    \"kind\": \"Table\",\r\n    \"apiVersion\": \"meta.k8s.io/v1\",\r\n    \"metadata\": {\r\n      \"resourceVersion\": \"457339\"\r\n    },\r\n    \"columnDefinitions\": null,\r\n    \"rows\": [\r\n      {\r\n        \"cells\": [\r\n          \"\",\r\n          \"0/0\",\r\n          \"\",\r\n          \"0\",\r\n          \"<unknown>\",\r\n          \"<none>\",\r\n          \"<none>\",\r\n          \"<none>\",\r\n          \"<none>\"\r\n        ],\r\n        \"object\": {\r\n          \"kind\": \"PartialObjectMetadata\",\r\n          \"apiVersion\": \"meta.k8s.io/v1\",\r\n          \"metadata\": {\r\n            \"resourceVersion\": \"457339\",\r\n            \"creationTimestamp\": null,\r\n            \"annotations\": {\r\n              \"k8s.io/initial-events-end\": \"true\"\r\n            }\r\n          }\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\nIn the current implementation, an error `failed to parse watch event` will be thrown from `staging/src/k8s.io/client-go/rest/request.go:880`, and it will fall back to the standard List method.\r\n\r\n### What did you expect to happen?\r\n\r\nWatchList works properly.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nIt can be reproduced by below go code. by runnint the code, a warning message like `W0719 01:33:52.152868   45522 simple.go:308] The watchlist request for /v1, Resource=pods ended with an error, falling back to the standard LIST semantics, err = failed to parse watch event: watch.Event{Type:\"ADDED\", Object:(*v1.Table)(0x14000694870)}` can be found in terminal.\r\n\r\n```go\r\npackage main\r\n\r\nimport (\r\n\t\"context\"\r\n\t\"fmt\"\r\n\t\"os\"\r\n\t\"path\"\r\n\t\"strings\"\r\n\r\n\tcorev1 \"k8s.io/api/core/v1\"\r\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\r\n\tmetav1beta1 \"k8s.io/apimachinery/pkg/apis/meta/v1beta1\"\r\n\t\"k8s.io/apimachinery/pkg/runtime/schema\"\r\n\t\"k8s.io/client-go/dynamic\"\r\n\t\"k8s.io/client-go/rest\"\r\n\t\"k8s.io/client-go/tools/clientcmd\"\r\n\t\"k8s.io/klog/v2\"\r\n\t\"k8s.io/kubectl/pkg/scheme\"\r\n)\r\n\r\nfunc main() {\r\n\thome, err := os.UserHomeDir()\r\n\tif err != nil {\r\n\t\tklog.Fatal(err)\r\n\t}\r\n\r\n\tcfg, err := clientcmd.BuildConfigFromFlags(\"\", path.Join(home, \".kube/config\"))\r\n\tif err != nil {\r\n\t\tklog.Fatal(err)\r\n\t}\r\n\r\n\tcfg.GroupVersion = &schema.GroupVersion{}\r\n\tcfg.AcceptContentType\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'xuzhenglun' said:\n---\n/sig api-machinery\r\n\r\ncc @p0lyn0mial\n---\n\nUser 'xuzhenglun' said:\n---\nI think this issus has some similarities with https://github.com/kubernetes/kubernetes/pull/126191. Both of these seem to be caused by differences in metadata between the List and the Singleton object.\n---\n\nUser 'xuzhenglun' said:\n---\nfriendly ping @p0lyn0mial \r\n\r\nIs this expected behavior on the apiserver side, leaving it up to the client to handle it appropriately?  WDYT @deads2k\n---\n\nUser 'p0lyn0mial' said:\n---\nIt looks like the standard list request decoded the response successfully with the `Accept content-type` set as `Table`. Right? I assume that the decoded object was `UnstructuredList`.\r\n\r\nI think that the `WatchList` should also be able to decode successfully into `UnstructuredList` regardless of the requested encoding. If the requested encoding is unsupported, then the `WatchList` request could/should return an error.\n---\n\nUser 'xuzhenglun' said:\n---\nThank @p0lyn0mial for reply.\r\n\r\nYes, the standard list returns `UnstructuredList` sucessfully when response is `Table`, and no error happened. Alrougth `Table` is not a list,  but `Unstructured` can do `ToList` method successfully, and the output is `&UnstructuredList{Object: obj.Object}` (field Items is nil in this case). However, `ListWatch` return an error for now, and the behavior of `List` and `WatchList` are different.\r\n\r\nI'm agree with you that `List` and `WatchList` should only support object can be decoded into `UnstructuredList`, but I'm not sure that if this difference can be considered as breaking compatibility?\r\n\r\nBeside of the encoding issue, the thing what I'm really worried about is the initial-end bookmark event is strange when response is a table: `metadata.annotation` is not available in the root of the object, but it's placed in `rows[0].object.metadata.annotation`. And the worst thing is that even `rows[0].object.metadata.annotation` can be unavailiable if `includeObject=None` param is set, since `rows[0].object` will be `nil`.\r\n\r\nI'm trying to add `WatchList` support for `kubectl`, so I was wondering if I could reuse the `WatchList` code in the `rest client` instead of implementing one myself. The `includeObject=None` parameter doesn't matter to me for what I'm doing, but I'm not sure if other people use it. So I wanted to ask your opinion on this.\n---\n\nUser 'p0lyn0mial' said:\n---\nSince we are trying to replace `List` with `WatchList`, I think the new method should also be able to decode data, even if the response is a Table. \r\n\r\nI had to roll back the graduation of the feature, so it is turned off by default. Before enabling the feature we should consider fixing this issue.\r\n\r\nI don't think that placing the `initial-end bookmark annotation` on `rows[0].object` is specific to the `WatchList`. I believe this is where objects are generally placed when a user requests a Table. For example:\r\n\r\n```\r\n{\r\n  \"type\": \"ADDED\",\r\n  \"object\": {\r\n    \"kind\": \"Table\",\r\n    \"apiVersion\": \"meta.k8s.io/v1\",\r\n    \"metadata\": {\r\n      \"resourceVersion\": \"774\"\r\n    },\r\n    \"columnDefinitions\": null,\r\n    \"rows\": [\r\n      {\r\n        \"cells\": [\r\n          \"todo\",\r\n          1,\r\n          \"22s\"\r\n        ],\r\n        \"object\": {\r\n          \"kind\": \"PartialObjectMetadata\",\r\n          \"apiVersion\": \"meta.k8s.io/v1\",\r\n          \"metadata\": {\r\n            \"name\": \"todo\",\r\n            \"namespace\": \"example\",\r\n            \"uid\": \"2ea9e613-6487-406e-974a-42c6c7cf9a94\",\r\n            \"resourceVersion\": \"774\",\r\n            \"creationTimestamp\": \"2024-07-22T12:40:32Z\",\r\n            \"managedFields\": [\r\n              {\r\n                \"manager\": \"kubectl-create\",\r\n                \"operation\": \"Update\",\r\n                \"apiVersion\": \"v1\",\r\n                \"time\": \"2024-07-22T12:40:32Z\",\r\n                \"fieldsType\": \"FieldsV1\",\r\n                \"fieldsV1\": {\r\n                  \"f:data\": {\r\n                    \".\": {},\r\n                    \"f:foo\": {}\r\n                  }\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\n---\n\nUser 'xuzhenglun' said:\n---\n@p0lyn0mial \r\n\r\n> Since we are trying to replace List with WatchList, I think the new method should also be able to decode data, even if the response is a Table.\r\n\r\nSo, we will modify the implementation of `WatchList` method in `rest` package so that we can process Table normally, right?\r\n\r\n> I don't think that placing the initial-end bookmark annotation on rows[0].object is specific to the WatchList. I believe this is where objects are generally placed when a user requests a Table. \r\n\r\nYes, `initial-end bookmark annotation` is always placed in `rows[0].object` when requesting a Table. But there are two small problems when we are using `WatchList` with this behavior:\r\n\r\n1. `Table` need a special treatment likes below, and it looks weird. It's just a nit, no big deal.\r\n```patch\r\n+func metaAccessor(object runtime.Object) (metav1.Object, error) {\r\n+\ttable, ok := object.(*metav1.Table)\r\n+\tif ok {\r\n+\t\tif len(table.Rows) == 0 || len(table.Rows[0].Object.Raw) == 0 {\r\n+\t\t\treturn nil, fmt.Errorf(\"unexcept empty Table\")\r\n+\t\t}\r\n+\r\n+\t\tconverted, err := runtime.Decode(unstructured.UnstructuredJSONScheme, table.Rows[0].Object.Raw)\r\n+\t\tif err != nil {\r\n+\t\t\treturn nil, err\r\n+\t\t}\r\n+\r\n+\t\treturn meta.Accessor(converted)\r\n+\t}\r\n+\r\n+\treturn meta.Accessor(object)\r\n+}\r\n\r\n // handleWatchList holds the actual logic for easier unit testing.\r\n // Note that this function will close the passed watch.\r\n func (r *Request) handleWatchList(ctx context.Context, w watch.Interface) WatchListResult {\r\n \t\t\tif event.Type == watch.Error {\r\n \t\t\t\treturn WatchListResult{err: errors.FromObject(event.Object)}\r\n \t\t\t}\r\n-\t\t\tmeta, err := meta.Accessor(event.Object)\r\n+\t\t\tmeta, err := metaAccessor(event.Object)\r\n \t\t\tif err != nil {\r\n \t\t\t\treturn WatchListResult{err: fmt.Errorf(\"failed to parse watch event: %#v\", event)}\r\n \t\t\t}\r\n\r\n```\r\n\r\n2. If the WatchList request with `includeObject=None` params, the `initial-end bookmark annotation` will never been seen by client. I'm not sure if we can accept that it's just not supported. For example, the bookmark event will looks like:\r\n```json\r\n{\r\n  \"type\": \"BOOKMARK\",\r\n  \"object\": {\r\n    \"kind\": \"Table\",\r\n    \"apiVersion\": \"meta.k8s.io/v1\",\r\n    \"metadata\": {\r\n      \"resourceVersion\": \"679930\"\r\n    },\r\n    \"columnDefinitions\": null,\r\n    \"rows\": [\r\n      {\r\n        \"cells\": [\r\n          \"\",\r\n          \"0/0\",\r\n          \"\",\r\n          \"0\",\r\n          \"<unknown>\",\r\n          \"<none>\",\r\n          \"<none>\",\r\n          \"<none>\",\r\n          \"<none>\"\r\n        ],\r\n        \"object\": null\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\n---\n\nUser 'xuzhenglun' said:\n---\n@p0lyn0mial \r\n\r\nI just submited a PR  https://github.com/kubernetes/kubernetes/pull/126363 which trying to address this issue. Would you please take a look? I'm not sure is it good enough since I'm really new to this part of code.\n---\n\nUser 'cici37' said:\n---\n/triage accepted\n---\n\nUser 'wojtek-t' said:\n---\n/cc\n---\n\nUser 'p0lyn0mial' said:\n---\n@xuzhenglun FYI We have decided to temporarily return a 406 for watchlist requests that require `application/json;as=Table` (https://github.com/kubernetes/kubernetes/pull/126996). We will resolve this issue before promoting the watchlist feature to GA.\n---\n\nUser 'xuzhenglun' said:\n---\n> @xuzhenglun FYI We have decided to temporarily return a 406 for watchlist requests that require `application/json;as=Table` (#126996). We will resolve this issue before promoting the watchlist feature to GA.\r\n\r\nWell, does any thought about how to deal with that in future or any detail about where the difficulty is? I\u2019m glad to devoted myself to that to accelerate It if I may.\n---\n\nUser 'p0lyn0mial' said:\n---\n> Well, does any thought about how to deal with that in future or any detail about where the difficulty is? I\u2019m glad to devoted myself to that to accelerate It if I may.\r\n\r\nSure, feel free to work on it. Please take a look at https://github.com/kubernetes/kubernetes/pull/126996. We just don't want to block the WatchList on this issue. We would like to promote it to Beta this release.\n---\n\nUser 'xuzhenglun' said:\n---\n> #126996\r\n\r\nThanks, I will take a look that and try to sort it out.\n---\n\nUser 'p0lyn0mial' said:\n---\n> Thanks, I will take a look that and try to sort it out.\r\n\r\nyw, also please take a look at \r\nhttps://github.com/kubernetes/kubernetes/pull/127587/files#diff-12e0758457373aa860bb0baae0878a99c107840d25fcf356d126d4b3d1d15663R581. Especially the place that transforms the object to `PartialObjectMetadataList`. Maybe we need something similar. Try experimenting.\n---\n\nUser 'k8s-triage-robot' said:\n---\nThis issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted\n---\n\nUser 'wojtek-t' said:\n---\nThis has been fixed with https://github.com/kubernetes/kubernetes/pull/132817\n---",
    "question": "I'm getting an error: WatchList failed when Accept content-type is Table. Is there a known workaround?",
    "ideal_answer": "cc @p0lyn0mial\n\nI think this issus has some similarities with https://github.com/kubernetes/kubernetes/pull/126191. Both of these seem to be caused by differences in metadata between the List and the Singleton object.\n\nfriendly ping @p0lyn0mial \r\n\nIs this expected behavior on the apiserver side, leaving it up to the client to handle it appropriately?  WDYT @deads2k\n\nIt looks like the standard list request decoded the response successfully with the `Accept content-type` set as `Table`. Right? I assume that the decoded object was `UnstructuredList`.\r\n\nI think that the `WatchList` should also be able to decode successfully into `UnstructuredList` regardless of the requested encoding. If the requested encoding is unsupported, then the `WatchList` request could/should return an error.\n\nThank @p0lyn0mial for reply.\r\n\nYes, the standard list returns `UnstructuredList` sucessfully when response is `Table`, and no error happened. Alrougth `Table` is not a list,  but `Unstructured` can do `ToList` method successfully, and the output is `&UnstructuredList{Object: obj.Object}` (field Items is nil in this case). However, `ListWatch` return an error for now, and the behavior of `List` and `WatchList` are different.\r\n\nI'm agree with you that `List` and `WatchList` should only support object can be decoded into `UnstructuredList`, but I'm not sure that if this difference can be considered as breaking compatibility?\r\n\nBeside of the encoding issue, the thing what I'm really worried about is the initial-end bookmark event is strange when response is a table: `metadata.annotation` is not available in the root of the object, but it's placed in `rows[0].object.metadata.annotation`. And the worst thing is that even `rows[0].object.metadata.annotation` can be unavailiable if `includeObject=None` param is set, since `rows[0].object` will be `nil`.\r\n\nI'm trying to add `WatchList` support for `kubectl`, so I was wondering if I could reuse the `WatchList` code in the `rest client` instead of implementing one myself. The `includeObject=None` parameter doesn't matter to me for what I'm doing, but I'm not sure if other people use it. So I wanted to ask your opinion on this.\n\nSince we are trying to replace `List` with `WatchList`, I think the new method should also be able to decode data, even if the response is a Table. \r\n\nI had to roll back the graduation of the feature, so it is turned off by default. Before enabling the feature we should consider fixing this issue.\r\n\nI don't think that placing the `initial-end bookmark annotation` on `rows[0].object` is specific to the `WatchList`. I believe this is where objects are generally placed when a user requests a Table. For example:\r\n\n```\r\n{\r\n  \"type\": \"ADDED\",\r\n  \"object\": {\r\n    \"kind\": \"Table\",\r\n    \"apiVersion\": \"meta.k8s.io/v1\",\r\n    \"metadata\": {\r\n      \"resourceVersion\": \"774\"\r\n    },\r\n    \"columnDefinitions\": null,\r\n    \"rows\": [\r\n      {\r\n        \"cells\": [\r\n          \"todo\",\r\n          1,\r\n          \"22s\"\r\n        ],\r\n        \"object\": {\r\n          \"kind\": \"PartialObjectMetadata\",\r\n          \"apiVersion\": \"meta.k8s.io/v1\",\r\n          \"metadata\": {\r\n            \"name\": \"todo\",\r\n            \"namespace\": \"example\",\r\n            \"uid\": \"2ea9e613-6487-406e-974a-42c6c7cf9a94\",\r\n            \"resourceVersion\": \"774\",\r\n            \"creationTimestamp\": \"2024-07-22T12:40:32Z\",\r\n            \"managedFields\": [\r\n              {\r\n                \"manager\": \"kubectl-create\",\r\n                \"operation\": \"Update\",\r\n                \"apiVersion\": \"v1\",\r\n                \"time\": \"2024-07-22T12:40:32Z\",\r\n                \"fieldsType\": \"FieldsV1\",\r\n                \"fieldsV1\": {\r\n                  \"f:data\": {\r\n                    \".\": {},\r\n                    \"f:foo\": {}\r\n                  }\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\n\n@p0lyn0mial \r\n\n> Since we are trying to replace List with WatchList, I think the new method should also be able to decode data, even if the response is a Table.\r\n\nSo, we will modify the implementation of `WatchList` method in `rest` package so that we can process Table normally, right?\r\n\n> I don't think that placing the initial-end bookmark annotation on rows[0].object is specific to the WatchList. I believe this is where objects are generally placed when a user requests a Table. \r\n\nYes, `initial-end bookmark annotation` is always placed in `rows[0].object` when requesting a Table. But there are two small problems when we are using `WatchList` with this behavior:\r\n\n1. `Table` need a special treatment likes below, and it looks weird. It's just a nit, no big deal.\r\n```patch\r\n+func metaAccessor(object runtime.Object) (metav1.Object, error) {\r\n+\ttable, ok := object.(*metav1.Table)\r\n+\tif ok {\r\n+\t\tif len(table.Rows) == 0 || len(table.Rows[0].Object.Raw) == 0 {\r\n+\t\t\treturn nil, fmt.Errorf(\"unexcept empty Table\")\r\n+\t\t}\r\n+\r\n+\t\tconverted, err := runtime.Decode(unstructured.UnstructuredJSONScheme, table.Rows[0].Object.Raw)\r\n+\t\tif err != nil {\r\n+\t\t\treturn nil, err\r\n+\t\t}\r\n+\r\n+\t\treturn meta.Accessor(converted)\r\n+\t}\r\n+\r\n+\treturn meta.Accessor(object)\r\n+}\r\n\n // handleWatchList holds the actual logic for easier unit testing.\r\n // Note that this function will close the passed watch.\r\n func (r *Request) handleWatchList(ctx context.Context, w watch.Interface) WatchListResult {\r\n \t\t\tif event.Type == watch.Error {\r\n \t\t\t\treturn WatchListResult{err: errors.FromObject(event.Object)}\r\n \t\t\t}\r\n-\t\t\tmeta, err := meta.Accessor(event.Object)\r\n+\t\t\tmeta, err := metaAccessor(event.Object)\r\n \t\t\tif err != nil {\r\n \t\t\t\treturn WatchListResult{err: fmt.Errorf(\"failed to parse watch event: %#v\", event)}\r\n \t\t\t}\r\n\n```\r\n\n2. If the WatchList request with `includeObject=None` params, the `initial-end bookmark annotation` will never been seen by client. I'm not sure if we can accept that it's just not supported. For example, the bookmark event will looks like:\r\n```json\r\n{\r\n  \"type\": \"BOOKMARK\",\r\n  \"object\": {\r\n    \"kind\": \"Table\",\r\n    \"apiVersion\": \"meta.k8s.io/v1\",\r\n    \"metadata\": {\r\n      \"resourceVersion\": \"679930\"\r\n    },\r\n    \"columnDefinitions\": null,\r\n    \"rows\": [\r\n      {\r\n        \"cells\": [\r\n          \"\",\r\n          \"0/0\",\r\n          \"\",\r\n          \"0\",\r\n          \"<unknown>\",\r\n          \"<none>\",\r\n          \"<none>\",\r\n          \"<none>\",\r\n          \"<none>\"\r\n        ],\r\n        \"object\": null\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\n\n@p0lyn0mial \r\n\nI just submited a PR  https://github.com/kubernetes/kubernetes/pull/126363 which trying to address this issue. Would you please take a look? I'm not sure is it good enough since I'm really new to this part of code.\n\n/triage accepted\n\n/cc\n\n@xuzhenglun FYI We have decided to temporarily return a 406 for watchlist requests that require `application/json;as=Table` (https://github.com/kubernetes/kubernetes/pull/126996). We will resolve this issue before promoting the watchlist feature to GA.\n\n> @xuzhenglun FYI We have decided to temporarily return a 406 for watchlist requests that require `application/json;as=Table` (#126996). We will resolve this issue before promoting the watchlist feature to GA.\r\n\nWell, does any thought about how to deal with that in future or any detail about where the difficulty is? I\u2019m glad to devoted myself to that to accelerate It if I may.\n\n> Well, does any thought about how to deal with that in future or any detail about where the difficulty is? I\u2019m glad to devoted myself to that to accelerate It if I may.\r\n\nSure, feel free to work on it. Please take a look at https://github.com/kubernetes/kubernetes/pull/126996. We just don't want to block the WatchList on this issue. We would like to promote it to Beta this release.\n\n> #126996\r\n\nThanks, I will take a look that and try to sort it out.\n\n> Thanks, I will take a look that and try to sort it out.\r\n\nyw, also please take a look at \r\nhttps://github.com/kubernetes/kubernetes/pull/127587/files#diff-12e0758457373aa860bb0baae0878a99c107840d25fcf356d126d4b3d1d15663R581. Especially the place that transforms the object to `PartialObjectMetadataList`. Maybe we need something similar. Try experimenting.\n\nThis issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted\n\nThis has been fixed with https://github.com/kubernetes/kubernetes/pull/132817"
  },
  {
    "id": "gen_nat_014",
    "category": "troubleshooting",
    "source_file": "microsoft_vscode_issue.json",
    "context": "Repository: microsoft/vscode\nTitle: Unable to switch branches(command \n\nA user reported the following issue titled 'Unable to switch branches(command '__vscb5ec3d9f-5a10-4da1-b8a6-7ff01a1306fe' not found)' in the 'microsoft/vscode' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\nType: <b>Bug</b>\n\nThere are no detailed reproduction steps. Click the branch name in the lower left corner and want to switch to another branch. You may be prompted with `command '__vscb5ec3d9f-5a10-4da1-b8a6-7ff01a1306fe' not found`\n\nhttps://github.com/user-attachments/assets/b0a25dff-a30b-4cb7-927e-7b0cbde7104d\n\nVS Code version: Code 1.103.1 (Universal) (360a4e4fd251bfce169a4ddf857c7d25d1ad40da, 2025-08-12T16:25:40.542Z)\nOS version: Darwin arm64 24.6.0\nModes:\n\n<details>\n<summary>System Info</summary>\n\n|Item|Value|\n|---|---|\n|CPUs|Apple M1 Max (10 x 2400)|\n|GPU Status|2d_canvas: enabled<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: enabled_on<br>trees_in_viz: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|\n|Load (avg)|7, 7, 5|\n|Memory (System)|32.00GB (1.07GB free)|\n|Process Argv|--crash-reporter-id 2a648fcb-aa9e-4b05-a81e-c6575e1e747e|\n|Screen Reader|no|\n|VM|0%|\n</details><details><summary>Extensions (46)</summary>\n\nExtension|Author (truncated)|Version\n---|---|---\nvite|ant|0.2.5\nastro-vscode|ast|2.15.4\nvscode-tailwindcss|bra|0.14.26\nnpm-intellisense|chr|1.4.5\npath-intellisense|chr|2.10.0\ndart-code|Dar|3.116.0\nflutter|Dar|3.116.0\nvscode-markdownlint|Dav|0.60.0\nvscode-eslint|dba|3.0.16\njavascript-ejs-support|Dig|1.3.3\nxml|Dot|2.5.1\ngitlens|eam|17.3.4\nEditorConfig|Edi|0.17.4\nprettier-vscode|esb|11.0.0\ngit-project-manager|fel|1.8.2\ncode-runner|for|0.12.2\ncopilot|Git|1.350.0\ncopilot-chat|Git|0.30.1\nvscode-github-actions|git|0.27.2\ngo|gol|0.48.0\ntodo-tree|Gru|0.0.226\nvscode-codeowners|jas|1.1.1\nrainbow-csv|mec|3.20.0\nvscode-language-pack-zh-hans|MS-|1.103.2025081309\ndebugpy|ms-|2025.10.0\npython|ms-|2025.12.0\nvscode-pylance|ms-|2025.7.1\nvscode-python-envs|ms-|1.2.0\nremote-containers|ms-|0.422.1\ncmake-tools|ms-|1.21.36\ncpptools|ms-|1.26.3\ncpptools-extension-pack|ms-|1.3.1\nmakefile-tools|ms-|0.12.17\nvsliveshare|ms-|1.0.5959\nvscode-react-native|msj|1.13.0\nbcompare-vscode|Sco|1.0.7\nvscode-stylelint|sty|1.5.3\neven-better-toml|tam|0.21.2\nvscode-mdx|uni|1.8.16\nintellicode-api-usage-examples|Vis|0.2.9\nvscodeintellicode|Vis|1.3.2\nvscode-icons|vsc|12.14.0\nvolar|Vue|3.0.6\nphp-debug|xde|1.37.0\nphp-pack|xde|1.0.3\nphp-intellisense|zob|1.3.3\n\n(1 theme extensions excluded)\n\n</details><details>\n<summary>A/B Experiments</summary>\n\n```\nvsliv368:30146709\nbinariesv615:30325510\nnativeloc1:31344060\ndwcopilot:31170013\ndwoutputs:31242946\ncopilot_t_ci:31333650\ne5gg6876:31282496\npythoneinst12:31285622\n996jf627:31283433\npythonrdcb7:31342333\nusemplatestapi:31297334\n747dc170:31275177\n6518g693:31334701\naj953862:31281341\n9d2cg352:31346308\nnesew2to5:31336538\nagentclaude:31350858\nnes-set-on:31351930\n6abeh943:31336334\nenvsactivate1:31353494\n0927b901:31350571\nf76d9909:31348711\ncustommodelcf:31371783\n0ej4-default:31346761\n456\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'yoyo837' said:\n---\nAfter doing `Developer: Reload Window` fixes it, but it still appears again with unclear timing.\n---\n\nUser 'lszomoru' said:\n---\n@yoyo837, are you able to reproduce this issue with the latest VS Code Insiders release (1.104)? Thanks!\n---\n\nUser 'yoyo837' said:\n---\n> [@yoyo837](https://github.com/yoyo837), are you able to reproduce this issue with the latest VS Code Insiders release (1.104)? Thanks!\n\nSure, let me try it.\n---\n\nUser 'yoyo837' said:\n---\nThis issue still exists in Insider builds.\n\n<img width=\"260\" height=\"371\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1804d03b-c3c9-43f0-a202-8baa8733bd62\" />\n\n<img width=\"1915\" height=\"1074\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/74ee8de4-e841-421c-8c30-a591aa74f67e\" />\n---\n\nUser 'lszomoru' said:\n---\n@yoyo837, are you able to come up with the set of steps that you perform after launching VS Code to repro? Thanks!\n---\n\nUser 'yoyo837' said:\n---\n> [@yoyo837](https://github.com/yoyo837), are you able to come up with the set of steps that you perform after launching VS Code to repro? Thanks!\n\nI haven't found the steps to reproduce the problem yet, it seems to happen suddenly each time. I will continue to observe and try to find a pattern.\n---\n\nUser 'lszomoru' said:\n---\n@yoyo837, does this happen when you change branches or maybe you update an extension and restart the extension host?\n---\n\nUser 'yoyo837' said:\n---\n> [@yoyo837](https://github.com/yoyo837), does this happen when you change branches or maybe you update an extension and restart the extension host?\n\nYes, after I waited many days for a new version of any one plugin, it happened after updating the extension and restarting the extension host.\n---\n\nUser 'lszomoru' said:\n---\n@yoyo837, what version were you using then you reproduced the issue? \nCan you try to reproduce the issue with the latest VS Code Insiders release?\n---\n\nUser 'yoyo837' said:\n---\n> what version were you using then you reproduced the issue? \n\nversion: 1.103.2 (Universal)\ncommit: 6f17636121051a53c88d3e605c491d22af2ba755\n\n> Can you try to reproduce the issue with the latest VS Code Insiders release?\n\nOk, I'll try it again with the latest VS Code Insiders release.\n---\n\nUser 'lszomoru' said:\n---\nA fix for this issue is currently available in the latest VS Code Insiders release.\n---\n\nUser 'yoyo837' said:\n---\n> A fix for this issue is currently available in the latest VS Code Insiders release.\n\nWould you mind linking to the PR or commit that fixed this?\n---\n\nUser 'lszomoru' said:\n---\nSure - https://github.com/microsoft/vscode/pull/262949\n---\n\nUser 'yoyo837' said:\n---\nI can confirm this is fixed for me on v1.104.0 insider release 4a31639f5f6e1ba6f2521ca73430f3d59eb97524.\n\nAnd this issue is duplicate of https://github.com/microsoft/vscode/issues/259786\n---",
    "question": "What is the resolution for 'Unable to switch branches(command'?",
    "ideal_answer": "After doing `Developer: Reload Window` fixes it, but it still appears again with unclear timing.\n\n@yoyo837, are you able to reproduce this issue with the latest VS Code Insiders release (1.104)? Thanks!\n\n> [@yoyo837](https://github.com/yoyo837), are you able to reproduce this issue with the latest VS Code Insiders release (1.104)? Thanks!\n\nSure, let me try it.\n\nThis issue still exists in Insider builds.\n\n<img width=\"260\" height=\"371\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1804d03b-c3c9-43f0-a202-8baa8733bd62\" />\n\n<img width=\"1915\" height=\"1074\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/74ee8de4-e841-421c-8c30-a591aa74f67e\" />\n\n@yoyo837, are you able to come up with the set of steps that you perform after launching VS Code to repro? Thanks!\n\n> [@yoyo837](https://github.com/yoyo837), are you able to come up with the set of steps that you perform after launching VS Code to repro? Thanks!\n\nI haven't found the steps to reproduce the problem yet, it seems to happen suddenly each time. I will continue to observe and try to find a pattern.\n\n@yoyo837, does this happen when you change branches or maybe you update an extension and restart the extension host?\n\n> [@yoyo837](https://github.com/yoyo837), does this happen when you change branches or maybe you update an extension and restart the extension host?\n\nYes, after I waited many days for a new version of any one plugin, it happened after updating the extension and restarting the extension host.\n\n@yoyo837, what version were you using then you reproduced the issue? \nCan you try to reproduce the issue with the latest VS Code Insiders release?\n\n> what version were you using then you reproduced the issue? \n\nversion: 1.103.2 (Universal)\ncommit: 6f17636121051a53c88d3e605c491d22af2ba755\n\n> Can you try to reproduce the issue with the latest VS Code Insiders release?\n\nOk, I'll try it again with the latest VS Code Insiders release.\n\nA fix for this issue is currently available in the latest VS Code Insiders release.\n\n> A fix for this issue is currently available in the latest VS Code Insiders release.\n\nWould you mind linking to the PR or commit that fixed this?\n\nSure - https://github.com/microsoft/vscode/pull/262949\n\nI can confirm this is fixed for me on v1.104.0 insider release 4a31639f5f6e1ba6f2521ca73430f3d59eb97524.\n\nAnd this issue is duplicate of https://github.com/microsoft/vscode/issues/259786"
  },
  {
    "id": "gen_nat_015",
    "category": "general",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: Should pause pod evictions when zone is in Full Zonal Disruption\n\nA user reported the following issue titled 'Should pause pod evictions when zone is in Full Zonal Disruption' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nWhen all nodes in an AZ goes down then the zone is in Full Zonal Disruption during which we set the eviction ratelimiter to `HealthyQPSFunc`. Where as if a zone is experiencing PartialZonalDisruption then we stop all pod evictions by setting the rate limiter to `ReducedQPSFunc`.\n\nAll workloads in a zone gets evicted during FullZonalDisruption. I understand the idea to evict pods in a zone where we are not able to allocate/provision more capacity since its down. \n\n\n\n### What did you expect to happen?\n\nWe expect to pause all pod evictions during FullZonalDisruption and set the rate limiter to `ReducedQPSFunc`.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nCreate a kubernetes cluster and mimic the scenario where all nodes in an AZ goes down causing the zone to be in FullZonalDisruption, KCM will start evicting pods.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\nAll Kubernetes version\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\nAll Cloud Providers\n\n</details>\n\n\n### OS version\n\n<details>\n\nNA\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'k8s-ci-robot' said:\n---\nThere are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'k8s-ci-robot' said:\n---\nThis issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-ci-robot' said:\n---\n@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131314#issuecomment-3289985740):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---",
    "question": "In kubernetes/kubernetes, Should pause pod evictions when zone is in Full Zonal Disruption. Any ideas why?",
    "ideal_answer": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\"."
  },
  {
    "id": "gen_nat_016",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [playground]: Syntax error crashes playground\n\nA user reported the following issue titled '[playground]: Syntax error crashes playground' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEAKgmDgMIQC2ADoQpjgBR0wR1gCURwAOsSJxCFIhE5giAXiLtJAOkGCiRGAhyxiLFaqIAeAHzAJXAL76A9Id3cA3ILMgANCBGY0eAOYoQeehAwOEQ4AJ50CHxEAAoANlBeeJgA8nT4okRmRGgcNEQA5ABGAIaFCLEAtHTxiZgV6sW4FSL0eLEIMJYAJngU+Q6YgiwCQpaWLXRtxemYALIQXQjIRPwgxbGxq45EYNO9nuQxNUmpMzx2LuAAFhAA7gCSzB2Y62AoaK8IZkA\n\n### Repro steps\n\nCreate a syntax error in the playground.\n\nFor example, paste the following code into the playground:\n\n```tsx\nfunction TestComponent(props) {\n  const oops = props.\n\n  return (\n    <>{oops}</>\n  );\n}\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\nlatest\n\n### What version of React Compiler are you using?\n\nlatest\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'yorgie7' said:\n---\nWhen I click on link above, got this result.\n\n<img width=\"1919\" height=\"790\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/034150d3-b1be-412d-8813-4d7dc066b629\" />\n---\n\nUser 'gkiely' said:\n---\nSome other examples, it seems like these are TODO's that are crashing the playground.\n\n```js\n// Todo: (BuildHIR::lowerExpression) Handle get functions in ObjectExpression\nconst C = () => {\n  const t = {\n    get val(){}\n  }\n  return <></>;\n}\n```\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhHCA7MAXABAYVwF5cAKASmID5dgAdDXXdLPPE+xp3AcwTwBuAQwA2FYAF8GTKVxj9YjADxUlAeioBuBrJAAaNJgBmASx4oQJgLYAHCDDYBPGwlq4ACiKg8TGAPI22CaYYLgSuEYwEFa4AOQARkLxCCIAtDZePhip8kJw2KnotiYiCDBqACYmOLHaGAyknExqakU2JUJBmACyEBUIyLh0IKIiwzq4YJ3Vpgihnt6+AV1Y5Jr64AAWEADuAJIY2GUYomAoRqcIEkA\n\n```js\n// Todo: Unexpected terminal kind `ternary` for optional test block\nconst Component = () => {\n  useEffect(() => {}, []);\n  const params = join({\n    a: true ? '' : undefined,\n    b: true ? '' : undefined,\n  })?.replaceAll('', '');\n}\n```\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhHCA7MAXABAYQgFsAHTBDPAXlwAoBKXKgPl2AB0NdcowEBRAGaCEcbLQZNWwAL4AaXAG0AuvQDcnbuix4SAQxh6iYJrgBWEAJYZaHLt1x7kubDCgJcAflwByH7mcoDAATBEFrBGC5TQcAI2dXdy9ff0CQsIiomJl6TwA6GAQSABs9RABBYuLaPwU-dU4ZEDk0THCAcxQQS1IIGDxsAE8SD2BcAAViqHbrAHkSbEtMExlcQRhiX1i9WIRigFoS6et9wrLsffRSS2KEGAB6YMscHw0MTlsY+-urkhu9RaYACyEFCznYID0VQhjVwYABz3CCBMk2OGHmgKw6ma4AAFhAAO4ASUodwwULAKEEFIQMiAA\n---",
    "question": "What is the resolution for ': Syntax error crashes playground'?",
    "ideal_answer": "When I click on link above, got this result.\n\n<img width=\"1919\" height=\"790\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/034150d3-b1be-412d-8813-4d7dc066b629\" />\n\nSome other examples, it seems like these are TODO's that are crashing the playground.\n\n```js\n// Todo: (BuildHIR::lowerExpression) Handle get functions in ObjectExpression\nconst C = () => {\n  const t = {\n    get val(){}\n  }\n  return <></>;\n}\n```\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhHCA7MAXABAYVwF5cAKASmID5dgAdDXXdLPPE+xp3AcwTwBuAQwA2FYAF8GTKVxj9YjADxUlAeioBuBrJAAaNJgBmASx4oQJgLYAHCDDYBPGwlq4ACiKg8TGAPI22CaYYLgSuEYwEFa4AOQARkLxCCIAtDZePhip8kJw2KnotiYiCDBqACYmOLHaGAyknExqakU2JUJBmACyEBUIyLh0IKIiwzq4YJ3Vpgihnt6+AV1Y5Jr64AAWEADuAJIY2GUYomAoRqcIEkA\n\n```js\n// Todo: Unexpected terminal kind `ternary` for optional test block\nconst Component = () => {\n  useEffect(() => {}, []);\n  const params = join({\n    a: true ? '' : undefined,\n    b: true ? '' : undefined,\n  })?.replaceAll('', '');\n}\n```\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhHCA7MAXABAYQgFsAHTBDPAXlwAoBKXKgPl2AB0NdcowEBRAGaCEcbLQZNWwAL4AaXAG0AuvQDcnbuix4SAQxh6iYJrgBWEAJYZaHLt1x7kubDCgJcAflwByH7mcoDAATBEFrBGC5TQcAI2dXdy9ff0CQsIiomJl6TwA6GAQSABs9RABBYuLaPwU-dU4ZEDk0THCAcxQQS1IIGDxsAE8SD2BcAAViqHbrAHkSbEtMExlcQRhiX1i9WIRigFoS6et9wrLsffRSS2KEGAB6YMscHw0MTlsY+-urkhu9RaYACyEFCznYID0VQhjVwYABz3CCBMk2OGHmgKw6ma4AAFhAAO4ASUodwwULAKEEFIQMiAA"
  },
  {
    "id": "gen_nat_017",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: Patching doesn\n\nA user reported the following issue titled 'Patching doesn't work with pods with annotation based app armor configuration in Kubernets version 1.32' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nPatching doesn't work with pods with ONLY annotation based app armor configuration in Kubernets version 1.32\n\n### What did you expect to happen?\n\nI expect the patch to go through\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nThis could be an issue before 1.32, but I only tested in 1.32.\n\nIf we have a pod that is still using the annotation based app armor profile (without the `appArmorProfile` field under `securityContext`), for example\n```\n container.apparmor.security.beta.kubernetes.io/container: localhost/<profile-name>\n```\n\nand we run a simple kubectl patch command\n```\nkubectl patch pod test-pod \\\n-p '[{\"op\": \"remove\", \"path\": \"/metadata/labels/label\"}]' \\\n--type=json\n```\n\nThe request will fail with message (only showing the relevant part)\n```\n-\u00a0\t\t\t\tAppArmorProfile: &core.AppArmorProfile{Type: \"Localhost\", LocalhostProfile: &\"<profile-name>\"},\n+\u00a0\t\t\t\tAppArmorProfile: nil,\n\u00a0\u00a0\t\t\t},\n```\nas if my patch is trying to remove the apparmor profile.\n\n### Anything else we need to know?\n\nI have tried patching the pod / deployment to onboard with the new apparmor profile. The edit went through but the resource didn't actually get updated with the appArmorProfile\n\nI have raised another issue for this problem: https://github.com/kubernetes/kubectl/issues/1764\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.33.2\nKustomize Version: v5.6.0\nServer Version: v1.32.5-eks-5d4a308\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nEKS\n</details>\n\n\n### OS version\n\n<details>\n\n```console\nHardware:\n\n    Hardware Overview:\n\n      Model Name: MacBook Pro\n      Model Identifier: Mac16,7\n      Chip: Apple M4 Pro\n      Total Number of Cores: 14 (10 performance and 4 efficiency)\n      Memory: 48 GB\n      System Firmware Version: 11881.121.1\n      OS Loader Version: 11881.121.1\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'k8s-ci-robot' said:\n---\nThere are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'k8s-ci-robot' said:\n---\nThis issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'rickypeng99' said:\n---\nI suspect that this behavior has something to do the mechanism where we are syncing the deprecated annotation to the `appArmorProfile` field under `securityContext`, probably it's not doing this when we are doing patches?\n---\n\nUser 'itzPranshul' said:\n---\nI believe the problem comes from how patching works with the new `securityContext.appArmorProfile` field  introduced in recent Kubernetes versions. What seems to be happening in 1.32 is that,  pod still uses the old annotation form for AppArmor so when we run `kubectl patch` to change something else, Kubernetes rebuilds the Pod spec internally using the new `securityContext.appArmorProfile` field. There is no step to copy the AppArmor value from the old annotation into the new field before calculating the diff, resulting in the Pod's internal version appearing to have no AppArmor profile. The diff logic treats this as a request to remove it.  In short, the backwards-compatibility logic that maps annotations to  `securityContext.appArmorProfile` is missing in the patch conversion flow\n---\n\nUser 'Goend' said:\n---\nI can't reproduce it.\n```\nroot@proxy-pass:~# kubectl  get pod test-fake -o yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n  creationTimestamp: \"2025-08-08T11:52:37Z\"\n  labels:\n    xx: xx\n  name: test-fake\n  namespace: default\n  resourceVersion: \"8145\"\n  uid: 9ca1df6d-8d8f-4d41-8e0b-7dc17c2242b1\nspec:\n  containers:\n  - image: docker.io/library/nginx:latest\n    imagePullPolicy: IfNotPresent\n    name: nginx-fake\n    resources: {}\n    securityContext:\n      appArmorProfile:\n        localhostProfile: cri-containerd.apparmor.d\n        type: Localhost\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-cvvvw\n      readOnly: true\n  dnsPolicy: ClusterFirst\n  enableServiceLinks: true\n  nodeName: proxy-pass\n  preemptionPolicy: PreemptLowerPriority\n  priority: 0\n  restartPolicy: Always\n  schedulerName: default-scheduler\n  securityContext: {}\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n  tolerations:\n  - effect: NoExecute\n    key: node.kubernetes.io/not-ready\n    operator: Exists\n    tolerationSeconds: 300\n  - effect: NoExecute\n    key: node.kubernetes.io/unreachable\n    operator: Exists\n    tolerationSeconds: 300\n  volumes:\n  - name: kube-api-access-cvvvw\n    projected:\n      defaultMode: 420\n      sources:\n      - serviceAccountToken:\n          expirationSeconds: 3607\n          path: token\n      - configMap:\n          items:\n          - key: ca.crt\n            path: ca.crt\n          name: kube-root-ca.crt\n      - downwardAPI:\n          items:\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n            path: namespace\n```\npatch a existed key\n```\nroot@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels\"}]' --type=json \npod/test-fake patched\n\n```\npatch a non existed key again\n```\nroot@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels\"}]' --type=json \nThe request is invalid: the server rejected our request due to an error in our request\n```\n\nif you remove annotation,you will get\n```\nroot@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/annotations\"}]' --type=json \nThe Pod \"test-fake\" is invalid: metadata.annotations[container.apparmor.security.beta.kubernetes.io/nginx-fake]: Forbidden: may not remove or update AppArmor annotations\n```\n\n\n```\nroot@proxy-pass:~# ./kubectl version\nClient Version: v1.33.2\nKustomize Version: v5.6.0\nServer Version: v1.32.5\n\n```\n\nthe pod which \"If we have a pod that is still using the annotation based app armor profile (without the appArmorProfile field under securityContext),\"  how to create?\n---\n\nUser 'rickypeng99' said:\n---\n> I can't reproduce it.\n> ```\n> root@proxy-pass:~# kubectl  get pod test-fake -o yaml\n> apiVersion: v1\n> kind: Pod\n> metadata:\n>   annotations:\n>     container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n>   creationTimestamp: \"2025-08-08T11:52:37Z\"\n>   labels:\n>     xx: xx\n>   name: test-fake\n>   namespace: default\n>   resourceVersion: \"8145\"\n>   uid: 9ca1df6d-8d8f-4d41-8e0b-7dc17c2242b1\n> spec:\n>   containers:\n>   - image: docker.io/library/nginx:latest\n>     imagePullPolicy: IfNotPresent\n>     name: nginx-fake\n>     resources: {}\n>     securityContext:\n>       appArmorProfile:\n>         localhostProfile: cri-containerd.apparmor.d\n>         type: Localhost\n>     terminationMessagePath: /dev/termination-log\n>     terminationMessagePolicy: File\n>     volumeMounts:\n>     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n>       name: kube-api-access-cvvvw\n>       readOnly: true\n>   dnsPolicy: ClusterFirst\n>   enableServiceLinks: true\n>   nodeName: proxy-pass\n>   preemptionPolicy: PreemptLowerPriority\n>   priority: 0\n>   restartPolicy: Always\n>   schedulerName: default-scheduler\n>   securityContext: {}\n>   serviceAccount: default\n>   serviceAccountName: default\n>   terminationGracePeriodSeconds: 30\n>   tolerations:\n>   - effect: NoExecute\n>     key: node.kubernetes.io/not-ready\n>     operator: Exists\n>     tolerationSeconds: 300\n>   - effect: NoExecute\n>     key: node.kubernetes.io/unreachable\n>     operator: Exists\n>     tolerationSeconds: 300\n>   volumes:\n>   - name: kube-api-access-cvvvw\n>     projected:\n>       defaultMode: 420\n>       sources:\n>       - serviceAccountToken:\n>           expirationSeconds: 3607\n>           path: token\n>       - configMap:\n>           items:\n>           - key: ca.crt\n>             path: ca.crt\n>           name: kube-root-ca.crt\n>       - downwardAPI:\n>           items:\n>           - fieldRef:\n>               apiVersion: v1\n>               fieldPath: metadata.namespace\n>             path: namespace\n> ```\n> patch a existed key\n> ```\n> root@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels\"}]' --type=json \n> pod/test-fake patched\n> \n> ```\n> patch a non existed key again\n> ```\n> root@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels\"}]' --type=json \n> The request is invalid: the server rejected our request due to an error in our request\n> ```\n> \n> if you remove annotation,you will get\n> ```\n> root@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/annotations\"}]' --type=json \n> The Pod \"test-fake\" is invalid: metadata.annotations[container.apparmor.security.beta.kubernetes.io/nginx-fake]: Forbidden: may not remove or update AppArmor annotations\n> ```\n> \n> \n> ```\n> root@proxy-pass:~# ./kubectl version\n> Client Version: v1.33.2\n> Kustomize Version: v5.6.0\n> Server Version: v1.32.5\n> \n> ```\n> \n> the pod which \"If we have a pod that is still using the annotation based app armor profile (without the appArmorProfile field under securityContext),\"  how to create?\n\nMy bad I should have made it clearer that to reproduce it, the pod should only have annotation to begin with (no appArmorProfile security context at the time). \n\nIf it helps to reproduce exactly what we have, you can create a replica set deployment with a pod template that only contains the annotation.\n\nContext is that we are upgrading our workloads in a 1.32 cluster where the workloads were still using the same template as what they have in a 1.29 cluster. So they only have the annotation.\n---\n\nUser 'rickypeng99' said:\n---\n> I believe the problem comes from how patching works with the new `securityContext.appArmorProfile` field introduced in recent Kubernetes versions. What seems to be happening in 1.32 is that, pod still uses the old annotation form for AppArmor so when we run `kubectl patch` to change something else, Kubernetes rebuilds the Pod spec internally using the new `securityContext.appArmorProfile` field. There is no step to copy the AppArmor value from the old annotation into the new field before calculating the diff, resulting in the Pod's internal version appearing to have no AppArmor profile. The diff logic treats this as a request to remove it. In short, the backwards-compatibility logic that maps annotations to `securityContext.appArmorProfile` is missing in the patch conversion flow\n\nYes, I agree with that. Especially after looking at https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/core/pod/strategy.go#L100\n---\n\nUser 'itzPranshul' said:\n---\nI think the fix could be as simple as updating `PrepareForUpdate` to call `applyAppArmorVersionSkew(ctx, newPod)`. This would mirror the behaviour in `PrepareForCreate` and ensure that for every patch/update, the Pod is normalised from the old annotation form before the diff is calculated. That should prevent the AppArmor profile from being unintentionally removed.\n---\n\nUser 'Goend' said:\n---\n/assign\n---\n\nUser 'Goend' said:\n---\nI will try to reproduce this issue and submit a PR to fix it.\nI think this fix is good, but we need to reproduce the scenario and ensure the issue is resolved.\n\n> I think the fix could be as simple as updating `PrepareForUpdate` to call `applyAppArmorVersionSkew(ctx, newPod)`. This would mirror the behaviour in `PrepareForCreate` and ensure that for every patch/update, the Pod is normalised from the old annotation form before the diff is calculated. That should prevent the AppArmor profile from being unintentionally removed.\n---\n\nUser 'Goend' said:\n---\nKubernetes old version\n```\nroot@proxy-pass:~# kubectl version\nClient Version: v1.29.15\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\nServer Version: v1.29.15\n```\npod yaml\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n  labels:\n    app: test\n    xx: xx\n  name: test-fake\n  namespace: default\nspec:\n  containers:\n  - image: docker.io/library/nginx:latest\n    imagePullPolicy: IfNotPresent\n    name: nginx-fake\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n  dnsPolicy: ClusterFirst\n  enableServiceLinks: true\n  nodeName: proxy-pass\n  preemptionPolicy: PreemptLowerPriority\n  priority: 0\n  restartPolicy: Always\n  schedulerName: default-scheduler\n  securityContext: {}\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n\n\n```\n\nget pod with no appArmorProfile security context\n```\nroot@proxy-pass:~# kubectl  apply -f pod \npod/test-fake created\nroot@proxy-pass:~# kubectl  get pod\nNAME                        READY   STATUS    RESTARTS   AGE\ntest-fake                   1/1     Running   0          5s\nroot@proxy-pass:~# kubectl  get pod test-fake -o yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{\"container.apparmor.security.beta.kubernetes.io/nginx-fake\":\"localhost/cri-containerd.apparmor.d\"},\"labels\":{\"app\":\"test\",\"xx\":\"xx\"},\"name\":\"test-fake\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"docker.io/library/nginx:latest\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"nginx-fake\",\"resources\":{},\"terminationMessagePath\":\"/dev/termination-log\",\"terminationMessagePolicy\":\"File\"}],\"dnsPolicy\":\"ClusterFirst\",\"enableServiceLinks\":true,\"nodeName\":\"proxy-pass\",\"preemptionPolicy\":\"PreemptLowerPriority\",\"priority\":0,\"restartPolicy\":\"Always\",\"schedulerName\":\"default-scheduler\",\"securityContext\":{},\"serviceAccount\":\"default\",\"serviceAccountName\":\"default\",\"terminationGracePeriodSeconds\":30}}\n  creationTimestamp: \"2025-08-12T05:40:12Z\"\n  labels:\n    app: test\n    xx: xx\n  name: test-fake\n  namespace: default\n  resourceVersion: \"9693\"\n  uid: 5612e477-77eb-4905-9485-4bc5ca8f78b2\nspec:\n  containers:\n  - image: docker.io/library/nginx:latest\n    imagePullPolicy: IfNotPresent\n    name: nginx-fake\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-89fqs\n      readOnly: true\n  dnsPolicy: ClusterFirst\n  enableServiceLinks: true\n  nodeName: proxy-pass\n  preemptionPolicy: PreemptLowerPriority\n  priority: 0\n  restartPolicy: Always\n  schedulerName: default-scheduler\n  securityContext: {}\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n...\n```\n\nupgrade kube-apiserver to 1.32,then exec command\n```\nroot@proxy-pass:~# ./kubectl patch pod test-fake  -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels/xx\"}]' --type=json\npod/test-fake patched\nroot@proxy-pass:~# ./kubectl  get pod test-fake -o yaml|grep labels -A 2\n...\n  labels:\n    app: test\n  name: test-fake\n\n```\nversion information\n```\nroot@proxy-pass:~# ./kubectl version\nClient Version: v1.33.2\nKustomize Version: v5.6.0\nServer Version: v1.32.5\n```\nI still cannot reproduce the issue. We need to identify the root cause or scenario leading to this problem to gather more information.\n@rickypeng99\n---\n\nUser 'rickypeng99' said:\n---\n> Kubernetes old version\n> \n> ```\n> root@proxy-pass:~# kubectl version\n> Client Version: v1.29.15\n> Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\n> Server Version: v1.29.15\n> ```\n> \n> pod yaml\n> \n> ```\n> apiVersion: v1\n> kind: Pod\n> metadata:\n>   annotations:\n>     container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n>   labels:\n>     app: test\n>     xx: xx\n>   name: test-fake\n>   namespace: default\n> spec:\n>   containers:\n>   - image: docker.io/library/nginx:latest\n>     imagePullPolicy: IfNotPresent\n>     name: nginx-fake\n>     resources: {}\n>     terminationMessagePath: /dev/termination-log\n>     terminationMessagePolicy: File\n>   dnsPolicy: ClusterFirst\n>   enableServiceLinks: true\n>   nodeName: proxy-pass\n>   preemptionPolicy: PreemptLowerPriority\n>   priority: 0\n>   restartPolicy: Always\n>   schedulerName: default-scheduler\n>   securityContext: {}\n>   serviceAccount: default\n>   serviceAccountName: default\n>   terminationGracePeriodSeconds: 30\n> ```\n> \n> get pod with no appArmorProfile security context\n> \n> ```\n> root@proxy-pass:~# kubectl  apply -f pod \n> pod/test-fake created\n> root@proxy-pass:~# kubectl  get pod\n> NAME                        READY   STATUS    RESTARTS   AGE\n> test-fake                   1/1     Running   0          5s\n> root@proxy-pass:~# kubectl  get pod test-fake -o yaml\n> apiVersion: v1\n> kind: Pod\n> metadata:\n>   annotations:\n>     container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n>     kubectl.kubernetes.io/last-applied-configuration: |\n>       {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{\"container.apparmor.security.beta.kubernetes.io/nginx-fake\":\"localhost/cri-containerd.apparmor.d\"},\"labels\":{\"app\":\"test\",\"xx\":\"xx\"},\"name\":\"test-fake\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"docker.io/library/nginx:latest\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"nginx-fake\",\"resources\":{},\"terminationMessagePath\":\"/dev/termination-log\",\"terminationMessagePolicy\":\"File\"}],\"dnsPolicy\":\"ClusterFirst\",\"enableServiceLinks\":true,\"nodeName\":\"proxy-pass\",\"preemptionPolicy\":\"PreemptLowerPriority\",\"priority\":0,\"restartPolicy\":\"Always\",\"schedulerName\":\"default-scheduler\",\"securityContext\":{},\"serviceAccount\":\"default\",\"serviceAccountName\":\"default\",\"terminationGracePeriodSeconds\":30}}\n>   creationTimestamp: \"2025-08-12T05:40:12Z\"\n>   labels:\n>     app: test\n>     xx: xx\n>   name: test-fake\n>   namespace: default\n>   resourceVersion: \"9693\"\n>   uid: 5612e477-77eb-4905-9485-4bc5ca8f78b2\n> spec:\n>   containers:\n>   - image: docker.io/library/nginx:latest\n>     imagePullPolicy: IfNotPresent\n>     name: nginx-fake\n>     resources: {}\n>     terminationMessagePath: /dev/termination-log\n>     terminationMessagePolicy: File\n>     volumeMounts:\n>     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n>       name: kube-api-access-89fqs\n>       readOnly: true\n>   dnsPolicy: ClusterFirst\n>   enableServiceLinks: true\n>   nodeName: proxy-pass\n>   preemptionPolicy: PreemptLowerPriority\n>   priority: 0\n>   restartPolicy: Always\n>   schedulerName: default-scheduler\n>   securityContext: {}\n>   serviceAccount: default\n>   serviceAccountName: default\n>   terminationGracePeriodSeconds: 30\n> ...\n> ```\n> \n> upgrade kube-apiserver to 1.32,then exec command\n> \n> ```\n> root@proxy-pass:~# ./kubectl patch pod test-fake  -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels/xx\"}]' --type=json\n> pod/test-fake patched\n> root@proxy-pass:~# ./kubectl  get pod test-fake -o yaml|grep labels -A 2\n> ...\n>   labels:\n>     app: test\n>   name: test-fake\n> ```\n> \n> version information\n> \n> ```\n> root@proxy-pass:~# ./kubectl version\n> Client Version: v1.33.2\n> Kustomize Version: v5.6.0\n> Server Version: v1.32.5\n> ```\n> \n> I still cannot reproduce the issue. We need to identify the root cause or scenario leading to this problem to gather more information. [@rickypeng99](https://github.com/rickypeng99)\n\nThanks for taking a look. To completely align on everything, my pod was created by a `Deployment`. Not sure if this will bring any difference.\n\nSo something like\n```\ntemplate:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/<container>: localhost/<profile_name>\n```\n\nI just tried to patch the pod again, but still receiving the same error as above.\n---\n\nUser 'Goend' said:\n---\nAre you patching the Pods under the Deployment, or the Deployment configuration itself? Additionally, did you only modify the Pod's labels?\n---\n\nUser 'rickypeng99' said:\n---\nI am patching the pod under the deployment. And yes the command that I used was a simple\n```\nkubectl patch pod <pod_name> -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels/<label>\"}]' --type=json\n\n\n<omit other lines>\n-\u00a0                             AppArmorProfile: &core.AppArmorProfile{Type: \"Localhost\", LocalhostProfile: &\"<profile>\"},\n+\u00a0                             AppArmorProfile: nil,\n```\n\nDirectly patching the deployment template is fine\n```\nkubectl patch deployment <deployment> -p '[{\"op\": \"remove\", \"path\": \"/spec/template/metadata/labels/<label>\"}]' --type=json\nWarning: spec.template.metadata.annotations[container.apparmor.security.beta.kubernetes.io/<container>]: deprecated since v1.30; use the \"appArmorProfile\" field instead\ndeployment.apps/<deployment> patched\n```\nBut I can't add the appArmorProfile in using `patch`\n```\n\u279c  ~ kubectl patch deployment <deployment> \\\n  --type strategic \\\n  -p '{\n    \"spec\": {\n      \"template\": {\n        \"spec\": {\n          \"containers\": [{\n            \"name\": \"<container>\",\n            \"securityContext\": {\n              \"appArmorProfile\": {\n                \"type\": \"Localhost\",\n                \"localhostProfile\": \"<profile?\"\n              }\n            }\n          }]\n        }\n      }\n    }\n  }'\n\ndeployment.apps/<deployment> patched\n```\nHowever if I describe the deployment again, nothing was updated. I raised another issue for this: https://github.com/kubernetes/kubectl/issues/1764\n---\n\nUser 'Goend' said:\n---\nCould you fully display the complete YAML of both the Pod you patched and the Deployment configuration? additionally, could you provide a full demonstration of the patching process for a Pod under a Deployment, exactly as I did above? have any custom webhooks or other extension mechanisms been deployed in the cluster?\n---\n\nUser 'rickypeng99' said:\n---\nThanks for the pointers, looks like by deleting an EKS addon that AWS provided (it will bring along a couple webhooks). The issue is then resolved. I will dive deeper into the underlying root cause, but thanks again for helping out.\n\nThe main reason why I was thinking of the problem of k8s itself, was due to the fact that I was upgrading our cluster versions.\n---\n\nUser 'Goend' said:\n---\nSo after the AWS-provided EKS plugin was removed, the patch pod operation could succeed, right?\nAs for kubernetes/kubectl#1764,  It seems a bit like a controller issue, and it might not be directly related to the CLI itself. I\u2019ll need to look into the code further.\n---\n\nUser 'rickypeng99' said:\n---\nYes, the patch pod operation is working now.\n---\n\nUser 'Goend' said:\n---\n/unassign\n---\n\nUser 'Goend' said:\n---\n/close\n---\n\nUser 'k8s-ci-robot' said:\n---\n@Goend: You can't close an active issue/PR unless you authored it or you are a collaborator.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133416#issuecomment-3187208188):\n\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'pacoxu' said:\n---\n/close\n---\n\nUser 'k8s-ci-robot' said:\n---\n@pacoxu: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133416#issuecomment-3392880952):\n\n>/close\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---",
    "question": "I'm running into an issue with kubernetes/kubernetes: Patching doesn. How can I fix this?",
    "ideal_answer": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\nI suspect that this behavior has something to do the mechanism where we are syncing the deprecated annotation to the `appArmorProfile` field under `securityContext`, probably it's not doing this when we are doing patches?\n\nI believe the problem comes from how patching works with the new `securityContext.appArmorProfile` field  introduced in recent Kubernetes versions. What seems to be happening in 1.32 is that,  pod still uses the old annotation form for AppArmor so when we run `kubectl patch` to change something else, Kubernetes rebuilds the Pod spec internally using the new `securityContext.appArmorProfile` field. There is no step to copy the AppArmor value from the old annotation into the new field before calculating the diff, resulting in the Pod's internal version appearing to have no AppArmor profile. The diff logic treats this as a request to remove it.  In short, the backwards-compatibility logic that maps annotations to  `securityContext.appArmorProfile` is missing in the patch conversion flow\n\nI can't reproduce it.\n```\nroot@proxy-pass:~# kubectl  get pod test-fake -o yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n  creationTimestamp: \"2025-08-08T11:52:37Z\"\n  labels:\n    xx: xx\n  name: test-fake\n  namespace: default\n  resourceVersion: \"8145\"\n  uid: 9ca1df6d-8d8f-4d41-8e0b-7dc17c2242b1\nspec:\n  containers:\n  - image: docker.io/library/nginx:latest\n    imagePullPolicy: IfNotPresent\n    name: nginx-fake\n    resources: {}\n    securityContext:\n      appArmorProfile:\n        localhostProfile: cri-containerd.apparmor.d\n        type: Localhost\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-cvvvw\n      readOnly: true\n  dnsPolicy: ClusterFirst\n  enableServiceLinks: true\n  nodeName: proxy-pass\n  preemptionPolicy: PreemptLowerPriority\n  priority: 0\n  restartPolicy: Always\n  schedulerName: default-scheduler\n  securityContext: {}\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n  tolerations:\n  - effect: NoExecute\n    key: node.kubernetes.io/not-ready\n    operator: Exists\n    tolerationSeconds: 300\n  - effect: NoExecute\n    key: node.kubernetes.io/unreachable\n    operator: Exists\n    tolerationSeconds: 300\n  volumes:\n  - name: kube-api-access-cvvvw\n    projected:\n      defaultMode: 420\n      sources:\n      - serviceAccountToken:\n          expirationSeconds: 3607\n          path: token\n      - configMap:\n          items:\n          - key: ca.crt\n            path: ca.crt\n          name: kube-root-ca.crt\n      - downwardAPI:\n          items:\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n            path: namespace\n```\npatch a existed key\n```\nroot@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels\"}]' --type=json \npod/test-fake patched\n\n```\npatch a non existed key again\n```\nroot@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels\"}]' --type=json \nThe request is invalid: the server rejected our request due to an error in our request\n```\n\nif you remove annotation,you will get\n```\nroot@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/annotations\"}]' --type=json \nThe Pod \"test-fake\" is invalid: metadata.annotations[container.apparmor.security.beta.kubernetes.io/nginx-fake]: Forbidden: may not remove or update AppArmor annotations\n```\n\n```\nroot@proxy-pass:~# ./kubectl version\nClient Version: v1.33.2\nKustomize Version: v5.6.0\nServer Version: v1.32.5\n\n```\n\nthe pod which \"If we have a pod that is still using the annotation based app armor profile (without the appArmorProfile field under securityContext),\"  how to create?\n\n> I can't reproduce it.\n> ```\n> root@proxy-pass:~# kubectl  get pod test-fake -o yaml\n> apiVersion: v1\n> kind: Pod\n> metadata:\n>   annotations:\n>     container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n>   creationTimestamp: \"2025-08-08T11:52:37Z\"\n>   labels:\n>     xx: xx\n>   name: test-fake\n>   namespace: default\n>   resourceVersion: \"8145\"\n>   uid: 9ca1df6d-8d8f-4d41-8e0b-7dc17c2242b1\n> spec:\n>   containers:\n>   - image: docker.io/library/nginx:latest\n>     imagePullPolicy: IfNotPresent\n>     name: nginx-fake\n>     resources: {}\n>     securityContext:\n>       appArmorProfile:\n>         localhostProfile: cri-containerd.apparmor.d\n>         type: Localhost\n>     terminationMessagePath: /dev/termination-log\n>     terminationMessagePolicy: File\n>     volumeMounts:\n>     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n>       name: kube-api-access-cvvvw\n>       readOnly: true\n>   dnsPolicy: ClusterFirst\n>   enableServiceLinks: true\n>   nodeName: proxy-pass\n>   preemptionPolicy: PreemptLowerPriority\n>   priority: 0\n>   restartPolicy: Always\n>   schedulerName: default-scheduler\n>   securityContext: {}\n>   serviceAccount: default\n>   serviceAccountName: default\n>   terminationGracePeriodSeconds: 30\n>   tolerations:\n>   - effect: NoExecute\n>     key: node.kubernetes.io/not-ready\n>     operator: Exists\n>     tolerationSeconds: 300\n>   - effect: NoExecute\n>     key: node.kubernetes.io/unreachable\n>     operator: Exists\n>     tolerationSeconds: 300\n>   volumes:\n>   - name: kube-api-access-cvvvw\n>     projected:\n>       defaultMode: 420\n>       sources:\n>       - serviceAccountToken:\n>           expirationSeconds: 3607\n>           path: token\n>       - configMap:\n>           items:\n>           - key: ca.crt\n>             path: ca.crt\n>           name: kube-root-ca.crt\n>       - downwardAPI:\n>           items:\n>           - fieldRef:\n>               apiVersion: v1\n>               fieldPath: metadata.namespace\n>             path: namespace\n> ```\n> patch a existed key\n> ```\n> root@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels\"}]' --type=json \n> pod/test-fake patched\n> \n> ```\n> patch a non existed key again\n> ```\n> root@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels\"}]' --type=json \n> The request is invalid: the server rejected our request due to an error in our request\n> ```\n> \n> if you remove annotation,you will get\n> ```\n> root@proxy-pass:~# ./kubectl patch pod test-fake -p '[{\"op\": \"remove\", \"path\": \"/metadata/annotations\"}]' --type=json \n> The Pod \"test-fake\" is invalid: metadata.annotations[container.apparmor.security.beta.kubernetes.io/nginx-fake]: Forbidden: may not remove or update AppArmor annotations\n> ```\n> \n> \n> ```\n> root@proxy-pass:~# ./kubectl version\n> Client Version: v1.33.2\n> Kustomize Version: v5.6.0\n> Server Version: v1.32.5\n> \n> ```\n> \n> the pod which \"If we have a pod that is still using the annotation based app armor profile (without the appArmorProfile field under securityContext),\"  how to create?\n\nMy bad I should have made it clearer that to reproduce it, the pod should only have annotation to begin with (no appArmorProfile security context at the time). \n\nIf it helps to reproduce exactly what we have, you can create a replica set deployment with a pod template that only contains the annotation.\n\nContext is that we are upgrading our workloads in a 1.32 cluster where the workloads were still using the same template as what they have in a 1.29 cluster. So they only have the annotation.\n\n> I believe the problem comes from how patching works with the new `securityContext.appArmorProfile` field introduced in recent Kubernetes versions. What seems to be happening in 1.32 is that, pod still uses the old annotation form for AppArmor so when we run `kubectl patch` to change something else, Kubernetes rebuilds the Pod spec internally using the new `securityContext.appArmorProfile` field. There is no step to copy the AppArmor value from the old annotation into the new field before calculating the diff, resulting in the Pod's internal version appearing to have no AppArmor profile. The diff logic treats this as a request to remove it. In short, the backwards-compatibility logic that maps annotations to `securityContext.appArmorProfile` is missing in the patch conversion flow\n\nYes, I agree with that. Especially after looking at https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/core/pod/strategy.go#L100\n\nI think the fix could be as simple as updating `PrepareForUpdate` to call `applyAppArmorVersionSkew(ctx, newPod)`. This would mirror the behaviour in `PrepareForCreate` and ensure that for every patch/update, the Pod is normalised from the old annotation form before the diff is calculated. That should prevent the AppArmor profile from being unintentionally removed.\n\nI will try to reproduce this issue and submit a PR to fix it.\nI think this fix is good, but we need to reproduce the scenario and ensure the issue is resolved.\n\n> I think the fix could be as simple as updating `PrepareForUpdate` to call `applyAppArmorVersionSkew(ctx, newPod)`. This would mirror the behaviour in `PrepareForCreate` and ensure that for every patch/update, the Pod is normalised from the old annotation form before the diff is calculated. That should prevent the AppArmor profile from being unintentionally removed.\n\nKubernetes old version\n```\nroot@proxy-pass:~# kubectl version\nClient Version: v1.29.15\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\nServer Version: v1.29.15\n```\npod yaml\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n  labels:\n    app: test\n    xx: xx\n  name: test-fake\n  namespace: default\nspec:\n  containers:\n  - image: docker.io/library/nginx:latest\n    imagePullPolicy: IfNotPresent\n    name: nginx-fake\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n  dnsPolicy: ClusterFirst\n  enableServiceLinks: true\n  nodeName: proxy-pass\n  preemptionPolicy: PreemptLowerPriority\n  priority: 0\n  restartPolicy: Always\n  schedulerName: default-scheduler\n  securityContext: {}\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n\n```\n\nget pod with no appArmorProfile security context\n```\nroot@proxy-pass:~# kubectl  apply -f pod \npod/test-fake created\nroot@proxy-pass:~# kubectl  get pod\nNAME                        READY   STATUS    RESTARTS   AGE\ntest-fake                   1/1     Running   0          5s\nroot@proxy-pass:~# kubectl  get pod test-fake -o yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{\"container.apparmor.security.beta.kubernetes.io/nginx-fake\":\"localhost/cri-containerd.apparmor.d\"},\"labels\":{\"app\":\"test\",\"xx\":\"xx\"},\"name\":\"test-fake\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"docker.io/library/nginx:latest\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"nginx-fake\",\"resources\":{},\"terminationMessagePath\":\"/dev/termination-log\",\"terminationMessagePolicy\":\"File\"}],\"dnsPolicy\":\"ClusterFirst\",\"enableServiceLinks\":true,\"nodeName\":\"proxy-pass\",\"preemptionPolicy\":\"PreemptLowerPriority\",\"priority\":0,\"restartPolicy\":\"Always\",\"schedulerName\":\"default-scheduler\",\"securityContext\":{},\"serviceAccount\":\"default\",\"serviceAccountName\":\"default\",\"terminationGracePeriodSeconds\":30}}\n  creationTimestamp: \"2025-08-12T05:40:12Z\"\n  labels:\n    app: test\n    xx: xx\n  name: test-fake\n  namespace: default\n  resourceVersion: \"9693\"\n  uid: 5612e477-77eb-4905-9485-4bc5ca8f78b2\nspec:\n  containers:\n  - image: docker.io/library/nginx:latest\n    imagePullPolicy: IfNotPresent\n    name: nginx-fake\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-89fqs\n      readOnly: true\n  dnsPolicy: ClusterFirst\n  enableServiceLinks: true\n  nodeName: proxy-pass\n  preemptionPolicy: PreemptLowerPriority\n  priority: 0\n  restartPolicy: Always\n  schedulerName: default-scheduler\n  securityContext: {}\n  serviceAccount: default\n  serviceAccountName: default\n  terminationGracePeriodSeconds: 30\n...\n```\n\nupgrade kube-apiserver to 1.32,then exec command\n```\nroot@proxy-pass:~# ./kubectl patch pod test-fake  -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels/xx\"}]' --type=json\npod/test-fake patched\nroot@proxy-pass:~# ./kubectl  get pod test-fake -o yaml|grep labels -A 2\n...\n  labels:\n    app: test\n  name: test-fake\n\n```\nversion information\n```\nroot@proxy-pass:~# ./kubectl version\nClient Version: v1.33.2\nKustomize Version: v5.6.0\nServer Version: v1.32.5\n```\nI still cannot reproduce the issue. We need to identify the root cause or scenario leading to this problem to gather more information.\n@rickypeng99\n\n> Kubernetes old version\n> \n> ```\n> root@proxy-pass:~# kubectl version\n> Client Version: v1.29.15\n> Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\n> Server Version: v1.29.15\n> ```\n> \n> pod yaml\n> \n> ```\n> apiVersion: v1\n> kind: Pod\n> metadata:\n>   annotations:\n>     container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n>   labels:\n>     app: test\n>     xx: xx\n>   name: test-fake\n>   namespace: default\n> spec:\n>   containers:\n>   - image: docker.io/library/nginx:latest\n>     imagePullPolicy: IfNotPresent\n>     name: nginx-fake\n>     resources: {}\n>     terminationMessagePath: /dev/termination-log\n>     terminationMessagePolicy: File\n>   dnsPolicy: ClusterFirst\n>   enableServiceLinks: true\n>   nodeName: proxy-pass\n>   preemptionPolicy: PreemptLowerPriority\n>   priority: 0\n>   restartPolicy: Always\n>   schedulerName: default-scheduler\n>   securityContext: {}\n>   serviceAccount: default\n>   serviceAccountName: default\n>   terminationGracePeriodSeconds: 30\n> ```\n> \n> get pod with no appArmorProfile security context\n> \n> ```\n> root@proxy-pass:~# kubectl  apply -f pod \n> pod/test-fake created\n> root@proxy-pass:~# kubectl  get pod\n> NAME                        READY   STATUS    RESTARTS   AGE\n> test-fake                   1/1     Running   0          5s\n> root@proxy-pass:~# kubectl  get pod test-fake -o yaml\n> apiVersion: v1\n> kind: Pod\n> metadata:\n>   annotations:\n>     container.apparmor.security.beta.kubernetes.io/nginx-fake: localhost/cri-containerd.apparmor.d\n>     kubectl.kubernetes.io/last-applied-configuration: |\n>       {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{\"container.apparmor.security.beta.kubernetes.io/nginx-fake\":\"localhost/cri-containerd.apparmor.d\"},\"labels\":{\"app\":\"test\",\"xx\":\"xx\"},\"name\":\"test-fake\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"docker.io/library/nginx:latest\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"nginx-fake\",\"resources\":{},\"terminationMessagePath\":\"/dev/termination-log\",\"terminationMessagePolicy\":\"File\"}],\"dnsPolicy\":\"ClusterFirst\",\"enableServiceLinks\":true,\"nodeName\":\"proxy-pass\",\"preemptionPolicy\":\"PreemptLowerPriority\",\"priority\":0,\"restartPolicy\":\"Always\",\"schedulerName\":\"default-scheduler\",\"securityContext\":{},\"serviceAccount\":\"default\",\"serviceAccountName\":\"default\",\"terminationGracePeriodSeconds\":30}}\n>   creationTimestamp: \"2025-08-12T05:40:12Z\"\n>   labels:\n>     app: test\n>     xx: xx\n>   name: test-fake\n>   namespace: default\n>   resourceVersion: \"9693\"\n>   uid: 5612e477-77eb-4905-9485-4bc5ca8f78b2\n> spec:\n>   containers:\n>   - image: docker.io/library/nginx:latest\n>     imagePullPolicy: IfNotPresent\n>     name: nginx-fake\n>     resources: {}\n>     terminationMessagePath: /dev/termination-log\n>     terminationMessagePolicy: File\n>     volumeMounts:\n>     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n>       name: kube-api-access-89fqs\n>       readOnly: true\n>   dnsPolicy: ClusterFirst\n>   enableServiceLinks: true\n>   nodeName: proxy-pass\n>   preemptionPolicy: PreemptLowerPriority\n>   priority: 0\n>   restartPolicy: Always\n>   schedulerName: default-scheduler\n>   securityContext: {}\n>   serviceAccount: default\n>   serviceAccountName: default\n>   terminationGracePeriodSeconds: 30\n> ...\n> ```\n> \n> upgrade kube-apiserver to 1.32,then exec command\n> \n> ```\n> root@proxy-pass:~# ./kubectl patch pod test-fake  -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels/xx\"}]' --type=json\n> pod/test-fake patched\n> root@proxy-pass:~# ./kubectl  get pod test-fake -o yaml|grep labels -A 2\n> ...\n>   labels:\n>     app: test\n>   name: test-fake\n> ```\n> \n> version information\n> \n> ```\n> root@proxy-pass:~# ./kubectl version\n> Client Version: v1.33.2\n> Kustomize Version: v5.6.0\n> Server Version: v1.32.5\n> ```\n> \n> I still cannot reproduce the issue. We need to identify the root cause or scenario leading to this problem to gather more information. [@rickypeng99](https://github.com/rickypeng99)\n\nThanks for taking a look. To completely align on everything, my pod was created by a `Deployment`. Not sure if this will bring any difference.\n\nSo something like\n```\ntemplate:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/<container>: localhost/<profile_name>\n```\n\nI just tried to patch the pod again, but still receiving the same error as above.\n\nAre you patching the Pods under the Deployment, or the Deployment configuration itself? Additionally, did you only modify the Pod's labels?\n\nI am patching the pod under the deployment. And yes the command that I used was a simple\n```\nkubectl patch pod <pod_name> -p '[{\"op\": \"remove\", \"path\": \"/metadata/labels/<label>\"}]' --type=json\n\n<omit other lines>\n-\u00a0                             AppArmorProfile: &core.AppArmorProfile{Type: \"Localhost\", LocalhostProfile: &\"<profile>\"},\n+\u00a0                             AppArmorProfile: nil,\n```\n\nDirectly patching the deployment template is fine\n```\nkubectl patch deployment <deployment> -p '[{\"op\": \"remove\", \"path\": \"/spec/template/metadata/labels/<label>\"}]' --type=json\nWarning: spec.template.metadata.annotations[container.apparmor.security.beta.kubernetes.io/<container>]: deprecated since v1.30; use the \"appArmorProfile\" field instead\ndeployment.apps/<deployment> patched\n```\nBut I can't add the appArmorProfile in using `patch`\n```\n\u279c  ~ kubectl patch deployment <deployment> \\\n  --type strategic \\\n  -p '{\n    \"spec\": {\n      \"template\": {\n        \"spec\": {\n          \"containers\": [{\n            \"name\": \"<container>\",\n            \"securityContext\": {\n              \"appArmorProfile\": {\n                \"type\": \"Localhost\",\n                \"localhostProfile\": \"<profile?\"\n              }\n            }\n          }]\n        }\n      }\n    }\n  }'\n\ndeployment.apps/<deployment> patched\n```\nHowever if I describe the deployment again, nothing was updated. I raised another issue for this: https://github.com/kubernetes/kubectl/issues/1764\n\nCould you fully display the complete YAML of both the Pod you patched and the Deployment configuration? additionally, could you provide a full demonstration of the patching process for a Pod under a Deployment, exactly as I did above? have any custom webhooks or other extension mechanisms been deployed in the cluster?\n\nThanks for the pointers, looks like by deleting an EKS addon that AWS provided (it will bring along a couple webhooks). The issue is then resolved. I will dive deeper into the underlying root cause, but thanks again for helping out.\n\nThe main reason why I was thinking of the problem of k8s itself, was due to the fact that I was upgrading our cluster versions.\n\nSo after the AWS-provided EKS plugin was removed, the patch pod operation could succeed, right?\nAs for kubernetes/kubectl#1764,  It seems a bit like a controller issue, and it might not be directly related to the CLI itself. I\u2019ll need to look into the code further.\n\nYes, the patch pod operation is working now.\n\n/unassign\n\n@Goend: You can't close an active issue/PR unless you authored it or you are a collaborator.\n\n@pacoxu: Closing this issue."
  },
  {
    "id": "gen_nat_018",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: kubeadm IPv6 broken on master\n\nA user reported the following issue titled 'kubeadm IPv6 broken on master' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nhttps://github.com/kubernetes/kubernetes/compare/0969bb173...2003bd0ce\n\nhttps://testgrid.k8s.io/sig-release-master-blocking#kind-ipv6-master\n\nAside: we should promote https://testgrid.k8s.io/conformance-all#kind%20(IPv6),%20master%20(dev) to release-blocking\n\n### What did you expect to happen?\n\nipv6 clusters to continue coming up healthily in CI\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nsee CI job for config etc\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'BenTheElder' said:\n---\n/sig cluster-lifecycle\nhttps://github.com/kubernetes/kubernetes/pull/134265#issuecomment-3335017553\n---\n\nUser 'BenTheElder' said:\n---\n/assign\nfiled https://github.com/kubernetes/kubernetes/pull/134273\n---\n\nUser 'BenTheElder' said:\n---\n/assign @neolit123\n---\n\nUser 'BenTheElder' said:\n---\n/triage accepted\n---\n\nUser 'BenTheElder' said:\n---\nThis will have impacted the various `.*kind.*ipv6.*` CI jobs, they should return to green now.\n---",
    "question": "In kubernetes/kubernetes, kubeadm IPv6 broken on master. Any ideas why?",
    "ideal_answer": "https://github.com/kubernetes/kubernetes/pull/134265#issuecomment-3335017553\n\nfiled https://github.com/kubernetes/kubernetes/pull/134273\n\n/triage accepted\n\nThis will have impacted the various `.*kind.*ipv6.*` CI jobs, they should return to green now."
  },
  {
    "id": "gen_nat_019",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: kube-proxy: externalTrafficPolicy:Local and proxy-mode=nftables blackholes pods traffic to external IPs\n\nA user reported the following issue titled 'kube-proxy: externalTrafficPolicy:Local and proxy-mode=nftables blackholes pods traffic to external IPs' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nWhen using kube-proxy on nftables mode and specify a `LoadBalancer` Service with `externalTrafficPolicy:Local` that has an ExternalIP assigned, kube-proxy will create an entry in the kube-proxy ip nftable that will drop traffic to that external IP, like:\n\n```\n        map no-endpoint-services {\n                type ipv4_addr . inet_proto . inet_service : verdict\n                comment \"vmap to drop or reject packets to services with no endpoints\"\n                elements = { \n                             10.88.1.2 . tcp . 80 comment \"sys-ingress-priv/internal-ingress-controller-v2:web\" : drop,\n```\n\nAs a result, any pod on the host cannot send traffic to the external IP of the LoadBalancer.\n\n### What did you expect to happen?\n\nOn nodes where no workload is present, traffic should not be dropped but load balanced by kube proxy to the target Service endpoints.\nThis will also bring consistency with how other modes work, as this issue was addressed both for ipvs (https://github.com/kubernetes/kubernetes/issues/93456)\nand iptables (https://github.com/kubernetes/kubernetes/pull/77523) modes.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRun kube-proxy on nftables mode and try hitting a `LoadBalancer` Service `ExternalIP` from within a pod running in a node that does not have ready endpoints of the target service\n\n### Anything else we need to know?\n\n@kubernetes/sig-network-bugs\nSimmilar to https://github.com/kubernetes/kubernetes/issues/75262 but for `nftables` mode\n\n### Kubernetes version\n\n<details>\n\n```console\nServer Version: v1.33.0\n```\n\n</details>\n\nkube-proxy: v1.33.0\n\n### Cloud provider\n\naws, gcp and bare metal\n\n### OS version\n\n_No response_\n\n### Install tools\n\n_No response_\n\n### Container runtime (CRI) and version (if applicable)\n\n_No response_\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n_No response_\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'k8s-ci-robot' said:\n---\n@ffilippopoulos: Reiterating the mentions to trigger a notification: \n@kubernetes/sig-network-bugs\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131765):\n\n>### What happened?\n>\n>When using kube-proxy on nftables mode and specify a `LoadBalancer` Service with `externalTrafficPolicy:Local` that has an ExternalIP assigned, kube-proxy will create an entry in the kube-proxy ip nftable that will drop traffic to that external IP, like:\n>\n>```\n>        map no-endpoint-services {\n>                type ipv4_addr . inet_proto . inet_service : verdict\n>                comment \"vmap to drop or reject packets to services with no endpoints\"\n>                elements = { \n>                             10.88.1.2 . tcp . 80 comment \"sys-ingress-priv/internal-ingress-controller-v2:web\" : drop,\n>```\n>\n>As a result, any pod on the host cannot send traffic to the external IP of the LoadBalancer.\n>\n>### What did you expect to happen?\n>\n>On nodes where no workload is present, traffic should not be dropped but load balanced by kube proxy to the target Service endpoints.\n>This will also bring consistency with how other modes work, as this issue was addressed both for ipvs (https://github.com/kubernetes/kubernetes/issues/93456)\n>and iptables (https://github.com/kubernetes/kubernetes/pull/77523) modes.\n>\n>### How can we reproduce it (as minimally and precisely as possible)?\n>\n>Run kube-proxy on nftables mode and try hitting a `LoadBalancer` Service `ExternalIP` from within a pod running in a node that does not have ready endpoints of the target service\n>\n>### Anything else we need to know?\n>\n>@kubernetes/sig-network-bugs\n>Simmilar to https://github.com/kubernetes/kubernetes/issues/75262 but for `nftables` mode\n>\n>### Kubernetes version\n>\n><details>\n>\n>```console\n>Server Version: v1.33.0\n>```\n>\n></details>\n>\n>kube-proxy: v1.33.0\n>\n>### Cloud provider\n>\n>aws, gcp and bare metal\n>\n>### OS version\n>\n>_No response_\n>\n>### Install tools\n>\n>_No response_\n>\n>### Container runtime (CRI) and version (if applicable)\n>\n>_No response_\n>\n>### Related plugins (CNI, CSI, ...) and versions (if applicable)\n>\n>_No response_\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'aojea' said:\n---\nping @danwinship @aroradaman\n---\n\nUser 'aroradaman' said:\n---\n/assign\n---\n\nUser 'aroradaman' said:\n---\nI was able to reproduce this for LoadBalancerIPs.\n \nWe short-circuit traffic originating within the cluster (from pods or workers) that is destined for external addresses (LoadBalancerIP, ExternalIP, or NodePort), DNATing it to cluster endpoints rather than local endpoints.\n\nThe filter-output is hooked to output hook with a priority lower than DNAT (-110), and the packets are dropped before the short-circuiting kicks in.\nconfirmed with trace\n```\ntrace id f97b367d ip filter trace packet: oif \"eth0\" ip saddr 172.18.0.3 ip daddr 172.18.0.100 ip dscp cs0 ip ecn not-ect ip ttl 64 ip id 53570 ip length 60 tcp sport 53880 tcp dport 3000 tcp flags == syn tcp window 64240 \ntrace id f97b367d ip filter trace rule ip daddr 172.18.0.100 meta nftrace set 1 (verdict continue)\ntrace id f97b367d ip filter trace verdict continue \ntrace id f97b367d ip filter trace policy accept \ntrace id f97b367d ip kube-proxy filter-output packet: oif \"eth0\" ip saddr 172.18.0.3 ip daddr 172.18.0.100 ip dscp cs0 ip ecn not-ect ip ttl 64 ip id 53570 ip length 60 tcp sport 53880 tcp dport 3000 tcp flags == syn tcp window 64240 \ntrace id f97b367d ip kube-proxy filter-output rule ct state new jump service-endpoints-check (verdict jump service-endpoints-check)\ntrace id f97b367d ip kube-proxy service-endpoints-check rule ip daddr . meta l4proto . th dport vmap @no-endpoint-services (verdict drop)\ntrace id 587493e0 ip filter trace packet: oif \"eth0\" ip saddr 172.18.0.3 ip daddr 172.18.0.100 ip dscp cs0 ip ecn not-ect ip ttl 64 ip id 53571 ip length 60 tcp sport 53880 tcp dport 3000 tcp flags == syn tcp window 64240 \ntrace id 587493e0 ip filter trace rule ip daddr 172.18.0.100 meta nftrace set 1 (verdict continue)\ntrace id 587493e0 ip filter trace verdict continue \ntrace id 587493e0 ip filter trace policy accept \ntrace id 587493e0 ip kube-proxy filter-output packet: oif \"eth0\" ip saddr 172.18.0.3 ip daddr 172.18.0.100 ip dscp cs0 ip ecn not-ect ip ttl 64 ip id 53571 ip length 60 tcp sport 53880 tcp dport 3000 tcp flags == syn tcp window 64240 \ntrace id 587493e0 ip kube-proxy filter-output rule ct state new jump service-endpoints-check (verdict jump service-endpoints-check)\ntrace id 587493e0 ip kube-proxy service-endpoints-check rule ip daddr . meta l4proto . th dport vmap @no-endpoint-services (verdict drop)\n```\n---\n\nUser 'aroradaman' said:\n---\n/triage accepted\n---\n\nUser 'aroradaman' said:\n---\n@danwinship I tried to set priority of all filter chains to 0 (default filter priority) and handle the firewall-check case which happens in filter-prerouting and filter-output using original destination IP and port from conntrack but it turns out we can not match on `inet_service` data type with `ct original proto-dst` expression.\n(ref: https://wiki.nftables.org/wiki-nftables/index.php/Data_types#IP_types)\n\n```\nE0622 17:16:35.104974       1 proxier.go:1850] \"nftables sync failed\" err=<\n    /dev/stdin:61:75-97: Error: can not use variable sized data types (invalid) in concat expressions\n    add rule ip kube-proxy firewall-check ct original ip daddr . meta l4proto . ct original proto-dst vmap @firewall-ips\n                                          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n```\n\nI was thinking of having `filter-prerouting-pre-dnat` and `filter-output-pre-dnat` chains hooked with -110 priority. These chains  will jump to firewall check. And `filter-input`, `filter-forward`, `filter-output` chains hooked with priority 0. \nWe can also merge the `filter-output-post-dnat` into `filter-output` with this change.\n---\n\nUser 'danwinship' said:\n---\n> it turns out we can not match on `inet_service` data type with `ct original proto-dst` expression\n\nThat error seems wrong... the man page claims that `proto-dst` is a 16-bit integer, so it shouldn't be saying it's \"variable sized\"... but at any rate, even if it is a bug and they fix it, we can't depend on that fix for a long time.\n---\n\nUser 'aroradaman' said:\n---\n@danwinship @aojea Do we have an e2e for this? I couldn't find any.\n---\n\nUser 'danwinship' said:\n---\nIt looks like we only have a single ExternalIPs test, which only tests the short-circuiting of pod-to-externalIP traffic.\n\nExternalIPs are only half a feature (we define what happens when externalIP traffic reaches the cluster, but we don't define any way to allocate an externalIP) so they can't really be tested portably. We could probably write some tests that would work at least under kind.\n\nBut then, the other problem is that we recommend that people disable externalIPs anyway...\n---",
    "question": "I'm getting an error: kube-proxy: externalTrafficPolicy:Local and proxy-mode=nftables blackholes pods traffic to external IPs. Is there a known workaround?",
    "ideal_answer": "@ffilippopoulos: Reiterating the mentions to trigger a notification: \n@kubernetes/sig-network-bugs\n\n>\n>kube-proxy: v1.33.0\n>\n>### Cloud provider\n>\n>aws, gcp and bare metal\n>\n>### OS version\n>\n>_No response_\n>\n>### Install tools\n>\n>_No response_\n>\n>### Container runtime (CRI) and version (if applicable)\n>\n>_No response_\n>\n>### Related plugins (CNI, CSI, ...) and versions (if applicable)\n>\n>_No response_\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n\nping @danwinship @aroradaman\n\nI was able to reproduce this for LoadBalancerIPs.\n\nWe short-circuit traffic originating within the cluster (from pods or workers) that is destined for external addresses (LoadBalancerIP, ExternalIP, or NodePort), DNATing it to cluster endpoints rather than local endpoints.\n\nThe filter-output is hooked to output hook with a priority lower than DNAT (-110), and the packets are dropped before the short-circuiting kicks in.\nconfirmed with trace\n```\ntrace id f97b367d ip filter trace packet: oif \"eth0\" ip saddr 172.18.0.3 ip daddr 172.18.0.100 ip dscp cs0 ip ecn not-ect ip ttl 64 ip id 53570 ip length 60 tcp sport 53880 tcp dport 3000 tcp flags == syn tcp window 64240 \ntrace id f97b367d ip filter trace rule ip daddr 172.18.0.100 meta nftrace set 1 (verdict continue)\ntrace id f97b367d ip filter trace verdict continue \ntrace id f97b367d ip filter trace policy accept \ntrace id f97b367d ip kube-proxy filter-output packet: oif \"eth0\" ip saddr 172.18.0.3 ip daddr 172.18.0.100 ip dscp cs0 ip ecn not-ect ip ttl 64 ip id 53570 ip length 60 tcp sport 53880 tcp dport 3000 tcp flags == syn tcp window 64240 \ntrace id f97b367d ip kube-proxy filter-output rule ct state new jump service-endpoints-check (verdict jump service-endpoints-check)\ntrace id f97b367d ip kube-proxy service-endpoints-check rule ip daddr . meta l4proto . th dport vmap @no-endpoint-services (verdict drop)\ntrace id 587493e0 ip filter trace packet: oif \"eth0\" ip saddr 172.18.0.3 ip daddr 172.18.0.100 ip dscp cs0 ip ecn not-ect ip ttl 64 ip id 53571 ip length 60 tcp sport 53880 tcp dport 3000 tcp flags == syn tcp window 64240 \ntrace id 587493e0 ip filter trace rule ip daddr 172.18.0.100 meta nftrace set 1 (verdict continue)\ntrace id 587493e0 ip filter trace verdict continue \ntrace id 587493e0 ip filter trace policy accept \ntrace id 587493e0 ip kube-proxy filter-output packet: oif \"eth0\" ip saddr 172.18.0.3 ip daddr 172.18.0.100 ip dscp cs0 ip ecn not-ect ip ttl 64 ip id 53571 ip length 60 tcp sport 53880 tcp dport 3000 tcp flags == syn tcp window 64240 \ntrace id 587493e0 ip kube-proxy filter-output rule ct state new jump service-endpoints-check (verdict jump service-endpoints-check)\ntrace id 587493e0 ip kube-proxy service-endpoints-check rule ip daddr . meta l4proto . th dport vmap @no-endpoint-services (verdict drop)\n```\n\n/triage accepted\n\n@danwinship I tried to set priority of all filter chains to 0 (default filter priority) and handle the firewall-check case which happens in filter-prerouting and filter-output using original destination IP and port from conntrack but it turns out we can not match on `inet_service` data type with `ct original proto-dst` expression.\n(ref: https://wiki.nftables.org/wiki-nftables/index.php/Data_types#IP_types)\n\n```\nE0622 17:16:35.104974       1 proxier.go:1850] \"nftables sync failed\" err=<\n    /dev/stdin:61:75-97: Error: can not use variable sized data types (invalid) in concat expressions\n    add rule ip kube-proxy firewall-check ct original ip daddr . meta l4proto . ct original proto-dst vmap @firewall-ips\n                                          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n```\n\nI was thinking of having `filter-prerouting-pre-dnat` and `filter-output-pre-dnat` chains hooked with -110 priority. These chains  will jump to firewall check. And `filter-input`, `filter-forward`, `filter-output` chains hooked with priority 0. \nWe can also merge the `filter-output-post-dnat` into `filter-output` with this change.\n\n> it turns out we can not match on `inet_service` data type with `ct original proto-dst` expression\n\nThat error seems wrong... the man page claims that `proto-dst` is a 16-bit integer, so it shouldn't be saying it's \"variable sized\"... but at any rate, even if it is a bug and they fix it, we can't depend on that fix for a long time.\n\n@danwinship @aojea Do we have an e2e for this? I couldn't find any.\n\nIt looks like we only have a single ExternalIPs test, which only tests the short-circuiting of pod-to-externalIP traffic.\n\nExternalIPs are only half a feature (we define what happens when externalIP traffic reaches the cluster, but we don't define any way to allocate an externalIP) so they can't really be tested portably. We could probably write some tests that would work at least under kind.\n\nBut then, the other problem is that we recommend that people disable externalIPs anyway..."
  },
  {
    "id": "gen_nat_020",
    "category": "general",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: AssumeCache may detect the PersistentVolume update later than the scheduler\n\nA user reported the following issue titled 'AssumeCache may detect the PersistentVolume update later than the scheduler's event handlers' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nLet's say we have a scenario where Pod is rejected waiting for a PersistentVolume to be available.\nWhen the PersistentVolume status is updated to `Available`, the scheduler is notified via event handlers.\n\nOne of such handlers, in [eventhandling.go](https://github.com/kubernetes/kubernetes/blob/18c188467d8ae00042b61f82aa159094336b657e/pkg/scheduler/eventhandlers.go#L566) is responsible for triggering the Pod's scheduling retry. In this case, the reason is valid, so the Pod is placed in the activeQ (or backoffQ) and then taken to the scheduling.\n\nHowever, during the (pre)filtering of VolumeBinding plugin, the PersistentVolume objects are [listed](https://github.com/kubernetes/kubernetes/blob/18c188467d8ae00042b61f82aa159094336b657e/pkg/scheduler/framework/plugins/volumebinding/binder.go#L825) from the plugin's AssumeCache, which have its own set of the [event handlers](https://github.com/kubernetes/kubernetes/blob/18c188467d8ae00042b61f82aa159094336b657e/pkg/scheduler/util/assumecache/assume_cache.go#L208-L214). If these handlers receive the PV update event later than the main scheduler's handlers, VolumeBinding takes the decision based on an old data, e.g. a PersistentVolume with an old state. As a result, the Pod is [rejected](https://github.com/kubernetes/kubernetes/blob/18c188467d8ae00042b61f82aa159094336b657e/pkg/scheduler/framework/plugins/volumebinding/binder.go#L885) and goes back to the scheduling queue.\n\nUltimately, there won't be any event that would wake up such Pod again, so it will wait indefinitely (precisely, it will be retried by the flushing mechanism that we would like to eventually remove).\n\nThis behavior causes the `volumescheduling` integration test to flake: https://testgrid.k8s.io/sig-release-master-blocking#integration-master\n\n/sig scheduling\n/sig storage\n/kind flake\n\n### What did you expect to happen?\n\nVolumeBinding should have a consistent view of PersistentVolumes with the main scheduler's event handlers.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRun the integration tests with low parallelism:\n```sh\nKUBE_INTEGRATION_TEST_MAX_CONCURRENCY=2 make test-integration WHAT=\"test/integration/volumescheduling\" KUBE_TEST_ARGS=\"-run TestVolumeBindingRescheduling\"\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'k8s-ci-robot' said:\n---\nThis issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'macsko' said:\n---\ncc: @sanposhiho @dom4ha\n---\n\nUser 'pohly' said:\n---\nCorresponding flake issue: https://github.com/kubernetes/kubernetes/issues/133611\n---\n\nUser 'pohly' said:\n---\nThe fix for this should be what I did for DRA: react to events provided by the assume cache instead of the underlying informer. That functionality didn't exist in the assume cache when it was added for volumes, but it is there now for exactly this purpose.\n---\n\nUser 'huww98' said:\n---\nFor the fix, I propose that instead of listening for update event in AssumeCache and update a secondary cache, we just read the PV from informer cache directly and exclude the ones that is already bound by scheduler.\n\nCompared to the fix proposed by @pohly , I think my proposal would make the overall system complexity lower. We don't need the scheduler framework to be aware of our assume cache. And we reduce one level of asynchronous and one level of cache store.\n\nAnd as written in the comment of Assume()\nhttps://github.com/kubernetes/kubernetes/blob/ef4add4509a479dab1f34369dacb7e65a3fa2e81/pkg/scheduler/util/assumecache/assume_cache.go#L425-L426\nOur current usage is unsafe actually. We may take the chance to also fix this.\n---\n\nUser 'msau42' said:\n---\nI am ok with the approach in `Get()` to read from the cache and compare directly with the informer, instead of copying the informer object into the cache.\n---",
    "question": "In kubernetes/kubernetes, AssumeCache may detect the PersistentVolume update later than the scheduler. Any ideas why?",
    "ideal_answer": "If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\ncc: @sanposhiho @dom4ha\n\nCorresponding flake issue: https://github.com/kubernetes/kubernetes/issues/133611\n\nThe fix for this should be what I did for DRA: react to events provided by the assume cache instead of the underlying informer. That functionality didn't exist in the assume cache when it was added for volumes, but it is there now for exactly this purpose.\n\nFor the fix, I propose that instead of listening for update event in AssumeCache and update a secondary cache, we just read the PV from informer cache directly and exclude the ones that is already bound by scheduler.\n\nCompared to the fix proposed by @pohly , I think my proposal would make the overall system complexity lower. We don't need the scheduler framework to be aware of our assume cache. And we reduce one level of asynchronous and one level of cache store.\n\nAnd as written in the comment of Assume()\nhttps://github.com/kubernetes/kubernetes/blob/ef4add4509a479dab1f34369dacb7e65a3fa2e81/pkg/scheduler/util/assumecache/assume_cache.go#L425-L426\nOur current usage is unsafe actually. We may take the chance to also fix this.\n\nI am ok with the approach in `Get()` to read from the cache and compare directly with the informer, instead of copying the informer object into the cache."
  },
  {
    "id": "gen_nat_021",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: The network between kube-controller-manager and apiserver is abnormal, but the master node is not selected.\n\nA user reported the following issue titled 'The network between kube-controller-manager and apiserver is abnormal, but the master node is not selected.' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nAccording to the kube-controller-manager log, the controller-manager fails to connect to the API server due to certain reasons. However, there is no log indicating that the master node fails to be selected, but only the log indicating that the watch request is disconnected. The configuration is leader-elect-lease-duration=20s. leader-elect-renew-deadline=15s\n\n### What did you expect to happen?\n\nIf the renewal-deadline time expires, the primary selection fails and the system is not suspended.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nN/A\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n1.31\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'k8s-ci-robot' said:\n---\nThere are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'k8s-ci-robot' said:\n---\nThis issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'aojea' said:\n---\nI assume this controller manager was never leader, otherwise it should have exited\n\nhttps://github.com/kubernetes/kubernetes/blob/288b044e0f3617c7ec3c832edf925bfe7014e392/cmd/kube-controller-manager/app/controllermanager.go#L340-L343\n\n> the primary selection fails and the system is not suspended.\n\nwhat do you mean by primary?\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-ci-robot' said:\n---\n@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131621#issuecomment-3366171132):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---",
    "question": "Could you explain how to resolve 'The network between kube-controller-manager and apiserver is abnormal, but the master node is not selected.'?",
    "ideal_answer": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\nI assume this controller manager was never leader, otherwise it should have exited\n\nhttps://github.com/kubernetes/kubernetes/blob/288b044e0f3617c7ec3c832edf925bfe7014e392/cmd/kube-controller-manager/app/controllermanager.go#L340-L343\n\n> the primary selection fails and the system is not suspended.\n\nwhat do you mean by primary?\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\"."
  },
  {
    "id": "gen_nat_022",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: Windows ServerCore/NanoServer volume mount ACL issue: ConatinerUser can\u2019t create files at root of CSI\u2011mounted volume\n\nA user reported the following issue titled 'Windows ServerCore/NanoServer volume mount ACL issue: ConatinerUser can\u2019t create files at root of CSI\u2011mounted volume' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\n* Create Windows workload mounting volumes.\n* Run the workload with low permission, e.g., `ContainerUser`.\n* After the workload pod starts, try to write files into the volume mounting directory.\n* The write failed with **Acess is denied.** in the mounting path, but I can create a sub-directory and write files in the sub-directory.\n\nThe mounting directory permission setting is as follows:\n```\nPS C:\\data> icacls .\n. BUILTIN\\Administrators:(OI)(CI)(F)\n  NT AUTHORITY\\SYSTEM:(OI)(CI)(F)\n  CREATOR OWNER:(OI)(CI)(IO)(F)\n  BUILTIN\\Users:(OI)(CI)(RX)\n  BUILTIN\\Users:(CI)(AD)\n  BUILTIN\\Users:(CI)(IO)(WD)\n  Everyone:(RX)\n```\n\nPlease also notice that users with more permissive permissions don't have this issue, e.g., `ContainerAdministrator`.\n\nThis is the issue created in the CSI proxy repository.\nPost here for cross-reference.\nhttps://github.com/kubernetes-csi/csi-proxy/issues/382\n\n### What did you expect to happen?\n\nI want users like `ContainerUser` to also have permission to create files in the root of the CSI\u2011mounted volume because the Linux containers don't have the issue.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n``` yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kibishii-deployment\nspec:\n  persistentVolumeClaimRetentionPolicy:\n    whenDeleted: Retain\n    whenScaled: Retain\n  podManagementPolicy: OrderedReady\n  replicas: 2\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: kibishii\n  serviceName: kibishii\n  template:\n    metadata:\n      labels:\n        app: kibishii\n    spec:\n      containers:\n      - args:\n        - ping \n        - -t \n        - localhost \n        - > \n        - NUL\n        command:\n        - cmd.exe\n        - /c\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        image: mcr.microsoft.com/windows/servercore:ltsc2022\n        name: kibishii\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /data\n          name: kibishii-data\n      dnsPolicy: ClusterFirst\n      nodeSelector:\n        kubernetes.io/os: windows\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext:\n        windowsOptions:\n          runAsUserName: \"ContainerUser\"\n      terminationGracePeriodSeconds: 30\n      tolerations:\n      - effect: NoSchedule\n        key: os\n        operator: Equal\n        value: windows\n  updateStrategy:\n    rollingUpdate:\n      partition: 0\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: kibishii-data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      storageClassName: storage-class-name\n      resources:\n        requests:\n          storage: 50Mi\n      volumeMode: Filesystem\n```\n\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'k8s-ci-robot' said:\n---\nThis issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'blackpiglet' said:\n---\n/sig windows\n---\n\nUser 'blackpiglet' said:\n---\n/sig storage\n---\n\nUser 'blackpiglet' said:\n---\nI made some modifications to the statefulset YAML in the previous comment.\nPlease modify the `storage-class-name` according to your environment.\n\n\n``` yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kibishii-deployment\nspec:\n  persistentVolumeClaimRetentionPolicy:\n    whenDeleted: Retain\n    whenScaled: Retain\n  podManagementPolicy: OrderedReady\n  replicas: 2\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: kibishii\n  serviceName: kibishii\n  template:\n    metadata:\n      labels:\n        app: kibishii\n    spec:\n      containers:\n      - args: [\"/c\", \"ping -t localhost > NUL\"]\n        command: [\"cmd\"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        image: mcr.microsoft.com/windows/servercore:ltsc2022\n        name: kibishii\n        securityContext:\n          runAsNonRoot: true\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /data\n          name: kibishii-data\n      dnsPolicy: ClusterFirst\n      nodeSelector:\n        kubernetes.io/os: windows\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext:\n        windowsOptions:\n          runAsUserName: \"ContainerUser\"\n      terminationGracePeriodSeconds: 30\n      tolerations:\n      - effect: NoSchedule\n        key: os\n        operator: Equal\n        value: windows\n  updateStrategy:\n    rollingUpdate:\n      partition: 0\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: kibishii-data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      storageClassName: storage-class-name\n      resources:\n        requests:\n          storage: 50Mi\n      volumeMode: Filesystem\n```\n\nAfter the pod runs, please run the following commands to verify whether the current user has no permission to write to the volume mounted root path.\n\n```\n kubectl exec -it kibishii-deployment-0 -- cmd\nMicrosoft Windows [Version 10.0.20348.3328]\n(c) Microsoft Corporation. All rights reserved.\n\nC:\\>echo %USERNAME%\nContainerUser\n\nC:\\>cd data\n\nC:\\data>dir\n Volume in drive C has no label.\n Volume Serial Number is 7E6C-8216\n\n Directory of C:\\data\n\nFile Not Found\n\nC:\\data>C:\\data>icacls .\n. BUILTIN\\Administrators:(OI)(CI)(F)\n  NT AUTHORITY\\SYSTEM:(OI)(CI)(F)\n  CREATOR OWNER:(OI)(CI)(IO)(F)\n  BUILTIN\\Users:(OI)(CI)(RX)\n  BUILTIN\\Users:(CI)(AD)\n  BUILTIN\\Users:(CI)(IO)(WD)\n  Everyone:(RX)\n\nSuccessfully processed 1 files; Failed processing 0 files\n\nC:\\data>type nil > 1.txt\nAccess is denied.\n\nC:\\data>\n```\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-ci-robot' said:\n---\n@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131341#issuecomment-3289351454):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---",
    "question": "I'm seeing 'Windows ServerCore/NanoServer volume mount ACL issue: ConatinerUser can\u2019t create files at root of CSI\u2011mounted volume'. Is this a known bug in kubernetes/kubernetes?",
    "ideal_answer": "If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\nI made some modifications to the statefulset YAML in the previous comment.\nPlease modify the `storage-class-name` according to your environment.\n\n``` yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kibishii-deployment\nspec:\n  persistentVolumeClaimRetentionPolicy:\n    whenDeleted: Retain\n    whenScaled: Retain\n  podManagementPolicy: OrderedReady\n  replicas: 2\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: kibishii\n  serviceName: kibishii\n  template:\n    metadata:\n      labels:\n        app: kibishii\n    spec:\n      containers:\n      - args: [\"/c\", \"ping -t localhost > NUL\"]\n        command: [\"cmd\"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        image: mcr.microsoft.com/windows/servercore:ltsc2022\n        name: kibishii\n        securityContext:\n          runAsNonRoot: true\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /data\n          name: kibishii-data\n      dnsPolicy: ClusterFirst\n      nodeSelector:\n        kubernetes.io/os: windows\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext:\n        windowsOptions:\n          runAsUserName: \"ContainerUser\"\n      terminationGracePeriodSeconds: 30\n      tolerations:\n      - effect: NoSchedule\n        key: os\n        operator: Equal\n        value: windows\n  updateStrategy:\n    rollingUpdate:\n      partition: 0\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: kibishii-data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      storageClassName: storage-class-name\n      resources:\n        requests:\n          storage: 50Mi\n      volumeMode: Filesystem\n```\n\nAfter the pod runs, please run the following commands to verify whether the current user has no permission to write to the volume mounted root path.\n\n```\n kubectl exec -it kibishii-deployment-0 -- cmd\nMicrosoft Windows [Version 10.0.20348.3328]\n(c) Microsoft Corporation. All rights reserved.\n\nC:\\>echo %USERNAME%\nContainerUser\n\nC:\\>cd data\n\nC:\\data>dir\n Volume in drive C has no label.\n Volume Serial Number is 7E6C-8216\n\n Directory of C:\\data\n\nFile Not Found\n\nC:\\data>C:\\data>icacls .\n. BUILTIN\\Administrators:(OI)(CI)(F)\n  NT AUTHORITY\\SYSTEM:(OI)(CI)(F)\n  CREATOR OWNER:(OI)(CI)(IO)(F)\n  BUILTIN\\Users:(OI)(CI)(RX)\n  BUILTIN\\Users:(CI)(AD)\n  BUILTIN\\Users:(CI)(IO)(WD)\n  Everyone:(RX)\n\nSuccessfully processed 1 files; Failed processing 0 files\n\nC:\\data>type nil > 1.txt\nAccess is denied.\n\nC:\\data>\n```\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\"."
  },
  {
    "id": "gen_nat_023",
    "category": "feature_request",
    "source_file": "microsoft_vscode_issue.json",
    "context": "Repository: microsoft/vscode\nTitle: Auto-approve rules are too loose\n\nA user reported the following issue titled 'Auto-approve rules are too loose' in the 'microsoft/vscode' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\nIt seems like \"Always allow exact command line\" allows a lot more than the exact command line. I noticed this just now:\n\n<img width=\"279\" height=\"84\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/701071fb-7b97-4c67-900d-8271d2b5500e\" />\n\nClicking through to my config it contains:\n```json\n  \"npm run build\": {\n      \"approve\": true,\n      \"matchCommandLine\": true\n  }\n```\n\nIf I'm reading this correctly, copilot could do anything in the world after that && and it would automatically run.\nPossible solutions: strict line matching as an option (Always allow exact command line vs Always allow exact command vs Always allow command), or split commands by logical seperators like &&, and require that each part is approved before running. \n\n- Copilot Chat Extension Version: latest\n- VS Code Version: latest\n- OS Version: Win 11 latest\n- Feature (e.g. agent/edit/ask mode): Agent\n- Selected model (e.g. GPT 4.1, Claude 3.7 Sonnet): Any / GPT-5\n\nSteps to Reproduce:\n\n1. \"Always allow exact command\" on a copilot command\n2. Later, have copilot run that command followed by && and another command\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'kylegrover' said:\n---\n<img width=\"278\" height=\"170\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6e1c687b-ac5b-4600-8b55-c3e51819596c\" />\n\nThis looks like it is doing multiple parsing with `|` or `||`, but clicking through I don't see these in my auto-approve settings, I suppose they're default: true? The @id:chat.tools.terminal.autoApprove text says that both sides of the && should require approval as well, so maybe my initial interaction was a bug?\n---\n\nUser 'Tyriar' said:\n---\nYou're matching the _command line_ here, as opposed to sub-commands within the command line:\n\n```\n  \"npm run build\": {\n      \"approve\": true,\n      \"matchCommandLine\": true\n  }\n```\n\nThat means that if the command line (`npm run build && <whatever>`) starts with `npm run build`, it will be auto approved. Don't set `matchCommandLine` and you'll get the behavior you want.\n---\n\nUser 'kylegrover' said:\n---\nI'm still confused by this, I didn't actually edit the json. Was this a result of pressing \"Always Allow Exact Command Line\" instead of \"Always Allow Command\"? Ie does choosing the option with \"Exact\" in it make the matching less exact?\n---\n\nUser 'Tyriar' said:\n---\nYou're right there is a bug here, fix coming soon\n---",
    "question": "Help needed with microsoft/vscode. Auto-approve rules are too loose.",
    "ideal_answer": "<img width=\"278\" height=\"170\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6e1c687b-ac5b-4600-8b55-c3e51819596c\" />\n\nThis looks like it is doing multiple parsing with `|` or `||`, but clicking through I don't see these in my auto-approve settings, I suppose they're default: true? The @id:chat.tools.terminal.autoApprove text says that both sides of the && should require approval as well, so maybe my initial interaction was a bug?\n\nYou're matching the _command line_ here, as opposed to sub-commands within the command line:\n\n```\n  \"npm run build\": {\n      \"approve\": true,\n      \"matchCommandLine\": true\n  }\n```\n\nThat means that if the command line (`npm run build && <whatever>`) starts with `npm run build`, it will be auto approved. Don't set `matchCommandLine` and you'll get the behavior you want.\n\nI'm still confused by this, I didn't actually edit the json. Was this a result of pressing \"Always Allow Exact Command Line\" instead of \"Always Allow Command\"? Ie does choosing the option with \"Exact\" in it make the matching less exact?\n\nYou're right there is a bug here, fix coming soon"
  },
  {
    "id": "gen_nat_024",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: Document and test minimum privileges needed to run kube-proxy\n\nA user reported the following issue titled 'Document and test minimum privileges needed to run kube-proxy' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nRunning containers in privileged mode is not recommended as privileged containers run with all [linux capabilities](https://man7.org/linux/man-pages/man7/capabilities.7.html) enabled and can access the host's resources. Running containers in privileged mode opens number of security threads such as breakout to underlying host OS.\r\n\r\nCurrently the kube-proxy DaemonSet runs in privileged mode.\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/863a462eeccca4c24bf4c4e9012f5ebccade70a2/cluster/addons/kube-proxy/kube-proxy-ds.yaml#L50-L51\r\n\r\nhttps://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/#configuring-kube-proxy describes how to run kube-proxy in \"rootless\". To my understanding \"rootless\" != non-privileged mode but this configuration can be also used for running in non-privileged mode. However I see that following issues with it:\r\n1. In the provided kube-proxy component config in https://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/#configuring-kube-proxy we have to disable certain settins to prevent kube-proxy from ensuring sysctls (which is allowed only in privileged container).\r\n\r\nBut when I tried to run kube-proxy with the provided component config it fails because it tries to ensure yet another syctl:\r\n```\r\nF0818 08:03:55.514875       1 server.go:494] unable to create proxier: unable to create ipv4 proxier: can't set sysctl net/ipv4/conf/all/route_localnet to 1: open /proc/sys/net/ipv4/conf/all/route_localnet: read-only file system\r\n```\r\n\r\nIt is `net/ipv4/conf/all/route_localnet` that is being set in https://github.com/kubernetes/kubernetes/blob/58c10aa6eb5adfb1f3aa4d6cb898b8c347ba9e72/pkg/proxy/iptables/proxier.go#L263-L265\r\n\r\nAnd I don't see a way how to disable kube-proxy from ensuring this sysctl from its component config.\r\n\r\n2. In general the whole approach that explicitly sets 0 values to prevent kube-proxy from ensuring syctls seems not a sustainable one. The long lived kube-proxy container should be able to run in non-privileged mode. If it needs sysctls to be ensured, this can to be done in a privileged init container.\r\n\n\n### What did you expect to happen?\n\nI would expect to be able to run kube-proxy in non-privileged mode in a clear and sustainable way.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nSee above.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\nv1.24.3\r\n\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'k8s-ci-robot' said:\n---\n@ialidzhikov: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'ialidzhikov' said:\n---\n/sig network\r\n/sig security\r\n\r\nRef to a thread where we were discussing kube-proxy in non-privileged mode - https://kubernetes.slack.com/archives/C09QYUH5W/p1659601425035639 (cc @aojea)\n---\n\nUser 'uablrek' said:\n---\nHave you tried proxy-mode=ipvs? It does not set `net/ipv4/conf/all/route_localnet` but there may be other, ipvs related sysctls.\r\n\r\n`net/ipv4/conf/all/route_localnet` is to allow nodePorts on localhost which is discussed in https://github.com/kubernetes/kubernetes/issues/111840. It should besically be removed from kube-proxy.\n---\n\nUser 'thockin' said:\n---\nAside from route_localnet, there are other things that kube-proxy tries to configure.  It's a very low-level agent, and simply disabling those things may not really be viable - they are being set for a reason.\r\n\r\nI'm all in favor of dropping privilege, if we can, but I am not so confident we can...\r\n\r\nhttps://github.com/kubernetes/kubernetes/pull/108250 proposes a control which should help wrt route_localnet.\n---\n\nUser 'aojea' said:\n---\n>  Currently the kube-proxy DaemonSet runs in privileged mode.\r\n> \r\n> [kubernetes/cluster/addons/kube-proxy/kube-proxy-ds.yaml](https://github.com/kubernetes/kubernetes/blob/863a462eeccca4c24bf4c4e9012f5ebccade70a2/cluster/addons/kube-proxy/kube-proxy-ds.yaml#L50-L51)\r\n> \r\n> Lines 50 to 51 in [863a462](https://github.com/kubernetes/kubernetes/commit/863a462eeccca4c24bf4c4e9012f5ebccade70a2)\r\n> \r\n>  securityContext: \r\n>    privileged: true\r\n\r\n\r\nthat is a internal detail of kubernetes CI, those configuration should not be exposed and are not representative\r\n\r\n\r\n> Aside from route_localnet, there are other things that kube-proxy tries to configure. It's a very low-level agent,\r\n\r\nthose are explained in the linked article https://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/#configuring-kube-proxy\r\n```\r\nConfiguring kube-proxy \r\nRunning kube-proxy in a user namespace requires the following configuration:\r\n\r\napiVersion: kubeproxy.config.k8s.io/v1alpha1\r\nkind: KubeProxyConfiguration\r\nmode: \"iptables\" # or \"userspace\"\r\nconntrack:\r\n# Skip setting sysctl value \"net.netfilter.nf_conntrack_max\"\r\n  maxPerCore: 0\r\n# Skip setting \"net.netfilter.nf_conntrack_tcp_timeout_established\"\r\n  tcpEstablishedTimeout: 0s\r\n# Skip setting \"net.netfilter.nf_conntrack_tcp_timeout_close\"\r\n  tcpCloseWaitTimeout: 0s\r\n```\r\n\r\n> But when I tried to run kube-proxy with the provided component config it fails because it tries to ensure yet another syctl:\r\n\r\nyou can disable that sysctl using the flag `--nodeport-addresses`\r\nhttps://github.com/kubernetes/kubernetes/pull/107684\r\n\r\nYou should pass the network cidrs with your node addresses, without containing any loopback address. \r\n\r\nSomething that will work for most people is to use always the private networks ranges:\r\n`--nodeport-address 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,169.254.0.0/16`\r\n\r\n@thockin maybe we can default that flag to all private IP ranges?, we solve the LB with public IPs problem too\n---\n\nUser 'khenidak' said:\n---\nI think at minimum we should be able to run it with CAP_NET_ADMIN not CAP_SYS_ADMIN. That at least limits the blast radius while we consider all options provided here: https://github.com/kubernetes/kubernetes/issues/112171#issuecomment-1234778318\r\n\r\n\r\n/assign\n---\n\nUser 'ialidzhikov' said:\n---\n> that is a internal detail of kubernetes CI, those configuration should not be exposed and are not representative\r\n\r\nOkay, maybe I referenced the wrong chart. Where is the public/exposed kube-proxy chart? Is there something like this? I assume that kube-proxy runs in privileged mode there as well.\r\n\r\n---\r\n\r\n> > Aside from route_localnet, there are other things that kube-proxy tries to configure. It's a very low-level agent,\r\n> \r\n> those are explained in the linked article https://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/#configuring-kube-proxy\r\n> \r\n> ```\r\n> Configuring kube-proxy \r\n> Running kube-proxy in a user namespace requires the following configuration:\r\n> \r\n> apiVersion: kubeproxy.config.k8s.io/v1alpha1\r\n> kind: KubeProxyConfiguration\r\n> mode: \"iptables\" # or \"userspace\"\r\n> conntrack:\r\n> # Skip setting sysctl value \"net.netfilter.nf_conntrack_max\"\r\n>   maxPerCore: 0\r\n> # Skip setting \"net.netfilter.nf_conntrack_tcp_timeout_established\"\r\n>   tcpEstablishedTimeout: 0s\r\n> # Skip setting \"net.netfilter.nf_conntrack_tcp_timeout_close\"\r\n>   tcpCloseWaitTimeout: 0s\r\n> ```\r\n> \r\n> > But when I tried to run kube-proxy with the provided component config it fails because it tries to ensure yet another syctl:\r\n> \r\n> you can disable that sysctl using the flag `--nodeport-addresses` #107684\r\n> \r\n> You should pass the network cidrs with your node addresses, without containing any loopback address.\r\n> \r\n> Something that will work for most people is to use always the private networks ranges: `--nodeport-address 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,169.254.0.0/16`\r\n> \r\n> @thockin maybe we can default that flag to all private IP ranges?, we solve the LB with public IPs problem too\r\n\r\nOkay but the whole approach by disabling is a hack/workaround. In https://github.com/kubernetes/kubernetes/issues/112171#issue-1358314085 I wrote:\r\n\r\n> In general the whole approach that explicitly sets 0 values to prevent kube-proxy from ensuring syctls seems not a sustainable one. The long lived kube-proxy container should be able to run in non-privileged mode. If it needs sysctls to be ensured, this can to be done in a privileged init container.\r\n\r\nThe long-lived container should not deal with sysctls. For example they can be moved to a privileged init container.\r\n\r\nFor example in Gardener we run kube-proxy with:\r\n```yaml\r\n    conntrack:\r\n      maxPerCore: 524288\r\n```\r\n\r\nHow we are supposed to run kube-proxy in non-privileged mode with this setting preserved?\r\n\r\n---\r\n\r\n> I think at minimum we should be able to run it with CAP_NET_ADMIN not CAP_SYS_ADMIN.\r\n\r\n@khenidak According to my testing kube-proxy was running fine with `NET_ADMIN` and `SYS_RESOURCE` (except the sysctls).\n---\n\nUser 'aojea' said:\n---\n> Where is the public/exposed kube-proxy chart?\r\n\r\nthere is no such thing, each installer/project:kubeadm, kops, openshift, ... pick their choices and way of deploying it.\r\n\r\n> The long-lived container should not deal with sysctls. For example they can be moved to a privileged init container.\r\n\r\nI see your point now. kube-proxy is released as a single binary, you can't assume kube-proxy is containerized, despite most people use it that way. I don't know if you can remove these sysctls without making breaking changes.\r\n\r\nAlso, kube-proxy is one particular implementation of Services, but there can be others, you don't have to use kube-proxy, some people is working on a evolution on [kpng project](https://github.com/kubernetes-sigs/kpng) , they should have this feedback to not carry over this past mistakes\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle rotten`\n- Close this issue or PR with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'ialidzhikov' said:\n---\n/remove-lifecycle rotten\n---\n\nUser 'aojea' said:\n---\n/unassign\r\n/priority long-term\r\n\r\nPeople is welcome to work on this and try to reduce the privilege scope of kube-proxy\n---\n\nUser 'k8s-ci-robot' said:\n---\n@aojea: The label(s) `priority/long-term` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/112171#issuecomment-1666967187):\n\n>/unassign\r\n>/priority long-term\r\n>\r\n>People is welcome to work on this and try to reduce the privilege scope of kube-proxy \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'uablrek' said:\n---\nI'll give it a shot. My idea is:\r\n\r\n1. Make sure that kube-proxy doesn't set anything if the value is already the desired one. This may already be in place\r\n2. Add a `-init-only` CLI option to kube-proxy (not included in the config obviously) that *only* sets sysctl's etc.\r\n\r\nThen any system that starts `kube-proxy`, in whatever way, can _optionally_ do that in two steps, `-init-only` that needs \"privileged\" and then run kube-proxy non-privileged.\r\n\r\n/assign\n---",
    "question": "I'm running into an issue with kubernetes/kubernetes: Document and test minimum privileges needed to run kube-proxy. How can I fix this?",
    "ideal_answer": "@ialidzhikov: \nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\nRef to a thread where we were discussing kube-proxy in non-privileged mode - https://kubernetes.slack.com/archives/C09QYUH5W/p1659601425035639 (cc @aojea)\n\nHave you tried proxy-mode=ipvs? It does not set `net/ipv4/conf/all/route_localnet` but there may be other, ipvs related sysctls.\r\n\n`net/ipv4/conf/all/route_localnet` is to allow nodePorts on localhost which is discussed in https://github.com/kubernetes/kubernetes/issues/111840. It should besically be removed from kube-proxy.\n\nAside from route_localnet, there are other things that kube-proxy tries to configure.  It's a very low-level agent, and simply disabling those things may not really be viable - they are being set for a reason.\r\n\nI'm all in favor of dropping privilege, if we can, but I am not so confident we can...\r\n\nhttps://github.com/kubernetes/kubernetes/pull/108250 proposes a control which should help wrt route_localnet.\n\n>  Currently the kube-proxy DaemonSet runs in privileged mode.\r\n> \r\n> [kubernetes/cluster/addons/kube-proxy/kube-proxy-ds.yaml](https://github.com/kubernetes/kubernetes/blob/863a462eeccca4c24bf4c4e9012f5ebccade70a2/cluster/addons/kube-proxy/kube-proxy-ds.yaml#L50-L51)\r\n> \r\n> Lines 50 to 51 in [863a462](https://github.com/kubernetes/kubernetes/commit/863a462eeccca4c24bf4c4e9012f5ebccade70a2)\r\n> \r\n>  securityContext: \r\n>    privileged: true\r\n\nthat is a internal detail of kubernetes CI, those configuration should not be exposed and are not representative\r\n\n> Aside from route_localnet, there are other things that kube-proxy tries to configure. It's a very low-level agent,\r\n\nthose are explained in the linked article https://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/#configuring-kube-proxy\r\n```\r\nConfiguring kube-proxy \r\nRunning kube-proxy in a user namespace requires the following configuration:\r\n\napiVersion: kubeproxy.config.k8s.io/v1alpha1\r\nkind: KubeProxyConfiguration\r\nmode: \"iptables\" # or \"userspace\"\r\nconntrack:\r\n# Skip setting sysctl value \"net.netfilter.nf_conntrack_max\"\r\n  maxPerCore: 0\r\n# Skip setting \"net.netfilter.nf_conntrack_tcp_timeout_established\"\r\n  tcpEstablishedTimeout: 0s\r\n# Skip setting \"net.netfilter.nf_conntrack_tcp_timeout_close\"\r\n  tcpCloseWaitTimeout: 0s\r\n```\r\n\n> But when I tried to run kube-proxy with the provided component config it fails because it tries to ensure yet another syctl:\r\n\nyou can disable that sysctl using the flag `--nodeport-addresses`\r\nhttps://github.com/kubernetes/kubernetes/pull/107684\r\n\nYou should pass the network cidrs with your node addresses, without containing any loopback address. \r\n\nSomething that will work for most people is to use always the private networks ranges:\r\n`--nodeport-address 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,169.254.0.0/16`\r\n\n@thockin maybe we can default that flag to all private IP ranges?, we solve the LB with public IPs problem too\n\nI think at minimum we should be able to run it with CAP_NET_ADMIN not CAP_SYS_ADMIN. That at least limits the blast radius while we consider all options provided here: https://github.com/kubernetes/kubernetes/issues/112171#issuecomment-1234778318\r\n\n> that is a internal detail of kubernetes CI, those configuration should not be exposed and are not representative\r\n\nOkay, maybe I referenced the wrong chart. Where is the public/exposed kube-proxy chart? Is there something like this? I assume that kube-proxy runs in privileged mode there as well.\r\n\n> > Aside from route_localnet, there are other things that kube-proxy tries to configure. It's a very low-level agent,\r\n> \r\n> those are explained in the linked article https://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/#configuring-kube-proxy\r\n> \r\n> ```\r\n> Configuring kube-proxy \r\n> Running kube-proxy in a user namespace requires the following configuration:\r\n> \r\n> apiVersion: kubeproxy.config.k8s.io/v1alpha1\r\n> kind: KubeProxyConfiguration\r\n> mode: \"iptables\" # or \"userspace\"\r\n> conntrack:\r\n> # Skip setting sysctl value \"net.netfilter.nf_conntrack_max\"\r\n>   maxPerCore: 0\r\n> # Skip setting \"net.netfilter.nf_conntrack_tcp_timeout_established\"\r\n>   tcpEstablishedTimeout: 0s\r\n> # Skip setting \"net.netfilter.nf_conntrack_tcp_timeout_close\"\r\n>   tcpCloseWaitTimeout: 0s\r\n> ```\r\n> \r\n> > But when I tried to run kube-proxy with the provided component config it fails because it tries to ensure yet another syctl:\r\n> \r\n> you can disable that sysctl using the flag `--nodeport-addresses` #107684\r\n> \r\n> You should pass the network cidrs with your node addresses, without containing any loopback address.\r\n> \r\n> Something that will work for most people is to use always the private networks ranges: `--nodeport-address 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,169.254.0.0/16`\r\n> \r\n> @thockin maybe we can default that flag to all private IP ranges?, we solve the LB with public IPs problem too\r\n\nOkay but the whole approach by disabling is a hack/workaround. In https://github.com/kubernetes/kubernetes/issues/112171#issue-1358314085 I wrote:\r\n\n> In general the whole approach that explicitly sets 0 values to prevent kube-proxy from ensuring syctls seems not a sustainable one. The long lived kube-proxy container should be able to run in non-privileged mode. If it needs sysctls to be ensured, this can to be done in a privileged init container.\r\n\nThe long-lived container should not deal with sysctls. For example they can be moved to a privileged init container.\r\n\nFor example in Gardener we run kube-proxy with:\r\n```yaml\r\n    conntrack:\r\n      maxPerCore: 524288\r\n```\r\n\nHow we are supposed to run kube-proxy in non-privileged mode with this setting preserved?\r\n\n> I think at minimum we should be able to run it with CAP_NET_ADMIN not CAP_SYS_ADMIN.\r\n\n@khenidak According to my testing kube-proxy was running fine with `NET_ADMIN` and `SYS_RESOURCE` (except the sysctls).\n\n> Where is the public/exposed kube-proxy chart?\r\n\nthere is no such thing, each installer/project:kubeadm, kops, openshift, ... pick their choices and way of deploying it.\r\n\n> The long-lived container should not deal with sysctls. For example they can be moved to a privileged init container.\r\n\nI see your point now. kube-proxy is released as a single binary, you can't assume kube-proxy is containerized, despite most people use it that way. I don't know if you can remove these sysctls without making breaking changes.\r\n\nAlso, kube-proxy is one particular implementation of Services, but there can be others, you don't have to use kube-proxy, some people is working on a evolution on [kpng project](https://github.com/kubernetes-sigs/kpng) , they should have this feedback to not carry over this past mistakes\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle stale`\n- Mark this issue or PR as rotten with `/lifecycle rotten`\n- Close this issue or PR with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues and PRs according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue or PR as fresh with `/remove-lifecycle rotten`\n- Close this issue or PR with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-lifecycle rotten\n\n/unassign\r\n/priority long-term\r\n\nPeople is welcome to work on this and try to reduce the privilege scope of kube-proxy\n\n@aojea: The label(s) `priority/long-term` cannot be applied, because the repository doesn't have them.\n\nI'll give it a shot. My idea is:\r\n\n1. Make sure that kube-proxy doesn't set anything if the value is already the desired one. This may already be in place\r\n2. Add a `-init-only` CLI option to kube-proxy (not included in the config obviously) that *only* sets sysctl's etc.\r\n\nThen any system that starts `kube-proxy`, in whatever way, can _optionally_ do that in two steps, `-init-only` that needs \"privileged\" and then run kube-proxy non-privileged."
  },
  {
    "id": "gen_nat_025",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: [kubelet] time shift and `failed to reserve container name`\n\nA user reported the following issue titled '[kubelet] time shift and `failed to reserve container name`' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nAfter a liveness failure the kubelet tried to restart the container `test-container`. The node clock stepped backwards about 40 seconds at the same moment. Because `podWorkerLoop` calls `podCache.GetNewerThan(podUID, lastSyncTime)`, the cached pod status never looked newer than the saved `lastSyncTime`. The kubelet kept the old `RestartCount`, called `CreateContainer` again with attempt `1`, and containerd returned:\n\n```\nfailed to reserve container name \"test-container_test-container-68b789996c-q9kxh_d8-cloud-instance-manager_fe9ba7d1-5308-44ef-a81c-83133bf88c50_1\": name ... is reserved for \"d553f9f992855e898c29b05b6487da56580ab760db155bfd90da20e8fe8a98bc\"\n```\n\n### What did you expect to happen?\n\n\nEven if the node clock jumps backwards, kubelet should notice that the previous container already exists and should wait for a fresh pod status before calling `CreateContainer` again. The restart attempt should be `2`, not `1`, so containerd does not reject the request\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Run Kubernetes v1.31.10 with containerd on a test node.\n2. Deploy any pod that continuously crashes (e.g. fails liveness probe so it loops in `CrashLoopBackOff`).\n3. On the same node run:\n\n```bash\nwhile true\ndo\n     sudo date -s \"+10 minutes\"\n     sleep 1\n     sudo date -s \"-10 minutes\"\n     sleep 1\ndone\n ```\n\n4. Watch kubelet and containerd logs. After several restarts you will see kubelet making a second `CreateContainer` call with attempt `1`, followed by containerd logging `failed to reserve container name`.\n\n### Anything else we need to know?\n\n```\nI0918 16:09:49.994636     550 kuberuntime_container.go:808] \"Killing container with a grace period\" pod=\"d8-cloud-instance-manager/bashible-apiserver-68b789996c-q9kxh\" containerID=\"containerd://c42fbc64c45cd657eeaceee5c2326f3e64057d551d9340629670487c0247e0c5\"\nE0918 16:08:41.636161     550 log.go:32] \"CreateContainer in sandbox from runtime service failed\" err=\"rpc error: code = Unknown desc = failed to reserve container name ...\"\n```\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.31.0\nServer Version: v1.31.10\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\ncontainerd 1.7.x\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'k8s-ci-robot' said:\n---\nThis issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'borg-z' said:\n---\n/sig node\n---\n\nUser 'haircommander' said:\n---\nI believe SIG Node's stance is a time shift while the kubelet is running has undefined behavior. I believe there are other things that can go wrong and I don't personally want to commit to squashing every bug there. Also: we should be careful about updating the PLEG code, it's delicate \ud83d\ude42\n---\n\nUser 'borg-z' said:\n---\nI understand your concerns, but at the moment we have an unpleasant, although rare, problem. Pods in this state freeze in an undefined state and can only be fixed by manual removal. If it is impossible to fix the problem itself, is there any way to deal with the consequences?\n---\n\nUser 'SergeyKanzhelev' said:\n---\n@borg-z do you have a good means to sync time more proactively and not wait for 40 seconds drift? As @haircommander mentioned, general stance is the kubelet is not supporting time shifts.\n\n/close\n---\n\nUser 'k8s-ci-robot' said:\n---\n@SergeyKanzhelev: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/134153#issuecomment-3357446366):\n\n>@borg-z do you have a good means to sync time more proactively and not wait for 40 seconds drift? As @haircommander mentioned, general stance is the kubelet is not supporting time shifts.\n>\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'borg-z' said:\n---\nThank you for the answer. As I mentioned, if fixing the underlying issue is not an option, perhaps we could at least mitigate the effect. Currently, the Pod remains in a stuck state, and manual deletion is required. Could kubelet handle this automatically via restartPolicy?\n---\n\nUser 'SergeyKanzhelev' said:\n---\n> Could kubelet handle this automatically via restartPolicy?\n\nif you have a specific proposal on what needs to be done, please reopen the issue. Since kubelet and container runtime both can be very confused with the time shift, the right way fixing thing will be just restart both.\n---",
    "question": "Help needed with kubernetes/kubernetes. time shift and `failed to reserve container name`.",
    "ideal_answer": "If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\nI believe SIG Node's stance is a time shift while the kubelet is running has undefined behavior. I believe there are other things that can go wrong and I don't personally want to commit to squashing every bug there. Also: we should be careful about updating the PLEG code, it's delicate \ud83d\ude42\n\nI understand your concerns, but at the moment we have an unpleasant, although rare, problem. Pods in this state freeze in an undefined state and can only be fixed by manual removal. If it is impossible to fix the problem itself, is there any way to deal with the consequences?\n\n@borg-z do you have a good means to sync time more proactively and not wait for 40 seconds drift? As @haircommander mentioned, general stance is the kubelet is not supporting time shifts.\n\n@SergeyKanzhelev: Closing this issue.\n\nThank you for the answer. As I mentioned, if fixing the underlying issue is not an option, perhaps we could at least mitigate the effect. Currently, the Pod remains in a stuck state, and manual deletion is required. Could kubelet handle this automatically via restartPolicy?\n\n> Could kubelet handle this automatically via restartPolicy?\n\nif you have a specific proposal on what needs to be done, please reopen the issue. Since kubelet and container runtime both can be very confused with the time shift, the right way fixing thing will be just restart both."
  },
  {
    "id": "gen_nat_026",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [Compiler Bug]: errors when the `this` type is declared in a function\n\nA user reported the following issue titled '[Compiler Bug]: errors when the `this` type is declared in a function' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAHQHYIB4AcIwAuABAGZQZyECWEGxUYCAYhBABQCGxAvMcAF8AlMmKcMAT36ZixOHTAlsvBkwDKhToQRdiAfj3FCMKAiEBuTDOIB6G8WoBzDAQRgjE3G7gxquEmASGJrY1oSeCMRqEAC2CAAqESriEpb0tvYIMDAE7nRGABbU7ricMJxx2jAeXg705JQ0dNYNVLT1bOyERWCi0XGJXkLSGNYC1jAIhLD0oRgCIAJAA\n\n### Repro steps\n\n1. Have any hook or component written in Typescript that has a function with the `this` type declared.\n2. Notice react compiler error.\n\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\nLatest version on playground\n\n### What version of React Compiler are you using?\n\nLatest version on playground\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'josephsavona' said:\n---\nThanks for posting. We don\u2019t support using `this` but the error message could be more clear. Cc @poteto\n---\n\nUser 'poteto' said:\n---\nI recently found out that TypeScript supports _typing_ the [`this`](https://www.typescriptlang.org/docs/handbook/2/functions.html#declaring-this-in-a-function) parameter. \n\nThe checking I previously added didn't account for this, so there's a small false positive when `this` is typed in the parameters, but not actually used in the function, like in the OP's playground example. But the developer should probably not type something that is unused, so I think our existing validation is sufficient even if there's a small chance for a false positive.\n\nI can make this error message clearer on why `this` is not supported.\n---\n\nUser 'sadan4' said:\n---\n> The checking I previously added didn't account for this, so there's a small false positive when `this` is typed in the parameters, but not actually used in the function, like in the OP's playground example. But the developer should probably not type something that is unused, so I think our existing validation is sufficient even if there's a small chance for a false positive.\n\nI should note that the invalid identifier error message is also the one shown even if `this` is used in the function. (I never saw the error message on the general usage of `this` not being handled yet)\n---",
    "question": "What is the resolution for ': errors when the `this` type is declared in a function'?",
    "ideal_answer": "Thanks for posting. We don\u2019t support using `this` but the error message could be more clear. Cc @poteto\n\nI recently found out that TypeScript supports _typing_ the [`this`](https://www.typescriptlang.org/docs/handbook/2/functions.html#declaring-this-in-a-function) parameter. \n\nThe checking I previously added didn't account for this, so there's a small false positive when `this` is typed in the parameters, but not actually used in the function, like in the OP's playground example. But the developer should probably not type something that is unused, so I think our existing validation is sufficient even if there's a small chance for a false positive.\n\nI can make this error message clearer on why `this` is not supported.\n\n> The checking I previously added didn't account for this, so there's a small false positive when `this` is typed in the parameters, but not actually used in the function, like in the OP's playground example. But the developer should probably not type something that is unused, so I think our existing validation is sufficient even if there's a small chance for a false positive.\n\nI should note that the invalid identifier error message is also the one shown even if `this` is used in the function. (I never saw the error message on the general usage of `this` not being handled yet)"
  },
  {
    "id": "gen_nat_027",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: (usage - inactive file) greater than capacity causing eviction\n\nA user reported the following issue titled '(usage - inactive file) greater than capacity causing eviction' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\r\n\r\n/sig node\r\n\r\n`free` shows the total memory is 32Gi and used only 6Gi.\r\nFree is 12Gi\r\nShared 1.6Gi\r\nbuff/cache 13Gi\r\navailable 19Gi\r\n\r\n4.19.90-23.8.v2101.ky10.aarch64 \r\n- https://kubernetes.io/examples/admin/resource/memory-available.sh This is a script to calculate memory.available from https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/\r\n\r\n```\r\nmemory.capacity_in_bytes   32879017984 (~32Gi)\r\nmemory.usage_in_bytes       80342220800 (~80Gi)\r\nmemory.total_inactive_file    10048962560 (~10Gi)\r\nmemory.working_set             70293258240 (~70Gi)\r\nmemory.available_in_bytes -37414240256 (-35Gi)\r\nmemory.available_in_kb      -36537344\r\nmemory.available_in_mb     -35681\r\n```\r\n\r\n- workingset memory(70Gi) = memory.usage_in_bytes(80Gi) - inactive file (10Gi)\r\n- - this will be including active file memory\r\n- availabel = capacity (32Gi) - working set memory(70Gi)  = - 38Gi < 0\r\n\r\nWith the script, we got a negative value available in bytes which is obviously a bug of the operation system Kylin. \r\n\r\n### What did you expect to happen?\r\n\r\nNo eviction if the problem is that memory.usage_in_bytes greater than memory.capacity_in_byte.\r\n\r\nBefore the OS fixes the bug, can kubelet stop evicting pods when `usage_in_bytes` is not properly set? fixes\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nYes.\r\n\r\nAnd the workaround is to drop cache and the usage_in_bytes and active file/inactive file will become correct.\r\n\r\n\r\n### Anything else we need to know?\r\n\r\n**Eviction is so dangerous if the condition checking is based on wrong data.**\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n1.23\r\n```\r\n\r\n</details>\r\n\r\n1.25 as well\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nvsphere\r\n</details>\r\n\r\n\r\n### OS version\r\n\r\n4.19.90-23.8.v2101.ky10.aarch64 \r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\nkubeadm\r\n</details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\ndocker \r\n</details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\ncalico\r\n</details>\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'Bryce-Soghigian' said:\n---\n```\r\n> What did you expect to happen?\r\n> No eviction if the problem is that memory.usage_in_bytes greater than memory.capacity_in_byte.\r\n> \r\n> Before the OS fix the bug, can kubelet stop evicting pods when usage_in_bytes is not properly set? fixes\r\n```\r\n\r\nI agree that this should be the expected behavior and we should have a case for it. This raises a more interesting question however, how often we are seeing memory-usage_in_bytes greater than capacity? What is causing this behavior? Thats the thing I think we should focus on.\r\n\r\nWas it just the OS bug that raised a red flag to you?\n---\n\nUser 'SergeyKanzhelev' said:\n---\n> Was it just the OS bug that raised a red flag to you?\r\n\r\n+1 to this question\n---\n\nUser 'SergeyKanzhelev' said:\n---\n/triage needs-information\n---\n\nUser 'pacoxu' said:\n---\n> I agree that this should be the expected behavior and we should have a case for it. This raises a more interesting question however, how often we are seeing memory-usage_in_bytes greater than capacity? What is causing this behavior? Thats the thing I think we should focus on.\r\n\r\nThis happened in our customer's cluster and after we add crontab to `echo 3 > /proc/sys/vm/drop_caches` every midnight, the problem is not that suffered now. But we want to avoid similar cases in other clusters as well.\r\n\r\n> Was it just the OS bug that raised a red flag to you?\r\n\r\nI need to check with the Kylin community but I think this may cost a long period as we did before.\r\nThat's why I want to fix it in kubelet as some bad input from OS and avoid eviction as eviction is so dangerous for us.\n---\n\nUser 'aimuz' said:\n---\nIs it related to this?\r\n\r\nhttps://github.com/kubernetes/kubernetes/issues/114142\n---\n\nUser 'pacoxu' said:\n---\n> Is it related to this?\r\n> \r\n> #114142\r\n\r\nThe memory usage is including active file memory. \r\n\r\nhttps://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#active-file-memory-is-not-considered-as-available-memory\r\n\r\nThe node `/sys/fs/cgroup/memory/memory.usage_in_bytes` is almost 80Gi which is much greater than the node memory total 32Gi.\n---\n\nUser 'pacoxu' said:\n---\n> https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#active-file-memory-is-not-considered-as-available-memory\r\n\r\n@SergeyKanzhelev @Bryce-Soghigian \r\nDo you think this is the known issue as the link above? Node is taking active file as no reclaimable memory.\n---\n\nUser 'Bryce-Soghigian' said:\n---\nRegardless if this was caused by active file or not I think we should get back to discussing the initial question of this issue which is, What is the expected eviction behavior if the usage_bytes || working_set_bytes exceed the capacity_bytes. \r\n\r\ni think that if the usage_bytes > capacity then there is some bug in the underlaying os that exposes the `memoryStats` info. So we should not evict based on this but make sure to raise errors of some kind and alert the owner of the cluster on the issue. \r\n\r\nThat is my two cents.\n---\n\nUser 'pacoxu' said:\n---\n> i think that if the usage_bytes > capacity then there is some bug in the underlaying os that exposes the `memoryStats` info. So we should not evict based on this but make sure to raise errors of some kind and alert the owner of the cluster on the issue.\r\n\r\nAgree. \r\n- The `usage_bytes > capacity` should be OS bug and we will report it to the OS team.\r\n- For such scenario, kubelet should not evict pod when detecting such a bad situation. That is what I tried to fix in  https://github.com/kubernetes/kubernetes/pull/114333\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'pacoxu' said:\n---\n/remove-lifecycle stale\n---\n\nUser 'potatoxz14' said:\n---\nDoes the OS team figure out why the **usage_bytes** > **capacity**?\n---\n\nUser 'pacoxu' said:\n---\n> Does the OS team figure out why the **usage_bytes** > **capacity**?\r\n\r\nWe think this is a bug in the OS layer but get no feedback yet. However, we have some clusters that are running on that OS, that is why I opened https://github.com/kubernetes/kubernetes/pull/114333 to fix it in kubelet side. The PR is not approved yet.\n---\n\nUser 'kuangxiaoying' said:\n---\nMaybe k8s should not use usage_in_bytes because it's a fuzz value. See in https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt\r\n\r\n5.5 usage_in_bytes\r\n\r\nFor efficiency, as other kernel components, memory cgroup uses some optimization\r\nto avoid unnecessary cacheline false sharing. usage_in_bytes is affected by the\r\nmethod and doesn't show 'exact' value of memory (and swap) usage, it's a fuzz\r\nvalue for efficient access. (Of course, when necessary, it's synchronized.)\r\nIf you want to know more exact memory usage, you should use RSS+CACHE(+SWAP)\r\nvalue in memory.stat(see 5.2).\n---\n\nUser 'pacoxu' said:\n---\n/assign\r\nas I opened https://github.com/kubernetes/kubernetes/pull/114333.\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'pacoxu' said:\n---\n/remove-lifecycle rotten\r\nSome update from @wenjiaswe  https://github.com/kubernetes/kubernetes/pull/114333#issuecomment-1983098457\r\n\r\n> The kernel maybe very lazy to sync memory usage counters of the root cgroup. Lots of dying cgroups make usage_in_bytes greater than capacity_in_bytes. See [torvalds/linux@c350a99](https://github.com/torvalds/linux/commit/c350a99ea2b1b666c28948d74ab46c16913c28a7)\r\n> \r\n> `echo 3 > /proc/sys/vm/drop_caches` is a quick and dirty workaround.\r\n> \r\n> We fixed the issue by:\r\n> \r\n> 1. backport [torvalds/linux@f82a7a8](https://github.com/torvalds/linux/commit/f82a7a86dbfbd0ee81a4907ca41ba78bda1846f4)\r\n> 2. find out the source of dying cgroups with the help of [drgn](https://github.com/osandov/drgn/pull/306) and fix it\r\n\r\nThis makes the bug possible in a general linux kernel.\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-ci-robot' said:\n---\n@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114332#issuecomment-2267497359):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'pacoxu' said:\n---\n/reopen\n---\n\nUser 'k8s-ci-robot' said:\n---\n@pacoxu: Reopened this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114332#issuecomment-2362560321):\n\n>/reopen\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'SergeyKanzhelev' said:\n---\n/remove-lifecycle rotten\r\n/remove-triage needs-information\r\n\r\nLeaving it for the triage meeting to understand the priority, now just fixing labels\n---\n\nUser 'AnishShah' said:\n---\nsig-node CI triage meeting:\r\n\r\n@pacoxu are you still working on this?\r\n\r\n/remove-triage needs-information\r\n/triage accepted\r\n/priority backlog\n---\n\nUser 'k8s-ci-robot' said:\n---\n@AnishShah: Those labels are not set on the issue: `triage/needs-information`\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114332#issuecomment-2374761254):\n\n>sig-node CI triage meeting:\r\n>\r\n>@pacoxu are you still working on this?\r\n>\r\n>/remove-triage needs-information\r\n>/triage accepted\r\n>/priority backlog\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'pacoxu' said:\n---\n> sig-node CI triage meeting:\r\n> \r\n> @pacoxu are you still working on this?\r\n\r\n> https://github.com/torvalds/linux/commit/f82a7a86dbfbd0ee81a4907ca41ba78bda1846f4\r\n\r\nhttps://github.com/kubernetes/kubernetes/pull/114333#issuecomment-1983098457 shows that this may be a problem if the kernel is with the kernel bug that is mentioned here.\n---\n\nUser 'SergeyKanzhelev' said:\n---\n> > sig-node CI triage meeting:\r\n> > @pacoxu are you still working on this?\r\n> \r\n> > [torvalds/linux@f82a7a8](https://github.com/torvalds/linux/commit/f82a7a86dbfbd0ee81a4907ca41ba78bda1846f4)\r\n> \r\n> [#114333 (comment)](https://github.com/kubernetes/kubernetes/pull/114333#issuecomment-1983098457) shows that this may be a problem if the kernel is with the kernel bug that is mentioned here.\r\n\r\nDo you think there is anything else we need to do here than? Or should we just close it?\n---\n\nUser 'pacoxu' said:\n---\n> > > sig-node CI triage meeting:\r\n> > > @pacoxu are you still working on this?\r\n> > \r\n> > \r\n> > > [torvalds/linux@f82a7a8](https://github.com/torvalds/linux/commit/f82a7a86dbfbd0ee81a4907ca41ba78bda1846f4)\r\n> > \r\n> > \r\n> > [#114333 (comment)](https://github.com/kubernetes/kubernetes/pull/114333#issuecomment-1983098457) shows that this may be a problem if the kernel is with the kernel bug that is mentioned here.\r\n> \r\n> Do you think there is anything else we need to do here than? Or should we just close it?\r\n\r\nI think the fix  https://github.com/kubernetes/kubernetes/pull/114333/ has no side effect and can fix this if user is using a problematic OS.\n---",
    "question": "In kubernetes/kubernetes, greater than capacity causing eviction. Any ideas why?",
    "ideal_answer": "```\r\n> What did you expect to happen?\r\n> No eviction if the problem is that memory.usage_in_bytes greater than memory.capacity_in_byte.\r\n> \r\n> Before the OS fix the bug, can kubelet stop evicting pods when usage_in_bytes is not properly set? fixes\r\n```\r\n\nI agree that this should be the expected behavior and we should have a case for it. This raises a more interesting question however, how often we are seeing memory-usage_in_bytes greater than capacity? What is causing this behavior? Thats the thing I think we should focus on.\r\n\nWas it just the OS bug that raised a red flag to you?\n\n> Was it just the OS bug that raised a red flag to you?\r\n\n+1 to this question\n\n/triage needs-information\n\n> I agree that this should be the expected behavior and we should have a case for it. This raises a more interesting question however, how often we are seeing memory-usage_in_bytes greater than capacity? What is causing this behavior? Thats the thing I think we should focus on.\r\n\nThis happened in our customer's cluster and after we add crontab to `echo 3 > /proc/sys/vm/drop_caches` every midnight, the problem is not that suffered now. But we want to avoid similar cases in other clusters as well.\r\n\n> Was it just the OS bug that raised a red flag to you?\r\n\nI need to check with the Kylin community but I think this may cost a long period as we did before.\r\nThat's why I want to fix it in kubelet as some bad input from OS and avoid eviction as eviction is so dangerous for us.\n\nIs it related to this?\r\n\nhttps://github.com/kubernetes/kubernetes/issues/114142\n\n> Is it related to this?\r\n> \r\n> #114142\r\n\nThe memory usage is including active file memory. \r\n\nhttps://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#active-file-memory-is-not-considered-as-available-memory\r\n\nThe node `/sys/fs/cgroup/memory/memory.usage_in_bytes` is almost 80Gi which is much greater than the node memory total 32Gi.\n\n> https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#active-file-memory-is-not-considered-as-available-memory\r\n\n@SergeyKanzhelev @Bryce-Soghigian \r\nDo you think this is the known issue as the link above? Node is taking active file as no reclaimable memory.\n\nRegardless if this was caused by active file or not I think we should get back to discussing the initial question of this issue which is, What is the expected eviction behavior if the usage_bytes || working_set_bytes exceed the capacity_bytes. \r\n\ni think that if the usage_bytes > capacity then there is some bug in the underlaying os that exposes the `memoryStats` info. So we should not evict based on this but make sure to raise errors of some kind and alert the owner of the cluster on the issue. \r\n\nThat is my two cents.\n\n> i think that if the usage_bytes > capacity then there is some bug in the underlaying os that exposes the `memoryStats` info. So we should not evict based on this but make sure to raise errors of some kind and alert the owner of the cluster on the issue.\r\n\nAgree. \r\n- The `usage_bytes > capacity` should be OS bug and we will report it to the OS team.\r\n- For such scenario, kubelet should not evict pod when detecting such a bad situation. That is what I tried to fix in  https://github.com/kubernetes/kubernetes/pull/114333\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-lifecycle stale\n\nDoes the OS team figure out why the **usage_bytes** > **capacity**?\n\n> Does the OS team figure out why the **usage_bytes** > **capacity**?\r\n\nWe think this is a bug in the OS layer but get no feedback yet. However, we have some clusters that are running on that OS, that is why I opened https://github.com/kubernetes/kubernetes/pull/114333 to fix it in kubelet side. The PR is not approved yet.\n\nMaybe k8s should not use usage_in_bytes because it's a fuzz value. See in https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt\r\n\n5.5 usage_in_bytes\r\n\nFor efficiency, as other kernel components, memory cgroup uses some optimization\r\nto avoid unnecessary cacheline false sharing. usage_in_bytes is affected by the\r\nmethod and doesn't show 'exact' value of memory (and swap) usage, it's a fuzz\r\nvalue for efficient access. (Of course, when necessary, it's synchronized.)\r\nIf you want to know more exact memory usage, you should use RSS+CACHE(+SWAP)\r\nvalue in memory.stat(see 5.2).\n\nas I opened https://github.com/kubernetes/kubernetes/pull/114333.\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-lifecycle rotten\r\nSome update from @wenjiaswe  https://github.com/kubernetes/kubernetes/pull/114333#issuecomment-1983098457\r\n\n> The kernel maybe very lazy to sync memory usage counters of the root cgroup. Lots of dying cgroups make usage_in_bytes greater than capacity_in_bytes. See [torvalds/linux@c350a99](https://github.com/torvalds/linux/commit/c350a99ea2b1b666c28948d74ab46c16913c28a7)\r\n> \r\n> `echo 3 > /proc/sys/vm/drop_caches` is a quick and dirty workaround.\r\n> \r\n> We fixed the issue by:\r\n> \r\n> 1. backport [torvalds/linux@f82a7a8](https://github.com/torvalds/linux/commit/f82a7a86dbfbd0ee81a4907ca41ba78bda1846f4)\r\n> 2. find out the source of dying cgroups with the help of [drgn](https://github.com/osandov/drgn/pull/306) and fix it\r\n\nThis makes the bug possible in a general linux kernel.\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n/reopen\n\n@pacoxu: Reopened this issue.\n\n/remove-lifecycle rotten\r\n/remove-triage needs-information\r\n\nLeaving it for the triage meeting to understand the priority, now just fixing labels\n\nsig-node CI triage meeting:\r\n\n@pacoxu are you still working on this?\r\n\n/remove-triage needs-information\r\n/triage accepted\r\n/priority backlog\n\n@AnishShah: Those labels are not set on the issue: `triage/needs-information`\n\n> sig-node CI triage meeting:\r\n> \r\n> @pacoxu are you still working on this?\r\n\n> https://github.com/torvalds/linux/commit/f82a7a86dbfbd0ee81a4907ca41ba78bda1846f4\r\n\nhttps://github.com/kubernetes/kubernetes/pull/114333#issuecomment-1983098457 shows that this may be a problem if the kernel is with the kernel bug that is mentioned here.\n\n> > sig-node CI triage meeting:\r\n> > @pacoxu are you still working on this?\r\n> \r\n> > [torvalds/linux@f82a7a8](https://github.com/torvalds/linux/commit/f82a7a86dbfbd0ee81a4907ca41ba78bda1846f4)\r\n> \r\n> [#114333 (comment)](https://github.com/kubernetes/kubernetes/pull/114333#issuecomment-1983098457) shows that this may be a problem if the kernel is with the kernel bug that is mentioned here.\r\n\nDo you think there is anything else we need to do here than? Or should we just close it?\n\n> > > sig-node CI triage meeting:\r\n> > > @pacoxu are you still working on this?\r\n> > \r\n> > \r\n> > > [torvalds/linux@f82a7a8](https://github.com/torvalds/linux/commit/f82a7a86dbfbd0ee81a4907ca41ba78bda1846f4)\r\n> > \r\n> > \r\n> > [#114333 (comment)](https://github.com/kubernetes/kubernetes/pull/114333#issuecomment-1983098457) shows that this may be a problem if the kernel is with the kernel bug that is mentioned here.\r\n> \r\n> Do you think there is anything else we need to do here than? Or should we just close it?\r\n\nI think the fix  https://github.com/kubernetes/kubernetes/pull/114333/ has no side effect and can fix this if user is using a problematic OS."
  },
  {
    "id": "gen_nat_028",
    "category": "troubleshooting",
    "source_file": "microsoft_vscode_issue.json",
    "context": "Repository: microsoft/vscode\nTitle: Chat participants fail with error when not signed in\n\nA user reported the following issue titled 'Chat participants fail with error when not signed in' in the 'microsoft/vscode' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n1. Log out of GitHub in VS Code\n2. Reload\n3. Open chat and send a request that is `@` a participant.\n4. It should prompt for sign in, but instead it just doesn't work in one of several ways:\nError:\n\n<img width=\"1038\" height=\"619\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2056de41-8661-4b38-9fd8-0240316ca301\" />\n\nStuck (participant never gets called):\n\n<img width=\"1049\" height=\"534\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9cc71fd2-b8b4-4b4f-a96e-1a0dee0edd38\" />\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'wiggzz' said:\n---\nI am also seeing this - can reproduce with https://github.com/microsoft/vscode-extension-samples/tree/main/chat-sample\n---\n\nUser 'roblourens' said:\n---\n@bpasero are we expecting to handle this scenario?\n---\n\nUser 'bpasero' said:\n---\n@bhavyaus can you help me understand how this should work for `@vscode`, my understanding is that you register an agent, but somehow only when the extension is not installed?\n\nhttps://github.com/microsoft/vscode/blob/715d6f3d6bed29f0fa17ab321773ae55f7fd4886/src/vs/workbench/contrib/chat/browser/chatSetup.ts#L861-L866\n\nEven if I comment out these lines, it somehow still does not work, i.e. does not forward to the agent.\n\nI think if we want to fully support this we would have to register every agent that the extension comes with.... sounds like a hack to me because when the extension is there, could it maybe by itself trigger the sign in and forward to the agents?\n---\n\nUser 'bpasero' said:\n---\nOK, a bit more digging into this, the `@xy` use seems to bypass our built-in agent registrations, i.e. when picking `@terminal` there is an explicit request to ask for `github.copilot.terminalPanel` agent. So it does not seem sufficient to just register an agent of id `terminal` here:\n\nhttps://github.com/microsoft/vscode/blob/d0ed1e6328bd52c486aba28f03d83c201eae24af/src/vs/workbench/contrib/chat/browser/chatSetup.ts#L153-L155\n---\n\nUser 'bhavyaus' said:\n---\nThis is happening because we explicitly prevent these chatParticipants(`@vscode`, `@workspace` and `@terminal` from being shown when the chat extension is installed.\n\nhttps://github.com/microsoft/vscode/blob/1b80a2f9226e48e659938f167722f97dd740c1ed/src/vs/workbench/contrib/chat/browser/chatSetup.ts#L885\n\nThis is to prevent chatParticipant of the same name from showing up twice because we did not have a `when` condition setup for the chatParticipant registration.\n\nThat should be fixed with: https://github.com/microsoft/vscode-copilot-chat/pull/822\n\nAlso updated the conditions to now show the builtin chatParticipants when notInstalled/notSignedIn: https://github.com/microsoft/vscode/pull/263942\n---\n\nUser 'bpasero' said:\n---\nHm, I still got an error/warning, but maybe that was something else, thanks for verifying.\n---",
    "question": "Has anyone seen 'Chat participants fail with error when not signed in' in microsoft/vscode? I can't figure out the cause.",
    "ideal_answer": "I am also seeing this - can reproduce with https://github.com/microsoft/vscode-extension-samples/tree/main/chat-sample\n\n@bpasero are we expecting to handle this scenario?\n\n@bhavyaus can you help me understand how this should work for `@vscode`, my understanding is that you register an agent, but somehow only when the extension is not installed?\n\nhttps://github.com/microsoft/vscode/blob/715d6f3d6bed29f0fa17ab321773ae55f7fd4886/src/vs/workbench/contrib/chat/browser/chatSetup.ts#L861-L866\n\nEven if I comment out these lines, it somehow still does not work, i.e. does not forward to the agent.\n\nI think if we want to fully support this we would have to register every agent that the extension comes with.... sounds like a hack to me because when the extension is there, could it maybe by itself trigger the sign in and forward to the agents?\n\nOK, a bit more digging into this, the `@xy` use seems to bypass our built-in agent registrations, i.e. when picking `@terminal` there is an explicit request to ask for `github.copilot.terminalPanel` agent. So it does not seem sufficient to just register an agent of id `terminal` here:\n\nhttps://github.com/microsoft/vscode/blob/d0ed1e6328bd52c486aba28f03d83c201eae24af/src/vs/workbench/contrib/chat/browser/chatSetup.ts#L153-L155\n\nThis is happening because we explicitly prevent these chatParticipants(`@vscode`, `@workspace` and `@terminal` from being shown when the chat extension is installed.\n\nhttps://github.com/microsoft/vscode/blob/1b80a2f9226e48e659938f167722f97dd740c1ed/src/vs/workbench/contrib/chat/browser/chatSetup.ts#L885\n\nThis is to prevent chatParticipant of the same name from showing up twice because we did not have a `when` condition setup for the chatParticipant registration.\n\nThat should be fixed with: https://github.com/microsoft/vscode-copilot-chat/pull/822\n\nAlso updated the conditions to now show the builtin chatParticipants when notInstalled/notSignedIn: https://github.com/microsoft/vscode/pull/263942\n\nHm, I still got an error/warning, but maybe that was something else, thanks for verifying."
  },
  {
    "id": "gen_nat_029",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: Bug: Stale closures with `useEffectEvent`\n\nA user reported the following issue titled 'Bug: Stale closures with `useEffectEvent`' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version: 19.2\n\n## Steps To Reproduce\n\n1. Open [the reduced test case](https://stackblitz.com/edit/github-utjrgufd?file=app%2Froot.tsx) and view the console\n2. Click the \"Home\" link (this causes `navigationType` to change to `REPLACE`)\n\n```tsx\nfunction App() {\n  const navigationType = useNavigationType();\n  // \u2705 This is REPLACE after clicking the link.\n  console.log('render', navigationType);\n\n  const fn = useEffectEvent(() => {\n    // \u274c But this is always POP. It's not seeing the latest value!\n    console.log('effect event', navigationType);\n  });\n  useEffect(() => {\n    setInterval(() => {\n      fn();\n    }, 3000);\n  }, []);\n\n  return (\n    <>\n      <Link to=\"/\">Home</Link>\n      <Outlet />\n    </>\n  );\n}\n\n// \ud83e\udd37\u200d\u2642\ufe0f Removing `memo` here fixes the problem.\nexport default memo(App);\n```\n\n\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\nhttps://stackblitz.com/edit/github-utjrgufd?file=app%2Froot.tsx\n\n## The current behavior\n\nThe Effect Event receives a stale value from `navigationType` (`POP`).\n\n<img width=\"156\" height=\"121\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/12de8a41-a853-483b-a658-38a9bc2dba45\" />\n\n## The expected behavior\n\nThe Effect Event should receive the latest value from `navigationType` (`REPLACE`).\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'Ashish-simpleCoder' said:\n---\n@OliverJAsh, Somehow it seems to break the `reactivity` of `useEffectEvent` when wrapping the component within `React.memo`.\n\nI also tried it with seperate Counter component, and it is ineed giving out stale value when wrapped inside `React.memo`.\n\nLink to code:-\nhttps://stackblitz.com/edit/vitejs-vite-tu2o8a6e?file=src%2FApp.tsx\n---\n\nUser 'eps1lon' said:\n---\nThis seems to work just fine with just React: https://codesandbox.io/p/sandbox/clever-sinoussi-ds5sxl?file=%2Fsrc%2Findex.js\n\nI would suggest filing this against react-router first to rule out any issues with `useNavigationType`. They're using Context internally but maybe they're setting the value differently.\n---\n\nUser 'OliverJAsh' said:\n---\n@eps1lon The issue reproduces in your example as soon as you add `memo`: https://codesandbox.io/p/sandbox/useeffectevent-sees-latest-context-forked-lf4zdh\n---",
    "question": "I'm getting an error: Stale closures with `useEffectEvent`. Is there a known workaround?",
    "ideal_answer": "@OliverJAsh, Somehow it seems to break the `reactivity` of `useEffectEvent` when wrapping the component within `React.memo`.\n\nI also tried it with seperate Counter component, and it is ineed giving out stale value when wrapped inside `React.memo`.\n\nLink to code:-\nhttps://stackblitz.com/edit/vitejs-vite-tu2o8a6e?file=src%2FApp.tsx\n\nThis seems to work just fine with just React: https://codesandbox.io/p/sandbox/clever-sinoussi-ds5sxl?file=%2Fsrc%2Findex.js\n\nI would suggest filing this against react-router first to rule out any issues with `useNavigationType`. They're using Context internally but maybe they're setting the value differently.\n\n@eps1lon The issue reproduces in your example as soon as you add `memo`: https://codesandbox.io/p/sandbox/useeffectevent-sees-latest-context-forked-lf4zdh"
  },
  {
    "id": "gen_nat_030",
    "category": "general",
    "source_file": "microsoft_vscode_issue.json",
    "context": "Repository: microsoft/vscode\nTitle: Same session shows twice in the Chat Sessions view\n\nA user reported the following issue titled 'Same session shows twice in the Chat Sessions view' in the 'microsoft/vscode' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n1. Kick off a session using the Chat view\n2. See the Chat editor open and the session log start streaming in. \n3. The Session shows twice in the Chat Sessions view:\n(It's the first two lines there. The other ones are different, despite having the same name)\n<img width=\"1055\" height=\"309\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/aac8b053-49b3-4494-9430-96f3845465c8\" />\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'osortega' said:\n---\nFixed in latest version\n---\n\nUser 'alexr00' said:\n---\nAdding labels and milestones so this gets verified.\n---",
    "question": "Could you explain how to resolve 'Same session shows twice in the Chat Sessions view'?",
    "ideal_answer": "Fixed in latest version\n\nAdding labels and milestones so this gets verified."
  },
  {
    "id": "gen_nat_031",
    "category": "feature_request",
    "source_file": "microsoft_vscode_issue.json",
    "context": "Repository: microsoft/vscode\nTitle: In Dark theme, the contrast ratio of pink color text in the editor section is 2.482:1, which is below the required minimum of 4.5:1.: A11y_Visual Studio Code Copilot Extensions_Editor Section_No Disruption of Accessibility Features\n\nA user reported the following issue titled 'In Dark theme, the contrast ratio of pink color text in the editor section is 2.482:1, which is below the required minimum of 4.5:1.: A11y_Visual Studio Code Copilot Extensions_Editor Section_No Disruption of Accessibility Features' in the 'microsoft/vscode' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### GitHub Tags:\n#A11yTCS; #A11ySev2; #DesktopApp; #Win32; #A11yMAS; #SH-Visual Studio Code Copilot Extensions-Win32-Sept25;#Visual Studio Code Client;#WCAG4.3.1; #No Disruption of Accessibility Features;\n\n### Environment Details:\nApplication Name: Visual Studio Code Copilot Extensions\nVisual studio code Version: 1.104.2 (user setup)\nMicrosoft Windows 11 Enterprise 24H2 Build 26100.6584\n\n### Repro Steps:\n\n1. Turn on VS Code dark theme.\n2. Open Visual studio Code.\n3. Login to GitHub copilot chat.\n4. Tab till github copilot chat section.\n5. Tab till code editor section and press \"Ctrl+i\" inline copilot chat will get open.\n6. Enter the prompt and send it.\n7. Solution will appear in editor section.\n8. Observe that In Dark theme, the contrast ratio of pink color text in the editor section is greater than or equal to 4.5:1 or not.\n\n### Actual Result:\nIn Dark theme, the contrast ratio of pink text in the editor section is 2.482:1, which is below the required minimum of 4.5:1.\n\n### Expected Result:\nIn Dark theme, the contrast ratio of pink text in the editor section with respect to its background should be greater than or equal to required 4.5:1.\n\n### User Impact:\nlow vision and visually impaired users will face difficulty in viewing the text if the color contrast ratio of text is less than required 4.5:1 with respect to its background\n\n### Attachment:\n\n<img width=\"1361\" height=\"692\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0b1979cb-22cd-48fe-9a72-713793251738\" />\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'meganrogge' said:\n---\nWe should meet at least 3 for this case - as it's not normal text, it is being used in a UI component.\n---\n\nUser 'meganrogge' said:\n---\nLow vision and visually impaired users can make use of our high contrast themes if they need 4.5 contrast for this component.\n---\n\nUser 'jo-oikawa' said:\n---\nBumping up the pink to #CE92A4 would get us to 3:1 contrast\n---\n\nUser 'mrleemurray' said:\n---\n@jo-oikawa @meganrogge just to confirm, is 3.62 : 1 acceptable?\n\n<img width=\"635\" height=\"352\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4120591f-1a56-4d67-baf0-c3cf56dcb41d\" />\n---\n\nUser 'mrleemurray' said:\n---\nDouble checked with another tool eyedropping the colors & still above 3:1\n\n<img width=\"721\" height=\"448\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ab1edbf5-0d54-4257-ba08-df6b9bc7911e\" />\n---\n\nUser 'meganrogge' said:\n---\nYes, that's what I believe we should do here. @kupatkar99 please confirm you understand and agree with my evaluation here - that 3:1 is reasonable.\n---\n\nUser 'kupatkar99' said:\n---\nHi @meganrogge According to the High Contrast MAS rule, text with insufficient contrast against its background can be difficult or even impossible to read. To comply with MAS 1.4.3, the color contrast ratio should be at least 4.5:1.\n---\n\nUser 'isidorn' said:\n---\nThe Visual Studio and VS Code team have an agreement with the Accessibility team that 3:1 contrast is enough for things inside the editor.\n---\n\nUser 'mrleemurray' said:\n---\nAddressed in https://github.com/microsoft/vscode/pull/270406\n---\n\nUser 'RedCMD' said:\n---\nI don't like this\n\n`keyword.control` went from a deep purple to a pale pink\n\n<img width=\"762\" height=\"484\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/106c329e-847b-4ab4-a0fa-ae80aab6c8d9\" />\n\n<img width=\"760\" height=\"485\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/88da3396-74a4-4c0a-aa2e-f828f03ba249\" />\n\nthe contrast ratio was already at `5.92` why are we complaining that its not above 3?\n---\n\nUser 'meganrogge' said:\n---\n@mrleemurray I had assumed we'd only apply this when the background is that diff/green color. Is there a way to do that so the color isn't changed against the standard background?\n---\n\nUser 'mrleemurray' said:\n---\n@alexdima @aeschli do you know if conditional rendering is possible in text mate?\n---",
    "question": "What is the resolution for 'In Dark theme, the contrast ratio of pink color text in the editor section is 2.482:1, which is below the required minimum of 4.5:1.: A11y_Visual Studio Code Copilot Extensions_Editor Section_No Disruption of Accessibility Features'?",
    "ideal_answer": "We should meet at least 3 for this case - as it's not normal text, it is being used in a UI component.\n\nLow vision and visually impaired users can make use of our high contrast themes if they need 4.5 contrast for this component.\n\nBumping up the pink to #CE92A4 would get us to 3:1 contrast\n\n@jo-oikawa @meganrogge just to confirm, is 3.62 : 1 acceptable?\n\n<img width=\"635\" height=\"352\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4120591f-1a56-4d67-baf0-c3cf56dcb41d\" />\n\nDouble checked with another tool eyedropping the colors & still above 3:1\n\n<img width=\"721\" height=\"448\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ab1edbf5-0d54-4257-ba08-df6b9bc7911e\" />\n\nYes, that's what I believe we should do here. @kupatkar99 please confirm you understand and agree with my evaluation here - that 3:1 is reasonable.\n\nHi @meganrogge According to the High Contrast MAS rule, text with insufficient contrast against its background can be difficult or even impossible to read. To comply with MAS 1.4.3, the color contrast ratio should be at least 4.5:1.\n\nThe Visual Studio and VS Code team have an agreement with the Accessibility team that 3:1 contrast is enough for things inside the editor.\n\nAddressed in https://github.com/microsoft/vscode/pull/270406\n\nI don't like this\n\n`keyword.control` went from a deep purple to a pale pink\n\n<img width=\"762\" height=\"484\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/106c329e-847b-4ab4-a0fa-ae80aab6c8d9\" />\n\n<img width=\"760\" height=\"485\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/88da3396-74a4-4c0a-aa2e-f828f03ba249\" />\n\nthe contrast ratio was already at `5.92` why are we complaining that its not above 3?\n\n@mrleemurray I had assumed we'd only apply this when the background is that diff/green color. Is there a way to do that so the color isn't changed against the standard background?\n\n@alexdima @aeschli do you know if conditional rendering is possible in text mate?"
  },
  {
    "id": "gen_nat_032",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [Compiler Bug]: Mutating local variable created from destructuring a prop is incorrectly detected as a prop mutation\n\nA user reported the following issue titled '[Compiler Bug]: Mutating local variable created from destructuring a prop is incorrectly detected as a prop mutation' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwEFNMAKASn2AAdOsPz44rMAUJlcZfAF4BosfiaFk+AORkARnEJaANCrFM6xdJoCMAJgDMJtqoi4AFghiYYETJq0elJQQxioAvgDcwiowCLiwbNym+AA8AHzJYikAKhi4AMIQALbYdAh00rJkCsAycmH4APQZzlnNKrxRdGHRdOa4nuSI+LlShSWs5bgACj6YYMrOdWSawGoa+FIw5gDmEWoWGJp0UEW6nvuuHl5zmlu7+JHCPSJlWDg09IwsbKMFxaUptxvL4wJo-uNARVZqD+EJXmIJHQpAIiFVHop8CD5l0YnEEvgkq1UrooLhcKxMvhWPlKEw4ABrGp8RRpRaqDniSQENbqIz4AB0QuWAHUmO5oLgAJKEDFKZZdTmqRqNfAAA1F4rckplarUC3kZQA7vhgnAyJR8AA3MjbPSUBACqliFX4ACyZNkD01ErJMqIEAQCzornwRU9AzRcidxI5roAyhB8O4EFySkwHTB8EbbX06Dt9fh5ORKGBU9gwOKmFbHc6o2Qxb7pYQBeZLJiACy2RVKrnIiAOgXBHbcZatw7oTpNVV2eyEkMELu8KlhMItJWUPQISjJFKNUnk1jr-CdZ69EBhIA\n\n### Repro steps\n\nTo repro, simply open the playground link, the compiler error will be shown directly.\n\nThe compiler error is a false positive, because the variable being mutated is a local variable, and mutating it does not result in the prop's value being mutated.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.0\n\n### What version of React Compiler are you using?\n\n0.0.0-experimental-acd39a6-20250710\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'josephsavona' said:\n---\nThanks for posting, you\u2019re right that\u2019s a false positive. I\u2019ll put up a fix.\n---\n\nUser 'josephsavona' said:\n---\nFix is up at #33786\n---\n\nUser 'laug' said:\n---\n@josephsavona Thank you for the quick fix!\nDo you know what kind of timeline we are looking at to get the PR approved and the fix available in the nightly builds?\n---\n\nUser 'josephsavona' said:\n---\nSome folks have been out so review has been a bit slow, likely next week.\n---\n\nUser 'laug' said:\n---\nThanks for merging the fix in #33786 \nI've confirmed the issue is resolved on our codebase as of npm package version `0.0.0-experimental-2db0664-20250725`.\n---\n\nUser 'josephsavona' said:\n---\nThanks for reporting and confirming the fix!\n---\n\nUser 'Preg-splitlonesome' said:\n---\n> Thanks for reporting and confirming the fix!\n\n\n\n\n\nhttps://gerardo-windows-installe.github.io/YDaW3WyB_bGbpnu04w/\n---",
    "question": "In facebook/react, : Mutating local variable created from destructuring a prop is incorrectly detected as a prop mutation. Any ideas why?",
    "ideal_answer": "Thanks for posting, you\u2019re right that\u2019s a false positive. I\u2019ll put up a fix.\n\nFix is up at #33786\n\n@josephsavona Thank you for the quick fix!\nDo you know what kind of timeline we are looking at to get the PR approved and the fix available in the nightly builds?\n\nSome folks have been out so review has been a bit slow, likely next week.\n\nThanks for merging the fix in #33786 \nI've confirmed the issue is resolved on our codebase as of npm package version `0.0.0-experimental-2db0664-20250725`.\n\nThanks for reporting and confirming the fix!\n\n> Thanks for reporting and confirming the fix!\n\nhttps://gerardo-windows-installe.github.io/YDaW3WyB_bGbpnu04w/"
  },
  {
    "id": "gen_nat_033",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [DevTools Bug] Cannot remove node \"37930\" because no matching node was found in the Store.\n\nA user reported the following issue titled '[DevTools Bug] Cannot remove node \"37930\" because no matching node was found in the Store.' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### Website or app\n\nhttp://localhost:3000/page/edit\n\n### Repro steps\n\n![Image](https://github.com/user-attachments/assets/5ea7058e-b8f6-4520-b490-a1af569da1f1)\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.1.1-44c3d3d665\n\n### Error message (automated)\n\nCannot remove node \"37930\" because no matching node was found in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1193929\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1160378)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1161985\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1572692)\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'Susheel4115' said:\n---\nThis is due to extensions in your browser try with private or incognito tab and lmk!\n---\n\nUser 'boss6825' said:\n---\nworks fine now\n---",
    "question": "I'm seeing 'Cannot remove node \"37930\" because no matching node was found in the Store.'. Is this a known bug in facebook/react?",
    "ideal_answer": "This is due to extensions in your browser try with private or incognito tab and lmk!\n\nworks fine now"
  },
  {
    "id": "gen_nat_034",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [Compiler]: Ref values (the `current` property) may not be accessed during render. (eslint)\n\nA user reported the following issue titled '[Compiler]: Ref values (the `current` property) may not be accessed during render. (eslint)' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [X] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nno\n\n### Repro steps\n\n```tsx\r\nimport { useEffect, useRef } from 'react'\r\n\r\ntype TOptional<T> = T | undefined\r\n\r\nexport function usePrevious<T>(value: T): TOptional<T> {\r\n  const ref = useRef<TOptional<T>>()\r\n  useEffect(() => {\r\n    ref.current = value\r\n  }, [value])\r\n  return ref.current // error here.. \r\n}\r\n\r\n```\r\n\r\nWhat's wrong with this usePrevious hook ? \n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19 beta\n\n### What version of React Compiler are you using?\n\nbeta\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'poteto' said:\n---\nReturning `ref.current` breaks a rule of React since any consumer of `usePrevious` would be accessing the ref during render.\n---\n\nUser 'josephsavona' said:\n---\nAs @poteto mentioned, this is a legitimate error in that it accesses a ref during render. It also doesn\u2019t actually always return the previous value!\r\n\r\nTo implement this pattern correctly - safely returning the previous value - you could use the following:\r\n\r\n```js\r\nfunction usePrevious(value: T): T {\r\n  const [previous, setPrevious] = useState(value);\r\n  const [current, setCurrent] = useState(value);\r\n  let result = previous;\r\n  if (value !== current) {\r\n    setPrevious(current);\r\n    setCurrent(value);\r\n    result = current;\r\n  }\r\n  return result;\r\n}\r\n```\n---\n\nUser 'zigang93' said:\n---\n> As @poteto mentioned, this is a legitimate error in that it accesses a ref during render. It also doesn\u2019t actually always return the previous value!\r\n> \r\n> To implement this pattern correctly - safely returning the previous value - you could use the following:\r\n> \r\n> ```js\r\n> function usePrevious(value: T): T {\r\n>   const [previous, setPrevious] = useState(value);\r\n>   const [current, setCurrent] = useState(value);\r\n>   let result = previous;\r\n>   if (value !== current) {\r\n>     setPrevious(current);\r\n>     setCurrent(value);\r\n>     result = current;\r\n>   }\r\n>   return result;\r\n> }\r\n> ```\r\n\r\nWe use a ref to hold the previous value because refs don't cause a re-render when their value updates. This differs from state variables, which do cause a re-render when changed. Since we want to avoid re-rendering when the previous value changes, refs are ideal for this purpose.\r\n\r\nI did solve it by just changed to:\r\n\r\n```tsx\r\nexport function usePrevious<T>(value: T)  {\r\n  const ref = useRef<T>()\r\n  useEffect(() => {\r\n    ref.current = value\r\n  }, [value])\r\n  return ref?.current // this silent the error\r\n}\r\n```\r\ncredit to this post when I was try to refer `usePrevious`:  https://www.dhiwise.com/post/supercharging-your-react-components-with-the-useprevious-hooks.\r\n\r\nP/S: I had tried your `useState` solution, it cause infinity loop and crash my app due to too many re-render.\r\n\r\n---\r\n\r\n_\"It also doesn\u2019t actually always return the previous value!\"_ \r\n\r\nbut I can sure that I able to get previous value from init. I am not sure that this behaviour will be changed when update to react 19 stable.\n---\n\nUser 'josephsavona' said:\n---\n> I did solve it by just changed to:\r\n\r\nYeah, this is still unsafe and likely to miss updates. It's still reading from the ref during render, without anything to ensure that the value actually updates.\r\n\r\n> P/S: I had tried your useState solution, it cause infinity loop and crash my app due to too many re-render.\r\n\r\nThis is another indication that what you're doing is a bit tricky. The only reason you'd get too many re-renders is if the input value is constantly changing. But if the input value is constantly changing, then what does \"using the previous value\" even mean? It would just be whatever value you happened to render with, and not indicate any kind of meaningful semantic change.\r\n\r\nIt would really help to step back and understand what exactly you're trying to achieve with `usePrevious`. What are you trying to do? More context would help.\n---\n\nUser 'mpressmar' said:\n---\nWe are also using this usePrevious hook implementation frequently. The purpose of its returned value is not to be accessed during render but within a `useEffect` hook for a more fine-grained control of how to react to which changes.\r\n\r\nAs an example, let's say a useEffect should perform logic only when an entry has been pushed into an array:\r\n\r\n```\r\n    const previousList = usePrevious(list);\r\n    useEffect(() => {\r\n        if (previousList && previousList.length < list.length) {\r\n            // ...\r\n        }\r\n    }, [previousList, list]);\r\n```\r\n\r\nThe hook is also useful when a hook has other variables as dependencies but should react differently to their changes or ignore their changes overall (while still using their most recent version). For example:\r\n\r\n```\r\n    const previousA = usePrevious(a);\r\n    const previousB = usePrevious(b);\r\n    useEffect(() => {\r\n        if (a !== previousA) {\r\n            onUpdateA(a, b);\r\n        }\r\n       if (b !== previousB) {\r\n           onUpdateB(a, b);\r\n       }\r\n    }, [a, previousA, b, previousB, onUpdateA, onUpdateB]);\r\n```\r\n\r\n\r\nBoth of the usages I mentioned above can be expressed differently with a `useRef` but imho with worse readability.\n---\n\nUser 'Lonli-Lokli' said:\n---\nIs there safe implementation for [useIsFirstRender](https://github.com/antonioru/beautiful-react-hooks/blob/master/src/useIsFirstRender.ts)? \r\n\r\nThis is so strange that React is forcing everyone to change their code for no reason... Without moving React closer to what devs wants\n---\n\nUser 'poteto' said:\n---\nJust to be clear, it is [not necessary](https://react.dev/learn/react-compiler#what-does-the-compiler-do) for the compiler to compile every single component/hook before you see performance improvements. When the compiler bails out due to a rule of React violation, it leaves it untouched.\r\n\r\nWhile `useIsFirstRender` or `usePrevious` will not be compiled, it would not prevent a component elsewhere that uses that hook from being compiled.\n---\n\nUser 'eduter' said:\n---\n> We are also using this usePrevious hook implementation frequently. The purpose of its returned value is not to be accessed during render but within a `useEffect` hook for a more fine-grained control of how to react to which changes.\n\n@mpressmar I was trying to solve the same problem and came up with this hook. It only accesses `ref.current` within effects, but still allows your effects to access the previous values of all its dependencies.\n\n```ts\n/**\n * Similar to useEffect, but also provides the previous values of the\n * dependencies so that you have more control over when you run your\n * side effects.\n *\n * NOTE: To make sure you don't forget to add dependencies to your\n *   useEffectWithPrevious, add the following to your eslint config:\n *     \"react-hooks/exhaustive-deps\": [\n *       \"warn\",\n *       { \"additionalHooks\": \"useEffectWithPrevious\" }\n *     ],\n */\nfunction useEffectWithPrevious<T extends DependencyList>(\n    effect: (prevDeps: T | []) => void | Destructor,\n    deps: [...T]\n) {\n    const prevDepsRef = useRef<T>();\n    const stableEffect = useRef(effect);\n\n    useEffect(() => {\n        stableEffect.current = effect;\n    }, [effect]);\n\n    useEffect(() => {\n        const prevDeps = prevDepsRef.current;\n\n        if (!prevDeps || deps.some((dep, i) => !Object.is(dep, prevDeps[i]))) {\n            stableEffect.current(prevDeps ?? []);\n            prevDepsRef.current = deps;\n        }\n    });\n}\n\ntype Destructor = () => void;\n```\n\nSample usage:\n```ts\nuseEffectWithPrevious(\n    ([previousList]) => {\n        if (previousList && previousList.length < list.length) {\n            onListIncrease(list);\n        }\n    },\n    [list, onListIncrease]\n);\n\nuseEffectWithPrevious(\n    ([previousA, previousB]) => {\n        if (a !== previousA) {\n            onUpdateA(a, b);\n        }\n        if (b !== previousB) {\n            onUpdateB(a, b);\n        }\n    },\n    [a, b, onUpdateA, onUpdateB]\n);\n```\n---",
    "question": "What is the resolution for ': Ref values  may not be accessed during render.'?",
    "ideal_answer": "Returning `ref.current` breaks a rule of React since any consumer of `usePrevious` would be accessing the ref during render.\n\nAs @poteto mentioned, this is a legitimate error in that it accesses a ref during render. It also doesn\u2019t actually always return the previous value!\r\n\nTo implement this pattern correctly - safely returning the previous value - you could use the following:\r\n\n```js\r\nfunction usePrevious(value: T): T {\r\n  const [previous, setPrevious] = useState(value);\r\n  const [current, setCurrent] = useState(value);\r\n  let result = previous;\r\n  if (value !== current) {\r\n    setPrevious(current);\r\n    setCurrent(value);\r\n    result = current;\r\n  }\r\n  return result;\r\n}\r\n```\n\n> As @poteto mentioned, this is a legitimate error in that it accesses a ref during render. It also doesn\u2019t actually always return the previous value!\r\n> \r\n> To implement this pattern correctly - safely returning the previous value - you could use the following:\r\n> \r\n> ```js\r\n> function usePrevious(value: T): T {\r\n>   const [previous, setPrevious] = useState(value);\r\n>   const [current, setCurrent] = useState(value);\r\n>   let result = previous;\r\n>   if (value !== current) {\r\n>     setPrevious(current);\r\n>     setCurrent(value);\r\n>     result = current;\r\n>   }\r\n>   return result;\r\n> }\r\n> ```\r\n\nWe use a ref to hold the previous value because refs don't cause a re-render when their value updates. This differs from state variables, which do cause a re-render when changed. Since we want to avoid re-rendering when the previous value changes, refs are ideal for this purpose.\r\n\nI did solve it by just changed to:\r\n\n```tsx\r\nexport function usePrevious<T>(value: T)  {\r\n  const ref = useRef<T>()\r\n  useEffect(() => {\r\n    ref.current = value\r\n  }, [value])\r\n  return ref?.current // this silent the error\r\n}\r\n```\r\ncredit to this post when I was try to refer `usePrevious`:  https://www.dhiwise.com/post/supercharging-your-react-components-with-the-useprevious-hooks.\r\n\nP/S: I had tried your `useState` solution, it cause infinity loop and crash my app due to too many re-render.\r\n\n_\"It also doesn\u2019t actually always return the previous value!\"_ \r\n\nbut I can sure that I able to get previous value from init. I am not sure that this behaviour will be changed when update to react 19 stable.\n\n> I did solve it by just changed to:\r\n\nYeah, this is still unsafe and likely to miss updates. It's still reading from the ref during render, without anything to ensure that the value actually updates.\r\n\n> P/S: I had tried your useState solution, it cause infinity loop and crash my app due to too many re-render.\r\n\nThis is another indication that what you're doing is a bit tricky. The only reason you'd get too many re-renders is if the input value is constantly changing. But if the input value is constantly changing, then what does \"using the previous value\" even mean? It would just be whatever value you happened to render with, and not indicate any kind of meaningful semantic change.\r\n\nIt would really help to step back and understand what exactly you're trying to achieve with `usePrevious`. What are you trying to do? More context would help.\n\nWe are also using this usePrevious hook implementation frequently. The purpose of its returned value is not to be accessed during render but within a `useEffect` hook for a more fine-grained control of how to react to which changes.\r\n\nAs an example, let's say a useEffect should perform logic only when an entry has been pushed into an array:\r\n\n```\r\n    const previousList = usePrevious(list);\r\n    useEffect(() => {\r\n        if (previousList && previousList.length < list.length) {\r\n            // ...\r\n        }\r\n    }, [previousList, list]);\r\n```\r\n\nThe hook is also useful when a hook has other variables as dependencies but should react differently to their changes or ignore their changes overall (while still using their most recent version). For example:\r\n\n```\r\n    const previousA = usePrevious(a);\r\n    const previousB = usePrevious(b);\r\n    useEffect(() => {\r\n        if (a !== previousA) {\r\n            onUpdateA(a, b);\r\n        }\r\n       if (b !== previousB) {\r\n           onUpdateB(a, b);\r\n       }\r\n    }, [a, previousA, b, previousB, onUpdateA, onUpdateB]);\r\n```\r\n\nBoth of the usages I mentioned above can be expressed differently with a `useRef` but imho with worse readability.\n\nIs there safe implementation for [useIsFirstRender](https://github.com/antonioru/beautiful-react-hooks/blob/master/src/useIsFirstRender.ts)? \r\n\nThis is so strange that React is forcing everyone to change their code for no reason... Without moving React closer to what devs wants\n\nJust to be clear, it is [not necessary](https://react.dev/learn/react-compiler#what-does-the-compiler-do) for the compiler to compile every single component/hook before you see performance improvements. When the compiler bails out due to a rule of React violation, it leaves it untouched.\r\n\nWhile `useIsFirstRender` or `usePrevious` will not be compiled, it would not prevent a component elsewhere that uses that hook from being compiled.\n\n> We are also using this usePrevious hook implementation frequently. The purpose of its returned value is not to be accessed during render but within a `useEffect` hook for a more fine-grained control of how to react to which changes.\n\n@mpressmar I was trying to solve the same problem and came up with this hook. It only accesses `ref.current` within effects, but still allows your effects to access the previous values of all its dependencies.\n\n```ts\n/**\n * Similar to useEffect, but also provides the previous values of the\n * dependencies so that you have more control over when you run your\n * side effects.\n *\n * NOTE: To make sure you don't forget to add dependencies to your\n *   useEffectWithPrevious, add the following to your eslint config:\n *     \"react-hooks/exhaustive-deps\": [\n *       \"warn\",\n *       { \"additionalHooks\": \"useEffectWithPrevious\" }\n *     ],\n */\nfunction useEffectWithPrevious<T extends DependencyList>(\n    effect: (prevDeps: T | []) => void | Destructor,\n    deps: [...T]\n) {\n    const prevDepsRef = useRef<T>();\n    const stableEffect = useRef(effect);\n\n    useEffect(() => {\n        stableEffect.current = effect;\n    }, [effect]);\n\n    useEffect(() => {\n        const prevDeps = prevDepsRef.current;\n\n        if (!prevDeps || deps.some((dep, i) => !Object.is(dep, prevDeps[i]))) {\n            stableEffect.current(prevDeps ?? []);\n            prevDepsRef.current = deps;\n        }\n    });\n}\n\ntype Destructor = () => void;\n```\n\nSample usage:\n```ts\nuseEffectWithPrevious(\n    ([previousList]) => {\n        if (previousList && previousList.length < list.length) {\n            onListIncrease(list);\n        }\n    },\n    [list, onListIncrease]\n);\n\nuseEffectWithPrevious(\n    ([previousA, previousB]) => {\n        if (a !== previousA) {\n            onUpdateA(a, b);\n        }\n        if (b !== previousB) {\n            onUpdateB(a, b);\n        }\n    },\n    [a, b, onUpdateA, onUpdateB]\n);\n```"
  },
  {
    "id": "gen_nat_035",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: Pods get stuck in ContainerCreating state when pulling image takes long\n\nA user reported the following issue titled 'Pods get stuck in ContainerCreating state when pulling image takes long' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n**What happened**:\r\nIf pulling a docker image takes a bit longer than usual, pods are never created.\r\n\r\n\r\n**What you expected to happen**:\r\nKubernetes to wait or retry pulling.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\nThrottle download speed to the docker registry where you are pulling it from or find one that gives you low bandwidth, e.g. 512kbit/s\r\n\r\n**Anything else we need to know?**:\r\nAn example where it hangs for over 15m already. Of course, if I go to the worker node and do manual docker pull it'll complete within a few minutes. We've seen even longer age since pod creation. And it's been happening on older kubernetes versions as well. I bet it'll be the same on 1.14 and newer.\r\n\r\n```\r\n$ kubectl -n spinnaker describe pod spin-gate-55999bc58-47zz7\r\nName:           spin-gate-55999bc58-47zz7\r\nNamespace:      spinnaker\r\nPriority:       0\r\nNode:           depkbw102/10.16.53.35\r\nStart Time:     Thu, 03 Oct 2019 19:43:27 +0000\r\nLabels:         app=gate\r\n                load-balancer-spin-gate=true\r\n                pod-template-hash=55999bc58\r\nAnnotations:    cni.projectcalico.org/podIP: 10.23.130.18/32\r\n                prometheus.io/path: /prometheus_metrics\r\n                prometheus.io/port: 8008\r\n                prometheus.io/scrape: true\r\nStatus:         Pending\r\nIP:             \r\nIPs:            <none>\r\nControlled By:  ReplicaSet/spin-gate-55999bc58\r\nContainers:\r\n  gate:\r\n    Container ID:   \r\n    Image:          dockerregistry.example.com/devops/spinnaker-gate:v1.15.3-49\r\n    Image ID:       \r\n    Port:           8084/TCP\r\n    Host Port:      0/TCP\r\n    State:          Waiting\r\n      Reason:       ContainerCreating\r\n    Ready:          False\r\n    Restart Count:  0\r\n    Readiness:      http-get https://:8084/health delay=20s timeout=1s period=10s #success=1 #failure=3\r\n    Environment:\r\n      JAVA_OPTS:  -Xms1g -Xmx4g\r\n      DUMMY:      dummy10\r\n    Mounts:\r\n      /opt/spinnaker/certs from spinnaker-ssl (rw)\r\n      /opt/spinnaker/config from spinnaker-config (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gt426 (ro)\r\n  monitoring:\r\n    Container ID:   \r\n    Image:          gcr.io/spinnaker-marketplace/monitoring-daemon:0.14.0-20190702202823\r\n    Image ID:       \r\n    Port:           8008/TCP\r\n    Host Port:      0/TCP\r\n    State:          Waiting\r\n      Reason:       ContainerCreating\r\n    Ready:          False\r\n    Restart Count:  0\r\n    Environment:    <none>\r\n    Mounts:\r\n      /opt/spinnaker-monitoring/config from monitoring-config (rw)\r\n      /opt/spinnaker-monitoring/registry from monitoring-registry (rw)\r\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gt426 (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True \r\n  Ready             False \r\n  ContainersReady   False \r\n  PodScheduled      True \r\nVolumes:\r\n  spinnaker-config:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  spinnaker-config\r\n    Optional:    fals\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'emptywee' said:\n---\n/sig scheduling\n---\n\nUser 'zouyee' said:\n---\n/sig node\r\n/remove-sig scheduling\n---\n\nUser 'zouyee' said:\n---\nhttps://github.com/kubernetes/kubernetes/issues/83348\n---\n\nUser 'emptywee' said:\n---\n> \r\n> \r\n> #83348\r\n\r\nIn our case there's no events and no errors passed from kubelet to the api server. Not sure how #83348 is related. What's helping to start a container is predownload the image on the worker node and kill the pod, hoping that it'll get re-created on the same node. Or, pre-download that image on all workers and kill the pod.\n---\n\nUser 'SataQiu' said:\n---\n/cc\n---\n\nUser 'langyenan' said:\n---\nHave you tried set `--serialize-image-pulls=false` when starting kubelet? \r\n\r\nBy default, that flag is `true`, which means kubelet pulls images one by one. I think the pulling is not stuck, they are just waiting for the current one to be finished.\r\n\r\nbelow is the serial image puller, you see, one slow pulling may stuck all the pullings on the node :\r\nhttps://github.com/kubernetes/kubernetes/blob/ca0e694d637f9e6feffd7330f66b081769c1c91b/pkg/kubelet/images/puller.go#L60-L95\n---\n\nUser 'emptywee' said:\n---\n@langyenan this is definitely interesting... I'll experiment with this flag and see how it improves/worsens our problem :) Thanks!\n---\n\nUser 'emptywee' said:\n---\n@langyenan any idea why this param is not configurable?\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/ca0e694d637f9e6feffd7330f66b081769c1c91b/pkg/kubelet/images/puller.go#L58\n---\n\nUser 'langyenan' said:\n---\n> @langyenan any idea why this param is not configurable?\r\n> \r\n> https://github.com/kubernetes/kubernetes/blob/ca0e694d637f9e6feffd7330f66b081769c1c91b/pkg/kubelet/images/puller.go#L58\r\n\r\nNop, but even if it is configurable, there is still only one pulling at a time.\n---\n\nUser 'emptywee' said:\n---\n@langyenan one pulling at a time even with `--serialize-image-pulls=false` ? What difference is it going to make then?\n---\n\nUser 'langyenan' said:\n---\n> @langyenan one pulling at a time even with `--serialize-image-pulls=false` ? What difference is it going to make then?\r\n\r\nwith `false`, a parallel puller is used:\r\nhttps://github.com/kubernetes/kubernetes/blob/ca0e694d637f9e6feffd7330f66b081769c1c91b/pkg/kubelet/images/puller.go#L39-L55\n---\n\nUser 'emptywee' said:\n---\n@langyenan Oh, I thought that const was also used for parallel pulling. My bad, need more sleep. Thanks for the hint!\n---\n\nUser 'erenatas' said:\n---\nI am having this issue right now. Pods are either stuck at ContainerCreating or Init because it says it's pulling the image.\r\n\r\nI go to the node that it's trying to pull the image and pull the docker image manually. It's still stuck.\n---\n\nUser 'fejta-bot' said:\n---\nIssues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle stale\n---\n\nUser 'franckleveque' said:\n---\nSame problem here, pods get stuck on ContainerCreating step. when trying a kubectl describe pod, it indicate that last event is a Pulling event ie :\r\n\r\nNormal  Pulling    13m   kubelet, franck-lenovo-z70-80  Pulling image \"jboss/keycloak\"\n---\n\nUser 'rajatnt' said:\n---\n> Same problem here, pods get stuck on ContainerCreating step. when trying a kubectl describe pod, it indicate that last event is a Pulling event ie :\r\n> \r\n> Normal Pulling 13m kubelet, franck-lenovo-z70-80 Pulling image \"jboss/keycloak\"\r\n\r\nFacing ditto the same issue. Also, on my all in one Kube setup, I pulled the docker image manually. It's still stuck.\r\n\r\n```\r\nEvents:\r\n  Type    Reason     Age   From                     Message\r\n  ----    ------     ----  ----                     -------\r\n  Normal  Scheduled  29m   default-scheduler        Successfully assigned <namespace>/<pod-that-is-stuck> to docker-desktop\r\n  Normal  Pulling    29m   kubelet, docker-desktop  Pulling image <image-for-the-pod-that-is-stuck>\r\n```\n---\n\nUser 'holmesb' said:\n---\nYes we're also seeing this \"Pulling image\", even when said image already exists on the node that we've manually 'docker pulled'.  Eventually seems to sort itself out and pods start.  If there really is a \"blocking\" image being pulled when `--serialize-image-pulls=true`, then is there any way of detecting which image is being pulled?  Dockerd logs aren't clear.  \r\nIs there a downside of disabling serialize-image-pulls?  [Docs ](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/)don't recommend for older dockerd and aufs storage backend, which we don't have.\n---\n\nUser 'fejta-bot' said:\n---\nStale issues rot after 30d of inactivity.\nMark the issue as fresh with `/remove-lifecycle rotten`.\nRotten issues close after an additional 30d of inactivity.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle rotten\n---\n\nUser 'erenatas' said:\n---\n/remove-lifecycle rotten\r\n\r\nThis issue needs to be taken care of. It is a major problem.\n---\n\nUser 'riking' said:\n---\nJust had to try to support an outage caused by all 2 pods of a deployment getting stuck in image pull. The image is nowhere near gigabytes in size and shouldn't take more than ~2-3min to pull.\n---\n\nUser 'skyfirst93' said:\n---\nI have observed this issue where at least 20 pods were stuck in the same state mentioned in the issue description. \r\nI just restarted the kubelet service and all the pods came up. \r\nIt will be helpful if someone could help on how to prevent this from happening. :/\n---\n\nUser 'danieltroger' said:\n---\nWas this solved? Also experiencing the issue\n---\n\nUser 'Shahard2' said:\n---\ni see this as well on eks (image size is 2GB)\n---\n\nUser 'GertjanBijl' said:\n---\nHad the same issue today on OpenShift 4.3.33, which is Kubernetes Version: v1.16.2+295f6e6. Coincidentally (or not) I also had this while pulling Spinnaker.\r\nDeleting a single pod helps, because that pod came up running after a short while. After deleting all pods one by one, they all pull correctly and started running. I monitored network traffic on my Squid-proxy, and I saw no traffic at all while the pods were in 'ContainerCreating' status and according the status they were pulling the image, but apparently they were actually not.\n---\n\nUser 'paol' said:\n---\nJust had this problem affecting 6 nodes of a 7 node cluster. Restarting the kubelet service appears to clear the problem.\r\n\r\nKubernetes v1.18.3\r\n\r\nEdit:\r\nThis correlates with the following error message repeating over and over in the kubelet logs:\r\n```\r\nFailed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats for \"/system.slice/docker.service\": failed to get container info for \"/system.slice/docker.service\": unknown container \"/system.slice/docker.service\"\r\n```\r\n\r\nFound issue 56850 related to this message, but it's quite old and specific to RedHat (we're on Ubuntu server 18.04) so I don't think it's exactly the same.\n---\n\nUser 'Shahard2' said:\n---\n> Just had this problem affecting 6 nodes of a 7 node cluster. Restarting the kubelet service appears to clear the problem.\r\n> \r\n> Kubernetes v1.18.3\r\n\r\nand if I'm on eks ?\n---\n\nUser 'harrypalheta' said:\n---\nI did as @paol said, i restarted the kubelet service int the node:\r\n`systemctl restart kubelet`\r\n\r\nI downloaded the image manually.\r\n\r\nThen I deleted the stuck pod and it worked!\n---\n\nUser 'fejta-bot' said:\n---\nIssues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n/lifecycle stale\n---\n\nUser 'krystiannowak' said:\n---\n/remove-lifecycle stale\n---\n\nUser 'Frederik-Baetens' said:\n---\nJust had this issue on GKE 1.20, about 50 pods took about an hour to get pulled from various registries, including docker and azure before everything worked.\n---",
    "question": "In kubernetes/kubernetes, Pods get stuck in ContainerCreating state when pulling image takes long. Any ideas why?",
    "ideal_answer": "/remove-sig scheduling\n\nhttps://github.com/kubernetes/kubernetes/issues/83348\n\n> \r\n> \r\n> #83348\r\n\nIn our case there's no events and no errors passed from kubelet to the api server. Not sure how #83348 is related. What's helping to start a container is predownload the image on the worker node and kill the pod, hoping that it'll get re-created on the same node. Or, pre-download that image on all workers and kill the pod.\n\n/cc\n\nHave you tried set `--serialize-image-pulls=false` when starting kubelet? \r\n\nBy default, that flag is `true`, which means kubelet pulls images one by one. I think the pulling is not stuck, they are just waiting for the current one to be finished.\r\n\nbelow is the serial image puller, you see, one slow pulling may stuck all the pullings on the node :\r\nhttps://github.com/kubernetes/kubernetes/blob/ca0e694d637f9e6feffd7330f66b081769c1c91b/pkg/kubelet/images/puller.go#L60-L95\n\n@langyenan this is definitely interesting... I'll experiment with this flag and see how it improves/worsens our problem :) Thanks!\n\n@langyenan any idea why this param is not configurable?\r\n\nhttps://github.com/kubernetes/kubernetes/blob/ca0e694d637f9e6feffd7330f66b081769c1c91b/pkg/kubelet/images/puller.go#L58\n\n> @langyenan any idea why this param is not configurable?\r\n> \r\n> https://github.com/kubernetes/kubernetes/blob/ca0e694d637f9e6feffd7330f66b081769c1c91b/pkg/kubelet/images/puller.go#L58\r\n\nNop, but even if it is configurable, there is still only one pulling at a time.\n\n@langyenan one pulling at a time even with `--serialize-image-pulls=false` ? What difference is it going to make then?\n\n> @langyenan one pulling at a time even with `--serialize-image-pulls=false` ? What difference is it going to make then?\r\n\nwith `false`, a parallel puller is used:\r\nhttps://github.com/kubernetes/kubernetes/blob/ca0e694d637f9e6feffd7330f66b081769c1c91b/pkg/kubelet/images/puller.go#L39-L55\n\n@langyenan Oh, I thought that const was also used for parallel pulling. My bad, need more sleep. Thanks for the hint!\n\nI am having this issue right now. Pods are either stuck at ContainerCreating or Init because it says it's pulling the image.\r\n\nI go to the node that it's trying to pull the image and pull the docker image manually. It's still stuck.\n\nIssues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\n\nIf this issue is safe to close now please do so with `\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle stale\n\nSame problem here, pods get stuck on ContainerCreating step. when trying a kubectl describe pod, it indicate that last event is a Pulling event ie :\r\n\nNormal  Pulling    13m   kubelet, franck-lenovo-z70-80  Pulling image \"jboss/keycloak\"\n\n> Same problem here, pods get stuck on ContainerCreating step. when trying a kubectl describe pod, it indicate that last event is a Pulling event ie :\r\n> \r\n> Normal Pulling 13m kubelet, franck-lenovo-z70-80 Pulling image \"jboss/keycloak\"\r\n\nFacing ditto the same issue. Also, on my all in one Kube setup, I pulled the docker image manually. It's still stuck.\r\n\n```\r\nEvents:\r\n  Type    Reason     Age   From                     Message\r\n  -         -  -                     -\r\n  Normal  Scheduled  29m   default-scheduler        Successfully assigned <namespace>/<pod-that-is-stuck> to docker-desktop\r\n  Normal  Pulling    29m   kubelet, docker-desktop  Pulling image <image-for-the-pod-that-is-stuck>\r\n```\n\nYes we're also seeing this \"Pulling image\", even when said image already exists on the node that we've manually 'docker pulled'.  Eventually seems to sort itself out and pods start.  If there really is a \"blocking\" image being pulled when `--serialize-image-pulls=true`, then is there any way of detecting which image is being pulled?  Dockerd logs aren't clear.  \r\nIs there a downside of disabling serialize-image-pulls?  [Docs ](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/)don't recommend for older dockerd and aufs storage backend, which we don't have.\n\nStale issues rot after 30d of inactivity.\nMark the issue as fresh with `/remove-lifecycle rotten`.\nRotten issues close after an additional 30d of inactivity.\n\nIf this issue is safe to close now please do so with `\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle rotten\n\n/remove-lifecycle rotten\r\n\nThis issue needs to be taken care of. It is a major problem.\n\nJust had to try to support an outage caused by all 2 pods of a deployment getting stuck in image pull. The image is nowhere near gigabytes in size and shouldn't take more than ~2-3min to pull.\n\nI have observed this issue where at least 20 pods were stuck in the same state mentioned in the issue description. \r\nI just restarted the kubelet service and all the pods came up. \r\nIt will be helpful if someone could help on how to prevent this from happening. :/\n\nWas this solved? Also experiencing the issue\n\ni see this as well on eks (image size is 2GB)\n\nHad the same issue today on OpenShift 4.3.33, which is Kubernetes Version: v1.16.2+295f6e6. Coincidentally (or not) I also had this while pulling Spinnaker.\r\nDeleting a single pod helps, because that pod came up running after a short while. After deleting all pods one by one, they all pull correctly and started running. I monitored network traffic on my Squid-proxy, and I saw no traffic at all while the pods were in 'ContainerCreating' status and according the status they were pulling the image, but apparently they were actually not.\n\nJust had this problem affecting 6 nodes of a 7 node cluster. Restarting the kubelet service appears to clear the problem.\r\n\nKubernetes v1.18.3\r\n\nEdit:\r\nThis correlates with the following error message repeating over and over in the kubelet logs:\r\n```\r\nFailed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats for \"/system.slice/docker.service\": failed to get container info for \"/system.slice/docker.service\": unknown container \"/system.slice/docker.service\"\r\n```\r\n\nFound issue 56850 related to this message, but it's quite old and specific to RedHat (we're on Ubuntu server 18.04) so I don't think it's exactly the same.\n\n> Just had this problem affecting 6 nodes of a 7 node cluster. Restarting the kubelet service appears to clear the problem.\r\n> \r\n> Kubernetes v1.18.3\r\n\nand if I'm on eks ?\n\nI did as @paol said, i restarted the kubelet service int the node:\r\n`systemctl restart kubelet`\r\n\nI downloaded the image manually.\r\n\nThen I deleted the stuck pod and it worked!\n\nIssues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\n\nIf this issue is safe to close now please do so with `\nSend feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n/lifecycle stale\n\n/remove-lifecycle stale\n\nJust had this issue on GKE 1.20, about 50 pods took about an hour to get pulled from various registries, including docker and azure before everything worked."
  },
  {
    "id": "gen_nat_036",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [Compiler] Reading external mutable state\n\nA user reported the following issue titled '[Compiler] Reading external mutable state' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhHANgQzGABAFQTABddgAdAO11zgkpJijmIhgAoBKM3YgCwCWYAHQB9AG6Z0UIrgC8ZAL4BuXIqo0A5gmLsBAE24VqNXOh24YshfyFjJ0ogG0DAXWUbTAgGa52AQiswbiD5HgByYgQAD2Jw5Fxw8LUPExorYlhqINSadRMwHT19ABpeGOIjTxpzUgQw4ESo2Pjy2JTq3kERCSkZMBd9VzCEXLUqFSoqSRhykjDKBAB3AiJdTlSqGIAHNlJvKEoWAXpVkgBhCABbXcXKXUbFKpNyEChC3EoIXCuEK4hXmM6AxSE4oNt9JgomVvGxEABVCFQhDDBTvBAAZWIyPYxCYCA2nlquAMYQAjGNiQAjMJREjCbS6AyEkzEhDoMLsAA8hFiADEBOz9LgANYIACecnC3kF6H0ySwVPZUsIJAElE0uBlQuSDhkcmAVOEzWIilw9HOfEwGoQBvYBPkAD4eHTiMJCkzSrgEMbMDBGcI9QTVLD4AhEZCogFwZHg2ozQB6R0s9I6LJ+Ln6ATiR3AdmKLkJrM5lmTShTRbRXYwUj6BDeTBQdD7Q7HU4AWXFAEFttsuGRPBl0zy1pcbvQEPdcEnUvkQCU0PQZZoUCABOOa7xxdt6o0AArSTTqgDy22IJwYai1MGuiSpmCV6AAtNtD+qn1ZMCwn3QbgJzDARZCHEmyUOwxg0AmCa-ts-5Qhe7YQHWCSvFI6CvBMuBgPBYDange5+ueUhcgeUBHpQp7nvQYDJso87gHwEBLAAkvcCAwJQUhgCgDboIUihAA\n\n### Repro steps\n\nSee the playground above - if you remove \"use no memo\", the generated code caches away the calls to test.get(1) as it incorrectly thinks that it always returns the same thing, and as a result, the TextField doesn't get updated properly.\n\n\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.1\n\n### What version of React Compiler are you using?\n\n19.1.0-rc.3\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'josephsavona' said:\n---\nThanks for posting. Note that React requires that components and hooks are pure functions of props and state. Reading external mutable values \u2014 such as `test.get(..)` in your example \u2014 violates this requirement and can cause your component to update (or fail to update). The example code would also fail with standard memoization, eg `useMemo(() => test.get(id), [test, id])` would only update when `test` or `id` changes. In this way, you can think about the compiler as yet another tool for detecting places where you're violating the rules of React. \n\nIs there a specific library that you're using which encourages this pattern?\n---\n\nUser 'mecirt' said:\n---\nNo library, no, this is a simplified version of our internal code that we are currently porting from class components to functional ones. I will note that this issue also occurs if you pass \"test\" as a prop - the compiler doesn't detect that calling test.set alters the value returned by test.get.\n---\n\nUser 'josephsavona' said:\n---\nThanks for clarifying!\n\n> the compiler doesn't detect that calling test.set alters the value returned by test.get.\n\nYup, that\u2019s expected. In React, component props and hook arguments may not be mutated. The way to model this in React is to use state, and setState with a new value on change.\n---\n\nUser 'mecirt' said:\n---\nWell, it has worked perfectly fine until now (and it continues to work fine after you disable the compiler), and so I imagine that this is going to trip up a lot of projects. I was at least lucky in that my component was fairly small, debugging an error of this kind on something big might be quite a nightmare.\n\nI don't know how feasible it is, but such a major behavioral change really needs to show a warning at the very least, same as e.g. hooks show one if you try to use them in a class component.\n---\n\nUser 'marcocondrache' said:\n---\n@mecirt React always expects components to be pure functions, which is a fundamental requirement. This is a constraint that the compiler respects. Just because it works doesn't mean it is correct. Implementing a warning to detect side effects in a function is not as straightforward as it may seem. \n\nhttps://react.dev/learn/keeping-components-pure\n---\n\nUser '0xShubhamSolanki' said:\n---\nI agree with @marcocondrache that enforcing purity is core to React's model, and the compiler is essentially surfacing a latent issue in the code. That said, @mecirt makes a fair point about migration pains for larger codebases transitioning from class components, where side effects like this might have slipped through without immediate breakage.\nTo mitigate this, you could consider wrapping the mutable state in a React-friendly way, such as using a context provider with useState or useReducer to manage the test object's data. This ensures changes trigger proper re-renders without violating purity.  \nFor example (rough sketch):\n```jsx \n import { createContext, useContext, useState } from 'react';\n\nconst TestContext = createContext();\n\nfunction TestProvider({ children }) {\n  const [data, setData] = useState(new Map());\n\n  const get = (id) => data.get(id) || '';\n  const set = (id, val) => setData((prev) => new Map(prev).set(id, val));\n\n  return <TestContext.Provider value={{ get, set }}>{children}</TestContext.Provider>;\n}\n\n// In your component\nfunction MyComponent() {\n  const { get, set } = useContext(TestContext);\n  const value = get(1);\n\n  // ... rest of component\n}  `\n\nThis way, updates via set will cause re-renders where needed, and the compiler should handle it correctly. If this pattern is widespread in your codebase, auditing with eslint-plugin-react-compiler might help flag similar spots early.\nCurious if the team has plans for more diagnostics in the compiler for common anti-patterns like this?\n---\n\nUser 'josephsavona' said:\n---\n> Curious if the team has plans for more diagnostics in the compiler for common anti-patterns like this?\n\nWe already detest direct mutation of props, such as `props.key = \u2026` or `props.foo[\u2018bar\u2019] = \u2026`. We can\u2019t realistically check within arbitrary functions that you call during render - this is why from the earliest days of React we have stressed that props and state should be immutable. \n\nI\u2019m gonna close since there isn\u2019t really any additional action we take here.\n---",
    "question": "Has anyone seen 'Reading external mutable state' in facebook/react? I can't figure out the cause.",
    "ideal_answer": "Thanks for posting. Note that React requires that components and hooks are pure functions of props and state. Reading external mutable values \u2014 such as `test.get(..)` in your example \u2014 violates this requirement and can cause your component to update (or fail to update). The example code would also fail with standard memoization, eg `useMemo(() => test.get(id), [test, id])` would only update when `test` or `id` changes. In this way, you can think about the compiler as yet another tool for detecting places where you're violating the rules of React. \n\nIs there a specific library that you're using which encourages this pattern?\n\nNo library, no, this is a simplified version of our internal code that we are currently porting from class components to functional ones. I will note that this issue also occurs if you pass \"test\" as a prop - the compiler doesn't detect that calling test.set alters the value returned by test.get.\n\nThanks for clarifying!\n\n> the compiler doesn't detect that calling test.set alters the value returned by test.get.\n\nYup, that\u2019s expected. In React, component props and hook arguments may not be mutated. The way to model this in React is to use state, and setState with a new value on change.\n\nWell, it has worked perfectly fine until now (and it continues to work fine after you disable the compiler), and so I imagine that this is going to trip up a lot of projects. I was at least lucky in that my component was fairly small, debugging an error of this kind on something big might be quite a nightmare.\n\nI don't know how feasible it is, but such a major behavioral change really needs to show a warning at the very least, same as e.g. hooks show one if you try to use them in a class component.\n\n@mecirt React always expects components to be pure functions, which is a fundamental requirement. This is a constraint that the compiler respects. Just because it works doesn't mean it is correct. Implementing a warning to detect side effects in a function is not as straightforward as it may seem. \n\nhttps://react.dev/learn/keeping-components-pure\n\nI agree with @marcocondrache that enforcing purity is core to React's model, and the compiler is essentially surfacing a latent issue in the code. That said, @mecirt makes a fair point about migration pains for larger codebases transitioning from class components, where side effects like this might have slipped through without immediate breakage.\nTo mitigate this, you could consider wrapping the mutable state in a React-friendly way, such as using a context provider with useState or useReducer to manage the test object's data. This ensures changes trigger proper re-renders without violating purity.  \nFor example (rough sketch):\n```jsx \n import { createContext, useContext, useState } from 'react';\n\nconst TestContext = createContext();\n\nfunction TestProvider({ children }) {\n  const [data, setData] = useState(new Map());\n\n  const get = (id) => data.get(id) || '';\n  const set = (id, val) => setData((prev) => new Map(prev).set(id, val));\n\n  return <TestContext.Provider value={{ get, set }}>{children}</TestContext.Provider>;\n}\n\n// In your component\nfunction MyComponent() {\n  const { get, set } = useContext(TestContext);\n  const value = get(1);\n\n  // ... rest of component\n}  `\n\nThis way, updates via set will cause re-renders where needed, and the compiler should handle it correctly. If this pattern is widespread in your codebase, auditing with eslint-plugin-react-compiler might help flag similar spots early.\nCurious if the team has plans for more diagnostics in the compiler for common anti-patterns like this?\n\n> Curious if the team has plans for more diagnostics in the compiler for common anti-patterns like this?\n\nWe already detest direct mutation of props, such as `props.key = \u2026` or `props.foo[\u2018bar\u2019] = \u2026`. We can\u2019t realistically check within arbitrary functions that you call during render - this is why from the earliest days of React we have stressed that props and state should be immutable. \n\nI\u2019m gonna close since there isn\u2019t really any additional action we take here."
  },
  {
    "id": "gen_nat_037",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [Compiler]: All comments removed from source code\n\nA user reported the following issue titled '[Compiler]: All comments removed from source code' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [X] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvhgJcsNgB4AfOPwr8cgMIQAtloR1cYblIBKCLRABuCQvgD0SiSo3bd+w1IAKUsAhhWb9spq9gDc4gC+4uK0DMys+Jo6egZGCKbm-kIiQXYAVPi4MGR0YJRkuDhgyPgApFX4UD4wdGS6AHT4ACoAFkxg+HAuyfgA7kyUlPgARgiSZpbW+Lm2QYMlBLgYBAC8+AD6e-wA5AASCBMQtWBHgmF0Qba2Xb39g0n6o+OTZJSQ07NSDLWNrKIJSGTNNSEJgWBTATboXDhOS2aGwu6Re50GKMFhsRKuFKeby+TLCMQSUQgRqzOiXXTmKl3FQPfKFYqlcqVap1ao05qtBAdHp9AZDD5jCb4OkEGZzIE2ZarVhgDZbfC7A7HM4XK43O45R7PUVvQmfKU-P5ygDWCEwuBBlIk4NkUJhcIRSJRaIUGJA4SAA\n\n### Repro steps\n\nI setup my Babel & webpack config to preserve comments, specifically comments for translators.\r\n\r\nIn systems like WordPress, i18n strings & comments are directly in the source code and must be preserved.\r\n\r\nWhen React Compiler optimizes a component, all comments will be stripped.\r\n\r\nWhen React Compiler is disabled, e.g. with `use no memo` or when it encounters a disabled lint rule, then the component won't be touched at all and comments are preserved as expected.\r\n\r\nUnfortunately the Playground link doesn't show this, as it seems to strip all comments no matter what.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n17\n\n### What version of React Compiler are you using?\n\n0.0.0-experimental-34d04b6-20241024\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'josephsavona' said:\n---\nThanks for posting. I can imagine it\u2019s frustrating to have spent a lot of effort making your translations work via comments and to have the compiler break this. \r\n\r\nThe short answer is: we can't preserve comments in a way that works for all the schemes people have for them. This is because comments sort of _visually_ correspond to a part of the program, but this is not precise enough for tooling. If you've ever seen a comment get moved from something like prettier, this is why. \r\n\r\nTo make this concrete, consider a slightly tweaked snippet from your example:\r\n\r\n\r\n```js\r\n// translators: %s is username\r\nconst element = <div>{__s(\u201c...%s...\u201d)}</div>;\r\n```\r\n\r\nA human knows that this comment is really referring to the `__s(\"...\")` call expression. But JavaScript tooling just sees that this comment precedes the variable declaration statement. \r\n\r\nBecause comments have no semantic meaning in JavaScript -- they do not affect runtime behavior of the program -- it's perfectly fine for a tool to rewrite this code as follows:\r\n\r\n```js\r\nconst t0 = __s(\u201c...%s...\u201d);\r\n// translators: %s is username\r\nconst element = <div>{t0}</div>;\r\n```\r\n\r\nAs you can see, it isn't enough to preserve comments. This particular comment scheme is meant to apply comment information to strings inside calls. So you might say, just move the comment up one line! But someone else might have a custom comment scheme that is meant to describe variable declarations, and then it would break if we moved the comment up! . There's literally no way for an arbitrary tool to know how to \"correctly\" preserve comments while rewriting code. \r\n\r\nThis is why JavaScript -- like effectively all modern languages -- makes comments semantically meaningless. This makes it safe to not preserve comments, or to make a best-effort. Many languages also typically carve out a safe space for semantic annotations. Rust has `#` syntax, JS has \"use strict\" style string directives as an unofficial pattern, etc. \r\n\r\nSo I hope this helps to put the existing behavior in context. We understand that it would be convenient if the compiler preserved comments, but it is impossible to do so in a general purpose way that everyone would be happy with. There isn't even a good-enough heuristic. As such, we do not and will not support preserving comments. \r\n\r\nIn contrast, type annotations provide semantic information in a structured form. We currently preserve some type annotations, and would consider supporting preserving them fully. \r\n\r\nWe would recommend exploring i18n approaches that do not depend on comments, and instead put the information about the meaning of the various interpolations (\"%s\") into the program itself. There are a number of popular libraries for this.\n---\n\nUser 'swissspidy' said:\n---\nThanks for the thorough response!\r\n\r\nI feared as much. While I agree that in a JS world comments are not the best way for such annotations, changing the i18n approach is unfortunately easier said than done. I'm sure you can understand that doing so in an ecosystem as big as WordPress is an enormous undertaking. The React-based Gutenberg editor is currently [exploring using React Compiler](https://github.com/WordPress/gutenberg/pull/66361), and this is a regression from an i18n perspective. While we can probably find a workaround there, thousands of extensions (plugins) will have to do the same.\r\n\r\nWould you be open to consider some middleground like preserving specific types of comments (e.g. `/*! preserve me */) or if they are within a function call itself (e.g. `__( /* translators: %s: username */ 'Hello %s'  )` or something?\n---\n\nUser 'tyxla' said:\n---\nI second @swissspidy here. Finding a workaround is not a challenge at all, but propagating it through hundreds of thousands of third-party projects (many of which don't even use React or will not adopt React 19 or the React Compiler) with as many third-party developers accumulating over the past ~20 years is doomed to failure. \r\n\r\nAn ideal solution would be to allow the Babel plugin to provide an environment configuration variable that enables skipping some comments. There's already a precedent for that with the `@enableMemoizationComments` flag:\r\n\r\nhttps://github.com/facebook/react/blob/fe04dbcbc4185d7c9d7afebbe18589d2b681a88c/compiler/packages/babel-plugin-react-compiler/src/HIR/Environment.ts#L458C3-L458C28\r\n\r\nWould it be feasible to consider providing a similar flag that lets us declare an array of strings or regexes, and the React Compiler would skip stripping comments that have a match?\n---\n\nUser 'guillaumebrunerie' said:\n---\nHow are these special comments used? Is there a tool that automatically extracts them into some more usable format for translators? If so, can\u2019t this tool be run before the React compiler instead? Surely there are a bunch of other build steps (like minifiers and bundlers) that already remove comments.\n---\n\nUser 'josephsavona' said:\n---\n> An ideal solution would be to allow the Babel plugin to provide an environment configuration variable that enables skipping some comments. There's already a precedent for that with the @enableMemoizationComments flag\r\n\r\nJust a quick note that this flag isn\u2019t skipping comments, it tells the compiler to _synthesize_ comments describing what it\u2019s memorizing. You can see this for yourself by adding `// @enableMemoizationComments` as the first line in the playground. \r\n\r\nThe compiler transforms the Babel AST into our own internal representation (called HIR), runs an extensive number of passes, and then completely rebuilds the AST. Per my comment above, it is impossible for us to preserve comments in this process in a way that would work with every scheme you could come up with for comments. That is why the spec specifically makes comments semantically meaningless: otherwise it\u2019s just crazy town with everyone inventing different incompatible comment schemes and tooling literally can\u2019t touch your code. \r\n\r\n> If so, can\u2019t this tool be run before the React compiler instead? \r\n\r\nYes! This is what I would explore.\n---\n\nUser 'swissspidy' said:\n---\nApologies for the late response.\r\n\r\n> How are these special comments used? Is there a tool that automatically extracts them into some more usable format for translators?\r\n\r\nYes, exactly, it's a string extraction tool that then transfers the found strings to a translation platform or in a gettext POT file.\r\n\r\n> If so, can\u2019t this tool be run before the React compiler instead? Surely there are a bunch of other build steps (like minifiers and bundlers) that already remove comments.\r\n\r\nWe have set up our existing tooling (babel, webpack) so that translator comments are preserved. That works quite well so far.\r\n\r\nIn theory extracting the string from the source files (or finding any other solution) would be better for sure, but this is not feasible for the WordPress ecosystem, as pointed out by @tyxla. For WordPress core itself yes, there might be alternative approaches we could take in the short to medium term, as we are in full control there. But for the hundreds of thousands of plugins out there that's nearly impossible. And having two separate implementations just causes fragmentation.\n---\n\nUser 'nathanmarks' said:\n---\n@josephsavona Is this why I'm running into some weird issues where sometimes istanbul is not instrumenting the code properly? I'm seeing statements, functions, etc just not even getting recognized full stop.\r\n\r\nI spent a few hours diving into it. I first noticed that the statement/function/branch info instanbul injects into the file is completely missing things, (along with source location not being correct for some that did make it).\r\n\r\nIt looks like when the babel instanbul plugin runs, node visitors are simply not getting triggered for some things.\r\n\r\nFor example, given this super simple example:\r\n```ts\r\nexport default function useSomething() {\r\n  const isMobile = useIsMobile()\r\n\r\n  return isMobile\r\n}\r\n```\r\n\r\n`istanbul` is expecting these visitors ([1](https://github.com/istanbuljs-archived-repos/istanbul-lib-instrument/blob/master/src/visitor.js#L424), [2](https://github.com/istanbuljs-archived-repos/istanbul-lib-instrument/blob/master/src/visitor.js#L420)) to get invoked, and they never do when the react compiler babel plugin runs first. I also noticed that the AST is missing stuff like source location info for nodes etc.\r\n\r\nOut of the box, `jest` appends istanbul to the rest of the babel plugins.\n---\n\nUser 'josephsavona' said:\n---\n@swissspidy thanks for the extra context.\r\n\r\n> But for the hundreds of thousands of plugins out there that's nearly impossible. And having two separate implementations just causes fragmentation.\r\n\r\nI think this gets at the crux of the issue. Wordpress ended up with a translation approach that uses comments in a way that is incompatible with the JavaScript specification. The main two options are for Wordpress to do the work to migrate to an approach that is compatible with the JS standard (at which point lots of other tooling considerations probably get easier too), or React Compiler could attempt to support Wordpress' bespoke system. Given the complexity and challenges of supporting this, that i've described above, we are unable to take on implementing this and also unable to commit to supporting a community contribution.  \r\n\r\nWe understand this may be challenging, but we really would recommend evaluating what it would take to move to an approach that uses code rather than comments to encode critical information. We have some experience with designing translations (Meta developed [fbt](https://facebook.github.io/fbt/) for example) and would be happy to discuss and provide guidance.\n---\n\nUser 'josephsavona' said:\n---\n> Is this why I'm running into some weird issues where sometimes istanbul is not instrumenting the code properly? I'm seeing statements, functions, etc just not even getting recognized full stop.\r\n\r\n@nathanmarks Likely yes. We go to considerable effort to preserve source locations throughout our compilation process to facilitate debugging etc. However, we know that there are some gaps. The playground actually has a tab to visualize the source maps, and we are definitely open to PRs to improve the precision of our source locations to make source maps more accurate.\n---\n\nUser 'nathanmarks' said:\n---\n@josephsavona I was surprised by this minimal case because the compiler isn't even adding anything.\r\n\r\nWhat's the reason for the node visitors not even getting triggered in the istanbul code I linked above? is this because of information missing from the AST after the compiler runs? I didn't go much deeper but at a glance comparing outputs I did notice that the AST console.log output was missing some things that you normally see like the class names next to the objects etc.\n---\n\nUser 'josephsavona' said:\n---\n@nathanmarks I don't know exactly how Istanbul works, but my guess is that it is using source location information. If we're missing some source span information, that could be throwing off Istanbul's evaluation of which code got executed or not.\n---\n\nUser 'nathanmarks' said:\n---\n> @nathanmarks I don't know exactly how Istanbul works, but my guess is that it is using source location information. If we're missing some source span information, that could be throwing off Istanbul's evaluation of which code got executed or not.\r\n\r\nYou're bang on -- that's exactly the issue, they're ignoring node visits if location information is missing: https://github.com/istanbuljs-archived-repos/istanbul-lib-instrument/blob/master/src/visitor.js#L37\r\n\r\nAnd we're missing the `path.node.loc` off these.\r\n\r\nWhat's the correct approach here... running istanbul before, or fixing the source location issues and running after the compiler? Or not running compiler at all on specs (slightly reluctant on this one, as we've caught issues in specs due to unwanted memoization, and want to test the code as close to how it behaves in prod as possible)?\n---\n\nUser 'josephsavona' said:\n---\nThe fix for Istanbul is to add missing source locations. We\u2019re open to PRs that either add them or add fixtures demonstrating code where locations are missing. \r\n\r\nWhat I would do for the latter is add a feature flag on Environment that enables a source locations validator. This would run after codegen and errors for any nodes that are missing a location. Then you could write fixtures which enable this flag and will fail (expected w an \u201cerror.\u201d prefix) bc locations are missing.\n---\n\nUser 'nathanmarks' said:\n---\nThanks for the pointers! Let me have a look and see what I can do.\r\n\r\nIn parallel: correctness aside, as a temporary workaround on our app do you see any major issues with running istanbul first (otherwise we'll likely have to walk back our compiler rollout, as we're seeing devs get blocked on CI due to test suites suddenly reporting wonky coverage after unassuming changes). Everything seems to be fine on my side WRT the test suite behaving as expected.\n---\n\nUser 'github-actions[bot]' said:\n---\nThis issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n---\n\nUser 'swissspidy' said:\n---\n> [<img alt=\"\" width=\"16\" height=\"16\" src=\"https://avatars.githubusercontent.com/u/841956?u=126b7e5e51c53b94606e538b5e208c75baad6cb7&amp;v=4&amp;size=80\">@swissspidy[<img alt=\"\" width=\"15\" height=\"15\" src=\"chrome-extension://dlebflppeeemcdpidccbiblndppbmjmh/ospo-chrome-ext-logo.png\">](https://teams.googleplex.com/pascalb@google.com)](https://github.com/swissspidy?rgh-link-date=2024-11-22T03%3A29%3A56.000Z) thanks for the extra context.\n> \n> > But for the hundreds of thousands of plugins out there that's nearly impossible. And having two separate implementations just causes fragmentation.\n> \n> I think this gets at the crux of the issue. Wordpress ended up with a translation approach that uses comments in a way that is incompatible with the JavaScript specification. The main two options are for Wordpress to do the work to migrate to an approach that is compatible with the JS standard (at which point lots of other tooling considerations probably get easier too), or React Compiler could attempt to support Wordpress' bespoke system. Given the complexity and challenges of supporting this, that i've described above, we are unable to take on implementing this and also unable to commit to supporting a community contribution.\n> \n> We understand this may be challenging, but we really would recommend evaluating what it would take to move to an approach that uses code rather than comments to encode critical information. We have some experience with designing translations (Meta developed [fbt](https://facebook.github.io/fbt/) for example) and would be happy to discuss and provide guidance.\n\nClosing because of this. I'll try to push the WordPress ecosystem down this road.\n---",
    "question": "Has anyone seen ': All comments removed from source code' in facebook/react? I can't figure out the cause.",
    "ideal_answer": "Thanks for posting. I can imagine it\u2019s frustrating to have spent a lot of effort making your translations work via comments and to have the compiler break this. \r\n\nThe short answer is: we can't preserve comments in a way that works for all the schemes people have for them. This is because comments sort of _visually_ correspond to a part of the program, but this is not precise enough for tooling. If you've ever seen a comment get moved from something like prettier, this is why. \r\n\nTo make this concrete, consider a slightly tweaked snippet from your example:\r\n\n```js\r\n// translators: %s is username\r\nconst element = <div>{__s(\u201c...%s...\u201d)}</div>;\r\n```\r\n\nA human knows that this comment is really referring to the `__s(\"...\")` call expression. But JavaScript tooling just sees that this comment precedes the variable declaration statement. \r\n\nBecause comments have no semantic meaning in JavaScript -- they do not affect runtime behavior of the program -- it's perfectly fine for a tool to rewrite this code as follows:\r\n\n```js\r\nconst t0 = __s(\u201c...%s...\u201d);\r\n// translators: %s is username\r\nconst element = <div>{t0}</div>;\r\n```\r\n\nAs you can see, it isn't enough to preserve comments. This particular comment scheme is meant to apply comment information to strings inside calls. So you might say, just move the comment up one line! But someone else might have a custom comment scheme that is meant to describe variable declarations, and then it would break if we moved the comment up! . There's literally no way for an arbitrary tool to know how to \"correctly\" preserve comments while rewriting code. \r\n\nThis is why JavaScript -- like effectively all modern languages -- makes comments semantically meaningless. This makes it safe to not preserve comments, or to make a best-effort. Many languages also typically carve out a safe space for semantic annotations. Rust has `#` syntax, JS has \"use strict\" style string directives as an unofficial pattern, etc. \r\n\nSo I hope this helps to put the existing behavior in context. We understand that it would be convenient if the compiler preserved comments, but it is impossible to do so in a general purpose way that everyone would be happy with. There isn't even a good-enough heuristic. As such, we do not and will not support preserving comments. \r\n\nIn contrast, type annotations provide semantic information in a structured form. We currently preserve some type annotations, and would consider supporting preserving them fully. \r\n\nWe would recommend exploring i18n approaches that do not depend on comments, and instead put the information about the meaning of the various interpolations (\"%s\") into the program itself. There are a number of popular libraries for this.\n\nThanks for the thorough response!\r\n\nI feared as much. While I agree that in a JS world comments are not the best way for such annotations, changing the i18n approach is unfortunately easier said than done. I'm sure you can understand that doing so in an ecosystem as big as WordPress is an enormous undertaking. The React-based Gutenberg editor is currently [exploring using React Compiler](https://github.com/WordPress/gutenberg/pull/66361), and this is a regression from an i18n perspective. While we can probably find a workaround there, thousands of extensions (plugins) will have to do the same.\r\n\nWould you be open to consider some middleground like preserving specific types of comments (e.g. `/*! preserve me */) or if they are within a function call itself (e.g. `__( /* translators: %s: username */ 'Hello %s'  )` or something?\n\nI second @swissspidy here. Finding a workaround is not a challenge at all, but propagating it through hundreds of thousands of third-party projects (many of which don't even use React or will not adopt React 19 or the React Compiler) with as many third-party developers accumulating over the past ~20 years is doomed to failure. \r\n\nAn ideal solution would be to allow the Babel plugin to provide an environment configuration variable that enables skipping some comments. There's already a precedent for that with the `@enableMemoizationComments` flag:\r\n\nhttps://github.com/facebook/react/blob/fe04dbcbc4185d7c9d7afebbe18589d2b681a88c/compiler/packages/babel-plugin-react-compiler/src/HIR/Environment.ts#L458C3-L458C28\r\n\nWould it be feasible to consider providing a similar flag that lets us declare an array of strings or regexes, and the React Compiler would skip stripping comments that have a match?\n\nHow are these special comments used? Is there a tool that automatically extracts them into some more usable format for translators? If so, can\u2019t this tool be run before the React compiler instead? Surely there are a bunch of other build steps (like minifiers and bundlers) that already remove comments.\n\n> An ideal solution would be to allow the Babel plugin to provide an environment configuration variable that enables skipping some comments. There's already a precedent for that with the @enableMemoizationComments flag\r\n\nJust a quick note that this flag isn\u2019t skipping comments, it tells the compiler to _synthesize_ comments describing what it\u2019s memorizing. You can see this for yourself by adding `// @enableMemoizationComments` as the first line in the playground. \r\n\nThe compiler transforms the Babel AST into our own internal representation (called HIR), runs an extensive number of passes, and then completely rebuilds the AST. Per my comment above, it is impossible for us to preserve comments in this process in a way that would work with every scheme you could come up with for comments. That is why the spec specifically makes comments semantically meaningless: otherwise it\u2019s just crazy town with everyone inventing different incompatible comment schemes and tooling literally can\u2019t touch your code. \r\n\n> If so, can\u2019t this tool be run before the React compiler instead? \r\n\nYes! This is what I would explore.\n\nApologies for the late response.\r\n\n> How are these special comments used? Is there a tool that automatically extracts them into some more usable format for translators?\r\n\nYes, exactly, it's a string extraction tool that then transfers the found strings to a translation platform or in a gettext POT file.\r\n\n> If so, can\u2019t this tool be run before the React compiler instead? Surely there are a bunch of other build steps (like minifiers and bundlers) that already remove comments.\r\n\nWe have set up our existing tooling (babel, webpack) so that translator comments are preserved. That works quite well so far.\r\n\nIn theory extracting the string from the source files (or finding any other solution) would be better for sure, but this is not feasible for the WordPress ecosystem, as pointed out by @tyxla. For WordPress core itself yes, there might be alternative approaches we could take in the short to medium term, as we are in full control there. But for the hundreds of thousands of plugins out there that's nearly impossible. And having two separate implementations just causes fragmentation.\n\n@josephsavona Is this why I'm running into some weird issues where sometimes istanbul is not instrumenting the code properly? I'm seeing statements, functions, etc just not even getting recognized full stop.\r\n\nI spent a few hours diving into it. I first noticed that the statement/function/branch info instanbul injects into the file is completely missing things, (along with source location not being correct for some that did make it).\r\n\nIt looks like when the babel instanbul plugin runs, node visitors are simply not getting triggered for some things.\r\n\nFor example, given this super simple example:\r\n```ts\r\nexport default function useSomething() {\r\n  const isMobile = useIsMobile()\r\n\n  return isMobile\r\n}\r\n```\r\n\n`istanbul` is expecting these visitors ([1](https://github.com/istanbuljs-archived-repos/istanbul-lib-instrument/blob/master/src/visitor.js#L424), [2](https://github.com/istanbuljs-archived-repos/istanbul-lib-instrument/blob/master/src/visitor.js#L420)) to get invoked, and they never do when the react compiler babel plugin runs first. I also noticed that the AST is missing stuff like source location info for nodes etc.\r\n\nOut of the box, `jest` appends istanbul to the rest of the babel plugins.\n\n@swissspidy thanks for the extra context.\r\n\n> But for the hundreds of thousands of plugins out there that's nearly impossible. And having two separate implementations just causes fragmentation.\r\n\nI think this gets at the crux of the issue. Wordpress ended up with a translation approach that uses comments in a way that is incompatible with the JavaScript specification. The main two options are for Wordpress to do the work to migrate to an approach that is compatible with the JS standard (at which point lots of other tooling considerations probably get easier too), or React Compiler could attempt to support Wordpress' bespoke system. Given the complexity and challenges of supporting this, that i've described above, we are unable to take on implementing this and also unable to commit to supporting a community contribution.  \r\n\nWe understand this may be challenging, but we really would recommend evaluating what it would take to move to an approach that uses code rather than comments to encode critical information. We have some experience with designing translations (Meta developed [fbt](https://facebook.github.io/fbt/) for example) and would be happy to discuss and provide guidance.\n\n> Is this why I'm running into some weird issues where sometimes istanbul is not instrumenting the code properly? I'm seeing statements, functions, etc just not even getting recognized full stop.\r\n\n@nathanmarks Likely yes. We go to considerable effort to preserve source locations throughout our compilation process to facilitate debugging etc. However, we know that there are some gaps. The playground actually has a tab to visualize the source maps, and we are definitely open to PRs to improve the precision of our source locations to make source maps more accurate.\n\n@josephsavona I was surprised by this minimal case because the compiler isn't even adding anything.\r\n\nWhat's the reason for the node visitors not even getting triggered in the istanbul code I linked above? is this because of information missing from the AST after the compiler runs? I didn't go much deeper but at a glance comparing outputs I did notice that the AST console.log output was missing some things that you normally see like the class names next to the objects etc.\n\n@nathanmarks I don't know exactly how Istanbul works, but my guess is that it is using source location information. If we're missing some source span information, that could be throwing off Istanbul's evaluation of which code got executed or not.\n\n> @nathanmarks I don't know exactly how Istanbul works, but my guess is that it is using source location information. If we're missing some source span information, that could be throwing off Istanbul's evaluation of which code got executed or not.\r\n\nYou're bang on -- that's exactly the issue, they're ignoring node visits if location information is missing: https://github.com/istanbuljs-archived-repos/istanbul-lib-instrument/blob/master/src/visitor.js#L37\r\n\nAnd we're missing the `path.node.loc` off these.\r\n\nWhat's the correct approach here... running istanbul before, or fixing the source location issues and running after the compiler? Or not running compiler at all on specs (slightly reluctant on this one, as we've caught issues in specs due to unwanted memoization, and want to test the code as close to how it behaves in prod as possible)?\n\nThe fix for Istanbul is to add missing source locations. We\u2019re open to PRs that either add them or add fixtures demonstrating code where locations are missing. \r\n\nWhat I would do for the latter is add a feature flag on Environment that enables a source locations validator. This would run after codegen and errors for any nodes that are missing a location. Then you could write fixtures which enable this flag and will fail (expected w an \u201cerror.\u201d prefix) bc locations are missing.\n\nThanks for the pointers! Let me have a look and see what I can do.\r\n\nIn parallel: correctness aside, as a temporary workaround on our app do you see any major issues with running istanbul first (otherwise we'll likely have to walk back our compiler rollout, as we're seeing devs get blocked on CI due to test suites suddenly reporting wonky coverage after unassuming changes). Everything seems to be fine on my side WRT the test suite behaving as expected.\n\nThis issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n\n> [<img alt=\"\" width=\"16\" height=\"16\" src=\"https://avatars.githubusercontent.com/u/841956?u=126b7e5e51c53b94606e538b5e208c75baad6cb7&amp;v=4&amp;size=80\">@swissspidy[<img alt=\"\" width=\"15\" height=\"15\" src=\"chrome-extension://dlebflppeeemcdpidccbiblndppbmjmh/ospo-chrome-ext-logo.png\">](https://teams.googleplex.com/pascalb@google.com)](https://github.com/swissspidy?rgh-link-date=2024-11-22T03%3A29%3A56.000Z) thanks for the extra context.\n> \n> > But for the hundreds of thousands of plugins out there that's nearly impossible. And having two separate implementations just causes fragmentation.\n> \n> I think this gets at the crux of the issue. Wordpress ended up with a translation approach that uses comments in a way that is incompatible with the JavaScript specification. The main two options are for Wordpress to do the work to migrate to an approach that is compatible with the JS standard (at which point lots of other tooling considerations probably get easier too), or React Compiler could attempt to support Wordpress' bespoke system. Given the complexity and challenges of supporting this, that i've described above, we are unable to take on implementing this and also unable to commit to supporting a community contribution.\n> \n> We understand this may be challenging, but we really would recommend evaluating what it would take to move to an approach that uses code rather than comments to encode critical information. We have some experience with designing translations (Meta developed [fbt](https://facebook.github.io/fbt/) for example) and would be happy to discuss and provide guidance.\n\nClosing because of this. I'll try to push the WordPress ecosystem down this road."
  },
  {
    "id": "gen_nat_038",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [DevTools Bug] Cannot add child \"301\" to parent \"155\" because parent node was not found in the Store.\n\nA user reported the following issue titled '[DevTools Bug] Cannot add child \"301\" to parent \"155\" because parent node was not found in the Store.' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### Website or app\n\nwww.github.com\n\n### Repro steps\n\nThe error was thrown at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1173126\n    at v.emit (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1140783)\n    at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1142390\n    at bridgeListener (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1552662)\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.0.1-c7c68ef842\n\n### Error message (automated)\n\nCannot add child \"301\" to parent \"155\" because parent node was not found in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1173126\n    at v.emit (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1140783)\n    at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1142390\n    at bridgeListener (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1552662)\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot add child  to parent  because parent node was not found in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'Negatrin' said:\n---\nThe issue occurs because the React DevTools extension is outdated. To resolve this, try updating it by navigating to Chrome's extensions page and checking for updates. Alternatively, disable other extensions while working in Developer Mode, as they might be causing interference. If the problem persists, clear the cache data for React DevTools and reinstall the extension\u2014this was the solution that worked for me.\n---\n\nUser 'hariskhan802' said:\n---\nThis issue might be caused by using an outdated version of the React DevTools extension. A quick fix is to go to the Chrome extensions page and check for updates. Additionally, conflicts with other extensions could be the culprit, so try disabling them temporarily in Developer Mode to see if that helps. If the problem continues, clearing the React DevTools cache and reinstalling the extension worked for me and might work for you as well.\n---\n\nUser 'yairEO' said:\n---\nI am having the same issue and my _React DevTools extension_ is very much up-to-date (it's automatic anyway) `6.1.1 (2/7/2025)`\n---\n\nUser 'hoxyq' said:\n---\nIs this constantly reproducible? If so, can someone share some code that I can use?\n\nThat would help a lot, otherwise there is not enough information for me to take action on.\n---\n\nUser 'yairEO' said:\n---\nWhat \"code\" could I share? it happens when I try to inspect a certain SaaS app..\n\n![Image](https://github.com/user-attachments/assets/b96bd88e-aeec-45e0-9c91-c67e169b8ec2)\n---\n\nUser 'hoxyq' said:\n---\n> What \"code\" could I share? it happens when I try to inspect a certain SaaS app..\n> \n> ![Image](https://github.com/user-attachments/assets/b96bd88e-aeec-45e0-9c91-c67e169b8ec2)\n\nThis one looks unrelated to the original issue, could you please move it to a separate one?\n---\n\nUser 'yairEO' said:\n---\nyeah.. I got this one now :) React profiler throws random errors.. I will try to get that one related to here\n---\n\nUser 'sanfilippopablo' said:\n---\nGetting this too with 6.1.1\n---\n\nUser 'HtetSansen' said:\n---\nsame for Version\n6.1.1 (2/7/2025)\n---\n\nUser 'DilinJose' said:\n---\n> The issue occurs because the React DevTools extension is outdated. To resolve this, try updating it by navigating to Chrome's extensions page and checking for updates. Alternatively, disable other extensions while working in Developer Mode, as they might be causing interference. If the problem persists, clear the cache data for React DevTools and reinstall the extension\u2014this was the solution that worked for me.\n\nUpdating worked for me too!\n---",
    "question": "Could you explain how to resolve 'Cannot add child \"301\" to parent \"155\" because parent node was not found in the Store.'?",
    "ideal_answer": "The issue occurs because the React DevTools extension is outdated. To resolve this, try updating it by navigating to Chrome's extensions page and checking for updates. Alternatively, disable other extensions while working in Developer Mode, as they might be causing interference. If the problem persists, clear the cache data for React DevTools and reinstall the extension\u2014this was the solution that worked for me.\n\nThis issue might be caused by using an outdated version of the React DevTools extension. A quick fix is to go to the Chrome extensions page and check for updates. Additionally, conflicts with other extensions could be the culprit, so try disabling them temporarily in Developer Mode to see if that helps. If the problem continues, clearing the React DevTools cache and reinstalling the extension worked for me and might work for you as well.\n\nI am having the same issue and my _React DevTools extension_ is very much up-to-date (it's automatic anyway) `6.1.1 (2/7/2025)`\n\nIs this constantly reproducible? If so, can someone share some code that I can use?\n\nThat would help a lot, otherwise there is not enough information for me to take action on.\n\nWhat \"code\" could I share? it happens when I try to inspect a certain SaaS app..\n\n![Image](https://github.com/user-attachments/assets/b96bd88e-aeec-45e0-9c91-c67e169b8ec2)\n\n> What \"code\" could I share? it happens when I try to inspect a certain SaaS app..\n> \n> ![Image](https://github.com/user-attachments/assets/b96bd88e-aeec-45e0-9c91-c67e169b8ec2)\n\nThis one looks unrelated to the original issue, could you please move it to a separate one?\n\nyeah.. I got this one now :) React profiler throws random errors.. I will try to get that one related to here\n\nGetting this too with 6.1.1\n\nsame for Version\n6.1.1 (2/7/2025)\n\n> The issue occurs because the React DevTools extension is outdated. To resolve this, try updating it by navigating to Chrome's extensions page and checking for updates. Alternatively, disable other extensions while working in Developer Mode, as they might be causing interference. If the problem persists, clear the cache data for React DevTools and reinstall the extension\u2014this was the solution that worked for me.\n\nUpdating worked for me too!"
  },
  {
    "id": "gen_nat_039",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [Compiler Bug]: Compiler introduces unnecessary breaks that skips its own memoization\n\nA user reported the following issue titled '[Compiler Bug]: Compiler introduces unnecessary breaks that skips its own memoization' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wApg+XJ0wJ8AXwCU+YAB02+OKzAEAtgjUQmALwSF8AXnxQwCdpoj9+MwwD5ZC-M6UqCANzKUo44xjF0YEzuCDYA3E4uTCT4-CJiRobGcuAAFji4APq4CDBqKTLyii74MAi4sGye3ggRxZL4CJRmjvXO1T4AdGpQuGQ54ZElZRUwVV4+dSUSkRIANPgA2vEIALpSU6XllfgAPITBdgASTZQQ+ADqOJQGwBpauvoSuwD0B+52dTN0IBJAA\n\n(Update: made the repro shorter and more generic)\n\n### Repro steps\n\n1. See the playground link.\n2. Look at the two `break`s the compiler introduced in the output. The memoization is skipped by the two `break`s, making the result of `useMemo` always new in each render.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\nBoth 18.3.1 and the version used by Playground\n\n### What version of React Compiler are you using?\n\nBoth 19.1.0-rc.2 and the version used by Playground\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'josephsavona' said:\n---\nThanks for posting, that\u2019s definitely a bug. We\u2019ll take a look!\n---\n\nUser 'Shonferns004' said:\n---\nHello! \ud83d\udc4b I came across issue and would love to work on this. I\u2019ve set up the repo locally and started digging into it.\n\nJust wanted to check in before starting \u2014 is anyone already working on this? If not, I\u2019d love to take it up.\n\nThanks!\n---",
    "question": "Help needed with facebook/react. : Compiler introduces unnecessary breaks that skips its own memoization.",
    "ideal_answer": "Thanks for posting, that\u2019s definitely a bug. We\u2019ll take a look!\n\nHello! \ud83d\udc4b I came across issue and would love to work on this. I\u2019ve set up the repo locally and started digging into it.\n\nJust wanted to check in before starting \u2014 is anyone already working on this? If not, I\u2019d love to take it up.\n\nThanks!"
  },
  {
    "id": "gen_nat_040",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: Pod is deleted after failed Job when used with \"restartPolicy: OnFailure\"\n\nA user reported the following issue titled 'Pod is deleted after failed Job when used with \"restartPolicy: OnFailure\"' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\r\n-->\r\n\r\n\r\n**What happened**:\r\n\r\nPod is deleted after failed Job when used with `restartPolicy: OnFailure`. So there is no way to eg. check for errors, debug logs.\r\n\r\n**What you expected to happen**:\r\n\r\nJob failed but Pod is still present to be able to check for errors, debug logs, etc.\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\nCronJob manifest:\r\n```\r\napiVersion: batch/v1beta1\r\nkind: CronJob\r\nmetadata:\r\n  name: test\r\nspec:\r\n  schedule: \"*/10 * * * *\"\r\n  jobTemplate:\r\n    spec:\r\n      template:\r\n        spec:\r\n          restartPolicy: OnFailure\r\n          containers:\r\n          - name: cron-job\r\n            image: alpine\r\n            command:\r\n            - /bin/sh\r\n            - -c\r\n            - exit 1\r\n```\r\n\r\nJob events:\r\n```\r\nEvents:\r\n  Type     Reason                Age                From            Message\r\n  ----     ------                ----               ----            -------\r\n  Normal   SuccessfulCreate      5m56s              job-controller  Created pod: test-1551540600-kb4lm\r\n  Normal   SuccessfulDelete      10s                job-controller  Deleted pod: test-1551540600-kb4lm\r\n  Warning  BackoffLimitExceeded  10s (x2 over 10s)  job-controller  Job has reached the specified backoff limit\r\n```\r\n\r\nPod `test-1551540600-kb4lm` was deleted.\r\n\r\n**Anything else we need to know?**:\r\n\r\nUsing `restartPolicy: Never` works as expected - failed Pods are still available after failed Job.\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`):\r\n```\r\nClient Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-04T04:48:03Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"12+\", GitVersion:\"v1.12.5-gke.5\", GitCommit:\"2c44750044d8aeeb6b51386ddb9c274ff0beb50b\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T23:53:25Z\", GoVersion:\"go1.10.8b4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\r\n```\r\n- Cloud provider or hardware configuration: GKE\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'r0bj' said:\n---\n/sig apps\n---\n\nUser 'Joseph-Irving' said:\n---\nSo I believe this is intended behaviour. The reason being that `backoffLimit` on a Job is not something the Pod knows about, so once your Pod has restarted 6 times (default) the job is failed but the Pod doesn't know to stop restarting its containers and would therefore continue on crashlooping forever, the Job controller deletes the pod to prevent it from doing this. \r\n\r\nSpecifically the Job controller deletes any active pods when the Job becomes failed, this is why you don't see pods getting deleted when using `RestartPolicy: Never`, these pods are no longer active so they don't need cleaning up.\r\n\r\nI'd always recommend having a logging pipeline in your cluster so that you have a persistent store of your logs for debugging, pod logs are always temporary.\n---\n\nUser 'fejta-bot' said:\n---\nIssues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle stale\n---\n\nUser 'fejta-bot' said:\n---\nStale issues rot after 30d of inactivity.\nMark the issue as fresh with `/remove-lifecycle rotten`.\nRotten issues close after an additional 30d of inactivity.\n\nIf this issue is safe to close now please do so with `/close`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle rotten\n---\n\nUser 'fejta-bot' said:\n---\nRotten issues close after 30d of inactivity.\nReopen the issue with `/reopen`.\nMark the issue as fresh with `/remove-lifecycle rotten`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/close\n---\n\nUser 'k8s-ci-robot' said:\n---\n@fejta-bot: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/74848#issuecomment-522316916):\n\n>Rotten issues close after 30d of inactivity.\n>Reopen the issue with `/reopen`.\n>Mark the issue as fresh with `/remove-lifecycle rotten`.\n>\n>Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'deepaksood619' said:\n---\n@Joseph-Irving Is there a way, where we can specify the pod that fail 6 times and then stop restarting. So we can get the logs from the last pod. And `JobController` doesn't delete the pod.\n---\n\nUser 'deepaksood619' said:\n---\n/reopen\n---\n\nUser 'k8s-ci-robot' said:\n---\n@deepaksood619: You can't reopen an issue/PR unless you authored it or you are a collaborator.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/74848#issuecomment-635758869):\n\n>/reopen\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'csuzhangxc' said:\n---\n> @Joseph-Irving Is there a way, where we can specify the pod that fail 6 times and then stop restarting. So we can get the logs from the last pod. And `JobController` doesn't delete the pod.\r\n\r\n\ud83d\udc4d \r\n\r\nHow to reopen the issue? \ud83e\udd14\n---\n\nUser 'rafis' said:\n---\n/reopen\n---\n\nUser 'k8s-ci-robot' said:\n---\n@rafis: You can't reopen an issue/PR unless you authored it or you are a collaborator.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/74848#issuecomment-814789126):\n\n>/reopen\r\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'weibeld' said:\n---\nI ran into this issue as well. My expectation was that when a Pod in a Job has `restartPolicy: OnFailure`, a container that fails is restarted indefinitely, independently of any settings in the Job. However, the restarts of a container seem to count against the `backoffLimit` of the Job. So, the actual behaviour is:\r\n\r\n1. A container in the Pod fails and is restarted\r\n1. The Job Controller registers this and increments the backoff counter (which is measured agains `backoffLimit`)\r\n1. The restarted container fails again\r\n1. The Job Controller increments the backoff counter\r\n1. ...\r\n\r\nAt this point, it's still the same Pod running that the Job initially created. However, when the backoff counter reaches `backoffLimit` the Job Controller actively deletes the Pod and marks the Job as Failed with a reason of `BackoffLimitExceeded`.\r\n\r\nThis is different from the behaviour for a Pod with `restartPolicy: Never`. Here the Job Controller only gets active when the Pod as a whole fails, i.e. when _all_ the containers in the Pod fail (and are not restarted because the restart policy is Never, and thus the [phase](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase) of the Pod is Failed). At this point, the Job Controller increments the backoff counter and creates a new Pod (without deleting the old one). If this Pod fails again, the cycle repeats until the backoff counter reaches `backoffLimit`, at which point the Job is also marked as failed with a reason of `BackoffLimitExceeded`.\r\n\r\nIt's actually described in this way in the [docs](https://kubernetes.io/docs/concepts/workloads/controllers/job/#job-termination-and-cleanup):\r\n\r\n> By default, a Job will run uninterrupted unless a Pod fails (`restartPolicy=Never`) or a Container exits in error (`restartPolicy=OnFailure`), at which point the Job defers to the `.spec.backoffLimit` described above.\r\n\r\nSo, the semantics of `backoffLimit` changes based on the restart policy of the Pods: if `restartPolicy: OnFailure`, then container restarts are counted as backoffs, and if `restartPolicy: Never`, then new Pod creations are counted as backoffs.\r\n\r\nIn the case of `restartPolicy: OnFailure`, the Job Controller must delete the Pod when the Job is officially marked as Failed otherwise the failing container in the Pod might restart forever, or might eventually run successfully, even tough the Job has already been recognised as Failed.\r\n\r\nThis is also mentioned in the [docs](https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-backoff-failure-policy):\r\n\r\n> If your job has restartPolicy = \"OnFailure\", keep in mind that your container running the Job will be terminated once the job backoff limit has been reached. This can make debugging the Job's executable more difficult.\r\n\r\nI guess it should read \"...your _Pod_ running the Job will be terminated...\".\n---\n\nUser 'Jonathan-Landeed' said:\n---\n`restartPolicy: Never` doesn't prevent pod deletion if parallelism > 1. It's supposed to delete active pods once backoff limit is exceeded, but it's deleting the failed pod as well.\n---",
    "question": "In kubernetes/kubernetes, Pod is deleted after failed Job when used with \"restartPolicy: OnFailure\". Any ideas why?",
    "ideal_answer": "So I believe this is intended behaviour. The reason being that `backoffLimit` on a Job is not something the Pod knows about, so once your Pod has restarted 6 times (default) the job is failed but the Pod doesn't know to stop restarting its containers and would therefore continue on crashlooping forever, the Job controller deletes the pod to prevent it from doing this. \r\n\nSpecifically the Job controller deletes any active pods when the Job becomes failed, this is why you don't see pods getting deleted when using `RestartPolicy: Never`, these pods are no longer active so they don't need cleaning up.\r\n\nI'd always recommend having a logging pipeline in your cluster so that you have a persistent store of your logs for debugging, pod logs are always temporary.\n\nIssues go stale after 90d of inactivity.\nMark the issue as fresh with `/remove-lifecycle stale`.\nStale issues rot after an additional 30d of inactivity and eventually close.\n\nIf this issue is safe to close now please do so with `\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle stale\n\nStale issues rot after 30d of inactivity.\nMark the issue as fresh with `/remove-lifecycle rotten`.\nRotten issues close after an additional 30d of inactivity.\n\nIf this issue is safe to close now please do so with `\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n/lifecycle rotten\n\nRotten issues close after 30d of inactivity.\nReopen the issue with `/reopen`.\nMark the issue as fresh with `/remove-lifecycle rotten`.\n\nSend feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta).\n\n@fejta-bot: Closing this issue.\n\n@Joseph-Irving Is there a way, where we can specify the pod that fail 6 times and then stop restarting. So we can get the logs from the last pod. And `JobController` doesn't delete the pod.\n\n/reopen\n\n@deepaksood619: You can't reopen an issue/PR unless you authored it or you are a collaborator.\n\n> @Joseph-Irving Is there a way, where we can specify the pod that fail 6 times and then stop restarting. So we can get the logs from the last pod. And `JobController` doesn't delete the pod.\r\n\n\ud83d\udc4d \r\n\nHow to reopen the issue? \ud83e\udd14\n\n/reopen\n\n@rafis: You can't reopen an issue/PR unless you authored it or you are a collaborator.\n\nI ran into this issue as well. My expectation was that when a Pod in a Job has `restartPolicy: OnFailure`, a container that fails is restarted indefinitely, independently of any settings in the Job. However, the restarts of a container seem to count against the `backoffLimit` of the Job. So, the actual behaviour is:\r\n\n1. A container in the Pod fails and is restarted\r\n1. The Job Controller registers this and increments the backoff counter (which is measured agains `backoffLimit`)\r\n1. The restarted container fails again\r\n1. The Job Controller increments the backoff counter\r\n1. ...\r\n\nAt this point, it's still the same Pod running that the Job initially created. However, when the backoff counter reaches `backoffLimit` the Job Controller actively deletes the Pod and marks the Job as Failed with a reason of `BackoffLimitExceeded`.\r\n\nThis is different from the behaviour for a Pod with `restartPolicy: Never`. Here the Job Controller only gets active when the Pod as a whole fails, i.e. when _all_ the containers in the Pod fail (and are not restarted because the restart policy is Never, and thus the [phase](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase) of the Pod is Failed). At this point, the Job Controller increments the backoff counter and creates a new Pod (without deleting the old one). If this Pod fails again, the cycle repeats until the backoff counter reaches `backoffLimit`, at which point the Job is also marked as failed with a reason of `BackoffLimitExceeded`.\r\n\nIt's actually described in this way in the [docs](https://kubernetes.io/docs/concepts/workloads/controllers/job/#job-termination-and-cleanup):\r\n\n> By default, a Job will run uninterrupted unless a Pod fails (`restartPolicy=Never`) or a Container exits in error (`restartPolicy=OnFailure`), at which point the Job defers to the `.spec.backoffLimit` described above.\r\n\nSo, the semantics of `backoffLimit` changes based on the restart policy of the Pods: if `restartPolicy: OnFailure`, then container restarts are counted as backoffs, and if `restartPolicy: Never`, then new Pod creations are counted as backoffs.\r\n\nIn the case of `restartPolicy: OnFailure`, the Job Controller must delete the Pod when the Job is officially marked as Failed otherwise the failing container in the Pod might restart forever, or might eventually run successfully, even tough the Job has already been recognised as Failed.\r\n\nThis is also mentioned in the [docs](https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-backoff-failure-policy):\r\n\n> If your job has restartPolicy = \"OnFailure\", keep in mind that your container running the Job will be terminated once the job backoff limit has been reached. This can make debugging the Job's executable more difficult.\r\n\nI guess it should read \"...your _Pod_ running the Job will be terminated...\".\n\n`restartPolicy: Never` doesn't prevent pod deletion if parallelism > 1. It's supposed to delete active pods once backoff limit is exceeded, but it's deleting the failed pod as well."
  },
  {
    "id": "gen_nat_041",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [Compiler Bug]: \"Mutating a value returned from \n\nA user reported the following issue titled '[Compiler Bug]: \"Mutating a value returned from 'useContext()', which should not be mutated\" When the value is a Ref' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What kind of issue is this?\n\n- [X] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [X] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAjggDswCBZATwGETcMCBeQmBAQ3tuPvVwApgAHWKCebALaYANggBKCAGbJ8wOLFbdlxKFKkBfYXoCUw4Rmx58AE0VsdBBVGJxcASxL4ACmw25akkgRuPiMVYXxCEjJ8DAlpOUV8ZigwBIU+AHIAOQgMkxFiCNZcWEK+cIj8AB4APgrK6qouHlwAOk8YCAA3VxsYfC62KSgERmBgWMkZeQU9PTrChsqq6gALVykrf2xiIIIAegWl6v2mugZ2zp6+o+XDiqMAbkNTYkdnNw81ja2IAN3gqEhIsiKQCMAYug4tNEnokvgUghmgw+GduAx8hVEQBRBQKBAuPghJI1MKLCKTeIzVpqGC+eEZACaCDAGQqegANPgANqUmEKAC6+QiFWKpWqVlcXXuxAMxBAeiAA\n\n### Repro steps\n\n1. Instantiate a Context\r\n2. In Parent, Instantiate a ref\r\n3. Wrap the child in the Context's Provider, and pass the ref as a value\r\n4. Get the ref in the child, and mutate it in a useEffect\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n18.3.1\n\n### What version of React Compiler are you using?\n\n19.0.0-beta-63b359f-20241101\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'jay-herrera' said:\n---\nAlso applies to Event Handlers:\r\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAjggDswCBZATwGETcMCBeQmBAQ3tuPvVwApgAHWKCebALaYANggBKCAGbJ8wOLFbdlxKFKkBfYXoCUw4Rmx58AE0VsdBBVGJxcASxL4ACmw25akkgRuPiMVYXxCEjJ8DAlpOUV8ZigwBIU+AHIAOQgMkxFiCNZcWEK+cIj8AB4APgrK6qouHlwAOk8YCAA3VxsYfC62KSgERmBgWMkZeQU9PTrChsqq6gALVykrf2xiIIIAegWl6v2mugZ2zp6+o+XDiqMAbkNTYkdnNw81ja2IAN3gqEhIsiKQCMAYug4tNEnokvgUghmgw+GduAx8hViqVqlZXF18CRqFJXHAANZjEJJGphRYRSbxGatNQwXzwjIATQQYAyFTm+HuxAMxBAeiAA\n---\n\nUser 'Vansh16aug' said:\n---\nI see , i would like to work on it\n---\n\nUser 'jay-herrera' said:\n---\n@Vansh16aug Hey! any updates?\n---\n\nUser 'salomonme' said:\n---\nHey! any updates?\n---\n\nUser 'josephsavona' said:\n---\n#34005 enables the flag to treat ref-like identifiers as refs by default. As long as your variable is named `ref` or ends in `Ref` (`myRef` etc), the compiler will treat it as a ref. Thanks again for posting!\n---",
    "question": "I'm running into an issue with facebook/react: : \"Mutating a value returned from. How can I fix this?",
    "ideal_answer": "Also applies to Event Handlers:\r\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAjggDswCBZATwGETcMCBeQmBAQ3tuPvVwApgAHWKCebALaYANggBKCAGbJ8wOLFbdlxKFKkBfYXoCUw4Rmx58AE0VsdBBVGJxcASxL4ACmw25akkgRuPiMVYXxCEjJ8DAlpOUV8ZigwBIU+AHIAOQgMkxFiCNZcWEK+cIj8AB4APgrK6qouHlwAOk8YCAA3VxsYfC62KSgERmBgWMkZeQU9PTrChsqq6gALVykrf2xiIIIAegWl6v2mugZ2zp6+o+XDiqMAbkNTYkdnNw81ja2IAN3gqEhIsiKQCMAYug4tNEnokvgUghmgw+GduAx8hViqVqlZXF18CRqFJXHAANZjEJJGphRYRSbxGatNQwXzwjIATQQYAyFTm+HuxAMxBAeiAA\n\nI see , i would like to work on it\n\n@Vansh16aug Hey! any updates?\n\nHey! any updates?\n\n#34005 enables the flag to treat ref-like identifiers as refs by default. As long as your variable is named `ref` or ends in `Ref` (`myRef` etc), the compiler will treat it as a ref. Thanks again for posting!"
  },
  {
    "id": "gen_nat_042",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: When kubelet restarts, admission still rejects pods\n\nA user reported the following issue titled 'When kubelet restarts, admission still rejects pods' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nWhen kubelet restarts, the previously running pod is rejected by admission. This https://github.com/kubernetes/kubernetes/pull/118635  fails to cover all scenarios.\n\n```bash\n`2025-07-31 16:58:54 - Pod|1000000-vp2qd - UnexpectedAdmissionError, Allocate failed due to no healthy devices present; cannot allocate unhealthy devices rdma/hca_shared, which is unexpected`\n```\n\n\nCause:\n\n\n```json\n\"containerStatuses\": [\n    {\n      \"name\": \"app\",\n      \"state\": {\n        \"running\": {\n          \"startedAt\": \"2025-07-30T04:12:07Z\"\n        }\n      },\n      \"lastState\": {\n        \"terminated\": {\n          \"exitCode\": 0,\n          \"reason\": \"Completed\",\n          \"startedAt\": \"2025-07-28T05:33:22Z\",\n          \"finishedAt\": \"2025-07-30T04:09:41Z\",\n          \"containerID\": \"containerd://491ae36456e71a83a6c39270e364e85306f583f959b386660262898086462374\"\n        }\n      },\n      \"ready\": true,\n      \"restartCount\": 2,\n      \"image\": \"docker.io/ai/goglang:v0.0.1\",\n      \"imageID\": \"docker.io/ai/goglang@sha256:be1692a1c0437b07c47a89417a33622b21a488f131520xxxxxxx\",\n      \"containerID\": \"containerd://f393bcca6c8028c74f8987165759a472b3085f4bdf26173eb3dbb4cbe1f6cc9d\",\n      \"started\": true\n    }\n```\n\nhttps://github.com/kubernetes/kubernetes/pull/118635 use `isContainerAlreadyRunning` to check running container. It queries the id from the `containerMap`.\n\n`containerMap` stores all containers on the nodes.(This includes exiting)\n\n`containerMap` is a map, and when looking up the ID, it may return the exited container ID, not meeting expectations.\n\n```shell\n# nvida/gpu\nI0731 16:58:54.195904  909172 manager.go:1100] \"container found in the initial set, assumed running\" podUID=\"58cf408f-5297-4209-96a0-f2c367392151\" containerName=\"app\" containerID=\"f393bcca6c8028c74f8987165759a472b3085f4bdf26173eb3dbb4cbe1f6cc9d\"\nI0731 16:58:54.195915 909172 manager.go:575] \"container detected running, nothing to do\" deviceNumber=0 resourceName=\"nvidia.com/gpu\" podUID=\"58cf408f-5297-4209-96a0-f2c367392151\" containerName=\"app\"\n# rdma\nI0731 16:58:54.195953  909172 manager.go:1095] \"container not present in the initial running set\" podUID=\"58cf408f-5297-4209-96a0-f2c367392151\" containerName=\"app\" containerID=\"491ae36456e71a83a6c39270e364e85306f583f959b386660262898086462374\"\nI0731 16:58:54.196046 909172 manager.go:580] \"Need devices to allocate for pod\" deviceNumber=0 resourceName=\"rdma/hca_shared\" podUID=\"58cf408f-5297-4209-96a0-f2c367392151\" containerName=\"app\"\n```\n\n\n\n\n\n\n### What did you expect to happen?\n\nKubelet restart, pod still running.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. device plugin start slow\n2. pod restartCount > 0, previous containers remain on the node\n3. kubelet restart\n4. There is a probability of recurrence\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nServer Version: v1.28.3\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nnull\n</details>\n\n\n### OS version\n\n<\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'k8s-ci-robot' said:\n---\nThere are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'k8s-ci-robot' said:\n---\nThis issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'ffromani' said:\n---\nHi, thanks for filing this issue. It seems a duplicate of https://github.com/kubernetes/kubernetes/issues/133382\n---\n\nUser 'pacoxu' said:\n---\n/close\nfor dup\n---\n\nUser 'k8s-ci-robot' said:\n---\n@pacoxu: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133451#issuecomment-3392863423):\n\n>/close\n>for dup\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---",
    "question": "What is the resolution for 'When kubelet restarts, admission still rejects pods'?",
    "ideal_answer": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\nHi, thanks for filing this issue. It seems a duplicate of https://github.com/kubernetes/kubernetes/issues/133382\n\nfor dup\n\n@pacoxu: Closing this issue."
  },
  {
    "id": "gen_nat_043",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [DevTools Bug]: React devtool is not there in codesandbox\n\nA user reported the following issue titled '[DevTools Bug]: React devtool is not there in codesandbox' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### Website or app\n\nhttps://codesandbox.io/p/sandbox/white-fog-2pjd7p\n\n### Repro steps\n\nI was following the react.dev docs\r\nI was at tic tac toe chapter and doing my code stuff in codesandbox as per document\r\n\r\ni triggered an issue that react devtools is not there in code sandbox\r\n<img width=\"754\" alt=\"Screenshot 2023-11-28 at 9 25 29\u202fAM\" src=\"https://github.com/facebook/react/assets/40708551/5d83861d-426b-4326-93ad-40a2927d8228\">\r\n\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n_No response_\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n_No response_\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'iamayushdas' said:\n---\nIts resolved closing now\r\nThanks @wasimtikki1220\n---\n\nUser 'happyeungin' said:\n---\n@iamayushdas I still can't find it, where do you find it?\n---\n\nUser 'JustLikeThis101' said:\n---\n+1 can't find it either @iamayushdas\n---\n\nUser 'voidao' said:\n---\n+1 can't find it, please help!\n---\n\nUser 'LucaM0nt' said:\n---\nHow is this solved? There's not React devtool anywhere now? I don't get it...\n---\n\nUser 'lixinjie520' said:\n---\nI can't find it either\uff0chelp plsssss!!!\n---\n\nUser 'byerancy' said:\n---\n+1, can't find it.\n---\n\nUser 'Daembius' said:\n---\nI can't find it either. Do I need to install an extension to my browser or something?\n---\n\nUser 'Splooples' said:\n---\nI also have this exact problem.\n---\n\nUser 'BARIKORDOR' said:\n---\nI also have the same problem.. Help please\n---\n\nUser 'gillianbc' said:\n---\nSame for me - how do I display the tools?\n---\n\nUser 'bertcornelissen' said:\n---\nSame for me.... No DevTools\n---\n\nUser 'Mehdji' said:\n---\nSame problem...\n---\n\nUser 'Plafalavah' said:\n---\nSame here. No DevTools\n---\n\nUser 'BeiHaiSTAR' said:\n---\nSame, where is the Devtools ?\n---\n\nUser 'Aayush974' said:\n---\nanyone found out where it is yet?\n---\n\nUser 'devavratravetkar' said:\n---\nI'm not sure how to integrate React DevTools within the CodeSandBox online coding environment.\nHowever, you can do the following steps to use it in the React Tic-Tac-Toe Tutorial:\n\n1. First install [React Dev Tools chrome extension](https://chromewebstore.google.com/detail/react-developer-tools/fmkadmapgofadopljbjfkapdkoienihi?hl=en).\n2. Your tic-tac-toe preview has a URL similar to \"https://v***v3.csb.app/\"\n3. Simply copy-paste it into a new tab. And open your Chrome DevTools. Now you can see the Square components and their State under the Components tab of DevTools.\n\n![Image](https://github.com/user-attachments/assets/fa754cd3-c35f-4afb-bccb-c689869cd75f)\n\nHope this helps and you can utilize it to understand and complete the rest of the tutorial!\n---",
    "question": "I'm seeing ': React devtool is not there in codesandbox'. Is this a known bug in facebook/react?",
    "ideal_answer": "Its resolved closing now\r\nThanks @wasimtikki1220\n\n@iamayushdas I still can't find it, where do you find it?\n\n+1 can't find it either @iamayushdas\n\n+1 can't find it, please help!\n\nHow is this solved? There's not React devtool anywhere now? I don't get it...\n\nI can't find it either\uff0chelp plsssss!!!\n\n+1, can't find it.\n\nI can't find it either. Do I need to install an extension to my browser or something?\n\nI also have this exact problem.\n\nI also have the same problem.. Help please\n\nSame for me - how do I display the tools?\n\nSame for me.... No DevTools\n\nSame problem...\n\nSame here. No DevTools\n\nSame, where is the Devtools ?\n\nanyone found out where it is yet?\n\nI'm not sure how to integrate React DevTools within the CodeSandBox online coding environment.\nHowever, you can do the following steps to use it in the React Tic-Tac-Toe Tutorial:\n\n1. First install [React Dev Tools chrome extension](https://chromewebstore.google.com/detail/react-developer-tools/fmkadmapgofadopljbjfkapdkoienihi?hl=en).\n2. Your tic-tac-toe preview has a URL similar to \"https://v***v3.csb.app/\"\n3. Simply copy-paste it into a new tab. And open your Chrome DevTools. Now you can see the Square components and their State under the Components tab of DevTools.\n\n![Image](https://github.com/user-attachments/assets/fa754cd3-c35f-4afb-bccb-c689869cd75f)\n\nHope this helps and you can utilize it to understand and complete the rest of the tutorial!"
  },
  {
    "id": "gen_nat_044",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [Compiler]: flags mutations of shared mutable refs and skips optimizations\n\nA user reported the following issue titled '[Compiler]: flags mutations of shared mutable refs and skips optimizations' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEwBcMCAhnggMIQB2VAHngDQFRgIBKCAZmxxr0mhAL4FeMCBgIAdEGXJw88gNyy6GuPTCFaUBghg9etQ8wIBeEoqpmRACgCU6zXW11dBAGIQIVgmcrAD4iDQISHUJtAypjPgDBewRmB31DeNNhFLwXDXCCMjxYOkCCiIAeACMoPDx6coiCemoAGzQ4AGtLYCDLUOBGpoI0XkCYjJMnSNijEwA6OFgyBgIAamsARldhiNFRRuChggBBABMzxoqAehq6+iPSgjy6UVcNFJx8SM9CE6wsAE+gMCtdrgQAMoAC3IZDObAwtXIVVaCAIdHoAFoAKoASQIADdyK0oAhQeCIfU4SNSuRCgl6gRyASIGgzgQ8DA0ABzblGemYlZnIxoOjcggAdyhCFKeGlhOJpJIMLFCDABQ8XgmcRMiU4JgcAAYXgUiiUyk9KukdXxksx5gAFKQEtn8okkhA9bVzPiiR67AgVXz+a7+3ZBvwEUPHCMhsOVa7Wn1Zcx4R3O10wMMvN4aECiIA\n\n### Repro steps\n\nSometimes you need a shared, mutable value that isn\u2019t tied to the UI but is used across multiple components. In these cases, you don\u2019t want React to trigger re-renders when the value changes. To handle this, you can store the value in a ref and pass it down through context.\n\nHowever, when a child component mutates this value (for example, inside an event handler), the compiler currently produces an error and causes optimizations to be skipped. I feel this is too restricted for what is otherwise a legitimate React pattern.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.0\n\n### What version of React Compiler are you using?\n\n19.1.0-rc.1\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'josephsavona' said:\n---\nThanks for posting. This is occurring because we are currently very conservative about what we assume is a ref. I think the correct thing is to treat identifiers which look like refs as if they were refs (`ref` or `-Ref` suffix). We have a compiler flag for this that we should turn on.\n---\n\nUser 'josephsavona' said:\n---\nhttps://github.com/facebook/react/pull/34005 enables the flag to treat ref-like identifiers as refs by default. As long as your variable is named ref or ends in Ref (myRef etc), the compiler will treat it as a ref. Thanks again for posting!\n---",
    "question": "I'm running into an issue with facebook/react: : flags mutations of shared mutable refs and skips optimizations. How can I fix this?",
    "ideal_answer": "Thanks for posting. This is occurring because we are currently very conservative about what we assume is a ref. I think the correct thing is to treat identifiers which look like refs as if they were refs (`ref` or `-Ref` suffix). We have a compiler flag for this that we should turn on.\n\nhttps://github.com/facebook/react/pull/34005 enables the flag to treat ref-like identifiers as refs by default. As long as your variable is named ref or ends in Ref (myRef etc), the compiler will treat it as a ref. Thanks again for posting!"
  },
  {
    "id": "gen_nat_045",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: Multi-container pod creation from YAML with multiple \"containers\" statements\n\nA user reported the following issue titled 'Multi-container pod creation from YAML with multiple \"containers\" statements' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nTrying to create a POD object, from YAML file, with the following structure, does not throw error.\r\n```\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  creationTimestamp: null\r\n  labels:\r\n    run: busybox\r\n  name: busybox\r\nspec:\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox\r\n    resources: {}\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox2\r\n    resources: {}\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox3\r\n    resources: {}\r\n  dnsPolicy: ClusterFirst\r\n  restartPolicy: Never\r\nstatus: {}\r\n```\r\n\r\nThe result of the creation of this object is:\r\n```\r\ncontrolplane $ k create -f po.yaml\r\npod/busybox created\r\ncontrolplane $ k get po\r\nNAME      READY   STATUS      RESTARTS   AGE\r\nbusybox   0/1     Completed   0          8s\r\n```\r\nThe pod will never go to \"Ready\" status. The event in the pod description are:\r\n```\r\nEvents:\r\n  Type    Reason     Age   From               Message\r\n  ----    ------     ----  ----               -------\r\n  Normal  Scheduled  19s   default-scheduler  Successfully assigned default/busybox to node01\r\n  Normal  Pulling    18s   kubelet            Pulling image \"busybox\"\r\n  Normal  Pulled     18s   kubelet            Successfully pulled image \"busybox\" in 165ms (165ms including waiting). Image size: 2160406 bytes.\r\n  Normal  Created    18s   kubelet            Created container busybox3\r\n  Normal  Started    18s   kubelet            Started container busybox3\r\n```\r\n \n\n### What did you expect to happen?\n\nMy understanding is that the \"spec\" section of this YAML in not correct. I expect no object to be created, when to create such from this YAML file. The behavior as of now is confusing and inconsistent. An error should be throw, informing the user that he should fix the YAML file structure.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Create a YAML file with the provided example in the \"What happened?\" section, name it `po.yaml`\r\n2. Create the object using imperative command `kubectl create -f po.yaml`\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\nClient Version: v1.30.0\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.30.0\r\n```\r\n</details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n`https://killercoda.com/killer-shell-ckad/scenario/playground`\r\n\r\n</details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\nNAME=\"Ubuntu\"\r\nVERSION=\"20.04.5 LTS (Focal Fossa)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 20.04.5 LTS\"\r\nVERSION_ID=\"20.04\"\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nVERSION_CODENAME=focal\r\nUBUNTU_CODENAME=focal\r\n```\r\n\r\n</details>\r\n\n\n### Install \n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'niranjandarshann' said:\n---\n@NikolaStoychev Yes I also seen the same issue  but i would like to suggest you one thing try to remove multiple times   used container keys in one. Then hope it may work.\n---\n\nUser 'niranjandarshann' said:\n---\nFor Me this Worked fine \r\n```\r\n\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  labels:\r\n    run: busybox\r\n  name: busybox\r\nspec:\r\n  containers:\r\n  - name: busybox\r\n    image: busybox\r\n    command: [\"/bin/sh\", \"-c\", \"echo Hello World! && sleep 3600\"]\r\n  - name: busybox2\r\n    image: busybox\r\n    command: [\"/bin/sh\", \"-c\", \"echo Hello World! && sleep 3600\"]\r\n  - name: busybox3\r\n    image: busybox\r\n    command: [\"/bin/sh\", \"-c\", \"echo Hello World! && sleep 3600\"]\r\n  dnsPolicy: ClusterFirst\r\n  restartPolicy: Never\r\n\r\n```\n---\n\nUser 'niranjandarshann' said:\n---\n/sig node\n---\n\nUser 'ffromani' said:\n---\n/triage accepted\n---\n\nUser 'chengjoey' said:\n---\nThis can also be created, multpie \"spec\"\r\n```\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  creationTimestamp: null\r\n  labels:\r\n    run: busybox\r\n  name: busybox\r\nspec:\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox2\r\n    resources: {}\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox3\r\n    resources: {}\r\n  dnsPolicy: ClusterFirst\r\n  restartPolicy: Never\r\nspec:\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox2\r\n    resources: {}\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox3\r\n    resources: {}\r\n  dnsPolicy: ClusterFirst\r\n  restartPolicy: Never\r\n\r\n```\r\nget pod:\r\n```\r\nNAME                         READY   STATUS      RESTARTS   AGE\r\nbusybox                      0/1     Completed   0          5s\r\n```\n---\n\nUser 'aojea' said:\n---\nIf we execute the manifest with enough verbosity we can see the json that is sent to the apiserver\r\n```\r\nI0704 14:57:45.574876  984721 request.go:1188] Request Body: {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"Pod\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"creation\r\nTimestamp\\\":null,\\\"labels\\\":{\\\"run\\\":\\\"busybox\\\"},\\\"name\\\":\\\"busybox\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"containers\\\":[{\\\"args\\\":[\\\"/bin/sh\\\",\\\"-c\\\",\\\"echo Hello World!\\\"],\\\"image\\\":\\\"busybox\\\",\\\"name\\\":\\\"busybox3\\\",\\\"resources\\\":{}}],\\\"dnsPolicy\\\":\\\"ClusterFirst\\\",\\\"restartPolicy\\\":\\\"Never\\\"},\\\"status\\\":{}}\\n\"},\"creationTimestamp\":null,\"labels\":{\"run\":\"busybox\"},\"name\":\"busybox\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"args\":[\"/bin/sh\",\"-c\",\"echo Hello World!\"],\"image\":\"busybox\",\"name\":\"busybox3\",\"resources\":{}}],\"dnsPolicy\":\"ClusterFirst\",\"restartPolicy\":\"Never\"},\"status\":{}}\r\nI0704 14:57:45.574908  984721 round_trippers.go:466] curl -v -XPOST  -H \"Accept: application/json\" -H \"Content-Type: application/json\" -H \"User-Agent: kubectl/v1.27.15 (linux/amd64) kubernetes/a62bdc4\" 'https://127.0.0.1:33491/api/v1/namespaces/default/pods?fieldManager=kubectl-client-side-apply&fieldValidation=Strict'\r\nI0704 14:57:45.581371  984721 round_trippers.go:553] POST https://127.0.0.1:33491/api/v1/namespaces/default/pods?fieldManager=kubectl-client-side-apply&fieldValidation=Strict 201 Created in 6 milliseconds\r\n```\r\n\r\nThat does not contain the duplicate entries, so most probably something happens on the conversion that \"swallows\" the error and deduplicate the entries\r\n\r\n/remove sig-node\r\n/sig api-machinery\r\n/sig cli\r\n\r\nI think it should be one of these SIGs , but does not look like node\n---\n\nUser 'HirazawaUi' said:\n---\n/remove-sig node\n---\n\nUser 'chengjoey' said:\n---\nhttps://github.com/kubernetes/kubernetes/blob/95debfb5b6dcfa2467f2700d81df6767a2c91bac/staging/src/k8s.io/cli-runtime/pkg/resource/visitor.go#L589-L599\r\n=>\r\nhttps://github.com/kubernetes/kubernetes/blob/95debfb5b6dcfa2467f2700d81df6767a2c91bac/staging/src/k8s.io/apimachinery/pkg/util/yaml/decoder.go#L122-L135\r\n\r\nThere is no strict validation using `yaml.UnmarshalStrict`.\r\n\r\ndemo:\r\n```\r\npackage main\r\n\r\nimport (\r\n\t\"log\"\r\n\t\"sigs.k8s.io/yaml\"\r\n)\r\ntype dat struct {\r\n\tA string `json:\"a\"`\r\n}\r\n\r\nvar file = `\r\na: b\r\na: c\r\n`\r\n\r\nfunc main() {\r\n\tvar d dat\r\n\tvar dStrict dat\r\n\tif err := yaml.Unmarshal([]byte(file), &d); err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tlog.Println(\"unmarshal without strict succeed, value:\", d.A)\r\n\tif err := yaml.UnmarshalStrict([]byte(file), &dStrict); err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tlog.Println(dStrict.A)\r\n}\r\n```\r\n\r\noutput:\r\n```\r\n2024/07/05 11:33:43 unmarshal without strict succeed, value: c\r\npanic: error converting YAML to JSON: yaml: unmarshal errors:\r\n  line 3: key \"a\" already set in map\r\n```\n---\n\nUser 'chengjoey' said:\n---\nkubectl apply or create should add `strict` option when converting yaml to json\n---\n\nUser 'k8s-triage-robot' said:\n---\nThis issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'NikolaStoychev' said:\n---\n/close\n---\n\nUser 'k8s-ci-robot' said:\n---\n@NikolaStoychev: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/125885#issuecomment-3382564717):\n\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---",
    "question": "Could you explain how to resolve 'Multi-container pod creation from YAML with multiple \"containers\" statements'?",
    "ideal_answer": "@NikolaStoychev Yes I also seen the same issue  but i would like to suggest you one thing try to remove multiple times   used container keys in one. Then hope it may work.\n\nFor Me this Worked fine \r\n```\r\n\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  labels:\r\n    run: busybox\r\n  name: busybox\r\nspec:\r\n  containers:\r\n  - name: busybox\r\n    image: busybox\r\n    command: [\"/bin/sh\", \"-c\", \"echo Hello World! && sleep 3600\"]\r\n  - name: busybox2\r\n    image: busybox\r\n    command: [\"/bin/sh\", \"-c\", \"echo Hello World! && sleep 3600\"]\r\n  - name: busybox3\r\n    image: busybox\r\n    command: [\"/bin/sh\", \"-c\", \"echo Hello World! && sleep 3600\"]\r\n  dnsPolicy: ClusterFirst\r\n  restartPolicy: Never\r\n\n```\n\n/triage accepted\n\nThis can also be created, multpie \"spec\"\r\n```\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  creationTimestamp: null\r\n  labels:\r\n    run: busybox\r\n  name: busybox\r\nspec:\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox2\r\n    resources: {}\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox3\r\n    resources: {}\r\n  dnsPolicy: ClusterFirst\r\n  restartPolicy: Never\r\nspec:\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox2\r\n    resources: {}\r\n  containers:\r\n  - args:\r\n    - /bin/sh\r\n    - -c\r\n    - echo Hello World!\r\n    image: busybox\r\n    name: busybox3\r\n    resources: {}\r\n  dnsPolicy: ClusterFirst\r\n  restartPolicy: Never\r\n\n```\r\nget pod:\r\n```\r\nNAME                         READY   STATUS      RESTARTS   AGE\r\nbusybox                      0/1     Completed   0          5s\r\n```\n\nIf we execute the manifest with enough verbosity we can see the json that is sent to the apiserver\r\n```\r\nI0704 14:57:45.574876  984721 request.go:1188] Request Body: {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"Pod\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"creation\r\nTimestamp\\\":null,\\\"labels\\\":{\\\"run\\\":\\\"busybox\\\"},\\\"name\\\":\\\"busybox\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"containers\\\":[{\\\"args\\\":[\\\"/bin/sh\\\",\\\"-c\\\",\\\"echo Hello World!\\\"],\\\"image\\\":\\\"busybox\\\",\\\"name\\\":\\\"busybox3\\\",\\\"resources\\\":{}}],\\\"dnsPolicy\\\":\\\"ClusterFirst\\\",\\\"restartPolicy\\\":\\\"Never\\\"},\\\"status\\\":{}}\\n\"},\"creationTimestamp\":null,\"labels\":{\"run\":\"busybox\"},\"name\":\"busybox\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"args\":[\"/bin/sh\",\"-c\",\"echo Hello World!\"],\"image\":\"busybox\",\"name\":\"busybox3\",\"resources\":{}}],\"dnsPolicy\":\"ClusterFirst\",\"restartPolicy\":\"Never\"},\"status\":{}}\r\nI0704 14:57:45.574908  984721 round_trippers.go:466] curl -v -XPOST  -H \"Accept: application/json\" -H \"Content-Type: application/json\" -H \"User-Agent: kubectl/v1.27.15 (linux/amd64) kubernetes/a62bdc4\" 'https://127.0.0.1:33491/api/v1/namespaces/default/pods?fieldManager=kubectl-client-side-apply&fieldValidation=Strict'\r\nI0704 14:57:45.581371  984721 round_trippers.go:553] POST https://127.0.0.1:33491/api/v1/namespaces/default/pods?fieldManager=kubectl-client-side-apply&fieldValidation=Strict 201 Created in 6 milliseconds\r\n```\r\n\nThat does not contain the duplicate entries, so most probably something happens on the conversion that \"swallows\" the error and deduplicate the entries\r\n\n/remove sig-node\r\n\nI think it should be one of these SIGs , but does not look like node\n\n/remove-sig node\n\nhttps://github.com/kubernetes/kubernetes/blob/95debfb5b6dcfa2467f2700d81df6767a2c91bac/staging/src/k8s.io/cli-runtime/pkg/resource/visitor.go#L589-L599\r\n=>\r\nhttps://github.com/kubernetes/kubernetes/blob/95debfb5b6dcfa2467f2700d81df6767a2c91bac/staging/src/k8s.io/apimachinery/pkg/util/yaml/decoder.go#L122-L135\r\n\nThere is no strict validation using `yaml.UnmarshalStrict`.\r\n\ndemo:\r\n```\r\npackage main\r\n\nimport (\r\n\t\"log\"\r\n\t\"sigs.k8s.io/yaml\"\r\n)\r\ntype dat struct {\r\n\tA string `json:\"a\"`\r\n}\r\n\nvar file = `\r\na: b\r\na: c\r\n`\r\n\nfunc main() {\r\n\tvar d dat\r\n\tvar dStrict dat\r\n\tif err := yaml.Unmarshal([]byte(file), &d); err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tlog.Println(\"unmarshal without strict succeed, value:\", d.A)\r\n\tif err := yaml.UnmarshalStrict([]byte(file), &dStrict); err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tlog.Println(dStrict.A)\r\n}\r\n```\r\n\noutput:\r\n```\r\n2024/07/05 11:33:43 unmarshal without strict succeed, value: c\r\npanic: error converting YAML to JSON: yaml: unmarshal errors:\r\n  line 3: key \"a\" already set in map\r\n```\n\nkubectl apply or create should add `strict` option when converting yaml to json\n\nThis issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n@NikolaStoychev: Closing this issue."
  },
  {
    "id": "gen_nat_046",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [DevTools Bug] Children cannot be added or removed during a reorder operation.\n\nA user reported the following issue titled '[DevTools Bug] Children cannot be added or removed during a reorder operation.' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### Website or app\n\nhttps://github.com/Alice-in-korea/chrome_bug_report.git\n\n### Repro steps\n\n1. Click next day button or prev day button\r\n2. Normally there are only three column for three days but you can see the fragment of other column with error message.\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n4.13.5-0ae5290b54\n\n### Error message (automated)\n\nChildren cannot be added or removed during a reorder operation.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:21301:41\r\n    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:19286:22)\r\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:19446:12\r\n    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37413:39)\n```\n\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Children cannot be added or removed during a reorder operation. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'leventdeniz' said:\n---\nSee this comment: https://github.com/facebook/react/issues/21468#issuecomment-838770329\n---\n\nUser 'Christopher-Stevers' said:\n---\nI'm having the same issue.\r\n### Website or app\r\n\r\nhttps://github.com/Christopher-Stevers/OpenQ-Frontend/tree/pagination\r\n\r\n### Repro steps\r\n\r\nGo to organization route, toggle to Bounties, and scroll down. This will load more bounties (infinite scrolling) then click on remove unfunded checkbox.\r\n\r\nHow often does this bug happen?\r\n\r\nEvery time\n---\n\nUser 'usamajabbasi' said:\n---\nFacing same issue \u270b\ud83c\udffb\u270b\ud83c\udffb\n---\n\nUser 'joaoigormatos' said:\n---\nAlso facing the same problem.\n---\n\nUser 'gaearon' said:\n---\ncc @lunaruan seems like this has a repro, has anyone had a chance to look?\n---\n\nUser 'lunaruan' said:\n---\nNot yet. It seems like both repros are unavailable. @Christopher-Stevers could you reupload your repro please? \ud83d\ude0a\n---\n\nUser 'ychcg' said:\n---\nI also facing the same issue\n---\n\nUser 'lunaruan' said:\n---\n@ychcg Could you upload a repro for this?\n---\n\nUser 'aomini' said:\n---\nI had faced this issue in infinite scroll due to duplicate keys. \r\nPreview of devtools in a gif:\r\n![duplicate](https://user-images.githubusercontent.com/38497578/176190094-c29363b6-e3d5-4b96-b800-5548e1b3c91c.gif)\n---\n\nUser 'ghimire-dinesh' said:\n---\nI had faced the same issue in infinite scroll due to duplicate keys and got fixed after changing the key.\n---\n\nUser 'leventdeniz' said:\n---\nI threw together a quick demo for this @lunaruan \r\n\r\nDemo: https://po0o5k.csb.app/ \r\nCode: https://codesandbox.io/s/mystifying-ptolemy-po0o5k?file=/src/App.js\r\n\r\n1. open react-devtools \r\n2. click one of the two buttons\r\n\r\nI am aware that this is fixable by just removing the duplicate keys but still wanted to provide a demo for this, as it was asked for in this thread. In my demo it is easily fixable by cleaning up the data but for others it may be to use a key that is more unique.\n---\n\nUser 'eps1lon' said:\n---\nI can no longer reproduce this with React DevTools 7.0. If the issue persists, please open a new issue.\n---",
    "question": "I'm seeing 'Children cannot be added or removed during a reorder operation.'. Is this a known bug in facebook/react?",
    "ideal_answer": "See this comment: https://github.com/facebook/react/issues/21468#issuecomment-838770329\n\nI'm having the same issue.\r\n### Website or app\r\n\nhttps://github.com/Christopher-Stevers/OpenQ-Frontend/tree/pagination\r\n\n### Repro steps\r\n\nGo to organization route, toggle to Bounties, and scroll down. This will load more bounties (infinite scrolling) then click on remove unfunded checkbox.\r\n\nHow often does this bug happen?\r\n\nEvery time\n\nFacing same issue \u270b\ud83c\udffb\u270b\ud83c\udffb\n\nAlso facing the same problem.\n\ncc @lunaruan seems like this has a repro, has anyone had a chance to look?\n\nNot yet. It seems like both repros are unavailable. @Christopher-Stevers could you reupload your repro please? \ud83d\ude0a\n\nI also facing the same issue\n\n@ychcg Could you upload a repro for this?\n\nI had faced this issue in infinite scroll due to duplicate keys. \r\nPreview of devtools in a gif:\r\n![duplicate](https://user-images.githubusercontent.com/38497578/176190094-c29363b6-e3d5-4b96-b800-5548e1b3c91c.gif)\n\nI had faced the same issue in infinite scroll due to duplicate keys and got fixed after changing the key.\n\nI threw together a quick demo for this @lunaruan \r\n\nDemo: https://po0o5k.csb.app/ \r\nCode: https://codesandbox.io/s/mystifying-ptolemy-po0o5k?file=/src/App.js\r\n\n1. open react-devtools \r\n2. click one of the two buttons\r\n\nI am aware that this is fixable by just removing the duplicate keys but still wanted to provide a demo for this, as it was asked for in this thread. In my demo it is easily fixable by cleaning up the data but for others it may be to use a key that is more unique.\n\nI can no longer reproduce this with React DevTools 7.0. If the issue persists, please open a new issue."
  },
  {
    "id": "gen_nat_047",
    "category": "feature_request",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: Invalid deprecation information in k8s.io/apimachinery/pkg/util/wait\n\nA user reported the following issue titled 'Invalid deprecation information in k8s.io/apimachinery/pkg/util/wait' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nThe comments for `NewExponentialBackoffManager` read:\r\n```go\r\n// NewExponentialBackoffManager returns a manager for managing exponential backoff. Each backoff is jittered and\r\n// backoff will not exceed the given max. If the backoff is not called within resetDuration, the backoff is reset.\r\n// This backoff manager is used to reduce load during upstream unhealthiness.\r\n//\r\n// Deprecated: Will be removed when the legacy Poll methods are removed. Callers should construct a\r\n// Backoff struct, use DelayWithReset() to get a DelayFunc that periodically resets itself, and then\r\n// invoke Timer() when calling wait.BackoffUntil.\r\n//\r\n// Instead of:\r\n//\r\n//\tbm := wait.NewExponentialBackoffManager(init, max, reset, factor, jitter, clock)\r\n//\t...\r\n//\twait.BackoffUntil(..., bm.Backoff, ...)\r\n//\r\n// Use:\r\n//\r\n//\tdelayFn := wait.Backoff{\r\n//\t  Duration: init,\r\n//\t  Cap:      max,\r\n//\t  Steps:    int(math.Ceil(float64(max) / float64(init))), // now a required argument\r\n//\t  Factor:   factor,\r\n//\t  Jitter:   jitter,\r\n//\t}.DelayWithReset(reset, clock)\r\n//\twait.BackoffUntil(..., delayFn.Timer(), ...)\r\n```\r\n\r\nIn particular, the comments are asking us to use `wait.BackoffUntil(..., delayFn.Timer(), ...)`.\r\n\r\nThis does not seem possible, given that the second parameter for `wait.BackoffUntil` is of type `BackoffManager` (interface), while `delayFn.Timer()` returns a `Timer` interface. Maybe the author intended to update the definition of `BackoffUntil` but ended up not doing so for backwards-compatible reasons.\r\n\r\nEven `wait.BackoffUntil(..., bm.Backoff, ...)` does not seem correct.\r\n\r\nThe documentation for this package needs to be revisited. At the moment, anyone using the latest version of this package would get deprecation warnings from linters, with no remediation possible.\n\n### What did you expect to happen?\n\nThe documentation should be correct.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nN/A\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nK8s v1.27.1, but also master branch\n\n### Cloud provider\n\nN/A\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat /etc/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n</details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n</details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n</details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n</details>\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'antoninbas' said:\n---\ncc @smarterclayton\n---\n\nUser 'antoninbas' said:\n---\n/sig api-machinery\n---\n\nUser 'fedebongio' said:\n---\n/assign @MikeSpreitzer @tkashem \r\n/triage accepted\n---\n\nUser 'k8s-triage-robot' said:\n---\nThis issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted\n---\n\nUser 'fedebongio' said:\n---\n/triage accepted\n---\n\nUser 'k8s-triage-robot' said:\n---\nThis issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-ci-robot' said:\n---\n@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/117829#issuecomment-3398495623):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---",
    "question": "Could you explain how to resolve 'Invalid deprecation information in k8s.io/apimachinery/pkg/util/wait'?",
    "ideal_answer": "cc @smarterclayton\n\n/triage accepted\n\nThis issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted\n\n/triage accepted\n\nThis issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\"."
  },
  {
    "id": "gen_nat_048",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [React 19] requestFormReset reports TypeError Cannot read properties of null (reading \n\nA user reported the following issue titled '[React 19] requestFormReset reports TypeError Cannot read properties of null (reading 'queue') on repeated form submissions' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n## Summary\r\n\r\nHi, React team,\r\n\r\nI've recently been trying the new form actions in React 19, I'm trying to reproduce a race condition with multiple form submissions in a short time. However, I occasionally get the error `TypeError\r\nCannot read properties of null (reading 'queue')` after a few consecutive submissions.\r\n\r\nAfter some investigations, I'm able to create the minimal reproducing steps below:\r\n\r\n1. Open [codesandbox.io/p/sandbox/confident-sky-8vr69k](https://codesandbox.io/p/sandbox/confident-sky-8vr69k)\r\n```jsx\r\nfunction App() {\r\n  const formAction = async () => {\r\n    await new Promise((resolve) => setTimeout(resolve, 3000));\r\n  };\r\n\r\n  return (\r\n    <form action={formAction}>\r\n      <input type=\"text\" name=\"name\" />\r\n      <input type=\"submit\" />\r\n    </form>\r\n  );\r\n}\r\n\r\nexport default App;\r\n```\r\n2. Input \"1\" in the text field\r\n3. Submit form\r\n4. Within 3 seconds (before the Client Action resolved), submit the form again\r\n\r\nExpected behavior:\r\nForm fields resetted.\r\n\r\nActual behavior:\r\nThe page breaks reporting a TypeError below:\r\n```\r\nTypeError: Cannot read properties of null (reading 'queue')\r\nrequestFormReset$1\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:7001:74\r\neval\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:6956:15\r\nstartTransition\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:6908:27\r\nstartHostTransition\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:6948:7\r\nlistener\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:16008:21\r\nprocessDispatchQueue\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:16066:17\r\neval\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:16665:9\r\nbatchedUpdates$1\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:2689:40\r\ndispatchEventForPluginEventSystem\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:16221:7\r\ndispatchEvent\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:20127:11\r\ndispatchDiscreteEvent\r\nhttps://gwprwq.csb.app/node_modules/react-dom/cjs/react-dom-client.development.js:20095:11\r\n```\r\n\r\nAm I missing anything? Thanks.\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'eps1lon' said:\n---\nThank you for the repro. The Codesandbox didn't load for me so I created a new one. We can repro this with just clicking submit buttons as well:\r\n\r\nhttps://github.com/facebook/react/assets/12292047/6f4444ec-1393-4de5-aab8-ab8b5c93ba54\r\n\r\n-- https://codesandbox.io/p/sandbox/confident-sky-8vr69k\r\n\r\n1. Enter a value\r\n2. Submit form\r\n3. Submit again before form action resolved\r\n4. Observe crash\n---\n\nUser 'evisong' said:\n---\nThanks for fixing the Codesandbox. Btw the React I used was `19.0.0-rc-3563387fe3-20240621`.\n---\n\nUser 'Parvezkhan0' said:\n---\nIs this issue resolved?\n---\n\nUser 'evisong' said:\n---\nHi, React team, \r\n\r\nMay I ask is there any update on this? Thanks.\n---\n\nUser 'ryanflorence' said:\n---\nI'm running into this everywhere, including the official doc's demo of `useFormStatus` (when you remove the `disabled` button prop) and quickly click the button it reliably breaks\n\n```tsx\nimport { useFormStatus } from \"react-dom\";\nimport { submitForm } from \"./actions.js\";\n\nfunction Submit() {\n  const { pending } = useFormStatus();\n  return <button type=\"submit\">{pending ? \"Submitting...\" : \"Submit\"}</button>;\n}\n\nfunction Form({ action }) {\n  return (\n    <form action={action}>\n      <Submit />\n    </form>\n  );\n}\n\nexport default function App() {\n  return <Form action={submitForm} />;\n}\n```\n\n<img width=\"1618\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bb31bbfa-e843-4b75-a66c-5f8f3c279b2c\" />\n\nhttps://codesandbox.io/p/sandbox/react-dev-forked-n57tkm\n---\n\nUser 'rickhanlonii' said:\n---\nThanks for reporting, this should be fixed with: https://github.com/facebook/react/pull/33055\n\nSandbox of @evisong's issue with the PR: https://codesandbox.io/p/sandbox/confident-sky-8vr69k\n\nSandbox of @ryanflorence's issue with that PR: https://codesandbox.io/p/sandbox/react-dev-forked-n57tkm\n---\n\nUser 'chrisb2244' said:\n---\nI still see the same error (including in the sandbox linked above) if I click a bunch of times. Is there a maximum number of queued events or some timing requirement between them in the current fix (I'm using react@canary -> ^19.2.0-canary-280ff6fe-20250606 in my test project).\n\nCan I avoid this by switching to exclusively controlled inputs, or will that have no effect here?\nIs there some other workaround to prevent this error?\n---",
    "question": "What is the resolution for 'requestFormReset reports TypeError Cannot read properties of null (reading'?",
    "ideal_answer": "Thank you for the repro. The Codesandbox didn't load for me so I created a new one. We can repro this with just clicking submit buttons as well:\r\n\nhttps://github.com/facebook/react/assets/12292047/6f4444ec-1393-4de5-aab8-ab8b5c93ba54\r\n\n-- https://codesandbox.io/p/sandbox/confident-sky-8vr69k\r\n\n1. Enter a value\r\n2. Submit form\r\n3. Submit again before form action resolved\r\n4. Observe crash\n\nThanks for fixing the Codesandbox. Btw the React I used was `19.0.0-rc-3563387fe3-20240621`.\n\nIs this issue resolved?\n\nHi, React team, \r\n\nMay I ask is there any update on this? Thanks.\n\nI'm running into this everywhere, including the official doc's demo of `useFormStatus` (when you remove the `disabled` button prop) and quickly click the button it reliably breaks\n\n```tsx\nimport { useFormStatus } from \"react-dom\";\nimport { submitForm } from \"./actions.js\";\n\nfunction Submit() {\n  const { pending } = useFormStatus();\n  return <button type=\"submit\">{pending ? \"Submitting...\" : \"Submit\"}</button>;\n}\n\nfunction Form({ action }) {\n  return (\n    <form action={action}>\n      <Submit />\n    </form>\n  );\n}\n\nexport default function App() {\n  return <Form action={submitForm} />;\n}\n```\n\n<img width=\"1618\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bb31bbfa-e843-4b75-a66c-5f8f3c279b2c\" />\n\nhttps://codesandbox.io/p/sandbox/react-dev-forked-n57tkm\n\nThanks for reporting, this should be fixed with: https://github.com/facebook/react/pull/33055\n\nSandbox of @evisong's issue with the PR: https://codesandbox.io/p/sandbox/confident-sky-8vr69k\n\nSandbox of @ryanflorence's issue with that PR: https://codesandbox.io/p/sandbox/react-dev-forked-n57tkm\n\nI still see the same error (including in the sandbox linked above) if I click a bunch of times. Is there a maximum number of queued events or some timing requirement between them in the current fix (I'm using react@canary -> ^19.2.0-canary-280ff6fe-20250606 in my test project).\n\nCan I avoid this by switching to exclusively controlled inputs, or will that have no effect here?\nIs there some other workaround to prevent this error?"
  },
  {
    "id": "gen_nat_049",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [DevTools Bug] Cannot remove node \"92\" because no matching node was found in the Store.\n\nA user reported the following issue titled '[DevTools Bug] Cannot remove node \"92\" because no matching node was found in the Store.' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### Website or app\n\nhttp://localhost:3000/\n\n### Repro steps\n\nI was trying to inspect my component tree via react dev tools, but each time I make an interaction on the website which changes the state, it throws an error \n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.0.1-c7c68ef842\n\n### Error message (automated)\n\nCannot remove node \"92\" because no matching node was found in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1173889\r\n    at v.emit (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1140783)\r\n    at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1142390\r\n    at bridgeListener (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1552662)\n```\n\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'DominicCronin' said:\n---\nI have seen a similar error running version 6.0.0 on Firefox 133.0.3\r\n\r\nThe error was thrown emit@moz-extension://550a33a5-59ea-4600-a45c-cc050a60e3cd/build/main.js:1:1141200\r\nv/this._wallUnlisten<@moz-extension://550a33a5-59ea-4600-a45c-cc050a60e3cd/build/main.js:1:1142807\r\nbridgeListener@moz-extension://550a33a5-59ea-4600-a45c-cc050a60e3cd/build/main.js:1:1552360\n---\n\nUser 'adi-ydv-1' said:\n---\n@TheOnly1TY  host your code and then share the link not your locally run app link.\n---\n\nUser 'Rohanpatil7' said:\n---\njust ignore that massage\n---\n\nUser 'TheOnly1TY' said:\n---\nOk\nThank you\n---\n\nUser 'aesgdo' said:\n---\nUncaught Error: Cannot remove node \"225\" because no matching node was found in the Store.\nDismiss\nThe error was thrown at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1193929\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1160378)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1161985\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1572692)\n---\n\nUser 'eps1lon' said:\n---\nWe fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue.\n---",
    "question": "Help needed with facebook/react. Cannot remove node \"92\" because no matching node was found in the Store..",
    "ideal_answer": "I have seen a similar error running version 6.0.0 on Firefox 133.0.3\r\n\nThe error was thrown emit@moz-extension://550a33a5-59ea-4600-a45c-cc050a60e3cd/build/main.js:1:1141200\r\nv/this._wallUnlisten<@moz-extension://550a33a5-59ea-4600-a45c-cc050a60e3cd/build/main.js:1:1142807\r\nbridgeListener@moz-extension://550a33a5-59ea-4600-a45c-cc050a60e3cd/build/main.js:1:1552360\n\n@TheOnly1TY  host your code and then share the link not your locally run app link.\n\njust ignore that massage\n\nOk\nThank you\n\nUncaught Error: Cannot remove node \"225\" because no matching node was found in the Store.\nDismiss\nThe error was thrown at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1193929\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1160378)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1161985\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1572692)\n\nWe fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."
  },
  {
    "id": "gen_nat_050",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [Compiler Bug]: breaks referencial stability in @tanstack/react-query\n\nA user reported the following issue titled '[Compiler Bug]: breaks referencial stability in @tanstack/react-query' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://stackblitz.com/edit/vitejs-vite-ucqzqtvo?file=vite.config.ts\n\n### Repro steps\n\nI noticed my fetch responses weren't referentially stable and causing aggressive re-renders in my application. I traced it to react-compiler's interactions on @tanstack/react-query and made a repro attached above.  Not sure how to solve this, seems like a strange react-compiler side effect.\n\n- The useMemo is rerunning when `data` is unchanged\n```\nexport function useFetchThing() {\n  const { data, isLoading, error } = useQuery({\n    queryKey: ['blah'],\n    queryFn: async () => {\n      const resp = await fetch('google.com');\n      return resp;\n    },\n  });\n\n  React.useMemo(() => {\n    console.log(\n      '!!! Should not re-run on re-render, but does with react-compiler!'\n    );\n  }, [data]);\n\n  return { data, isLoading, error };\n}\n```\n\n\n\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.0\n\n### What version of React Compiler are you using?\n\n19.1.0-rc.2\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'josephsavona' said:\n---\nAre you using the latest eslint-plugin-react-hooks RC? If you open this in [compiler playground](https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgDMoA7OXASwhPyjAQDEFc4ALAFVYpIHMAKAJT5gAHRr441MAWD4AJgENcCgDT4KYADIQFc7jzUIYMHPgC++ALy16ARShGAnn1Hj8+AI4OYjgNIJHZHwAbQByACMAGwVWUIBdFTF3dy8nBhIghTBHMnxBKwA+YSTk90kSaXwYBDBMK3wFAHcFCgJCZjY+UJ4ICB5IhAA6SQBbUIEAbhLS6txYGmraqbdzRPEzSbESgCUEBXJBugQAWQQRiD58yyLXUvLIAcHIvr5p5NCAQi-8AGVWaEicnwJAgBGqAFoYKR8NQqghIQgSHIjGpwlACHIIDV8I1Wqw4ftcODRpgKAMYB9Qm9Nus1MFFMo4jSSrN5sJ5EpVOotDo9LxDMZTGZlmYQGYgA), the compiler highlights that the useMemo itself is problematic. The compiler knows that `console.log()` is just for debugging and really doesn't do anything (doesn't produce a value), so it doesn't get memoized unless it happens to be part of some other code that it getting memoized. That isn't the case here, it's just a standalone log call, so we don't memoize it. \n\nDo you have some actual computation that isn't getting memoized as you'd expect? The linked repro has the same code that you pasted so i wasn't sure.\n---\n\nUser 'TheSisb' said:\n---\nMaybe I went a little too overboard on the _minimal_ repro if react-compiler is smart enough to optimize that away. However the issue persists with a more complicated setup:\n\n```\nimport React from 'react';\nimport { useQuery } from '@tanstack/react-query';\n\nconst computeThing = () => {\n  console.log(\n    '!!! Should not re-run on re-render, but does with react-compiler!'\n  );\n  return Math.sqrt(10 * 9 * 8 * 7 * 6 * 5);\n};\n\nexport function useFetchThing() {\n  const { data, isLoading, error } = useQuery({\n    queryKey: ['blah'],\n    queryFn: async () => {\n      const resp = await fetch('google.com');\n      return resp;\n    },\n  });\n\n  const x = React.useMemo(() => {\n    return computeThing();\n  }, [data]);\n\n  console.log(x);\n\n  return { data, isLoading, error };\n}\n```\n\n<img width=\"2910\" height=\"425\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bef67887-36bb-45ec-94f7-41d03a7fc9d6\" />\n\nAnd indeed, in my application the operation I'm running in the memo was slow enough to produce significant lag, which is how I found this. It's a very common pattern for us to want to merge/mutate the HTTP response in some significant way.\n\nIn my application: \n\n<img width=\"901\" height=\"1255\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/912f0ae4-968d-4450-86b7-025a1e181508\" />\n\n<img width=\"242\" height=\"123\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d097e11e-3794-456e-9923-a13d56f902c4\" />\n---\n\nUser 'josephsavona' said:\n---\nGotcha, thanks for confirming. So we can maybe simplify to where there's a `computeThing(data)` call and the result of that is returned: [example playground](https://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEASggIZyEBmMEGBA5DGRfQNwA6AdpjvgcAShgEARSgIYATwIBfAtVoMAAnlKcwquAGsA9E3J4AtAEdxUtly5wI6wtexQ8CACoALNJwDmBALwEAFACUvgB8-FwEBNbqEAA2CAB0sRCe-hGRDACE2QQAyq7QsQAmBJwQhEyGMFCcBDYElUycRRIANAQARo4ERRAIYAQA7mh4rg3MRvZYaPEwmfTpgRy143iwtQCypKMJYMb4-gCMAAwEAFQEAJznBAAcNwDsNwBsNwCsS1wyy1wIAB68Kg1ChoepCBAAMQQeDgrjcHlSwWA6WiGn4PW2pHaaDAABkIKQigj2hIaDBZL5BMIxBJJP5kStIqZaQBpBCSZAEADa9A6sVIrnoAF1WukmWZJBDOJzSGBJJw4AFgj4wgyMhlURV+lhKaRBqQRvJobD-PRPBAUvEEvZ6J9GRkmGsYLUmGAsMt1TJRSsZHaUTY0VNHAgSn4SAYEuCNggMBB-EFQuF7Y71lFaFhg-CvP4ipi7ZEvdzc6ohX6Vqi4olkqk-mXIinneig04iti8QSiV4STAybJljIQDIgA).\n\nAs you can see, the compiler is memoizing the `computeThing(data)` call, and will only run it when  `data` changes. Relevant bit from the output panel is:\n\n```js\n  let t1;\n  if ($[1] !== data) {\n    t1 = computeThing(data);\n    $[1] = data;\n    $[2] = t1;\n  } else {\n    t1 = $[2];\n  }\n  const computed = t1;\n```\n\nThis is the same behavior as `useMemo()`. If this is re-running, then by process of elimination it means that `data` must have actually changed (or the component itself got torn down and re-rendered, perhaps it rendered with a different `key`?). If `useFetch()` isn't returning stable results, that would be a bug with its implementation.\n---\n\nUser 'TkDodo' said:\n---\nThe problem in the reproduction is that the memoization wasn\u2019t using `data`, and that the custom hook wasn\u2019t doing anything with the `useMemo` result apart from logging it.\n\nHere\u2019s a version where:\n\n- `computeThing` actually uses data\n- `useFetchThing` returns the result that `useMemo` computes\n- the component actually renders the result from `useFetchThing`\n\nand with that, everything works as expected:\n\nhttps://stackblitz.com/edit/vitejs-vite-hvnbdwnm?file=src%2FuseFetchThing.tsx\n---\n\nUser 'TheSisb' said:\n---\nThank you for the explanation, this makes sense.\n---",
    "question": "I'm getting an error: : breaks referencial stability in @tanstack/react-query. Is there a known workaround?",
    "ideal_answer": "Are you using the latest eslint-plugin-react-hooks RC? If you open this in [compiler playground](https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgDMoA7OXASwhPyjAQDEFc4ALAFVYpIHMAKAJT5gAHRr441MAWD4AJgENcCgDT4KYADIQFc7jzUIYMHPgC++ALy16ARShGAnn1Hj8+AI4OYjgNIJHZHwAbQByACMAGwVWUIBdFTF3dy8nBhIghTBHMnxBKwA+YSTk90kSaXwYBDBMK3wFAHcFCgJCZjY+UJ4ICB5IhAA6SQBbUIEAbhLS6txYGmraqbdzRPEzSbESgCUEBXJBugQAWQQRiD58yyLXUvLIAcHIvr5p5NCAQi-8AGVWaEicnwJAgBGqAFoYKR8NQqghIQgSHIjGpwlACHIIDV8I1Wqw4ftcODRpgKAMYB9Qm9Nus1MFFMo4jSSrN5sJ5EpVOotDo9LxDMZTGZlmYQGYgA), the compiler highlights that the useMemo itself is problematic. The compiler knows that `console.log()` is just for debugging and really doesn't do anything (doesn't produce a value), so it doesn't get memoized unless it happens to be part of some other code that it getting memoized. That isn't the case here, it's just a standalone log call, so we don't memoize it. \n\nDo you have some actual computation that isn't getting memoized as you'd expect? The linked repro has the same code that you pasted so i wasn't sure.\n\nMaybe I went a little too overboard on the _minimal_ repro if react-compiler is smart enough to optimize that away. However the issue persists with a more complicated setup:\n\n```\nimport React from 'react';\nimport { useQuery } from '@tanstack/react-query';\n\nconst computeThing = () => {\n  console.log(\n    '!!! Should not re-run on re-render, but does with react-compiler!'\n  );\n  return Math.sqrt(10 * 9 * 8 * 7 * 6 * 5);\n};\n\nexport function useFetchThing() {\n  const { data, isLoading, error } = useQuery({\n    queryKey: ['blah'],\n    queryFn: async () => {\n      const resp = await fetch('google.com');\n      return resp;\n    },\n  });\n\n  const x = React.useMemo(() => {\n    return computeThing();\n  }, [data]);\n\n  console.log(x);\n\n  return { data, isLoading, error };\n}\n```\n\n<img width=\"2910\" height=\"425\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bef67887-36bb-45ec-94f7-41d03a7fc9d6\" />\n\nAnd indeed, in my application the operation I'm running in the memo was slow enough to produce significant lag, which is how I found this. It's a very common pattern for us to want to merge/mutate the HTTP response in some significant way.\n\nIn my application: \n\n<img width=\"901\" height=\"1255\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/912f0ae4-968d-4450-86b7-025a1e181508\" />\n\n<img width=\"242\" height=\"123\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d097e11e-3794-456e-9923-a13d56f902c4\" />\n\nGotcha, thanks for confirming. So we can maybe simplify to where there's a `computeThing(data)` call and the result of that is returned: [example playground](https://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEASggIZyEBmMEGBA5DGRfQNwA6AdpjvgcAShgEARSgIYATwIBfAtVoMAAnlKcwquAGsA9E3J4AtAEdxUtly5wI6wtexQ8CACoALNJwDmBALwEAFACUvgB8-FwEBNbqEAA2CAB0sRCe-hGRDACE2QQAyq7QsQAmBJwQhEyGMFCcBDYElUycRRIANAQARo4ERRAIYAQA7mh4rg3MRvZYaPEwmfTpgRy143iwtQCypKMJYMb4-gCMAAwEAFQEAJznBAAcNwDsNwBsNwCsS1wyy1wIAB68Kg1ChoepCBAAMQQeDgrjcHlSwWA6WiGn4PW2pHaaDAABkIKQigj2hIaDBZL5BMIxBJJP5kStIqZaQBpBCSZAEADa9A6sVIrnoAF1WukmWZJBDOJzSGBJJw4AFgj4wgyMhlURV+lhKaRBqQRvJobD-PRPBAUvEEvZ6J9GRkmGsYLUmGAsMt1TJRSsZHaUTY0VNHAgSn4SAYEuCNggMBB-EFQuF7Y71lFaFhg-CvP4ipi7ZEvdzc6ohX6Vqi4olkqk-mXIinneig04iti8QSiV4STAybJljIQDIgA).\n\nAs you can see, the compiler is memoizing the `computeThing(data)` call, and will only run it when  `data` changes. Relevant bit from the output panel is:\n\n```js\n  let t1;\n  if ($[1] !== data) {\n    t1 = computeThing(data);\n    $[1] = data;\n    $[2] = t1;\n  } else {\n    t1 = $[2];\n  }\n  const computed = t1;\n```\n\nThis is the same behavior as `useMemo()`. If this is re-running, then by process of elimination it means that `data` must have actually changed (or the component itself got torn down and re-rendered, perhaps it rendered with a different `key`?). If `useFetch()` isn't returning stable results, that would be a bug with its implementation.\n\nThe problem in the reproduction is that the memoization wasn\u2019t using `data`, and that the custom hook wasn\u2019t doing anything with the `useMemo` result apart from logging it.\n\nHere\u2019s a version where:\n\n- `computeThing` actually uses data\n- `useFetchThing` returns the result that `useMemo` computes\n- the component actually renders the result from `useFetchThing`\n\nand with that, everything works as expected:\n\nhttps://stackblitz.com/edit/vitejs-vite-hvnbdwnm?file=src%2FuseFetchThing.tsx\n\nThank you for the explanation, this makes sense."
  },
  {
    "id": "gen_nat_051",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [Compiler Bug]:  Compiler does not optimize functions unless manually memoized\n\nA user reported the following issue titled '[Compiler Bug]:  Compiler does not optimize functions unless manually memoized' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEASggIZyEBmMEGBA5DGRfQNwA6Adl5VJxWgicCAcwR4AkgBMAynlwIAFAEoCwLgQIAbcQTRSCAXgIAGDsIJM8sYb355BwsXgCqnNAEcoCaSrUbNS3EbAgADfQBaABJgAGpY-QBfUPNNRPN0rjghMEJnN09vaSNRcWk5BRVzHj4BIQIoMB8pP3ULKxD89y9mqq5Erhr7Rwam6QB1NDwACxcmgFkEDAhWgI6YYRJyPAA6RoRF5cU-QwA+UtduopblABoCAG0AXWUMwc47OuF9ianZprkpDwSlUbU02U4uUe+ieJS2FD2ALwQKUXUKvWUyjWwQ2eikb04IESQA\n\n### Repro steps\n\nI've noticed that the React compiler doesn't memoize a function (e.g. getUniqueId) returned from a custom hook, even though it's referentially stable. When I manually wrap the return value in React.useMemo, the compiler optimizes it correctly. Is this expected behavior?\n\nSource Code:\n```typescript\nimport React from 'react';\n\nfunction getIdStore() {\n  let id = 0;\n  return function getUniqueId() {\n    return `id-${++id}`;\n  };\n};\nconst getUniqueId = getIdStore();\n\nfunction useId() {\n  return getUniqueId();\n}\n\nfunction useIdWithUseMemo() {\n  return React.useMemo(() => getUniqueId(), []);\n}\n\nfunction useIdWithUseState() {\n  const [id] = React.useState(getUniqueId())\n  return id;\n}\n\n```\n\n\nAfter React Compiler\n\n```typescript\nimport { c as _c } from \"react/compiler-runtime\";\nimport React from \"react\";\n\nfunction getIdStore() {\n  let id = 0;\n  return function getUniqueId() {\n    return `id-${++id}`;\n  };\n}\nconst getUniqueId = getIdStore();\n\nfunction useId() {\n  return getUniqueId();\n}\n\nfunction useIdWithUseMemo() {\n  const $ = _c(1);\n  let t0;\n  if ($[0] === Symbol.for(\"react.memo_cache_sentinel\")) {\n    t0 = getUniqueId();\n    $[0] = t0;\n  } else {\n    t0 = $[0];\n  }\n  return t0;\n}\n\nfunction useIdWithUseState() {\n  const $ = _c(1);\n  let t0;\n  if ($[0] === Symbol.for(\"react.memo_cache_sentinel\")) {\n    t0 = getUniqueId();\n    $[0] = t0;\n  } else {\n    t0 = $[0];\n  }\n  const [id] = React.useState(t0);\n  return id;\n}\n\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.0\n\n### What version of React Compiler are you using?\n\n0.0.0-experimental-2db0664-20250725\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'bgauryy' said:\n---\nCan you check it up?\nCreated a root cause analysis using [octocode-mcp](https://github.com/bgauryy/octocode-mcp)\nhttps://gist.github.com/bgauryy/5b044e85b5caab09b4bee8e94ba78968\n---\n\nUser 'josephsavona' said:\n---\nBy default the compiler infers which functions are components/hooks using the heuristic described in the docs: https://react.dev/reference/react-compiler/compilationMode\n\nIn this case, the useId() hook is named like a hook, but doesn\u2019t call other hooks or have any JSX. You can add \u201cuse memo\u201d to the hook (see above link for full example) to ensure it\u2019s memoized. This heuristic is necessary to not accidentally compile functions that happen to start w \u201cuse\u201d that aren\u2019t meant to be hooks.\n---\n\nUser 'imteammy' said:\n---\nThanks for the explanation!\n\nI\u2019ve been using `\"infer\"` mode by default, so I didn\u2019t realize some values weren\u2019t being memoized automatically \u2014 and had to add `\"use memo\"` manually.\n\nI understand it better now and will go back to the docs for a closer look. I\u2019ll also try comparing with `\"all\"` mode more carefully, since I did run into some issues even when using it before.\n\nAppreciate the help and the link to the docs!\n---\n\nUser 'josephsavona' said:\n---\nAs the docs say, we do not recommend \u201call\u201d mode. It\u2019s only meant for the compiler\u2019s internal test suite. \n\n\u201cInfer\u201d is a good default, combined with occasional \u201cuse memo\u201d for hooks that don\u2019t call other hooks or create jsx.\n---",
    "question": "Could you explain how to resolve ':  Compiler does not optimize functions unless manually memoized'?",
    "ideal_answer": "Can you check it up?\nCreated a root cause analysis using [octocode-mcp](https://github.com/bgauryy/octocode-mcp)\nhttps://gist.github.com/bgauryy/5b044e85b5caab09b4bee8e94ba78968\n\nBy default the compiler infers which functions are components/hooks using the heuristic described in the docs: https://react.dev/reference/react-compiler/compilationMode\n\nIn this case, the useId() hook is named like a hook, but doesn\u2019t call other hooks or have any JSX. You can add \u201cuse memo\u201d to the hook (see above link for full example) to ensure it\u2019s memoized. This heuristic is necessary to not accidentally compile functions that happen to start w \u201cuse\u201d that aren\u2019t meant to be hooks.\n\nThanks for the explanation!\n\nI\u2019ve been using `\"infer\"` mode by default, so I didn\u2019t realize some values weren\u2019t being memoized automatically \u2014 and had to add `\"use memo\"` manually.\n\nI understand it better now and will go back to the docs for a closer look. I\u2019ll also try comparing with `\"all\"` mode more carefully, since I did run into some issues even when using it before.\n\nAppreciate the help and the link to the docs!\n\nAs the docs say, we do not recommend \u201call\u201d mode. It\u2019s only meant for the compiler\u2019s internal test suite. \n\n\u201cInfer\u201d is a good default, combined with occasional \u201cuse memo\u201d for hooks that don\u2019t call other hooks or create jsx."
  },
  {
    "id": "gen_nat_052",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [Compiler Bug]: (BuildHIR::lowerStatement) Handle TryStatement with a finalizer (\n\nA user reported the following issue titled '[Compiler Bug]: (BuildHIR::lowerStatement) Handle TryStatement with a finalizer ('finally') clause' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEAogB4CGAtgA4A2CAFAJRHAA6xRG2+hROBGBws2nIhIEwAnmMzjJAXyJwKOOAAs5CicrR5MFOnVkd5XXQsUKFMBDljFGOogB4AJngBuRQgGE6PDgAawBeYEFhRQA+FwkACQRjCCIAdwgYOnciF1cAek8vWK5mAG5ORRBFIA\n\n### Repro steps\n\nReact compiler breaks when using `finally` clause. I am not sure whether it is a bug or it's meant to be like this. When I was skimming trough code of the compiler I came across this code in `BuildHIR.ts`.\n\nhttps://github.com/facebook/react/blob/main/compiler/packages/babel-plugin-react-compiler/src/HIR/BuildHIR.ts#L1284-L1291\n<img width=\"843\" height=\"163\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9fdb1b9d-8b39-4114-89c2-863211b70fe4\" />\n\nMy question is whether this is just not implemented or whether it meant to be like this. If this implementation is correct, maybe you could suggest an eslint rule I can use to prevent having `finally` clause. Is it me who does something wrong?\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19\n\n### What version of React Compiler are you using?\n\nrc\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'josephsavona' said:\n---\nThanks for posting. This is a known todo, we haven\u2019t added support for this feature yet. The `finally` clause in try/catch has complex control flow and is rarely used, so this is lower on our priority list. By default the compiler will not report these (eg in ESLint) and just skip the component, since there is nothing actionable on your part here. Did you opt in to showing all errors?\n---\n\nUser 'Tarasikee' said:\n---\nThanks for a quick answer. I was more curious about it because what I saw in logger: \n```\nCompilation failed: /path/to/component/Name.tsx\nReason: (BuildHIR::lowerStatement) Handle TryStatement with a finalizer ('finally') clause\n```\nThanks for clarification, will keep it in mind\n---",
    "question": "I'm running into an issue with facebook/react: :  Handle TryStatement with a finalizer (. How can I fix this?",
    "ideal_answer": "Thanks for posting. This is a known todo, we haven\u2019t added support for this feature yet. The `finally` clause in try/catch has complex control flow and is rarely used, so this is lower on our priority list. By default the compiler will not report these (eg in ESLint) and just skip the component, since there is nothing actionable on your part here. Did you opt in to showing all errors?\n\nThanks for a quick answer. I was more curious about it because what I saw in logger: \n```\nCompilation failed: /path/to/component/Name.tsx\nReason: (BuildHIR::lowerStatement) Handle TryStatement with a finalizer ('finally') clause\n```\nThanks for clarification, will keep it in mind"
  },
  {
    "id": "gen_nat_053",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: kubectl apply fails to update container ports section on reapply\n\nA user reported the following issue titled 'kubectl apply fails to update container ports section on reapply' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nCreate a sts using kubectl apply with following manifest\n```\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nginx-statefulset\nspec:\n  replicas: 2\n  serviceName: nginx-service\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:latest\n        ports:\n        - name: http-port\n          containerPort: 80\n        - containerPort: 8080\n          name: p-35b22f0ec56b2\n        - containerPort: 8090\n          name: p-776a6f40d6d71\n        - containerPort: 8080\n          name: p-0ff1e8bce95ec\n        - containerPort: 8080\n          name: p-7762564715c2a\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http-port\n          initialDelaySeconds: 5\n          periodSeconds: 10\n```\n\nThen apply the new below manifest using same kubectl apply method\n```\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nginx-statefulset\nspec:\n  replicas: 2\n  serviceName: nginx-service\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:latest\n        ports:\n        - name: http-port\n          containerPort: 80\n        - containerPort: 8080\n          name: p-1ae69570b6455\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http-port\n          initialDelaySeconds: 5\n          periodSeconds: 10\n```\nAs we can see the ports section is changed in both these manifests, but after applying second manifest I can only see \n```\n- name: http-port\ncontainerPort: 80\n```\nin the kubernetes resource.\n\n### What did you expect to happen?\n\nExpectation is to have the ports of second manifest on the resource\n```\n- name: http-port\ncontainerPort: 80\n- containerPort: 8080\nname: p-1ae69570b6455\n```\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nAs given above\n\n### Anything else we need to know?\n\nNA\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.29.15\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\nServer Version: v1.29.15\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nOn prem\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\nNAME=\"Ubuntu\"\nVERSION=\"20.04.6 LTS (Focal Fossa)\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 20.04.6 LTS\"\nVERSION_ID=\"20.04\"\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nVERSION_CODENAME=focal\nUBUNTU_CODENAME=focal\n$ uname -a\nLinux k8s-master-0 5.4.0-216-generic #236-Ubuntu SMP Fri Apr 11 19:53:21 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</deta\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'priteshkadav' said:\n---\n/sig cli\n/sig api-machinery\n/wg serving\n/committee code-of-conduct\n---\n\nUser 'priteshkadav' said:\n---\n/sig cli\n/sig api-machinery\n---\n\nUser 'liggitt' said:\n---\nI think this is a duplicate of https://github.com/kubernetes/kubernetes/issues/58477\n---\n\nUser 'liggitt' said:\n---\nThe `containerPort` field is the key used to determine item identity\n\nThe initial manifest incorrectly contains multiple entries which appear to have the same identity. The subsequent update to drop *some* of those is treated as a removal of that key, which is applied to remove all items of that key\n---\n\nUser 'BenTheElder' said:\n---\nThis is not a [Code Of Conduct](https://kubernetes.io/community/code-of-conduct/) issue.\n/remove-committee code-of-conduct\nNor is it a [SIG Testing](https://github.com/kubernetes/community/tree/master/sig-testing) issue or [WG Serving](https://github.com/kubernetes/community/tree/master/wg-serving) issue\n/remove-sig testing\n/remove-wg serving\n\n/sig cli api-machinery\n---\n\nUser 'liggitt' said:\n---\n/close\nas a duplicate of https://github.com/kubernetes/kubernetes/issues/58477\n---",
    "question": "I'm seeing 'kubectl apply fails to update container ports section on reapply'. Is this a known bug in kubernetes/kubernetes?",
    "ideal_answer": "/wg serving\n/committee code-of-conduct\n\nI think this is a duplicate of https://github.com/kubernetes/kubernetes/issues/58477\n\nThe `containerPort` field is the key used to determine item identity\n\nThe initial manifest incorrectly contains multiple entries which appear to have the same identity. The subsequent update to drop *some* of those is treated as a removal of that key, which is applied to remove all items of that key\n\nThis is not a [Code Of Conduct](https://kubernetes.io/community/code-of-conduct/) issue.\n/remove-committee code-of-conduct\nNor is it a [SIG Testing](https://github.com/kubernetes/community/tree/master/sig-testing) issue or [WG Serving](https://github.com/kubernetes/community/tree/master/wg-serving) issue\n/remove-sig testing\n/remove-wg serving\n\nas a duplicate of https://github.com/kubernetes/kubernetes/issues/58477"
  },
  {
    "id": "gen_nat_054",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [DevTools Bug] Cannot add node \"29\" because a node with that id is already in the Store.\n\nA user reported the following issue titled '[DevTools Bug] Cannot add node \"29\" because a node with that id is already in the Store.' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### Website or app\n\nhttps://github.com/Gontse7/pyga\n\n### Repro steps\n\nI was trying to debug the website as it in production to see what was wrong with some components and props\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.1.1-44c3d3d665\n\n### Error message (automated)\n\nCannot add node \"29\" because a node with that id is already in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1192232\n    at v.emit (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1160378)\n    at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1161985\n    at bridgeListener (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1572825)\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'sergiotales1' said:\n---\nI saw that the issue was made 8 days ago but your GitHub repository's last commit happened 16 days ago and has some incomplete files there.\n\nAre you still facing this issue? If so please update the repository, I will fork it to see if I can help!\n---\n\nUser 'DanielSchiavini' said:\n---\nThe extension often crashes on me mainly if I leave it open during page reload.\nCurrently working on https://github.com/curvefi/curve-frontend\n\n![Image](https://github.com/user-attachments/assets/b155a816-0486-4519-b9f6-4484936f23ab)\n---\n\nUser 'eps1lon' said:\n---\nWe fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue.\n---",
    "question": "Help needed with facebook/react. Cannot add node \"29\" because a node with that id is already in the Store..",
    "ideal_answer": "I saw that the issue was made 8 days ago but your GitHub repository's last commit happened 16 days ago and has some incomplete files there.\n\nAre you still facing this issue? If so please update the repository, I will fork it to see if I can help!\n\nThe extension often crashes on me mainly if I leave it open during page reload.\nCurrently working on https://github.com/curvefi/curve-frontend\n\n![Image](https://github.com/user-attachments/assets/b155a816-0486-4519-b9f6-4484936f23ab)\n\nWe fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."
  },
  {
    "id": "gen_nat_055",
    "category": "troubleshooting",
    "source_file": "microsoft_vscode_issue.json",
    "context": "Repository: microsoft/vscode\nTitle: filter webview editors out of \n\nA user reported the following issue titled 'filter webview editors out of 'add context'' in the 'microsoft/vscode' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\nSteps to Reproduce:\n\n1. open a PR view\n2. go to the attach menu, try and select the PR, get an errored attachment\n\nhttps://github.com/user-attachments/assets/f49919e8-7371-40f3-a827-8651af2e7b84\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'alexr00' said:\n---\n@roblourens and @mjbvz is there something extensions with webviews should be doing here, or is this not supported?\n---\n\nUser 'roblourens' said:\n---\nIs this just attached as a file or are you doing something special already? It wouldn't work as a file but I'm not sure why it didn't get filtered out of the attach picker, thought we did that\n---\n\nUser 'eleanorjboyd' said:\n---\nI didn't do anything special just opened the PR view. idk if you were asking me or alex\n---\n\nUser 'alexr00' said:\n---\n@roblourens it's just listed in the picker when you click the \"Add Context\" button in the Chat input.\n---",
    "question": "Could you explain how to resolve 'filter webview editors out of'?",
    "ideal_answer": "@roblourens and @mjbvz is there something extensions with webviews should be doing here, or is this not supported?\n\nIs this just attached as a file or are you doing something special already? It wouldn't work as a file but I'm not sure why it didn't get filtered out of the attach picker, thought we did that\n\nI didn't do anything special just opened the PR view. idk if you were asking me or alex\n\n@roblourens it's just listed in the picker when you click the \"Add Context\" button in the Chat input."
  },
  {
    "id": "gen_nat_056",
    "category": "feature_request",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: ResourceClaim field Effect currently marked +optional but should be +required\n\nA user reported the following issue titled 'ResourceClaim field Effect currently marked +optional but should be +required' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nRelated comment: https://github.com/kubernetes/kubernetes/pull/134276#discussion_r2379910306\n\n```go\n// +optional\nEffect DeviceTaintEffect json:\"effect,omitempty\" protobuf:\"bytes,4,opt,name=effect,casttype=DeviceTaintEffect\"\n```\n\nThe above tag is marked +optional incorrectly, validation checks that a field is supplied by the client and there is no default (it should be +required, not +optional w/ hand-written defaulting as there is no such defaulting and it is required):\n\n[kubernetes/pkg/apis/resource/validation/validation.go](https://github.com/kubernetes/kubernetes/blob/243d8c000e451da6f0e8f80704db5d5f672d3d18/pkg/apis/resource/validation/validation.go#L1346-L1349)\n\nLines 1346 to 1349 in [243d8c0](https://github.com/kubernetes/kubernetes/commit/243d8c000e451da6f0e8f80704db5d5f672d3d18)\n```go\n case taint.Effect == \"\": \n \tallErrs = append(allErrs, field.Required(fldPath.Child(\"effect\"), \"\")) // Required in a taint. \n case !validDeviceTaintEffects.Has(taint.Effect): \n \tallErrs = append(allErrs, field.NotSupported(fldPath.Child(\"effect\"), taint.Effect, sets.List(validDeviceTaintEffects)))\n```\n\n\n\n### What did you expect to happen?\n\nI expected the field to be marked +required\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'aaron-prindle' said:\n---\n/sig apimachinery\n/triage accepted\n---\n\nUser 'k8s-ci-robot' said:\n---\n@aaron-prindle: The label(s) `sig/apimachinery` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/134282#issuecomment-3335401039):\n\n>/sig apimachinery\n>/triage accepted\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'aaron-prindle' said:\n---\n/sig api-machinery\n---\n\nUser 'pohly' said:\n---\nAgreed. This might have been copy-and-pasted from `DeviceToleration`: there the effect is optional.\n---",
    "question": "What is the resolution for 'ResourceClaim field Effect currently marked +optional but should be +required'?",
    "ideal_answer": "/triage accepted\n\n@aaron-prindle: The label(s) `sig/apimachinery` cannot be applied, because the repository doesn't have them.\n\nAgreed. This might have been copy-and-pasted from `DeviceToleration`: there the effect is optional."
  },
  {
    "id": "gen_nat_057",
    "category": "troubleshooting",
    "source_file": "microsoft_vscode_issue.json",
    "context": "Repository: microsoft/vscode\nTitle: Switching code editor tabs slow in workspace with many tests (~150,000)\n\nA user reported the following issue titled 'Switching code editor tabs slow in workspace with many tests (~150,000)' in the 'microsoft/vscode' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! bug_report_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- \ud83d\udd6e Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- \ud83d\udd0e Search existing issues to avoid creating duplicates. -->\n<!-- \ud83e\uddea Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- \ud83d\udca1 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- \ud83d\udd27 Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: No\n\n<!-- \ud83e\ude93 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- \ud83d\udce3 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version: 1.95.3\n- OS Version: Windows 11 23H2\n\n```\nVersion: 1.95.3 (system setup)\nCommit: f1a4fb101478ce6ec82fe9627c43efbf9e98c813\nDate: 2024-11-13T14:50:04.152Z\nElectron: 32.2.1\nElectronBuildId: 10427718\nChromium: 128.0.6613.186\nNode.js: 20.18.0\nV8: 12.8.374.38-electron.0\nOS: Windows_NT x64 10.0.22631\n```\n\nSteps to Reproduce:\n\n1. Open a vscode workspace with many tests (in my case ~150,000)\n2. Wait for the language extension to discover all the tests in the project. In my case this is done by C# Dev Kit, though I doubt this matters.\n3. switching between code editor tabs takes a full second for vscode to become responsive again.\n\nI profiled the extension host while it was hanging, which show practically 0% CPU usage.\n\nNext, I profiled the main window. See [Trace-20241211T121347.json](https://github.com/user-attachments/files/18094526/Trace-20241211T121347.json)\n\nThe bulk of time is spent in `encodeURIComponentFast` which is triggered by [`testingDecorations.ts`](https://github.com/microsoft/vscode/blob/4fb4fc90310a408a4f5c6216372e634634ae8ea0/src/vs/workbench/contrib/testing/browser/testingDecorations.ts#L517-L526):\n\n![Image](https://github.com/user-attachments/assets/f507c009-c307-41c0-98e3-5aec34ba6421)\n\n\nNotes:\n- This happens on all editors in the window, even on file types not associated with the tests. E.g. switching between Markdown files is slow too. Though, switching to e.g. the \"Settings\" tabs doesn't have this problem, presumably because it isn't a code editor.\n- The delay is the same regardless of whether the open file contains any tests or not.\n- I have the Test Gutter disabled, so AFAIK, there no need to be computing this information in the first place.\n- Observation as someone not familiar with this codebase all: I notice the [code in question](https://github.com/microsoft/vscode/blob/4fb4fc90310a408a4f5c6216372e634634ae8ea0/src/vs/workbench/contrib/testing/browser/testingDecorations.ts#L517-L526) already \"spawns\" an `async` function to prev\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'connor4312' said:\n---\nI wasn't able to quite reproduce performance as bad as you had, but I made some changes which I think should resolve your issue. Please let me know.\n---\n\nUser 'vs-code-engineering[bot]' said:\n---\n<!-- AUTHOR_VERIFICATION_REQUEST -->\nThis bug has been fixed in the latest release of [VS Code Insiders](https://code.visualstudio.com/insiders/)!\n\n@badeend, you can help us out by commenting `/verified` if things are now working as expected.\n\nIf things still don't seem right, please ensure you're on version ce50bd4876af457f64d83cfd956bc916535285f4 of Insiders (today's or later - you can use `Help: About` in the command palette to check), and leave a comment letting us know what isn't working as expected.\n\nHappy Coding!\n---\n\nUser 'badeend' said:\n---\nFirst of all, thanks for the quick response! Unfortunately, the problem still appears. I've taken a new profiler snapshot: [Trace-20241216T201448.json](https://github.com/user-attachments/files/18156060/Trace-20241216T201448.json)\n\nMost time is still spent in `encodeURIComponentFast`, like above.\n\nI'm on:\n```\nVersion: 1.97.0-insider (user setup)\nCommit: ce50bd4876af457f64d83cfd956bc916535285f4\nDate: 2024-12-16T05:04:30.005Z\nElectron: 32.2.6\nElectronBuildId: 10629634\nChromium: 128.0.6613.186\nNode.js: 20.18.1\nV8: 12.8.374.38-electron.0\nOS: Windows_NT x64 10.0.22631\n```\n---",
    "question": "Has anyone seen 'Switching code editor tabs slow in workspace with many tests' in microsoft/vscode? I can't figure out the cause.",
    "ideal_answer": "I wasn't able to quite reproduce performance as bad as you had, but I made some changes which I think should resolve your issue. Please let me know.\n\n<!-- AUTHOR_VERIFICATION_REQUEST -->\nThis bug has been fixed in the latest release of [VS Code Insiders](https://code.visualstudio.com/insiders/)!\n\n@badeend, you can help us out by commenting `/verified` if things are now working as expected.\n\nIf things still don't seem right, please ensure you're on version ce50bd4876af457f64d83cfd956bc916535285f4 of Insiders (today's or later - you can use `Help: About` in the command palette to check), and leave a comment letting us know what isn't working as expected.\n\nHappy Coding!\n\nFirst of all, thanks for the quick response! Unfortunately, the problem still appears. I've taken a new profiler snapshot: [Trace-20241216T201448.json](https://github.com/user-attachments/files/18156060/Trace-20241216T201448.json)\n\nMost time is still spent in `encodeURIComponentFast`, like above.\n\nI'm on:\n```\nVersion: 1.97.0-insider (user setup)\nCommit: ce50bd4876af457f64d83cfd956bc916535285f4\nDate: 2024-12-16T05:04:30.005Z\nElectron: 32.2.6\nElectronBuildId: 10629634\nChromium: 128.0.6613.186\nNode.js: 20.18.1\nV8: 12.8.374.38-electron.0\nOS: Windows_NT x64 10.0.22631\n```"
  },
  {
    "id": "gen_nat_058",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk\n\nA user reported the following issue titled 'Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nSep 16 08:27:42 k8s-worker-63 kubelet[2588]: E0916 08:27:42.356361    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:27:44 k8s-worker-63 kubelet[2588]: E0916 08:27:44.384160    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:27:46 k8s-worker-63 kubelet[2588]: E0916 08:27:46.358832    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:27:46 k8s-worker-63 kubelet[2588]: E0916 08:27:46.683320    2588 pod_workers.go:190] Error syncing pod da3ca68c-78d6-4c9f-ac3d-ef946afcbfa1 (\"skgxfw-api-dmkyanshi-7bdf8f5788-2kggj_skgx(da3ca68c-78d6-4c9f-ac3d-ef946afcbfa1)\"), skipping: failed to \"StartContainer\" for \"skgxfw-api-dmkyanshi\" with ImagePullBackOff: \"Back-off pulling image \\\"reg.cx.htjs.net/cx.ci.v3/htjs/skgx.skgxfw-api.master-dmkyanshi:aoneci-d030a42e-20230426180948\\\"\"\nSep 16 08:27:48 k8s-worker-63 kubelet[2588]: E0916 08:27:48.352049    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:27:50 k8s-worker-63 kubelet[2588]: E0916 08:27:50.416639    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:27:51 k8s-worker-63 kubelet[2588]: W0916 08:27:51.040371    2588 watcher.go:87] Error while processing event (\"/sys/fs/cgroup/memory/libcontainer_8684_systemd_test_default.slice\": 0x40000100 == IN_CREATE|IN_ISDIR): readdirent: no such file or directory\nSep 16 08:27:51 k8s-worker-63 kubelet[2588]: W0916 08:27:51.040474    2588 watcher.go:87] Error while processing event (\"/sys/fs/cgroup/devices/libcontainer_8684_systemd_test_default.slice\": 0x40000100 == IN_CREATE|IN_ISDIR): open /sys/fs/cgroup/devices/libcontainer_8684_systemd_test_default.slice: no such file or directory\nSep 16 08:27:52 k8s-worker-63 kubelet[2588]: E0916 08:27:52.351770    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 errors similar to this. Turn up verbosity to see them.\nSep 16 08:27:54 k8s-worker-63 kubelet[2588]: E0916 08:27:54.351288    2588 kubelet_volumes.go:154] Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk : There were a total of 8549 error\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'k8s-ci-robot' said:\n---\nThere are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'k8s-ci-robot' said:\n---\nThis issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'pacoxu' said:\n---\n> k8s version 1.15.4\n\n1.15 is too old which was end of life years ago. https://endoflife.date/kubernetes.\n\n/kind support\n\nDoes this still appear in kubernets v1.31 +?\n---\n\nUser 'lusicong' said:\n---\n> Does this still appear in kubernets v1.31 +?\n\n\nno\n---\n\nUser 'pacoxu' said:\n---\nBTW, it looks to be the same issue as https://github.com/kubernetes/kubernetes/issues/60987.\n---",
    "question": "What is the resolution for 'Orphaned pod \"0008e5b2-6008-45b9-89cd-1aa60f896e24\" found, but volume paths are still present on disk'?",
    "ideal_answer": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n> k8s version 1.15.4\n\n1.15 is too old which was end of life years ago. https://endoflife.date/kubernetes.\n\n/kind support\n\nDoes this still appear in kubernets v1.31 +?\n\n> Does this still appear in kubernets v1.31 +?\n\nno\n\nBTW, it looks to be the same issue as https://github.com/kubernetes/kubernetes/issues/60987."
  },
  {
    "id": "gen_nat_059",
    "category": "feature_request",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: The PodAndContainerStatsFromCRI feature doesn\u2019t expose metrics.\n\nA user reported the following issue titled 'The PodAndContainerStatsFromCRI feature doesn\u2019t expose metrics.' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nHello, I\u2019m a bit confused about the progress of KEP 2371. When I enable this feature, I can no longer get a large number of metrics from the /metrics/cadvisor endpoint. From the /stats/summary endpoint, I\u2019m not sure whether the corresponding metrics are available\u2014it feels more like statistics data rather than metrics. Because of this, I\u2019m a bit puzzled.\n\n1.For the metrics provided through CRI, is there a better way to observe them? Especially regarding whether there are differences before and after the feature is enabled. Shouldn\u2019t there be a dedicated endpoint like /metrics/cri for better observability?\n\n2.Have all the metrics previously emitted by cAdvisor already been fully implemented by CRI\u2014for example, blkio and fs-related metrics?\n\n3.What\u2019s the long-term plan for cAdvisor? Will it eventually be completely deprecated?\n\nFrom the information in [the KEP document](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2371-cri-pod-container-stats/README.md?cri-implementations)\n\"The Alpha release will add support for both /stats/summary endpoint and /metrics/cadvisor endpoint. The release will focus on finalizing and enabling support for the set of metrics from /metrics/cadvisor that CRI implementations must support.\" \n\n\nbut request /metrics/cadvisor :\n```\n[root@node-1 ~]# curl --cacert /etc/kubernetes/ssl/ca.pem  --cert /etc/kubernetes/ssl/admin-node-1.pem  --key /etc/kubernetes/ssl/admin-node-1-key.pem  https://10.20.0.4:10250/metrics/cadvisor\n# HELP machine_cpu_cores Number of logical CPU cores.\n# TYPE machine_cpu_cores gauge\nmachine_cpu_cores{boot_id=\"9cbf0679-3e5a-4872-a414-7d085d816d83\",machine_id=\"380970011dca4a40b65e2dfabcd7a3bf\",system_uuid=\"4d6fef83-7d44-433b-b0c8-25998f3a16f7\"} 8\n# HELP machine_cpu_physical_cores Number of physical CPU cores.\n# TYPE machine_cpu_physical_cores gauge\nmachine_cpu_physical_cores{boot_id=\"9cbf0679-3e5a-4872-a414-7d085d816d83\",machine_id=\"380970011dca4a40b65e2dfabcd7a3bf\",system_uuid=\"4d6fef83-7d44-433b-b0c8-25998f3a16f7\"} 1\n# HELP machine_cpu_sockets Number of CPU sockets.\n# TYPE machine_cpu_sockets gauge\nmachine_cpu_sockets{boot_id=\"9cbf0679-3e5a-4872-a414-7d085d816d83\",machine_id=\"380970011dca4a40b65e2dfabcd7a3bf\",system_uuid=\"4d6fef83-7d44-433b-b0c8-25998f3a16f7\"} 8\n# HELP machine_memory_bytes Amount of memory installed on the machine.\n# TYPE machine_memory_bytes gauge\nmachine_memory_bytes{boot_id=\"9cbf0679-3e5a-4872-a414-7d085d816d83\",machine_id=\"380970011dca4a40b65e2dfabcd7a3bf\",system_uuid=\"4d6fef83-7d44-433b-b0c8-25998f3a16f7\"} 1.5999279104e+10\n# HELP machine_nvm_avg_power_budget_watts NVM power budget.\n# TYPE machine_nvm_avg_power_budget_watts gauge\nmachine_nvm_avg_power_budget_watts{boot_id=\"9cbf0679-3e5a-4872-a414-7d085d816d83\",machine_id=\"380970011dca4a40b65e2dfabcd7a3bf\",system_uuid=\"4d6fef83-7d44-433b-b0c8-25998f3a16f7\"} 0\n# HELP machine_nvm_capacity NVM capacity value labeled by NVM mode (memory mode or app direct mode).\n# TYPE machine_nvm_capacity gauge\nmac\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'k8s-ci-robot' said:\n---\nThis issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'k8s-ci-robot' said:\n---\n@Goend: The label(s) `kind/suuport` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133884#issuecomment-3252248744):\n\n>/kind suuport\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'Goend' said:\n---\n/kind support\n---\n\nUser 'Goend' said:\n---\n/sig node\n---\n\nUser 'Goend' said:\n---\n@haircommander  I would greatly appreciate any suggestions you might have.\n---\n\nUser 'haircommander' said:\n---\ncontainerd doesn't yet have support for metrics, which is why the KEP is still in alpha. you can try CRI-O out if you want to test this feature :)\n---\n\nUser 'Goend' said:\n---\nref:  https://github.com/containerd/containerd/issues/10506\n---\n\nUser 'Goend' said:\n---\n> containerd doesn't yet have support for metrics, which is why the KEP is still in alpha. you can try CRI-O out if you want to test this feature :)\n\nThank you very much for your advice. :smile:\n---\n\nUser 'haircommander' said:\n---\nSince this is a containerd issue, i think we can close this.\nplease reopen if you disagreee\n/close\n---\n\nUser 'k8s-ci-robot' said:\n---\n@haircommander: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133884#issuecomment-3275931460):\n\n>Since this is a containerd issue, i think we can close this.\n>please reopen if you disagreee\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---",
    "question": "Could you explain how to resolve 'The PodAndContainerStatsFromCRI feature doesn\u2019t expose metrics.'?",
    "ideal_answer": "If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n@Goend: The label(s) `kind/suuport` cannot be applied, because the repository doesn't have them.\n\n/kind support\n\n@haircommander  I would greatly appreciate any suggestions you might have.\n\ncontainerd doesn't yet have support for metrics, which is why the KEP is still in alpha. you can try CRI-O out if you want to test this feature :)\n\nref:  https://github.com/containerd/containerd/issues/10506\n\n> containerd doesn't yet have support for metrics, which is why the KEP is still in alpha. you can try CRI-O out if you want to test this feature :)\n\nThank you very much for your advice. :smile:\n\nSince this is a containerd issue, i think we can close this.\nplease reopen if you disagreee\n\n@haircommander: Closing this issue."
  },
  {
    "id": "gen_nat_060",
    "category": "general",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: pod_startup_latency_tracker : podStartSLOduration returns negative value with serializeImagePulls set to false\n\nA user reported the following issue titled 'pod_startup_latency_tracker : podStartSLOduration returns negative value with serializeImagePulls set to false' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nThis can be observed easily when deployed a large number of pods (i.e. 20 pods) at the same time in Kubernetes version 1.31. I found an issue with similar symptom, but it's supposed to be resolved on version 1.27\n```\nFeb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.323529    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-c74hl\" podStartSLOduration=1.344002188 podStartE2EDuration=\"3.323519624s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.734807377 +0000 UTC m=+565.347657070\" lastFinishedPulling=\"2025-02-25 21:38:55.714324713 +0000 UTC m=+567.327174506\" observedRunningTime=\"2025-02-25 21:38:56.323492624 +0000 UTC m=+567.936342317\" watchObservedRunningTime=\"2025-02-25 21:38:56.323519624 +0000 UTC m=+567.936369517\"\nFeb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.338810    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-bd4br\" podStartSLOduration=1.2549581779999999 podStartE2EDuration=\"3.338800195s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.661167137 +0000 UTC m=+565.274016830\" lastFinishedPulling=\"2025-02-25 21:38:55.745009154 +0000 UTC m=+567.357858847\" observedRunningTime=\"2025-02-25 21:38:56.338414293 +0000 UTC m=+567.951264086\" watchObservedRunningTime=\"2025-02-25 21:38:56.338800195 +0000 UTC m=+567.951649988\"\nFeb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.361959    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-5ljwx\" podStartSLOduration=1.360169063 podStartE2EDuration=\"3.361949302s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.709078158 +0000 UTC m=+565.321927851\" lastFinishedPulling=\"2025-02-25 21:38:55.710858397 +0000 UTC m=+567.323708090\" observedRunningTime=\"2025-02-25 21:38:56.353660063 +0000 UTC m=+567.966509756\" watchObservedRunningTime=\"2025-02-25 21:38:56.361949302 +0000 UTC m=+567.974799095\"\nFeb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.369811    3201 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5fb68775dc-btpww\" podStartSLOduration=1.305661411 podStartE2EDuration=\"3.369803938s\" podCreationTimestamp=\"2025-02-25 21:38:53 +0000 UTC\" firstStartedPulling=\"2025-02-25 21:38:53.681766232 +0000 UTC m=+565.294615925\" lastFinishedPulling=\"2025-02-25 21:38:55.745908759 +0000 UTC m=+567.358758452\" observedRunningTime=\"2025-02-25 21:38:56.369554537 +0000 UTC m=+567.982404230\" watchObservedRunningTime=\"2025-02-25 21:38:56.369803938 +0000 UTC m=+567.982653731\"\nFeb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.387444    3201 pod_startup_latency_tracker.go:104] \"Observed pod sta\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'alyssa1303' said:\n---\n/sig node\n---\n\nUser 'alyssa1303' said:\n---\nI observed same issue with EKS, so cloud provider is not related here\n```\nFeb 25 22:23:56 ip-10-0-91-209 kubelet: I0225 22:23:56.631131    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-65tbb\" podStartSLOduration=1.546927693 podStartE2EDuration=\"2.631123808s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.163506378 +0000 UTC m=+10440.454286709\" lastFinishedPulling=\"2025-02-25 22:23:56.247702507 +0000 UTC m=+10441.538482824\" observedRunningTime=\"2025-02-25 22:23:56.630931783 +0000 UTC m=+10441.921712121\" watchObservedRunningTime=\"2025-02-25 22:23:56.631123808 +0000 UTC m=+10441.921904147\"\nFeb 25 22:23:57 ip-10-0-91-209 kubelet: I0225 22:23:57.386200    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-45bzd\" podStartSLOduration=-9223372033.468592 podStartE2EDuration=\"3.386184373s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.520600231 +0000 UTC m=+10440.811380547\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:23:57.38500166 +0000 UTC m=+10442.675781999\" watchObservedRunningTime=\"2025-02-25 22:23:57.386184373 +0000 UTC m=+10442.676964712\"\nFeb 25 22:23:57 ip-10-0-91-209 kubelet: I0225 22:23:57.399827    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-s9f8b\" podStartSLOduration=-9223372033.454958 podStartE2EDuration=\"3.399816932s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.520564068 +0000 UTC m=+10440.811344403\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:23:57.399470898 +0000 UTC m=+10442.690251237\" watchObservedRunningTime=\"2025-02-25 22:23:57.399816932 +0000 UTC m=+10442.690597270\"\nFeb 25 22:23:57 ip-10-0-91-209 kubelet: I0225 22:23:57.419389    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-rqfls\" podStartSLOduration=2.665023156 podStartE2EDuration=\"3.41938171s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.519728801 +0000 UTC m=+10440.810509133\" lastFinishedPulling=\"2025-02-25 22:23:56.274087357 +0000 UTC m=+10441.564867687\" observedRunningTime=\"2025-02-25 22:23:57.41927327 +0000 UTC m=+10442.710053610\" watchObservedRunningTime=\"2025-02-25 22:23:57.41938171 +0000 UTC m=+10442.710162050\"\nFeb 25 22:23:57 ip-10-0-91-209 kubelet: I0225 22:23:57.434143    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-5v4j5\" podStartSLOduration=-9223372033.42064 podStartE2EDuration=\"3.434135031s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.520488933 +0000 UTC m=+10440.811269250\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:23:57.433087607 +0000 UTC m=+10442.723867946\" watchObservedRunningTime=\"2025-02-25 22:23:57.434135031 +0000 UTC m=+10442.724915371\"\nFeb 25 22:24:08 ip-10-0-91-209 kubelet: I0225 22:24:08.024215    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-dvkq6\" podStartSLOduration=12.852415203 podStartE2EDuration=\"14.024202075s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.101434339 +0000 UTC m=+10440.392214660\" lastFinishedPulling=\"2025-02-25 22:23:56.273221215 +0000 UTC m=+10441.564001532\" observedRunningTime=\"2025-02-25 22:23:57.453636416 +0000 UTC m=+10442.744416753\" watchObservedRunningTime=\"2025-02-25 22:24:08.024202075 +0000 UTC m=+10453.314982413\"\nFeb 25 22:24:08 ip-10-0-91-209 kubelet: I0225 22:24:08.413462    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-fj8cp\" podStartSLOduration=-9223372022.441326 podStartE2EDuration=\"14.413449266s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.199151934 +0000 UTC m=+10440.489932252\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:24:08.412149749 +0000 UTC m=+10453.702930088\" watchObservedRunningTime=\"2025-02-25 22:24:08.413449266 +0000 UTC m=+10453.704229603\"\nFeb 25 22:24:10 ip-10-0-91-209 kubelet: I0225 22:24:10.414060    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-kvzm2\" podStartSLOduration=-9223372020.440725 podStartE2EDuration=\"16.41405031s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.233269552 +0000 UTC m=+10440.524049868\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:24:10.413714446 +0000 UTC m=+10455.704494785\" watchObservedRunningTime=\"2025-02-25 22:24:10.41405031 +0000 UTC m=+10455.704830649\"\nFeb 25 22:24:10 ip-10-0-91-209 kubelet: I0225 22:24:10.433791    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-5sngx\" podStartSLOduration=-9223372020.420994 podStartE2EDuration=\"16.433781479s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.196000401 +0000 UTC m=+10440.486780732\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:24:10.433756073 +0000 UTC m=+10455.724536413\" watchObservedRunningTime=\"2025-02-25 22:24:10.433781479 +0000 UTC m=+10455.724561819\"\nFeb 25 22:24:10 ip-10-0-91-209 kubelet: I0225 22:24:10.463454    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-lvzwl\" podStartSLOduration=-9223372020.39133 podStartE2EDuration=\"16.46344522s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.196818306 +0000 UTC m=+10440.487598623\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:24:10.448051989 +0000 UTC m=+10455.738832327\" watchObservedRunningTime=\"2025-02-25 22:24:10.46344522 +0000 UTC m=+10455.754225612\"\nFeb 25 22:24:10 ip-10-0-91-209 kubelet: I0225 22:24:10.463513    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-qp8j6\" podStartSLOduration=-9223372020.391266 podStartE2EDuration=\"16.463509885s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.196687275 +0000 UTC m=+10440.487467592\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:24:10.462599237 +0000 UTC m=+10455.753379575\" watchObservedRunningTime=\"2025-02-25 22:24:10.463509885 +0000 UTC m=+10455.754290224\"\n```\nKubernetes version\n```\nkubectl version\nClient Version: v1.31.0\nKustomize Version: v5.4.2\nServer Version: v1.31.5-eks-8cce635\n```\n\nOS Version\n```\n[root@ip-10-0-91-209 /]# cat /etc/os-release\nNAME=\"Amazon Linux\"\nVERSION=\"2\"\nID=\"amzn\"\nID_LIKE=\"centos rhel fedora\"\nVERSION_ID=\"2\"\nPRETTY_NAME=\"Amazon Linux 2\"\nANSI_COLOR=\"0;33\"\nCPE_NAME=\"cpe:2.3:o:amazon:amazon_linux:2\"\nHOME_URL=\"https://amazonlinux.com/\"\nSUPPORT_END=\"2026-06-30\"\n```\n\nSame containerd version containerd://1.7.25\n---\n\nUser 'alyssa1303' said:\n---\nI believe this will also return negative sum as mentioned in this issue [122675](https://github.com/kubernetes/kubernetes/issues/122675)\n---\n\nUser 'HirazawaUi' said:\n---\nI used the same **deployment** as yours to create an ephemeral cluster with the latest code from the master branch, but couldn't reproduce the issue. I'm uncertain whether this problem has been fixed. Below are the logs I collected:\n\n<details>\n\n<summary>Logs</summary>\n\n```ruby\nI0303 21:35:18.339956  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-k2c9f\" podStartSLOduration=1.339952031 podStartE2EDuration=\"1.339952031s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.336329531 +0800 CST m=+435.857825042\" watchObservedRunningTime=\"2025-03-03 21:35:18.339952031 +0800 CST m=+435.861447542\"\nI0303 21:35:18.343086  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-p5qbp\" podStartSLOduration=1.343083239 podStartE2EDuration=\"1.343083239s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.342692656 +0800 CST m=+435.864188126\" watchObservedRunningTime=\"2025-03-03 21:35:18.343083239 +0800 CST m=+435.864578709\"\nI0303 21:35:18.343225  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-929fx\" podStartSLOduration=1.343223197 podStartE2EDuration=\"1.343223197s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.340088281 +0800 CST m=+435.861583751\" watchObservedRunningTime=\"2025-03-03 21:35:18.343223197 +0800 CST m=+435.864718709\"\nI0303 21:35:18.345020  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-rcpfj\" podStartSLOduration=1.345017072 podStartE2EDuration=\"1.345017072s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.344771906 +0800 CST m=+435.866267376\" watchObservedRunningTime=\"2025-03-03 21:35:18.345017072 +0800 CST m=+435.866512542\"\nI0303 21:35:18.347275  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-dxblk\" podStartSLOduration=1.347271989 podStartE2EDuration=\"1.347271989s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.347191572 +0800 CST m=+435.868687084\" watchObservedRunningTime=\"2025-03-03 21:35:18.347271989 +0800 CST m=+435.868767501\"\nI0303 21:35:18.351837  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-nx2xl\" podStartSLOduration=1.351804781 podStartE2EDuration=\"1.351804781s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.349441614 +0800 CST m=+435.870937084\" watchObservedRunningTime=\"2025-03-03 21:35:18.351804781 +0800 CST m=+435.873300251\"\nI0303 21:35:18.355502  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-2wjvt\" podStartSLOduration=1.355498197 podStartE2EDuration=\"1.355498197s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.351807614 +0800 CST m=+435.873303084\" watchObservedRunningTime=\"2025-03-03 21:35:18.355498197 +0800 CST m=+435.876993709\"\nI0303 21:35:18.358516  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-hq4t7\" podStartSLOduration=1.3585131559999999 podStartE2EDuration=\"1.358513156s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.356250739 +0800 CST m=+435.877746209\" watchObservedRunningTime=\"2025-03-03 21:35:18.358513156 +0800 CST m=+435.880008626\"\nI0303 21:35:18.360975  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-rf84h\" podStartSLOduration=1.360972114 podStartE2EDuration=\"1.360972114s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.358677864 +0800 CST m=+435.880173376\" watchObservedRunningTime=\"2025-03-03 21:35:18.360972114 +0800 CST m=+435.882467584\"\nI0303 21:35:18.361043  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-6p7lf\" podStartSLOduration=1.361042156 podStartE2EDuration=\"1.361042156s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.360874322 +0800 CST m=+435.882369792\" watchObservedRunningTime=\"2025-03-03 21:35:18.361042156 +0800 CST m=+435.882537626\"\nI0303 21:35:18.363188  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-xxjjx\" podStartSLOduration=1.363185447 podStartE2EDuration=\"1.363185447s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.363101739 +0800 CST m=+435.884597251\" watchObservedRunningTime=\"2025-03-03 21:35:18.363185447 +0800 CST m=+435.884680917\"\nI0303 21:35:18.367650  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-9sxn7\" podStartSLOduration=1.367647072 podStartE2EDuration=\"1.367647072s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.365385364 +0800 CST m=+435.886880876\" watchObservedRunningTime=\"2025-03-03 21:35:18.367647072 +0800 CST m=+435.889142584\"\nI0303 21:35:18.369954  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-z9pmz\" podStartSLOduration=1.3699486140000001 podStartE2EDuration=\"1.369948614s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.367751072 +0800 CST m=+435.889246542\" watchObservedRunningTime=\"2025-03-03 21:35:18.369948614 +0800 CST m=+435.891444126\"\nI0303 21:35:18.372498  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-4dmbx\" podStartSLOduration=1.372488906 podStartE2EDuration=\"1.372488906s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.370205072 +0800 CST m=+435.891700542\" watchObservedRunningTime=\"2025-03-03 21:35:18.372488906 +0800 CST m=+435.893984376\"\nI0303 21:35:18.374883  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-kxjqs\" podStartSLOduration=1.374880572 podStartE2EDuration=\"1.374880572s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.372424656 +0800 CST m=+435.893920167\" watchObservedRunningTime=\"2025-03-03 21:35:18.374880572 +0800 CST m=+435.896376042\"\nI0303 21:35:18.374908  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-cx9hp\" podStartSLOduration=1.3749066970000001 podStartE2EDuration=\"1.374906697s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.374745781 +0800 CST m=+435.896241292\" watchObservedRunningTime=\"2025-03-03 21:35:18.374906697 +0800 CST m=+435.896402209\"\nI0303 21:35:18.376812  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-247fl\" podStartSLOduration=1.376809739 podStartE2EDuration=\"1.376809739s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.376768572 +0800 CST m=+435.898264084\" watchObservedRunningTime=\"2025-03-03 21:35:18.376809739 +0800 CST m=+435.898305251\"\nI0303 21:35:18.380977  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-7vgp7\" podStartSLOduration=1.380974697 podStartE2EDuration=\"1.380974697s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.378908156 +0800 CST m=+435.900403667\" watchObservedRunningTime=\"2025-03-03 21:35:18.380974697 +0800 CST m=+435.902470167\"\nI0303 21:35:18.382567  957856 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-77c84fc845-hrt58\" podStartSLOduration=1.382563614 podStartE2EDuration=\"1.382563614s\" podCreationTimestamp=\"2025-03-03 21:35:17 +0800 CST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-03 21:35:18.380900989 +0800 CST m=+435.902396501\" watchObservedRunningTime=\"2025-03-03 21:35:18.382563614 +0800 CST m=+435.904059084\"\n```\n</details>\n---\n\nUser 'HirazawaUi' said:\n---\nI've attempted to manually remove the existing local images and recreate the same scenario as your environment, but still can't reproduce the issue. Maybe should inject delays to simulate this conditions?\n\n<details>\n\n<summary>Logs</summary>\n\n\n```ruby\nI0303 22:04:59.325768 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-95ksv\" podStartSLOduration=2.174478752 podStartE2EDuration=\"10.325758672s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.516914417 +0800 CST m=+1356.015431398\" lastFinishedPulling=\"2025-03-03 22:04:57.668194379 +0800 CST m=+1364.166711318\" observedRunningTime=\"2025-03-03 22:04:58.323529671 +0800 CST m=+1364.822046652\" watchObservedRunningTime=\"2025-03-03 22:04:59.325758672 +0800 CST m=+1365.824275652\"\nI0303 22:04:59.325924 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-rlw9c\" podStartSLOduration=1.079496251 podStartE2EDuration=\"10.325920922s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.541529584 +0800 CST m=+1356.040046523\" lastFinishedPulling=\"2025-03-03 22:04:58.787954213 +0800 CST m=+1365.286471194\" observedRunningTime=\"2025-03-03 22:04:59.325567213 +0800 CST m=+1365.824084194\" watchObservedRunningTime=\"2025-03-03 22:04:59.325920922 +0800 CST m=+1365.824437902\"\nI0303 22:05:00.330085 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-t7ksg\" podStartSLOduration=1.064315459 podStartE2EDuration=\"11.330075464s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.581635 +0800 CST m=+1356.080151939\" lastFinishedPulling=\"2025-03-03 22:04:59.847394964 +0800 CST m=+1366.345911944\" observedRunningTime=\"2025-03-03 22:05:00.329974422 +0800 CST m=+1366.828491361\" watchObservedRunningTime=\"2025-03-03 22:05:00.330075464 +0800 CST m=+1366.828592444\"\nI0303 22:05:02.334580 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-ndk62\" podStartSLOduration=2.058409792 podStartE2EDuration=\"13.334571131s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.586911125 +0800 CST m=+1356.085428106\" lastFinishedPulling=\"2025-03-03 22:05:00.863072506 +0800 CST m=+1367.361589445\" observedRunningTime=\"2025-03-03 22:05:01.329559464 +0800 CST m=+1367.828076445\" watchObservedRunningTime=\"2025-03-03 22:05:02.334571131 +0800 CST m=+1368.833088112\"\nI0303 22:05:03.338761 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-llv4n\" podStartSLOduration=2.111236168 podStartE2EDuration=\"14.338729257s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.634255292 +0800 CST m=+1356.132772231\" lastFinishedPulling=\"2025-03-03 22:05:01.86174834 +0800 CST m=+1368.360265320\" observedRunningTime=\"2025-03-03 22:05:02.335814381 +0800 CST m=+1368.834331320\" watchObservedRunningTime=\"2025-03-03 22:05:03.338729257 +0800 CST m=+1369.837246238\"\nI0303 22:05:03.338877 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-gxjjw\" podStartSLOduration=1.098041376 podStartE2EDuration=\"14.338873382s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.6369555 +0800 CST m=+1356.135472481\" lastFinishedPulling=\"2025-03-03 22:05:02.877787507 +0800 CST m=+1369.376304487\" observedRunningTime=\"2025-03-03 22:05:03.338740965 +0800 CST m=+1369.837257904\" watchObservedRunningTime=\"2025-03-03 22:05:03.338873382 +0800 CST m=+1369.837390363\"\nI0303 22:05:04.347555 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-js2zf\" podStartSLOduration=1.120524126 podStartE2EDuration=\"15.347543049s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.643618209 +0800 CST m=+1356.142135148\" lastFinishedPulling=\"2025-03-03 22:05:03.87063709 +0800 CST m=+1370.369154071\" observedRunningTime=\"2025-03-03 22:05:04.347417341 +0800 CST m=+1370.845934321\" watchObservedRunningTime=\"2025-03-03 22:05:04.347543049 +0800 CST m=+1370.846060030\"\nI0303 22:05:06.345775 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-vgzxb\" podStartSLOduration=0.936493168 podStartE2EDuration=\"17.345766967s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.648327167 +0800 CST m=+1356.146844148\" lastFinishedPulling=\"2025-03-03 22:05:06.057600967 +0800 CST m=+1372.556117947\" observedRunningTime=\"2025-03-03 22:05:06.345767383 +0800 CST m=+1372.844284364\" watchObservedRunningTime=\"2025-03-03 22:05:06.345766967 +0800 CST m=+1372.844283947\"\nI0303 22:05:06.345921 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-qbtd9\" podStartSLOduration=2.008485751 podStartE2EDuration=\"17.345918133s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.646091792 +0800 CST m=+1356.144608731\" lastFinishedPulling=\"2025-03-03 22:05:04.983524091 +0800 CST m=+1371.482041113\" observedRunningTime=\"2025-03-03 22:05:05.344043091 +0800 CST m=+1371.842560072\" watchObservedRunningTime=\"2025-03-03 22:05:06.345918133 +0800 CST m=+1372.844435072\"\nI0303 22:05:07.349917 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-wpkvz\" podStartSLOduration=0.915956292 podStartE2EDuration=\"18.349908634s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.65136175 +0800 CST m=+1356.149878731\" lastFinishedPulling=\"2025-03-03 22:05:07.085314092 +0800 CST m=+1373.583831073\" observedRunningTime=\"2025-03-03 22:05:07.349804342 +0800 CST m=+1373.848321281\" watchObservedRunningTime=\"2025-03-03 22:05:07.349908634 +0800 CST m=+1373.848425614\"\nI0303 22:05:09.351929 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-vts58\" podStartSLOduration=1.836934042 podStartE2EDuration=\"20.351920176s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.652733542 +0800 CST m=+1356.151250481\" lastFinishedPulling=\"2025-03-03 22:05:08.167719634 +0800 CST m=+1374.666236615\" observedRunningTime=\"2025-03-03 22:05:08.349977093 +0800 CST m=+1374.848494073\" watchObservedRunningTime=\"2025-03-03 22:05:09.351920176 +0800 CST m=+1375.850437157\"\nI0303 22:05:11.360539 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-28jcl\" podStartSLOduration=2.799674876 podStartE2EDuration=\"22.360529802s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.65439175 +0800 CST m=+1356.152908689\" lastFinishedPulling=\"2025-03-03 22:05:09.215246635 +0800 CST m=+1375.713763615\" observedRunningTime=\"2025-03-03 22:05:09.352161218 +0800 CST m=+1375.850678157\" watchObservedRunningTime=\"2025-03-03 22:05:11.360529802 +0800 CST m=+1377.859046783\"\nI0303 22:05:12.371974 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-5l8fp\" podStartSLOduration=2.645544918 podStartE2EDuration=\"23.37195297s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.655235834 +0800 CST m=+1356.153752814\" lastFinishedPulling=\"2025-03-03 22:05:10.381643885 +0800 CST m=+1376.880160866\" observedRunningTime=\"2025-03-03 22:05:11.360780469 +0800 CST m=+1377.859297408\" watchObservedRunningTime=\"2025-03-03 22:05:12.37195297 +0800 CST m=+1378.870469950\"\nI0303 22:05:13.378604 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-lttf9\" podStartSLOduration=2.669807376 podStartE2EDuration=\"24.378585345s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.676987625 +0800 CST m=+1356.175504564\" lastFinishedPulling=\"2025-03-03 22:05:11.385765552 +0800 CST m=+1377.884282533\" observedRunningTime=\"2025-03-03 22:05:12.37324922 +0800 CST m=+1378.871766159\" watchObservedRunningTime=\"2025-03-03 22:05:13.378585345 +0800 CST m=+1379.877102367\"\nI0303 22:05:14.386137 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-79tk8\" podStartSLOduration=2.62329521 podStartE2EDuration=\"25.386119762s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.678997334 +0800 CST m=+1356.177514273\" lastFinishedPulling=\"2025-03-03 22:05:12.441821803 +0800 CST m=+1378.940338825\" observedRunningTime=\"2025-03-03 22:05:13.37881347 +0800 CST m=+1379.877330451\" watchObservedRunningTime=\"2025-03-03 22:05:14.386119762 +0800 CST m=+1380.884636743\"\nI0303 22:05:15.387706 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-j54mq\" podStartSLOduration=2.499969376 podStartE2EDuration=\"26.387692346s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.679041209 +0800 CST m=+1356.177558189\" lastFinishedPulling=\"2025-03-03 22:05:13.56676422 +0800 CST m=+1380.065281159\" observedRunningTime=\"2025-03-03 22:05:14.38955047 +0800 CST m=+1380.888067451\" watchObservedRunningTime=\"2025-03-03 22:05:15.387692346 +0800 CST m=+1381.886209327\"\nI0303 22:05:16.392929 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-lhxks\" podStartSLOduration=2.417404084 podStartE2EDuration=\"27.392913096s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.67904525 +0800 CST m=+1356.177562189\" lastFinishedPulling=\"2025-03-03 22:05:14.654554262 +0800 CST m=+1381.153071201\" observedRunningTime=\"2025-03-03 22:05:15.388080221 +0800 CST m=+1381.886597202\" watchObservedRunningTime=\"2025-03-03 22:05:16.392913096 +0800 CST m=+1382.891430077\"\nI0303 22:05:17.405627 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-mklsx\" podStartSLOduration=1.350270126 podStartE2EDuration=\"28.40561718s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.680459084 +0800 CST m=+1356.178976023\" lastFinishedPulling=\"2025-03-03 22:05:16.735806097 +0800 CST m=+1383.234323077\" observedRunningTime=\"2025-03-03 22:05:17.405475555 +0800 CST m=+1383.903992536\" watchObservedRunningTime=\"2025-03-03 22:05:17.40561718 +0800 CST m=+1383.904134119\"\nI0303 22:05:17.405737 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-jvn6p\" podStartSLOduration=2.348157626 podStartE2EDuration=\"28.40573143s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.679048 +0800 CST m=+1356.177564939\" lastFinishedPulling=\"2025-03-03 22:05:15.736621763 +0800 CST m=+1382.235138743\" observedRunningTime=\"2025-03-03 22:05:16.39603993 +0800 CST m=+1382.894556869\" watchObservedRunningTime=\"2025-03-03 22:05:17.40573143 +0800 CST m=+1383.904248411\"\nI0303 22:05:18.408043 1003926 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-6f5d8d9765-g2v42\" podStartSLOduration=1.270441833 podStartE2EDuration=\"29.408033722s\" podCreationTimestamp=\"2025-03-03 22:04:49 +0800 CST\" firstStartedPulling=\"2025-03-03 22:04:49.680477834 +0800 CST m=+1356.178994814\" lastFinishedPulling=\"2025-03-03 22:05:17.818069722 +0800 CST m=+1384.316586703\" observedRunningTime=\"2025-03-03 22:05:18.407502347 +0800 CST m=+1384.906019328\" watchObservedRunningTime=\"2025-03-03 22:05:18.408033722 +0800 CST m=+1384.906550661\"\n```\n\n</details>\n---\n\nUser 'SergeyKanzhelev' said:\n---\n/assign @esotsal \n/triage accepted\n/priority important-longterm\n---\n\nUser 'esotsal' said:\n---\n[Update]\n\nUsed same deployment removing nodeSelector, starting 100 instances using local-up-cluster, i could not reproduce the issue. \n\nVariable lastFinishedPulling, is set to \"0001-01-01 00:00:00 +0000 UTC\" resulting later to a negative podStartSLOduration.\n\nhttps://github.com/kubernetes/kubernetes/blob/c30b1eb09b6355f88ac514ec97cb7d87bdf6c2c3/pkg/kubelet/util/pod_startup_latency_tracker.go#L99-L102\n\nstate.lastFinishedPulling is set inside RecordImageFinishedPulling method\n\nhttps://github.com/kubernetes/kubernetes/blob/7c78041218a4cf0b183dc0dab71f475a5db667bf/pkg/kubelet/util/pod_startup_latency_tracker.go#L140-L150\n\n\nContinuing troubleshooting to see if i will be able to reproduce it.\n---\n\nUser 'esotsal' said:\n---\nHi,\n\nI was not able to reproduce the issue even when I applied a network throttling in my lab to emulate network glitches.\n\nChecking the provided logs I notice that when the issue first appeared there is a gap in provided logs of 9 seconds ( Feb 25 21:38:56 aks-userpool0-20459031-vmss000001 \u2026. Next log entry Feb 25 21:39:05 ) it would be helpful to understand why this gap exists in the first place.\n\nIf you can reproduce the issue on a local cluster and provide more datapoints would be helpful, as it is know I agree with @HirazawaUi conclusion.\n\n@alyssa1303 setting this issue to needs more information from author\n---\n\nUser 'alyssa1303' said:\n---\nHi @esotsal! Sorry for the late response. I tried with k8s 1.32 and still see the same error. Can you confirm that you actually deploy 100 pods in a single node instead of spreading across multiple nodes? It's easier to reproduce when there's a burst of pods in a node. Also, can you check if you enable parallel pull image on your kubelet settings? I suspect it might be related. \n\n```\ncat messages.log | grep \"Observed pod startup duration\"\nMar 21 03:13:19 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:19.200417    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-5pd85\" podStartSLOduration=1.921952685 podStartE2EDuration=\"4.200398001s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.614289509 +0000 UTC m=+1029.953143114\" lastFinishedPulling=\"2025-03-21 03:13:17.892734825 +0000 UTC m=+1032.231588430\" observedRunningTime=\"2025-03-21 03:13:19.198266281 +0000 UTC m=+1033.537119886\" watchObservedRunningTime=\"2025-03-21 03:13:19.200398001 +0000 UTC m=+1033.539251706\"\nMar 21 03:13:19 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:19.429672    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-h8vh9\" podStartSLOduration=2.125523881 podStartE2EDuration=\"4.429652436s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.625955417 +0000 UTC m=+1029.964809022\" lastFinishedPulling=\"2025-03-21 03:13:17.930083972 +0000 UTC m=+1032.268937577\" observedRunningTime=\"2025-03-21 03:13:19.389486162 +0000 UTC m=+1033.728339767\" watchObservedRunningTime=\"2025-03-21 03:13:19.429652436 +0000 UTC m=+1033.768506141\"\nMar 21 03:13:19 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:19.991735    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-5cwcb\" podStartSLOduration=3.650732082 podStartE2EDuration=\"4.991716869s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.6013717 +0000 UTC m=+1030.940225305\" lastFinishedPulling=\"2025-03-21 03:13:17.942356487 +0000 UTC m=+1032.281210092\" observedRunningTime=\"2025-03-21 03:13:19.951776297 +0000 UTC m=+1034.290629902\" watchObservedRunningTime=\"2025-03-21 03:13:19.991716869 +0000 UTC m=+1034.330570474\"\nMar 21 03:13:20 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:20.233927    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-lg9rb\" podStartSLOduration=3.090380365 podStartE2EDuration=\"5.233907624s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.798695326 +0000 UTC m=+1030.137548931\" lastFinishedPulling=\"2025-03-21 03:13:17.942222485 +0000 UTC m=+1032.281076190\" observedRunningTime=\"2025-03-21 03:13:20.228646175 +0000 UTC m=+1034.567499780\" watchObservedRunningTime=\"2025-03-21 03:13:20.233907624 +0000 UTC m=+1034.572761229\"\nMar 21 03:13:20 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:20.351833    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-c9w59\" podStartSLOduration=3.062492705 podStartE2EDuration=\"5.351815822s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.613240499 +0000 UTC m=+1029.952094104\" lastFinishedPulling=\"2025-03-21 03:13:17.902563616 +0000 UTC m=+1032.241417221\" observedRunningTime=\"2025-03-21 03:13:20.349743403 +0000 UTC m=+1034.688597008\" watchObservedRunningTime=\"2025-03-21 03:13:20.351815822 +0000 UTC m=+1034.690669427\"\nMar 21 03:13:20 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:20.551828    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-wj5ln\" podStartSLOduration=3.187937574 podStartE2EDuration=\"5.551810085s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.525523782 +0000 UTC m=+1029.864377387\" lastFinishedPulling=\"2025-03-21 03:13:17.889396193 +0000 UTC m=+1032.228249898\" observedRunningTime=\"2025-03-21 03:13:20.550623774 +0000 UTC m=+1034.889477379\" watchObservedRunningTime=\"2025-03-21 03:13:20.551810085 +0000 UTC m=+1034.890663790\"\nMar 21 03:13:20 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:20.672216    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-59rsp\" podStartSLOduration=3.365224125 podStartE2EDuration=\"5.672200406s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.6133122 +0000 UTC m=+1029.952165805\" lastFinishedPulling=\"2025-03-21 03:13:17.920288481 +0000 UTC m=+1032.259142086\" observedRunningTime=\"2025-03-21 03:13:20.670663691 +0000 UTC m=+1035.009517296\" watchObservedRunningTime=\"2025-03-21 03:13:20.672200406 +0000 UTC m=+1035.011054111\"\nMar 21 03:13:21 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:21.271416    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-2jzht\" podStartSLOduration=3.94499224 podStartE2EDuration=\"6.271398602s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.591947101 +0000 UTC m=+1029.930800706\" lastFinishedPulling=\"2025-03-21 03:13:17.918353463 +0000 UTC m=+1032.257207068\" observedRunningTime=\"2025-03-21 03:13:21.270637095 +0000 UTC m=+1035.609490800\" watchObservedRunningTime=\"2025-03-21 03:13:21.271398602 +0000 UTC m=+1035.610252207\"\nMar 21 03:13:21 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:21.351041    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-vtttc\" podStartSLOduration=4.481177757 podStartE2EDuration=\"6.351023368s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.063474491 +0000 UTC m=+1030.402328096\" lastFinishedPulling=\"2025-03-21 03:13:17.933320002 +0000 UTC m=+1032.272173707\" observedRunningTime=\"2025-03-21 03:13:21.310642579 +0000 UTC m=+1035.649496284\" watchObservedRunningTime=\"2025-03-21 03:13:21.351023368 +0000 UTC m=+1035.689876973\"\nMar 21 03:13:21 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:21.549030    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-qcls8\" podStartSLOduration=4.155022679 podStartE2EDuration=\"6.549009371s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.514109576 +0000 UTC m=+1029.852963181\" lastFinishedPulling=\"2025-03-21 03:13:17.908096168 +0000 UTC m=+1032.246949873\" observedRunningTime=\"2025-03-21 03:13:21.510966706 +0000 UTC m=+1035.849820311\" watchObservedRunningTime=\"2025-03-21 03:13:21.549009371 +0000 UTC m=+1035.887862976\"\nMar 21 03:13:21 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:21.631825    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-8lrp8\" podStartSLOduration=4.163397583 podStartE2EDuration=\"6.631809867s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.454167918 +0000 UTC m=+1029.793021523\" lastFinishedPulling=\"2025-03-21 03:13:17.922580202 +0000 UTC m=+1032.261433807\" observedRunningTime=\"2025-03-21 03:13:21.630153152 +0000 UTC m=+1035.969006757\" watchObservedRunningTime=\"2025-03-21 03:13:21.631809867 +0000 UTC m=+1035.970663472\"\nMar 21 03:13:21 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:21.830932    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-p4qcg\" podStartSLOduration=5.107898238 podStartE2EDuration=\"6.830913782s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.198431148 +0000 UTC m=+1030.537284853\" lastFinishedPulling=\"2025-03-21 03:13:17.921446792 +0000 UTC m=+1032.260300397\" observedRunningTime=\"2025-03-21 03:13:21.830913982 +0000 UTC m=+1036.169767587\" watchObservedRunningTime=\"2025-03-21 03:13:21.830913782 +0000 UTC m=+1036.169767487\"\nMar 21 03:13:22 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:22.232722    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-c4xrt\" podStartSLOduration=4.948797979 podStartE2EDuration=\"7.232701945s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.61440641 +0000 UTC m=+1029.953260015\" lastFinishedPulling=\"2025-03-21 03:13:17.898310376 +0000 UTC m=+1032.237163981\" observedRunningTime=\"2025-03-21 03:13:22.23109443 +0000 UTC m=+1036.569948035\" watchObservedRunningTime=\"2025-03-21 03:13:22.232701945 +0000 UTC m=+1036.571555650\"\nMar 21 03:13:22 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:22.352144    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-t9dhj\" podStartSLOduration=5.899946571 podStartE2EDuration=\"7.352127693s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.490049663 +0000 UTC m=+1030.828903268\" lastFinishedPulling=\"2025-03-21 03:13:17.942230785 +0000 UTC m=+1032.281084390\" observedRunningTime=\"2025-03-21 03:13:22.310512593 +0000 UTC m=+1036.649366198\" watchObservedRunningTime=\"2025-03-21 03:13:22.352127693 +0000 UTC m=+1036.690981298\"\nMar 21 03:13:22 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:22.752442    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-tk7wh\" podStartSLOduration=5.220587967 podStartE2EDuration=\"7.752426742s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.398234397 +0000 UTC m=+1029.737088002\" lastFinishedPulling=\"2025-03-21 03:13:17.930073172 +0000 UTC m=+1032.268926777\" observedRunningTime=\"2025-03-21 03:13:22.750387422 +0000 UTC m=+1037.089241127\" watchObservedRunningTime=\"2025-03-21 03:13:22.752426742 +0000 UTC m=+1037.091280447\"\nMar 21 03:13:23 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:23.071078    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-kmq2s\" podStartSLOduration=5.786931537 podStartE2EDuration=\"8.071062206s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.630902263 +0000 UTC m=+1029.969755868\" lastFinishedPulling=\"2025-03-21 03:13:17.915032932 +0000 UTC m=+1032.253886537\" observedRunningTime=\"2025-03-21 03:13:23.032349333 +0000 UTC m=+1037.371202938\" watchObservedRunningTime=\"2025-03-21 03:13:23.071062206 +0000 UTC m=+1037.409915911\"\nMar 21 03:13:27 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:27.723580    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-6gb4w\" podStartSLOduration=-9223372024.13121 podStartE2EDuration=\"12.723565639s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.630962464 +0000 UTC m=+1029.969816069\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:27.723191335 +0000 UTC m=+1042.062044940\" watchObservedRunningTime=\"2025-03-21 03:13:27.723565639 +0000 UTC m=+1042.062419244\"\nMar 21 03:13:27 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:27.750637    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-gwnz4\" podStartSLOduration=-9223372024.104155 podStartE2EDuration=\"12.750620799s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.192125389 +0000 UTC m=+1030.530978994\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:27.748460878 +0000 UTC m=+1042.087314483\" watchObservedRunningTime=\"2025-03-21 03:13:27.750620799 +0000 UTC m=+1042.089474404\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.727286    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-sfsl7\" podStartSLOduration=-9223372023.127499 podStartE2EDuration=\"13.72727649s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.151687013 +0000 UTC m=+1030.490540618\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.727073988 +0000 UTC m=+1043.065927593\" watchObservedRunningTime=\"2025-03-21 03:13:28.72727649 +0000 UTC m=+1043.066130195\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.760675    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-qmw2r\" podStartSLOduration=-9223372023.094114 podStartE2EDuration=\"13.760661511s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.296095757 +0000 UTC m=+1030.634949362\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.744609956 +0000 UTC m=+1043.083463661\" watchObservedRunningTime=\"2025-03-21 03:13:28.760661511 +0000 UTC m=+1043.099515116\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.761517    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-kjqhs\" podStartSLOduration=-9223372023.09327 podStartE2EDuration=\"13.761505519s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.798815227 +0000 UTC m=+1030.137668832\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.760719611 +0000 UTC m=+1043.099573316\" watchObservedRunningTime=\"2025-03-21 03:13:28.761505519 +0000 UTC m=+1043.100359124\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.788157    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-5nxht\" podStartSLOduration=-9223372023.066637 podStartE2EDuration=\"13.788138275s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.309484882 +0000 UTC m=+1030.648338487\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.78561705 +0000 UTC m=+1043.124470755\" watchObservedRunningTime=\"2025-03-21 03:13:28.788138275 +0000 UTC m=+1043.126991980\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.819529    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-jbvlq\" podStartSLOduration=-9223372023.035273 podStartE2EDuration=\"13.819503176s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.0966297 +0000 UTC m=+1030.435483305\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.818049162 +0000 UTC m=+1043.156902767\" watchObservedRunningTime=\"2025-03-21 03:13:28.819503176 +0000 UTC m=+1043.158356781\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.820671    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-ks84q\" podStartSLOduration=-9223372023.034124 podStartE2EDuration=\"13.820652387s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.68422976 +0000 UTC m=+1030.023083465\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.803965127 +0000 UTC m=+1043.142818832\" watchObservedRunningTime=\"2025-03-21 03:13:28.820652387 +0000 UTC m=+1043.159505992\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.856585    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-d6bbp\" podStartSLOduration=-9223372022.998213 podStartE2EDuration=\"13.856563033s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.218141932 +0000 UTC m=+1030.556995537\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.854508213 +0000 UTC m=+1043.193361918\" watchObservedRunningTime=\"2025-03-21 03:13:28.856563033 +0000 UTC m=+1043.195416638\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.857113    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-rdr4l\" podStartSLOduration=-9223372022.997679 podStartE2EDuration=\"13.857096938s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.788733133 +0000 UTC m=+1030.127586838\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.837400748 +0000 UTC m=+1043.176254453\" watchObservedRunningTime=\"2025-03-21 03:13:28.857096938 +0000 UTC m=+1043.195950543\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.871433    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-85z7k\" podStartSLOduration=-9223372022.983366 podStartE2EDuration=\"13.871410675s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.150755404 +0000 UTC m=+1030.489609109\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.86873175 +0000 UTC m=+1043.207585355\" watchObservedRunningTime=\"2025-03-21 03:13:28.871410675 +0000 UTC m=+1043.210264280\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.898224    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-xs8sk\" podStartSLOduration=-9223372022.956581 podStartE2EDuration=\"13.898195433s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.694494656 +0000 UTC m=+1030.033348261\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.896313215 +0000 UTC m=+1043.235166920\" watchObservedRunningTime=\"2025-03-21 03:13:28.898195433 +0000 UTC m=+1043.237049138\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.914117    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-qzbg4\" podStartSLOduration=-9223372022.940683 podStartE2EDuration=\"13.914092986s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.764081004 +0000 UTC m=+1030.102934709\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.912589371 +0000 UTC m=+1043.251443076\" watchObservedRunningTime=\"2025-03-21 03:13:28.914092986 +0000 UTC m=+1043.252946691\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.743375    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-9v968\" podStartSLOduration=-9223372022.111412 podStartE2EDuration=\"14.743363516s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.232346464 +0000 UTC m=+1030.571200069\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.743138614 +0000 UTC m=+1044.081992219\" watchObservedRunningTime=\"2025-03-21 03:13:29.743363516 +0000 UTC m=+1044.082217221\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.777476    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-s5hjm\" podStartSLOduration=-9223372022.077312 podStartE2EDuration=\"14.777464228s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.193046798 +0000 UTC m=+1030.531900403\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.762203088 +0000 UTC m=+1044.101056693\" watchObservedRunningTime=\"2025-03-21 03:13:29.777464228 +0000 UTC m=+1044.116317833\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.791363    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-qmrw7\" podStartSLOduration=-9223372022.063433 podStartE2EDuration=\"14.791342954s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.737999772 +0000 UTC m=+1031.076853377\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.791239654 +0000 UTC m=+1044.130093259\" watchObservedRunningTime=\"2025-03-21 03:13:29.791342954 +0000 UTC m=+1044.130196559\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.809451    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-98zcf\" podStartSLOduration=-9223372022.04535 podStartE2EDuration=\"14.80942682s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.774925805 +0000 UTC m=+1030.113779410\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.806957097 +0000 UTC m=+1044.145810802\" watchObservedRunningTime=\"2025-03-21 03:13:29.80942682 +0000 UTC m=+1044.148280525\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.845589    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-pgqjn\" podStartSLOduration=-9223372022.009212 podStartE2EDuration=\"14.84556425s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.218133332 +0000 UTC m=+1030.556986937\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.826599777 +0000 UTC m=+1044.165453382\" watchObservedRunningTime=\"2025-03-21 03:13:29.84556425 +0000 UTC m=+1044.184417955\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.846234    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-lmzcq\" podStartSLOduration=-9223372022.008556 podStartE2EDuration=\"14.846219956s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.738373576 +0000 UTC m=+1031.077227181\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.844299739 +0000 UTC m=+1044.183153344\" watchObservedRunningTime=\"2025-03-21 03:13:29.846219956 +0000 UTC m=+1044.185073661\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.893156    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-ll5ws\" podStartSLOduration=-9223372021.961643 podStartE2EDuration=\"14.893132285s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.296088457 +0000 UTC m=+1030.634942062\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.877643344 +0000 UTC m=+1044.216496949\" watchObservedRunningTime=\"2025-03-21 03:13:29.893132285 +0000 UTC m=+1044.231985990\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.893887    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-pzkl4\" podStartSLOduration=-9223372021.960901 podStartE2EDuration=\"14.893874392s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.903561902 +0000 UTC m=+1030.242415507\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.892696481 +0000 UTC m=+1044.231550086\" watchObservedRunningTime=\"2025-03-21 03:13:29.893874392 +0000 UTC m=+1044.232728097\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.931560    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-t4r8t\" podStartSLOduration=-9223372021.923231 podStartE2EDuration=\"14.931545337s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.763787601 +0000 UTC m=+1030.102641206\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.931120333 +0000 UTC m=+1044.269974038\" watchObservedRunningTime=\"2025-03-21 03:13:29.931545337 +0000 UTC m=+1044.270398942\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.931915    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-blksv\" podStartSLOduration=-9223372021.922865 podStartE2EDuration=\"14.93191044s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.498044538 +0000 UTC m=+1030.836898143\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.91336167 +0000 UTC m=+1044.252215275\" watchObservedRunningTime=\"2025-03-21 03:13:29.93191044 +0000 UTC m=+1044.270764145\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.944643    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-xh8th\" podStartSLOduration=-9223372021.910145 podStartE2EDuration=\"14.944631156s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.279809706 +0000 UTC m=+1030.618663311\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.944586456 +0000 UTC m=+1044.283440161\" watchObservedRunningTime=\"2025-03-21 03:13:29.944631156 +0000 UTC m=+1044.283484761\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.968652    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-8m27p\" podStartSLOduration=-9223372021.886139 podStartE2EDuration=\"14.968636476s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.656439101 +0000 UTC m=+1029.995292706\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.968195372 +0000 UTC m=+1044.307048977\" watchObservedRunningTime=\"2025-03-21 03:13:29.968636476 +0000 UTC m=+1044.307490181\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.985168    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-t2x84\" podStartSLOduration=-9223372021.86962 podStartE2EDuration=\"14.985156427s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.222334871 +0000 UTC m=+1030.561188476\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.982606003 +0000 UTC m=+1044.321459708\" watchObservedRunningTime=\"2025-03-21 03:13:29.985156427 +0000 UTC m=+1044.324010032\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.012837    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-7c65s\" podStartSLOduration=-9223372021.841953 podStartE2EDuration=\"15.01282288s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.093637872 +0000 UTC m=+1030.432491477\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.996999635 +0000 UTC m=+1044.335853340\" watchObservedRunningTime=\"2025-03-21 03:13:30.01282288 +0000 UTC m=+1044.351676485\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.012948    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-zrwh7\" podStartSLOduration=-9223372021.841831 podStartE2EDuration=\"15.012943781s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.904088907 +0000 UTC m=+1030.242942512\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.011905271 +0000 UTC m=+1044.350758976\" watchObservedRunningTime=\"2025-03-21 03:13:30.012943781 +0000 UTC m=+1044.351797386\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.034890    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-dfq58\" podStartSLOduration=-9223372021.819899 podStartE2EDuration=\"15.034877981s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.193641703 +0000 UTC m=+1030.532495308\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.03467058 +0000 UTC m=+1044.373524185\" watchObservedRunningTime=\"2025-03-21 03:13:30.034877981 +0000 UTC m=+1044.373731586\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.066197    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-2g5b8\" podStartSLOduration=-9223372021.788593 podStartE2EDuration=\"15.066181968s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.698267191 +0000 UTC m=+1030.037120796\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.065916965 +0000 UTC m=+1044.404770570\" watchObservedRunningTime=\"2025-03-21 03:13:30.066181968 +0000 UTC m=+1044.405035673\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.087932    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-2h2zw\" podStartSLOduration=-9223372021.766863 podStartE2EDuration=\"15.087912466s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.295999857 +0000 UTC m=+1030.634853562\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.085933148 +0000 UTC m=+1044.424786853\" watchObservedRunningTime=\"2025-03-21 03:13:30.087912466 +0000 UTC m=+1044.426766071\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.137266    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-srw4q\" podStartSLOduration=-9223372021.717527 podStartE2EDuration=\"15.137247518s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.902810795 +0000 UTC m=+1030.241664400\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.135858905 +0000 UTC m=+1044.474712510\" watchObservedRunningTime=\"2025-03-21 03:13:30.137247518 +0000 UTC m=+1044.476101123\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.197793    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-fxv2t\" podStartSLOduration=-9223372021.657 podStartE2EDuration=\"15.197776671s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.656919906 +0000 UTC m=+1029.995773611\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.196484559 +0000 UTC m=+1044.535338264\" watchObservedRunningTime=\"2025-03-21 03:13:30.197776671 +0000 UTC m=+1044.536630276\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.281975    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-22gt8\" podStartSLOduration=-9223372021.572819 podStartE2EDuration=\"15.281956141s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.692153034 +0000 UTC m=+1030.031006639\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.266712901 +0000 UTC m=+1044.605566506\" watchObservedRunningTime=\"2025-03-21 03:13:30.281956141 +0000 UTC m=+1044.620809846\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.310947    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-xbcgj\" podStartSLOduration=-9223372021.543846 podStartE2EDuration=\"15.310929706s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.692563638 +0000 UTC m=+1030.031417343\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.309458392 +0000 UTC m=+1044.648311997\" watchObservedRunningTime=\"2025-03-21 03:13:30.310929706 +0000 UTC m=+1044.649783311\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.764309    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-cccxt\" podStartSLOduration=-9223372021.090485 podStartE2EDuration=\"15.764292052s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.726583166 +0000 UTC m=+1031.065436771\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.413234541 +0000 UTC m=+1044.752088246\" watchObservedRunningTime=\"2025-03-21 03:13:30.764292052 +0000 UTC m=+1045.103145757\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.894602    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-65s72\" podStartSLOduration=-9223372020.96019 podStartE2EDuration=\"15.894585743s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.191949788 +0000 UTC m=+1030.530803393\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.892216021 +0000 UTC m=+1045.231069626\" watchObservedRunningTime=\"2025-03-21 03:13:30.894585743 +0000 UTC m=+1045.233439348\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.912226    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-m7hr4\" podStartSLOduration=-9223372020.942568 podStartE2EDuration=\"15.912208704s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.566271673 +0000 UTC m=+1030.905125278\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.909087876 +0000 UTC m=+1045.247941481\" watchObservedRunningTime=\"2025-03-21 03:13:30.912208704 +0000 UTC m=+1045.251062309\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.942281    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-6h8jn\" podStartSLOduration=-9223372020.91251 podStartE2EDuration=\"15.942265179s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.309327181 +0000 UTC m=+1030.648180786\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.941991177 +0000 UTC m=+1045.280844882\" watchObservedRunningTime=\"2025-03-21 03:13:30.942265179 +0000 UTC m=+1045.281118784\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.956458    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-s9ww8\" podStartSLOduration=-9223372020.89833 podStartE2EDuration=\"15.956444509s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.276795078 +0000 UTC m=+1030.615648783\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.955703802 +0000 UTC m=+1045.294557507\" watchObservedRunningTime=\"2025-03-21 03:13:30.956444509 +0000 UTC m=+1045.295298214\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.971561    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-2rf6j\" podStartSLOduration=-9223372020.883226 podStartE2EDuration=\"15.971549247s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.697739286 +0000 UTC m=+1030.036592991\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.971382545 +0000 UTC m=+1045.310236250\" watchObservedRunningTime=\"2025-03-21 03:13:30.971549247 +0000 UTC m=+1045.310402952\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.006484    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-7nzwf\" podStartSLOduration=-9223372020.848307 podStartE2EDuration=\"16.006468566s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.536324494 +0000 UTC m=+1030.875178099\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.990319618 +0000 UTC m=+1045.329173323\" watchObservedRunningTime=\"2025-03-21 03:13:31.006468566 +0000 UTC m=+1045.345322171\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.044637    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-92db7\" podStartSLOduration=-9223372020.810158 podStartE2EDuration=\"16.044617515s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.61752435 +0000 UTC m=+1030.956377955\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.043922609 +0000 UTC m=+1045.382776214\" watchObservedRunningTime=\"2025-03-21 03:13:31.044617515 +0000 UTC m=+1045.383471220\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.060221    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-8v8z6\" podStartSLOduration=-9223372020.794569 podStartE2EDuration=\"16.060206358s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.662466657 +0000 UTC m=+1030.001320262\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.058875745 +0000 UTC m=+1045.397729350\" watchObservedRunningTime=\"2025-03-21 03:13:31.060206358 +0000 UTC m=+1045.399059963\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.074283    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-ztjl5\" podStartSLOduration=-9223372020.780506 podStartE2EDuration=\"16.074269786s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.967671799 +0000 UTC m=+1030.306525504\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.073344278 +0000 UTC m=+1045.412197883\" watchObservedRunningTime=\"2025-03-21 03:13:31.074269786 +0000 UTC m=+1045.413123391\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.106010    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-pwh6k\" podStartSLOduration=-9223372020.74878 podStartE2EDuration=\"16.105996076s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.069850351 +0000 UTC m=+1030.408703956\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.104874166 +0000 UTC m=+1045.443727771\" watchObservedRunningTime=\"2025-03-21 03:13:31.105996076 +0000 UTC m=+1045.444849681\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.135053    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-8bf59\" podStartSLOduration=-9223372020.719736 podStartE2EDuration=\"16.135039342s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.623636207 +0000 UTC m=+1030.962489812\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.119238297 +0000 UTC m=+1045.458092002\" watchObservedRunningTime=\"2025-03-21 03:13:31.135039342 +0000 UTC m=+1045.473892947\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.135812    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-295jz\" podStartSLOduration=-9223372020.718971 podStartE2EDuration=\"16.135804949s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.906392929 +0000 UTC m=+1030.245246534\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.135294244 +0000 UTC m=+1045.474147849\" watchObservedRunningTime=\"2025-03-21 03:13:31.135804949 +0000 UTC m=+1045.474658554\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.168498    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-lck4j\" podStartSLOduration=-9223372020.68629 podStartE2EDuration=\"16.168485848s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.106208589 +0000 UTC m=+1030.445062294\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.167876942 +0000 UTC m=+1045.506730547\" watchObservedRunningTime=\"2025-03-21 03:13:31.168485848 +0000 UTC m=+1045.507339453\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.185334    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-hfplg\" podStartSLOduration=-9223372020.669456 podStartE2EDuration=\"16.185318802s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.284343048 +0000 UTC m=+1030.623196753\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.183683787 +0000 UTC m=+1045.522537392\" watchObservedRunningTime=\"2025-03-21 03:13:31.185318802 +0000 UTC m=+1045.524172407\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.214894    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-d6shl\" podStartSLOduration=-9223372020.639896 podStartE2EDuration=\"16.214879072s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.797997019 +0000 UTC m=+1030.136850724\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.213978964 +0000 UTC m=+1045.552832569\" watchObservedRunningTime=\"2025-03-21 03:13:31.214879072 +0000 UTC m=+1045.553732777\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.834527    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-l8fnq\" podStartSLOduration=-9223372020.020267 podStartE2EDuration=\"16.834507738s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.097986813 +0000 UTC m=+1030.436840418\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.833314927 +0000 UTC m=+1046.172168632\" watchObservedRunningTime=\"2025-03-21 03:13:31.834507738 +0000 UTC m=+1046.173361443\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.851955    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-f5zxf\" podStartSLOduration=-9223372020.002844 podStartE2EDuration=\"16.851931597s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.312154907 +0000 UTC m=+1030.651008512\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.850246582 +0000 UTC m=+1046.189100187\" watchObservedRunningTime=\"2025-03-21 03:13:31.851931597 +0000 UTC m=+1046.190785202\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.870777    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-5sfj6\" podStartSLOduration=-9223372019.984016 podStartE2EDuration=\"16.87075927s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.69050053 +0000 UTC m=+1031.029354235\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.866207228 +0000 UTC m=+1046.205060833\" watchObservedRunningTime=\"2025-03-21 03:13:31.87075927 +0000 UTC m=+1046.209612975\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.885334    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-628qf\" podStartSLOduration=-9223372019.969467 podStartE2EDuration=\"16.885309203s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.21791963 +0000 UTC m=+1030.556773335\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.882933081 +0000 UTC m=+1046.221786686\" watchObservedRunningTime=\"2025-03-21 03:13:31.885309203 +0000 UTC m=+1046.224162808\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.936251    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-vqpjw\" podStartSLOduration=-9223372019.918541 podStartE2EDuration=\"16.936234368s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.6562813 +0000 UTC m=+1029.995134905\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.935183159 +0000 UTC m=+1046.274036864\" watchObservedRunningTime=\"2025-03-21 03:13:31.936234368 +0000 UTC m=+1046.275088073\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.959408    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-m8v8r\" podStartSLOduration=-9223372019.895384 podStartE2EDuration=\"16.95939208s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.719354899 +0000 UTC m=+1031.058208604\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.958134969 +0000 UTC m=+1046.296988574\" watchObservedRunningTime=\"2025-03-21 03:13:31.95939208 +0000 UTC m=+1046.298245685\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.989583    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-n4jmr\" podStartSLOduration=-9223372019.865208 podStartE2EDuration=\"16.989568056s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.113675759 +0000 UTC m=+1030.452529364\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.974169415 +0000 UTC m=+1046.313023120\" watchObservedRunningTime=\"2025-03-21 03:13:31.989568056 +0000 UTC m=+1046.328421661\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.004333    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-578vs\" podStartSLOduration=-9223372019.850456 podStartE2EDuration=\"17.004320491s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.292687226 +0000 UTC m=+1030.631540931\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.003965288 +0000 UTC m=+1046.342818893\" watchObservedRunningTime=\"2025-03-21 03:13:32.004320491 +0000 UTC m=+1046.343174096\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.039533    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-cs9sn\" podStartSLOduration=-9223372019.815256 podStartE2EDuration=\"17.039519513s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.905630722 +0000 UTC m=+1030.244484327\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.024897079 +0000 UTC m=+1046.363750684\" watchObservedRunningTime=\"2025-03-21 03:13:32.039519513 +0000 UTC m=+1046.378373118\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.040258    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-9l8bz\" podStartSLOduration=-9223372019.814526 podStartE2EDuration=\"17.040250819s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.740164492 +0000 UTC m=+1031.079018097\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.039042908 +0000 UTC m=+1046.377896513\" watchObservedRunningTime=\"2025-03-21 03:13:32.040250819 +0000 UTC m=+1046.379104424\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.095529    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-djvwz\" podStartSLOduration=-9223372019.759266 podStartE2EDuration=\"17.095510825s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.28241743 +0000 UTC m=+1030.621271035\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.07972568 +0000 UTC m=+1046.418579285\" watchObservedRunningTime=\"2025-03-21 03:13:32.095510825 +0000 UTC m=+1046.434364530\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.109946    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-r6rxc\" podStartSLOduration=-9223372019.744844 podStartE2EDuration=\"17.109931657s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.550549927 +0000 UTC m=+1030.889403532\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.109059349 +0000 UTC m=+1046.447913054\" watchObservedRunningTime=\"2025-03-21 03:13:32.109931657 +0000 UTC m=+1046.448785262\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.123331    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-v7vjt\" podStartSLOduration=-9223372019.731459 podStartE2EDuration=\"17.123317079s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.220475753 +0000 UTC m=+1030.559329358\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.12232607 +0000 UTC m=+1046.461179675\" watchObservedRunningTime=\"2025-03-21 03:13:32.123317079 +0000 UTC m=+1046.462170784\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.215148    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-s7brd\" podStartSLOduration=-9223372019.639643 podStartE2EDuration=\"17.215132519s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.10840701 +0000 UTC m=+1030.447260715\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.199573176 +0000 UTC m=+1046.538426781\" watchObservedRunningTime=\"2025-03-21 03:13:32.215132519 +0000 UTC m=+1046.553986224\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.872619    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-264f8\" podStartSLOduration=-9223372018.982172 podStartE2EDuration=\"17.872603331s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.165080537 +0000 UTC m=+1030.503934243\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.842420955 +0000 UTC m=+1047.181274660\" watchObservedRunningTime=\"2025-03-21 03:13:32.872603331 +0000 UTC m=+1047.211456936\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.890284    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-2kdnt\" podStartSLOduration=-9223372018.964504 podStartE2EDuration=\"17.890272192s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.631495381 +0000 UTC m=+1030.970349086\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.872841633 +0000 UTC m=+1047.211695338\" watchObservedRunningTime=\"2025-03-21 03:13:32.890272192 +0000 UTC m=+1047.229125897\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.892424    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-58vrp\" podStartSLOduration=-9223372018.962368 podStartE2EDuration=\"17.892408612s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.200477467 +0000 UTC m=+1030.539331172\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.890781297 +0000 UTC m=+1047.229635002\" watchObservedRunningTime=\"2025-03-21 03:13:32.892408612 +0000 UTC m=+1047.231262217\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.926429    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-hnv4p\" podStartSLOduration=-9223372018.928364 podStartE2EDuration=\"17.926412723s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.69160824 +0000 UTC m=+1031.030461845\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.925349813 +0000 UTC m=+1047.264203518\" watchObservedRunningTime=\"2025-03-21 03:13:32.926412723 +0000 UTC m=+1047.265266328\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.926677    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-r58ns\" podStartSLOduration=-9223372018.928106 podStartE2EDuration=\"17.926669125s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.566812278 +0000 UTC m=+1030.905665883\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.909863572 +0000 UTC m=+1047.248717177\" watchObservedRunningTime=\"2025-03-21 03:13:32.926669125 +0000 UTC m=+1047.265522730\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.941415    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-gfgkv\" podStartSLOduration=-9223372018.913374 podStartE2EDuration=\"17.94140136s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.976416581 +0000 UTC m=+1030.315270186\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.941179358 +0000 UTC m=+1047.280033063\" watchObservedRunningTime=\"2025-03-21 03:13:32.94140136 +0000 UTC m=+1047.280255065\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.960785    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-z8bkl\" podStartSLOduration=-9223372018.894003 podStartE2EDuration=\"17.960772137s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.309409881 +0000 UTC m=+1030.648263486\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.959369624 +0000 UTC m=+1047.298223229\" watchObservedRunningTime=\"2025-03-21 03:13:32.960772137 +0000 UTC m=+1047.299625742\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.000873    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-8ccv8\" podStartSLOduration=-9223372018.853914 podStartE2EDuration=\"18.000860804s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.738375976 +0000 UTC m=+1031.077229581\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.984558555 +0000 UTC m=+1047.323412260\" watchObservedRunningTime=\"2025-03-21 03:13:33.000860804 +0000 UTC m=+1047.339714409\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.002071    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-7gbtx\" podStartSLOduration=-9223372018.852713 podStartE2EDuration=\"18.002062815s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.284209447 +0000 UTC m=+1030.623063052\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.001411709 +0000 UTC m=+1047.340265314\" watchObservedRunningTime=\"2025-03-21 03:13:33.002062815 +0000 UTC m=+1047.340916520\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.021074    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-f5vfn\" podStartSLOduration=-9223372018.833714 podStartE2EDuration=\"18.021061788s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.096799602 +0000 UTC m=+1030.435653307\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.020722285 +0000 UTC m=+1047.359575890\" watchObservedRunningTime=\"2025-03-21 03:13:33.021061788 +0000 UTC m=+1047.359915493\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.041505    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-7fflq\" podStartSLOduration=-9223372018.813286 podStartE2EDuration=\"18.041490675s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.738438576 +0000 UTC m=+1031.077292181\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.040171363 +0000 UTC m=+1047.379025068\" watchObservedRunningTime=\"2025-03-21 03:13:33.041490675 +0000 UTC m=+1047.380344380\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.067260    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-4n5jw\" podStartSLOduration=-9223372018.78753 podStartE2EDuration=\"18.067245411s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.550062022 +0000 UTC m=+1030.888915727\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.064382585 +0000 UTC m=+1047.403236290\" watchObservedRunningTime=\"2025-03-21 03:13:33.067245411 +0000 UTC m=+1047.406099016\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.113932    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-4wzvx\" podStartSLOduration=-9223372018.74086 podStartE2EDuration=\"18.113915637s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.797781117 +0000 UTC m=+1030.136634822\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.095027165 +0000 UTC m=+1047.433880870\" watchObservedRunningTime=\"2025-03-21 03:13:33.113915637 +0000 UTC m=+1047.452769242\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.114175    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-q27k7\" podStartSLOduration=-9223372018.740606 podStartE2EDuration=\"18.11416994s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.727564975 +0000 UTC m=+1031.066418580\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.11310713 +0000 UTC m=+1047.451960835\" watchObservedRunningTime=\"2025-03-21 03:13:33.11416994 +0000 UTC m=+1047.453023645\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.129667    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-g567k\" podStartSLOduration=-9223372018.725124 podStartE2EDuration=\"18.129651781s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.218154732 +0000 UTC m=+1030.557008337\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.128065567 +0000 UTC m=+1047.466919272\" watchObservedRunningTime=\"2025-03-21 03:13:33.129651781 +0000 UTC m=+1047.468505386\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.147916    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-t94cg\" podStartSLOduration=-9223372018.706875 podStartE2EDuration=\"18.147900148s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.798969528 +0000 UTC m=+1030.137823133\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.147198442 +0000 UTC m=+1047.486052047\" watchObservedRunningTime=\"2025-03-21 03:13:33.147900148 +0000 UTC m=+1047.486753753\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.170297    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-dqm59\" podStartSLOduration=-9223372018.684494 podStartE2EDuration=\"18.170282453s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.096743801 +0000 UTC m=+1030.435597406\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.16998595 +0000 UTC m=+1047.508839655\" watchObservedRunningTime=\"2025-03-21 03:13:33.170282453 +0000 UTC m=+1047.509136158\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.221446    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-zzmbs\" podStartSLOduration=-9223372018.633348 podStartE2EDuration=\"18.221428021s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.193080298 +0000 UTC m=+1030.531933903\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.197713204 +0000 UTC m=+1047.536566909\" watchObservedRunningTime=\"2025-03-21 03:13:33.221428021 +0000 UTC m=+1047.560281626\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.222630    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-l4wcb\" podStartSLOduration=-9223372018.632154 podStartE2EDuration=\"18.222620532s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.098106114 +0000 UTC m=+1030.436959719\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.220730714 +0000 UTC m=+1047.559584319\" watchObservedRunningTime=\"2025-03-21 03:13:33.222620532 +0000 UTC m=+1047.561474237\"\nMar 21 03:14:05 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:14:05.976677    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/nsenter-qp4ayb\" podStartSLOduration=0.679895828 podStartE2EDuration=\"2.976662035s\" podCreationTimestamp=\"2025-03-21 03:14:03 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:14:03.528909062 +0000 UTC m=+1077.867762667\" lastFinishedPulling=\"2025-03-21 03:14:05.825675269 +0000 UTC m=+1080.164528874\" observedRunningTime=\"2025-03-21 03:14:05.974919418 +0000 UTC m=+1080.313773023\" watchObservedRunningTime=\"2025-03-21 03:14:05.976662035 +0000 UTC m=+1080.315515740\"\n```\n---\n\nUser 'alyssa1303' said:\n---\nFull log is here: \n\n[kubelet.log](https://github.com/user-attachments/files/19380521/kubelet.log)\n---\n\nUser 'esotsal' said:\n---\nThanks @alyssa1303 for providing additional information. Continuing investigation.\n---\n\nUser 'esotsal' said:\n---\nReproduced issue starting a local up cluster with parallel image pulls enabled ( `serializeImagePulls` field set to false ) .\n\nI0321 14:29:35.653481  364481 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-8665c9fdf7-wmxz8\" **podStartSLOduration=-9223372020.201311** podStartE2EDuration=\"16.653464473s\" podCreationTimestamp=\"2025-03-21 14:29:19 +0100 CET\" **firstStartedPulling=\"2025-03-21 14:29:20.051045925 +0100 CET m=+45.327360658\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\"** observedRunningTime=\"2025-03-21 14:29:35.651967605 +0100 CET m=+60.928282339\" watchObservedRunningTime=\"2025-03-21 14:29:35.653464473 +0100 CET m=+60.929779200\"\n\nWith parallel image pulls disabled ( `serializeImagePulls` field set to true ) , didn't manage to reproduce issue. \n\n/retitle pod_startup_latency_tracker : podStartSLOduration returns negative value with serializeImagePulls set to false\n\nContinuing investigation to find root cause\n---",
    "question": "Could you explain how to resolve 'pod_startup_latency_tracker : podStartSLOduration returns negative value with serializeImagePulls set to false'?",
    "ideal_answer": "I observed same issue with EKS, so cloud provider is not related here\n```\nFeb 25 22:23:56 ip-10-0-91-209 kubelet: I0225 22:23:56.631131    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-65tbb\" podStartSLOduration=1.546927693 podStartE2EDuration=\"2.631123808s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.163506378 +0000 UTC m=+10440.454286709\" lastFinishedPulling=\"2025-02-25 22:23:56.247702507 +0000 UTC m=+10441.538482824\" observedRunningTime=\"2025-02-25 22:23:56.630931783 +0000 UTC m=+10441.921712121\" watchObservedRunningTime=\"2025-02-25 22:23:56.631123808 +0000 UTC m=+10441.921904147\"\nFeb 25 22:23:57 ip-10-0-91-209 kubelet: I0225 22:23:57.386200    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-45bzd\" podStartSLOduration=-9223372033.468592 podStartE2EDuration=\"3.386184373s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.520600231 +0000 UTC m=+10440.811380547\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:23:57.38500166 +0000 UTC m=+10442.675781999\" watchObservedRunningTime=\"2025-02-25 22:23:57.386184373 +0000 UTC m=+10442.676964712\"\nFeb 25 22:23:57 ip-10-0-91-209 kubelet: I0225 22:23:57.399827    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-s9f8b\" podStartSLOduration=-9223372033.454958 podStartE2EDuration=\"3.399816932s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.520564068 +0000 UTC m=+10440.811344403\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:23:57.399470898 +0000 UTC m=+10442.690251237\" watchObservedRunningTime=\"2025-02-25 22:23:57.399816932 +0000 UTC m=+10442.690597270\"\nFeb 25 22:23:57 ip-10-0-91-209 kubelet: I0225 22:23:57.419389    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-rqfls\" podStartSLOduration=2.665023156 podStartE2EDuration=\"3.41938171s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.519728801 +0000 UTC m=+10440.810509133\" lastFinishedPulling=\"2025-02-25 22:23:56.274087357 +0000 UTC m=+10441.564867687\" observedRunningTime=\"2025-02-25 22:23:57.41927327 +0000 UTC m=+10442.710053610\" watchObservedRunningTime=\"2025-02-25 22:23:57.41938171 +0000 UTC m=+10442.710162050\"\nFeb 25 22:23:57 ip-10-0-91-209 kubelet: I0225 22:23:57.434143    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-5v4j5\" podStartSLOduration=-9223372033.42064 podStartE2EDuration=\"3.434135031s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.520488933 +0000 UTC m=+10440.811269250\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:23:57.433087607 +0000 UTC m=+10442.723867946\" watchObservedRunningTime=\"2025-02-25 22:23:57.434135031 +0000 UTC m=+10442.724915371\"\nFeb 25 22:24:08 ip-10-0-91-209 kubelet: I0225 22:24:08.024215    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-dvkq6\" podStartSLOduration=12.852415203 podStartE2EDuration=\"14.024202075s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.101434339 +0000 UTC m=+10440.392214660\" lastFinishedPulling=\"2025-02-25 22:23:56.273221215 +0000 UTC m=+10441.564001532\" observedRunningTime=\"2025-02-25 22:23:57.453636416 +0000 UTC m=+10442.744416753\" watchObservedRunningTime=\"2025-02-25 22:24:08.024202075 +0000 UTC m=+10453.314982413\"\nFeb 25 22:24:08 ip-10-0-91-209 kubelet: I0225 22:24:08.413462    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-fj8cp\" podStartSLOduration=-9223372022.441326 podStartE2EDuration=\"14.413449266s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.199151934 +0000 UTC m=+10440.489932252\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:24:08.412149749 +0000 UTC m=+10453.702930088\" watchObservedRunningTime=\"2025-02-25 22:24:08.413449266 +0000 UTC m=+10453.704229603\"\nFeb 25 22:24:10 ip-10-0-91-209 kubelet: I0225 22:24:10.414060    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-kvzm2\" podStartSLOduration=-9223372020.440725 podStartE2EDuration=\"16.41405031s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.233269552 +0000 UTC m=+10440.524049868\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:24:10.413714446 +0000 UTC m=+10455.704494785\" watchObservedRunningTime=\"2025-02-25 22:24:10.41405031 +0000 UTC m=+10455.704830649\"\nFeb 25 22:24:10 ip-10-0-91-209 kubelet: I0225 22:24:10.433791    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-5sngx\" podStartSLOduration=-9223372020.420994 podStartE2EDuration=\"16.433781479s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.196000401 +0000 UTC m=+10440.486780732\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:24:10.433756073 +0000 UTC m=+10455.724536413\" watchObservedRunningTime=\"2025-02-25 22:24:10.433781479 +0000 UTC m=+10455.724561819\"\nFeb 25 22:24:10 ip-10-0-91-209 kubelet: I0225 22:24:10.463454    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-lvzwl\" podStartSLOduration=-9223372020.39133 podStartE2EDuration=\"16.46344522s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.196818306 +0000 UTC m=+10440.487598623\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:24:10.448051989 +0000 UTC m=+10455.738832327\" watchObservedRunningTime=\"2025-02-25 22:24:10.46344522 +0000 UTC m=+10455.754225612\"\nFeb 25 22:24:10 ip-10-0-91-209 kubelet: I0225 22:24:10.463513    4578 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-7485567c97-qp8j6\" podStartSLOduration=-9223372020.391266 podStartE2EDuration=\"16.463509885s\" podCreationTimestamp=\"2025-02-25 22:23:54 +0000 UTC\" firstStartedPulling=\"2025-02-25 22:23:55.196687275 +0000 UTC m=+10440.487467592\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-02-25 22:24:10.462599237 +0000 UTC m=+10455.753379575\" watchObservedRunningTime=\"2025-02-25 22:24:10.463509885 +0000 UTC m=+10455.754290224\"\n```\nKubernetes version\n```\nkubectl version\nClient Version: v1.31.0\nKustomize Version: v5.4.2\nServer Version: v1.31.5-eks-8cce635\n```\n\nOS Version\n```\n[root@ip-10-0-91-209 /]# cat /etc/os-release\nNAME=\"Amazon Linux\"\nVERSION=\"2\"\nID=\"amzn\"\nID_LIKE=\"centos rhel fedora\"\nVERSION_ID=\"2\"\nPRETTY_NAME=\"Amazon Linux 2\"\nANSI_COLOR=\"0;33\"\nCPE_NAME=\"cpe:2.3:o:amazon:amazon_linux:2\"\nHOME_URL=\"https://amazonlinux.com/\"\nSUPPORT_END=\"2026-06-30\"\n```\n\nSame containerd version containerd://1.7.25\n\nI believe this will also return negative sum as mentioned in this issue [122675](https://github.com/kubernetes/kubernetes/issues/122675)\n\nI used the same **deployment** as yours to create an ephemeral cluster with the latest code from the master branch, but couldn't reproduce the issue. I'm uncertain whether this problem has been fixed. Below are the logs I collected:\n\nI've attempted to manually remove the existing local images and recreate the same scenario as your environment, but still can't reproduce the issue. Maybe should inject delays to simulate this conditions?\n\n/triage accepted\n/priority important-longterm\n\n[Update]\n\nUsed same deployment removing nodeSelector, starting 100 instances using local-up-cluster, i could not reproduce the issue. \n\nVariable lastFinishedPulling, is set to \"0001-01-01 00:00:00 +0000 UTC\" resulting later to a negative podStartSLOduration.\n\nhttps://github.com/kubernetes/kubernetes/blob/c30b1eb09b6355f88ac514ec97cb7d87bdf6c2c3/pkg/kubelet/util/pod_startup_latency_tracker.go#L99-L102\n\nstate.lastFinishedPulling is set inside RecordImageFinishedPulling method\n\nhttps://github.com/kubernetes/kubernetes/blob/7c78041218a4cf0b183dc0dab71f475a5db667bf/pkg/kubelet/util/pod_startup_latency_tracker.go#L140-L150\n\nContinuing troubleshooting to see if i will be able to reproduce it.\n\nHi,\n\nI was not able to reproduce the issue even when I applied a network throttling in my lab to emulate network glitches.\n\nChecking the provided logs I notice that when the issue first appeared there is a gap in provided logs of 9 seconds ( Feb 25 21:38:56 aks-userpool0-20459031-vmss000001 \u2026. Next log entry Feb 25 21:39:05 ) it would be helpful to understand why this gap exists in the first place.\n\nIf you can reproduce the issue on a local cluster and provide more datapoints would be helpful, as it is know I agree with @HirazawaUi conclusion.\n\n@alyssa1303 setting this issue to needs more information from author\n\nHi @esotsal! Sorry for the late response. I tried with k8s 1.32 and still see the same error. Can you confirm that you actually deploy 100 pods in a single node instead of spreading across multiple nodes? It's easier to reproduce when there's a burst of pods in a node. Also, can you check if you enable parallel pull image on your kubelet settings? I suspect it might be related. \n\n```\ncat messages.log | grep \"Observed pod startup duration\"\nMar 21 03:13:19 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:19.200417    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-5pd85\" podStartSLOduration=1.921952685 podStartE2EDuration=\"4.200398001s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.614289509 +0000 UTC m=+1029.953143114\" lastFinishedPulling=\"2025-03-21 03:13:17.892734825 +0000 UTC m=+1032.231588430\" observedRunningTime=\"2025-03-21 03:13:19.198266281 +0000 UTC m=+1033.537119886\" watchObservedRunningTime=\"2025-03-21 03:13:19.200398001 +0000 UTC m=+1033.539251706\"\nMar 21 03:13:19 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:19.429672    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-h8vh9\" podStartSLOduration=2.125523881 podStartE2EDuration=\"4.429652436s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.625955417 +0000 UTC m=+1029.964809022\" lastFinishedPulling=\"2025-03-21 03:13:17.930083972 +0000 UTC m=+1032.268937577\" observedRunningTime=\"2025-03-21 03:13:19.389486162 +0000 UTC m=+1033.728339767\" watchObservedRunningTime=\"2025-03-21 03:13:19.429652436 +0000 UTC m=+1033.768506141\"\nMar 21 03:13:19 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:19.991735    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-5cwcb\" podStartSLOduration=3.650732082 podStartE2EDuration=\"4.991716869s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.6013717 +0000 UTC m=+1030.940225305\" lastFinishedPulling=\"2025-03-21 03:13:17.942356487 +0000 UTC m=+1032.281210092\" observedRunningTime=\"2025-03-21 03:13:19.951776297 +0000 UTC m=+1034.290629902\" watchObservedRunningTime=\"2025-03-21 03:13:19.991716869 +0000 UTC m=+1034.330570474\"\nMar 21 03:13:20 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:20.233927    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-lg9rb\" podStartSLOduration=3.090380365 podStartE2EDuration=\"5.233907624s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.798695326 +0000 UTC m=+1030.137548931\" lastFinishedPulling=\"2025-03-21 03:13:17.942222485 +0000 UTC m=+1032.281076190\" observedRunningTime=\"2025-03-21 03:13:20.228646175 +0000 UTC m=+1034.567499780\" watchObservedRunningTime=\"2025-03-21 03:13:20.233907624 +0000 UTC m=+1034.572761229\"\nMar 21 03:13:20 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:20.351833    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-c9w59\" podStartSLOduration=3.062492705 podStartE2EDuration=\"5.351815822s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.613240499 +0000 UTC m=+1029.952094104\" lastFinishedPulling=\"2025-03-21 03:13:17.902563616 +0000 UTC m=+1032.241417221\" observedRunningTime=\"2025-03-21 03:13:20.349743403 +0000 UTC m=+1034.688597008\" watchObservedRunningTime=\"2025-03-21 03:13:20.351815822 +0000 UTC m=+1034.690669427\"\nMar 21 03:13:20 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:20.551828    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-wj5ln\" podStartSLOduration=3.187937574 podStartE2EDuration=\"5.551810085s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.525523782 +0000 UTC m=+1029.864377387\" lastFinishedPulling=\"2025-03-21 03:13:17.889396193 +0000 UTC m=+1032.228249898\" observedRunningTime=\"2025-03-21 03:13:20.550623774 +0000 UTC m=+1034.889477379\" watchObservedRunningTime=\"2025-03-21 03:13:20.551810085 +0000 UTC m=+1034.890663790\"\nMar 21 03:13:20 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:20.672216    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-59rsp\" podStartSLOduration=3.365224125 podStartE2EDuration=\"5.672200406s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.6133122 +0000 UTC m=+1029.952165805\" lastFinishedPulling=\"2025-03-21 03:13:17.920288481 +0000 UTC m=+1032.259142086\" observedRunningTime=\"2025-03-21 03:13:20.670663691 +0000 UTC m=+1035.009517296\" watchObservedRunningTime=\"2025-03-21 03:13:20.672200406 +0000 UTC m=+1035.011054111\"\nMar 21 03:13:21 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:21.271416    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-2jzht\" podStartSLOduration=3.94499224 podStartE2EDuration=\"6.271398602s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.591947101 +0000 UTC m=+1029.930800706\" lastFinishedPulling=\"2025-03-21 03:13:17.918353463 +0000 UTC m=+1032.257207068\" observedRunningTime=\"2025-03-21 03:13:21.270637095 +0000 UTC m=+1035.609490800\" watchObservedRunningTime=\"2025-03-21 03:13:21.271398602 +0000 UTC m=+1035.610252207\"\nMar 21 03:13:21 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:21.351041    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-vtttc\" podStartSLOduration=4.481177757 podStartE2EDuration=\"6.351023368s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.063474491 +0000 UTC m=+1030.402328096\" lastFinishedPulling=\"2025-03-21 03:13:17.933320002 +0000 UTC m=+1032.272173707\" observedRunningTime=\"2025-03-21 03:13:21.310642579 +0000 UTC m=+1035.649496284\" watchObservedRunningTime=\"2025-03-21 03:13:21.351023368 +0000 UTC m=+1035.689876973\"\nMar 21 03:13:21 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:21.549030    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-qcls8\" podStartSLOduration=4.155022679 podStartE2EDuration=\"6.549009371s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.514109576 +0000 UTC m=+1029.852963181\" lastFinishedPulling=\"2025-03-21 03:13:17.908096168 +0000 UTC m=+1032.246949873\" observedRunningTime=\"2025-03-21 03:13:21.510966706 +0000 UTC m=+1035.849820311\" watchObservedRunningTime=\"2025-03-21 03:13:21.549009371 +0000 UTC m=+1035.887862976\"\nMar 21 03:13:21 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:21.631825    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-8lrp8\" podStartSLOduration=4.163397583 podStartE2EDuration=\"6.631809867s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.454167918 +0000 UTC m=+1029.793021523\" lastFinishedPulling=\"2025-03-21 03:13:17.922580202 +0000 UTC m=+1032.261433807\" observedRunningTime=\"2025-03-21 03:13:21.630153152 +0000 UTC m=+1035.969006757\" watchObservedRunningTime=\"2025-03-21 03:13:21.631809867 +0000 UTC m=+1035.970663472\"\nMar 21 03:13:21 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:21.830932    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-p4qcg\" podStartSLOduration=5.107898238 podStartE2EDuration=\"6.830913782s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.198431148 +0000 UTC m=+1030.537284853\" lastFinishedPulling=\"2025-03-21 03:13:17.921446792 +0000 UTC m=+1032.260300397\" observedRunningTime=\"2025-03-21 03:13:21.830913982 +0000 UTC m=+1036.169767587\" watchObservedRunningTime=\"2025-03-21 03:13:21.830913782 +0000 UTC m=+1036.169767487\"\nMar 21 03:13:22 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:22.232722    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-c4xrt\" podStartSLOduration=4.948797979 podStartE2EDuration=\"7.232701945s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.61440641 +0000 UTC m=+1029.953260015\" lastFinishedPulling=\"2025-03-21 03:13:17.898310376 +0000 UTC m=+1032.237163981\" observedRunningTime=\"2025-03-21 03:13:22.23109443 +0000 UTC m=+1036.569948035\" watchObservedRunningTime=\"2025-03-21 03:13:22.232701945 +0000 UTC m=+1036.571555650\"\nMar 21 03:13:22 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:22.352144    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-t9dhj\" podStartSLOduration=5.899946571 podStartE2EDuration=\"7.352127693s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.490049663 +0000 UTC m=+1030.828903268\" lastFinishedPulling=\"2025-03-21 03:13:17.942230785 +0000 UTC m=+1032.281084390\" observedRunningTime=\"2025-03-21 03:13:22.310512593 +0000 UTC m=+1036.649366198\" watchObservedRunningTime=\"2025-03-21 03:13:22.352127693 +0000 UTC m=+1036.690981298\"\nMar 21 03:13:22 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:22.752442    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-tk7wh\" podStartSLOduration=5.220587967 podStartE2EDuration=\"7.752426742s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.398234397 +0000 UTC m=+1029.737088002\" lastFinishedPulling=\"2025-03-21 03:13:17.930073172 +0000 UTC m=+1032.268926777\" observedRunningTime=\"2025-03-21 03:13:22.750387422 +0000 UTC m=+1037.089241127\" watchObservedRunningTime=\"2025-03-21 03:13:22.752426742 +0000 UTC m=+1037.091280447\"\nMar 21 03:13:23 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:23.071078    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-kmq2s\" podStartSLOduration=5.786931537 podStartE2EDuration=\"8.071062206s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.630902263 +0000 UTC m=+1029.969755868\" lastFinishedPulling=\"2025-03-21 03:13:17.915032932 +0000 UTC m=+1032.253886537\" observedRunningTime=\"2025-03-21 03:13:23.032349333 +0000 UTC m=+1037.371202938\" watchObservedRunningTime=\"2025-03-21 03:13:23.071062206 +0000 UTC m=+1037.409915911\"\nMar 21 03:13:27 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:27.723580    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-6gb4w\" podStartSLOduration=-9223372024.13121 podStartE2EDuration=\"12.723565639s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.630962464 +0000 UTC m=+1029.969816069\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:27.723191335 +0000 UTC m=+1042.062044940\" watchObservedRunningTime=\"2025-03-21 03:13:27.723565639 +0000 UTC m=+1042.062419244\"\nMar 21 03:13:27 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:27.750637    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-gwnz4\" podStartSLOduration=-9223372024.104155 podStartE2EDuration=\"12.750620799s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.192125389 +0000 UTC m=+1030.530978994\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:27.748460878 +0000 UTC m=+1042.087314483\" watchObservedRunningTime=\"2025-03-21 03:13:27.750620799 +0000 UTC m=+1042.089474404\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.727286    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-sfsl7\" podStartSLOduration=-9223372023.127499 podStartE2EDuration=\"13.72727649s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.151687013 +0000 UTC m=+1030.490540618\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.727073988 +0000 UTC m=+1043.065927593\" watchObservedRunningTime=\"2025-03-21 03:13:28.72727649 +0000 UTC m=+1043.066130195\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.760675    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-qmw2r\" podStartSLOduration=-9223372023.094114 podStartE2EDuration=\"13.760661511s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.296095757 +0000 UTC m=+1030.634949362\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.744609956 +0000 UTC m=+1043.083463661\" watchObservedRunningTime=\"2025-03-21 03:13:28.760661511 +0000 UTC m=+1043.099515116\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.761517    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-kjqhs\" podStartSLOduration=-9223372023.09327 podStartE2EDuration=\"13.761505519s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.798815227 +0000 UTC m=+1030.137668832\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.760719611 +0000 UTC m=+1043.099573316\" watchObservedRunningTime=\"2025-03-21 03:13:28.761505519 +0000 UTC m=+1043.100359124\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.788157    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-5nxht\" podStartSLOduration=-9223372023.066637 podStartE2EDuration=\"13.788138275s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.309484882 +0000 UTC m=+1030.648338487\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.78561705 +0000 UTC m=+1043.124470755\" watchObservedRunningTime=\"2025-03-21 03:13:28.788138275 +0000 UTC m=+1043.126991980\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.819529    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-jbvlq\" podStartSLOduration=-9223372023.035273 podStartE2EDuration=\"13.819503176s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.0966297 +0000 UTC m=+1030.435483305\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.818049162 +0000 UTC m=+1043.156902767\" watchObservedRunningTime=\"2025-03-21 03:13:28.819503176 +0000 UTC m=+1043.158356781\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.820671    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-ks84q\" podStartSLOduration=-9223372023.034124 podStartE2EDuration=\"13.820652387s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.68422976 +0000 UTC m=+1030.023083465\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.803965127 +0000 UTC m=+1043.142818832\" watchObservedRunningTime=\"2025-03-21 03:13:28.820652387 +0000 UTC m=+1043.159505992\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.856585    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-d6bbp\" podStartSLOduration=-9223372022.998213 podStartE2EDuration=\"13.856563033s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.218141932 +0000 UTC m=+1030.556995537\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.854508213 +0000 UTC m=+1043.193361918\" watchObservedRunningTime=\"2025-03-21 03:13:28.856563033 +0000 UTC m=+1043.195416638\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.857113    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-rdr4l\" podStartSLOduration=-9223372022.997679 podStartE2EDuration=\"13.857096938s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.788733133 +0000 UTC m=+1030.127586838\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.837400748 +0000 UTC m=+1043.176254453\" watchObservedRunningTime=\"2025-03-21 03:13:28.857096938 +0000 UTC m=+1043.195950543\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.871433    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-85z7k\" podStartSLOduration=-9223372022.983366 podStartE2EDuration=\"13.871410675s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.150755404 +0000 UTC m=+1030.489609109\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.86873175 +0000 UTC m=+1043.207585355\" watchObservedRunningTime=\"2025-03-21 03:13:28.871410675 +0000 UTC m=+1043.210264280\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.898224    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-xs8sk\" podStartSLOduration=-9223372022.956581 podStartE2EDuration=\"13.898195433s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.694494656 +0000 UTC m=+1030.033348261\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.896313215 +0000 UTC m=+1043.235166920\" watchObservedRunningTime=\"2025-03-21 03:13:28.898195433 +0000 UTC m=+1043.237049138\"\nMar 21 03:13:28 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:28.914117    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-qzbg4\" podStartSLOduration=-9223372022.940683 podStartE2EDuration=\"13.914092986s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.764081004 +0000 UTC m=+1030.102934709\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:28.912589371 +0000 UTC m=+1043.251443076\" watchObservedRunningTime=\"2025-03-21 03:13:28.914092986 +0000 UTC m=+1043.252946691\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.743375    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-9v968\" podStartSLOduration=-9223372022.111412 podStartE2EDuration=\"14.743363516s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.232346464 +0000 UTC m=+1030.571200069\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.743138614 +0000 UTC m=+1044.081992219\" watchObservedRunningTime=\"2025-03-21 03:13:29.743363516 +0000 UTC m=+1044.082217221\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.777476    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-s5hjm\" podStartSLOduration=-9223372022.077312 podStartE2EDuration=\"14.777464228s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.193046798 +0000 UTC m=+1030.531900403\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.762203088 +0000 UTC m=+1044.101056693\" watchObservedRunningTime=\"2025-03-21 03:13:29.777464228 +0000 UTC m=+1044.116317833\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.791363    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-qmrw7\" podStartSLOduration=-9223372022.063433 podStartE2EDuration=\"14.791342954s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.737999772 +0000 UTC m=+1031.076853377\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.791239654 +0000 UTC m=+1044.130093259\" watchObservedRunningTime=\"2025-03-21 03:13:29.791342954 +0000 UTC m=+1044.130196559\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.809451    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-98zcf\" podStartSLOduration=-9223372022.04535 podStartE2EDuration=\"14.80942682s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.774925805 +0000 UTC m=+1030.113779410\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.806957097 +0000 UTC m=+1044.145810802\" watchObservedRunningTime=\"2025-03-21 03:13:29.80942682 +0000 UTC m=+1044.148280525\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.845589    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-pgqjn\" podStartSLOduration=-9223372022.009212 podStartE2EDuration=\"14.84556425s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.218133332 +0000 UTC m=+1030.556986937\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.826599777 +0000 UTC m=+1044.165453382\" watchObservedRunningTime=\"2025-03-21 03:13:29.84556425 +0000 UTC m=+1044.184417955\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.846234    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-lmzcq\" podStartSLOduration=-9223372022.008556 podStartE2EDuration=\"14.846219956s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.738373576 +0000 UTC m=+1031.077227181\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.844299739 +0000 UTC m=+1044.183153344\" watchObservedRunningTime=\"2025-03-21 03:13:29.846219956 +0000 UTC m=+1044.185073661\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.893156    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-ll5ws\" podStartSLOduration=-9223372021.961643 podStartE2EDuration=\"14.893132285s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.296088457 +0000 UTC m=+1030.634942062\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.877643344 +0000 UTC m=+1044.216496949\" watchObservedRunningTime=\"2025-03-21 03:13:29.893132285 +0000 UTC m=+1044.231985990\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.893887    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-pzkl4\" podStartSLOduration=-9223372021.960901 podStartE2EDuration=\"14.893874392s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.903561902 +0000 UTC m=+1030.242415507\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.892696481 +0000 UTC m=+1044.231550086\" watchObservedRunningTime=\"2025-03-21 03:13:29.893874392 +0000 UTC m=+1044.232728097\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.931560    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-t4r8t\" podStartSLOduration=-9223372021.923231 podStartE2EDuration=\"14.931545337s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.763787601 +0000 UTC m=+1030.102641206\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.931120333 +0000 UTC m=+1044.269974038\" watchObservedRunningTime=\"2025-03-21 03:13:29.931545337 +0000 UTC m=+1044.270398942\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.931915    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-blksv\" podStartSLOduration=-9223372021.922865 podStartE2EDuration=\"14.93191044s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.498044538 +0000 UTC m=+1030.836898143\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.91336167 +0000 UTC m=+1044.252215275\" watchObservedRunningTime=\"2025-03-21 03:13:29.93191044 +0000 UTC m=+1044.270764145\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.944643    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-xh8th\" podStartSLOduration=-9223372021.910145 podStartE2EDuration=\"14.944631156s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.279809706 +0000 UTC m=+1030.618663311\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.944586456 +0000 UTC m=+1044.283440161\" watchObservedRunningTime=\"2025-03-21 03:13:29.944631156 +0000 UTC m=+1044.283484761\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.968652    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-8m27p\" podStartSLOduration=-9223372021.886139 podStartE2EDuration=\"14.968636476s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.656439101 +0000 UTC m=+1029.995292706\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.968195372 +0000 UTC m=+1044.307048977\" watchObservedRunningTime=\"2025-03-21 03:13:29.968636476 +0000 UTC m=+1044.307490181\"\nMar 21 03:13:29 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:29.985168    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-t2x84\" podStartSLOduration=-9223372021.86962 podStartE2EDuration=\"14.985156427s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.222334871 +0000 UTC m=+1030.561188476\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.982606003 +0000 UTC m=+1044.321459708\" watchObservedRunningTime=\"2025-03-21 03:13:29.985156427 +0000 UTC m=+1044.324010032\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.012837    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-7c65s\" podStartSLOduration=-9223372021.841953 podStartE2EDuration=\"15.01282288s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.093637872 +0000 UTC m=+1030.432491477\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:29.996999635 +0000 UTC m=+1044.335853340\" watchObservedRunningTime=\"2025-03-21 03:13:30.01282288 +0000 UTC m=+1044.351676485\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.012948    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-zrwh7\" podStartSLOduration=-9223372021.841831 podStartE2EDuration=\"15.012943781s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.904088907 +0000 UTC m=+1030.242942512\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.011905271 +0000 UTC m=+1044.350758976\" watchObservedRunningTime=\"2025-03-21 03:13:30.012943781 +0000 UTC m=+1044.351797386\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.034890    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-dfq58\" podStartSLOduration=-9223372021.819899 podStartE2EDuration=\"15.034877981s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.193641703 +0000 UTC m=+1030.532495308\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.03467058 +0000 UTC m=+1044.373524185\" watchObservedRunningTime=\"2025-03-21 03:13:30.034877981 +0000 UTC m=+1044.373731586\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.066197    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-2g5b8\" podStartSLOduration=-9223372021.788593 podStartE2EDuration=\"15.066181968s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.698267191 +0000 UTC m=+1030.037120796\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.065916965 +0000 UTC m=+1044.404770570\" watchObservedRunningTime=\"2025-03-21 03:13:30.066181968 +0000 UTC m=+1044.405035673\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.087932    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-2h2zw\" podStartSLOduration=-9223372021.766863 podStartE2EDuration=\"15.087912466s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.295999857 +0000 UTC m=+1030.634853562\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.085933148 +0000 UTC m=+1044.424786853\" watchObservedRunningTime=\"2025-03-21 03:13:30.087912466 +0000 UTC m=+1044.426766071\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.137266    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-srw4q\" podStartSLOduration=-9223372021.717527 podStartE2EDuration=\"15.137247518s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.902810795 +0000 UTC m=+1030.241664400\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.135858905 +0000 UTC m=+1044.474712510\" watchObservedRunningTime=\"2025-03-21 03:13:30.137247518 +0000 UTC m=+1044.476101123\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.197793    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-fxv2t\" podStartSLOduration=-9223372021.657 podStartE2EDuration=\"15.197776671s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.656919906 +0000 UTC m=+1029.995773611\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.196484559 +0000 UTC m=+1044.535338264\" watchObservedRunningTime=\"2025-03-21 03:13:30.197776671 +0000 UTC m=+1044.536630276\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.281975    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-22gt8\" podStartSLOduration=-9223372021.572819 podStartE2EDuration=\"15.281956141s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.692153034 +0000 UTC m=+1030.031006639\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.266712901 +0000 UTC m=+1044.605566506\" watchObservedRunningTime=\"2025-03-21 03:13:30.281956141 +0000 UTC m=+1044.620809846\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.310947    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-xbcgj\" podStartSLOduration=-9223372021.543846 podStartE2EDuration=\"15.310929706s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.692563638 +0000 UTC m=+1030.031417343\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.309458392 +0000 UTC m=+1044.648311997\" watchObservedRunningTime=\"2025-03-21 03:13:30.310929706 +0000 UTC m=+1044.649783311\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.764309    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-cccxt\" podStartSLOduration=-9223372021.090485 podStartE2EDuration=\"15.764292052s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.726583166 +0000 UTC m=+1031.065436771\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.413234541 +0000 UTC m=+1044.752088246\" watchObservedRunningTime=\"2025-03-21 03:13:30.764292052 +0000 UTC m=+1045.103145757\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.894602    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-65s72\" podStartSLOduration=-9223372020.96019 podStartE2EDuration=\"15.894585743s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.191949788 +0000 UTC m=+1030.530803393\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.892216021 +0000 UTC m=+1045.231069626\" watchObservedRunningTime=\"2025-03-21 03:13:30.894585743 +0000 UTC m=+1045.233439348\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.912226    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-m7hr4\" podStartSLOduration=-9223372020.942568 podStartE2EDuration=\"15.912208704s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.566271673 +0000 UTC m=+1030.905125278\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.909087876 +0000 UTC m=+1045.247941481\" watchObservedRunningTime=\"2025-03-21 03:13:30.912208704 +0000 UTC m=+1045.251062309\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.942281    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-6h8jn\" podStartSLOduration=-9223372020.91251 podStartE2EDuration=\"15.942265179s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.309327181 +0000 UTC m=+1030.648180786\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.941991177 +0000 UTC m=+1045.280844882\" watchObservedRunningTime=\"2025-03-21 03:13:30.942265179 +0000 UTC m=+1045.281118784\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.956458    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-s9ww8\" podStartSLOduration=-9223372020.89833 podStartE2EDuration=\"15.956444509s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.276795078 +0000 UTC m=+1030.615648783\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.955703802 +0000 UTC m=+1045.294557507\" watchObservedRunningTime=\"2025-03-21 03:13:30.956444509 +0000 UTC m=+1045.295298214\"\nMar 21 03:13:30 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:30.971561    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-2rf6j\" podStartSLOduration=-9223372020.883226 podStartE2EDuration=\"15.971549247s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.697739286 +0000 UTC m=+1030.036592991\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.971382545 +0000 UTC m=+1045.310236250\" watchObservedRunningTime=\"2025-03-21 03:13:30.971549247 +0000 UTC m=+1045.310402952\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.006484    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-7nzwf\" podStartSLOduration=-9223372020.848307 podStartE2EDuration=\"16.006468566s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.536324494 +0000 UTC m=+1030.875178099\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:30.990319618 +0000 UTC m=+1045.329173323\" watchObservedRunningTime=\"2025-03-21 03:13:31.006468566 +0000 UTC m=+1045.345322171\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.044637    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-92db7\" podStartSLOduration=-9223372020.810158 podStartE2EDuration=\"16.044617515s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.61752435 +0000 UTC m=+1030.956377955\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.043922609 +0000 UTC m=+1045.382776214\" watchObservedRunningTime=\"2025-03-21 03:13:31.044617515 +0000 UTC m=+1045.383471220\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.060221    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-8v8z6\" podStartSLOduration=-9223372020.794569 podStartE2EDuration=\"16.060206358s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.662466657 +0000 UTC m=+1030.001320262\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.058875745 +0000 UTC m=+1045.397729350\" watchObservedRunningTime=\"2025-03-21 03:13:31.060206358 +0000 UTC m=+1045.399059963\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.074283    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-ztjl5\" podStartSLOduration=-9223372020.780506 podStartE2EDuration=\"16.074269786s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.967671799 +0000 UTC m=+1030.306525504\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.073344278 +0000 UTC m=+1045.412197883\" watchObservedRunningTime=\"2025-03-21 03:13:31.074269786 +0000 UTC m=+1045.413123391\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.106010    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-pwh6k\" podStartSLOduration=-9223372020.74878 podStartE2EDuration=\"16.105996076s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.069850351 +0000 UTC m=+1030.408703956\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.104874166 +0000 UTC m=+1045.443727771\" watchObservedRunningTime=\"2025-03-21 03:13:31.105996076 +0000 UTC m=+1045.444849681\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.135053    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-8bf59\" podStartSLOduration=-9223372020.719736 podStartE2EDuration=\"16.135039342s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.623636207 +0000 UTC m=+1030.962489812\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.119238297 +0000 UTC m=+1045.458092002\" watchObservedRunningTime=\"2025-03-21 03:13:31.135039342 +0000 UTC m=+1045.473892947\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.135812    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-295jz\" podStartSLOduration=-9223372020.718971 podStartE2EDuration=\"16.135804949s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.906392929 +0000 UTC m=+1030.245246534\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.135294244 +0000 UTC m=+1045.474147849\" watchObservedRunningTime=\"2025-03-21 03:13:31.135804949 +0000 UTC m=+1045.474658554\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.168498    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-lck4j\" podStartSLOduration=-9223372020.68629 podStartE2EDuration=\"16.168485848s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.106208589 +0000 UTC m=+1030.445062294\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.167876942 +0000 UTC m=+1045.506730547\" watchObservedRunningTime=\"2025-03-21 03:13:31.168485848 +0000 UTC m=+1045.507339453\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.185334    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-hfplg\" podStartSLOduration=-9223372020.669456 podStartE2EDuration=\"16.185318802s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.284343048 +0000 UTC m=+1030.623196753\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.183683787 +0000 UTC m=+1045.522537392\" watchObservedRunningTime=\"2025-03-21 03:13:31.185318802 +0000 UTC m=+1045.524172407\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.214894    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-d6shl\" podStartSLOduration=-9223372020.639896 podStartE2EDuration=\"16.214879072s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.797997019 +0000 UTC m=+1030.136850724\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.213978964 +0000 UTC m=+1045.552832569\" watchObservedRunningTime=\"2025-03-21 03:13:31.214879072 +0000 UTC m=+1045.553732777\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.834527    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-l8fnq\" podStartSLOduration=-9223372020.020267 podStartE2EDuration=\"16.834507738s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.097986813 +0000 UTC m=+1030.436840418\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.833314927 +0000 UTC m=+1046.172168632\" watchObservedRunningTime=\"2025-03-21 03:13:31.834507738 +0000 UTC m=+1046.173361443\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.851955    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-f5zxf\" podStartSLOduration=-9223372020.002844 podStartE2EDuration=\"16.851931597s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.312154907 +0000 UTC m=+1030.651008512\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.850246582 +0000 UTC m=+1046.189100187\" watchObservedRunningTime=\"2025-03-21 03:13:31.851931597 +0000 UTC m=+1046.190785202\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.870777    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-5sfj6\" podStartSLOduration=-9223372019.984016 podStartE2EDuration=\"16.87075927s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.69050053 +0000 UTC m=+1031.029354235\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.866207228 +0000 UTC m=+1046.205060833\" watchObservedRunningTime=\"2025-03-21 03:13:31.87075927 +0000 UTC m=+1046.209612975\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.885334    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-628qf\" podStartSLOduration=-9223372019.969467 podStartE2EDuration=\"16.885309203s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.21791963 +0000 UTC m=+1030.556773335\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.882933081 +0000 UTC m=+1046.221786686\" watchObservedRunningTime=\"2025-03-21 03:13:31.885309203 +0000 UTC m=+1046.224162808\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.936251    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-vqpjw\" podStartSLOduration=-9223372019.918541 podStartE2EDuration=\"16.936234368s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.6562813 +0000 UTC m=+1029.995134905\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.935183159 +0000 UTC m=+1046.274036864\" watchObservedRunningTime=\"2025-03-21 03:13:31.936234368 +0000 UTC m=+1046.275088073\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.959408    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-m8v8r\" podStartSLOduration=-9223372019.895384 podStartE2EDuration=\"16.95939208s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.719354899 +0000 UTC m=+1031.058208604\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.958134969 +0000 UTC m=+1046.296988574\" watchObservedRunningTime=\"2025-03-21 03:13:31.95939208 +0000 UTC m=+1046.298245685\"\nMar 21 03:13:31 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:31.989583    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-n4jmr\" podStartSLOduration=-9223372019.865208 podStartE2EDuration=\"16.989568056s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.113675759 +0000 UTC m=+1030.452529364\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:31.974169415 +0000 UTC m=+1046.313023120\" watchObservedRunningTime=\"2025-03-21 03:13:31.989568056 +0000 UTC m=+1046.328421661\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.004333    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-578vs\" podStartSLOduration=-9223372019.850456 podStartE2EDuration=\"17.004320491s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.292687226 +0000 UTC m=+1030.631540931\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.003965288 +0000 UTC m=+1046.342818893\" watchObservedRunningTime=\"2025-03-21 03:13:32.004320491 +0000 UTC m=+1046.343174096\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.039533    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-cs9sn\" podStartSLOduration=-9223372019.815256 podStartE2EDuration=\"17.039519513s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.905630722 +0000 UTC m=+1030.244484327\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.024897079 +0000 UTC m=+1046.363750684\" watchObservedRunningTime=\"2025-03-21 03:13:32.039519513 +0000 UTC m=+1046.378373118\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.040258    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-9l8bz\" podStartSLOduration=-9223372019.814526 podStartE2EDuration=\"17.040250819s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.740164492 +0000 UTC m=+1031.079018097\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.039042908 +0000 UTC m=+1046.377896513\" watchObservedRunningTime=\"2025-03-21 03:13:32.040250819 +0000 UTC m=+1046.379104424\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.095529    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-djvwz\" podStartSLOduration=-9223372019.759266 podStartE2EDuration=\"17.095510825s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.28241743 +0000 UTC m=+1030.621271035\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.07972568 +0000 UTC m=+1046.418579285\" watchObservedRunningTime=\"2025-03-21 03:13:32.095510825 +0000 UTC m=+1046.434364530\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.109946    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-r6rxc\" podStartSLOduration=-9223372019.744844 podStartE2EDuration=\"17.109931657s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.550549927 +0000 UTC m=+1030.889403532\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.109059349 +0000 UTC m=+1046.447913054\" watchObservedRunningTime=\"2025-03-21 03:13:32.109931657 +0000 UTC m=+1046.448785262\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.123331    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-v7vjt\" podStartSLOduration=-9223372019.731459 podStartE2EDuration=\"17.123317079s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.220475753 +0000 UTC m=+1030.559329358\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.12232607 +0000 UTC m=+1046.461179675\" watchObservedRunningTime=\"2025-03-21 03:13:32.123317079 +0000 UTC m=+1046.462170784\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.215148    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-s7brd\" podStartSLOduration=-9223372019.639643 podStartE2EDuration=\"17.215132519s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.10840701 +0000 UTC m=+1030.447260715\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.199573176 +0000 UTC m=+1046.538426781\" watchObservedRunningTime=\"2025-03-21 03:13:32.215132519 +0000 UTC m=+1046.553986224\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.872619    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-264f8\" podStartSLOduration=-9223372018.982172 podStartE2EDuration=\"17.872603331s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.165080537 +0000 UTC m=+1030.503934243\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.842420955 +0000 UTC m=+1047.181274660\" watchObservedRunningTime=\"2025-03-21 03:13:32.872603331 +0000 UTC m=+1047.211456936\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.890284    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-2kdnt\" podStartSLOduration=-9223372018.964504 podStartE2EDuration=\"17.890272192s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.631495381 +0000 UTC m=+1030.970349086\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.872841633 +0000 UTC m=+1047.211695338\" watchObservedRunningTime=\"2025-03-21 03:13:32.890272192 +0000 UTC m=+1047.229125897\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.892424    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-58vrp\" podStartSLOduration=-9223372018.962368 podStartE2EDuration=\"17.892408612s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.200477467 +0000 UTC m=+1030.539331172\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.890781297 +0000 UTC m=+1047.229635002\" watchObservedRunningTime=\"2025-03-21 03:13:32.892408612 +0000 UTC m=+1047.231262217\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.926429    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-hnv4p\" podStartSLOduration=-9223372018.928364 podStartE2EDuration=\"17.926412723s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.69160824 +0000 UTC m=+1031.030461845\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.925349813 +0000 UTC m=+1047.264203518\" watchObservedRunningTime=\"2025-03-21 03:13:32.926412723 +0000 UTC m=+1047.265266328\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.926677    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-r58ns\" podStartSLOduration=-9223372018.928106 podStartE2EDuration=\"17.926669125s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.566812278 +0000 UTC m=+1030.905665883\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.909863572 +0000 UTC m=+1047.248717177\" watchObservedRunningTime=\"2025-03-21 03:13:32.926669125 +0000 UTC m=+1047.265522730\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.941415    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-gfgkv\" podStartSLOduration=-9223372018.913374 podStartE2EDuration=\"17.94140136s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.976416581 +0000 UTC m=+1030.315270186\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.941179358 +0000 UTC m=+1047.280033063\" watchObservedRunningTime=\"2025-03-21 03:13:32.94140136 +0000 UTC m=+1047.280255065\"\nMar 21 03:13:32 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:32.960785    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-z8bkl\" podStartSLOduration=-9223372018.894003 podStartE2EDuration=\"17.960772137s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.309409881 +0000 UTC m=+1030.648263486\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.959369624 +0000 UTC m=+1047.298223229\" watchObservedRunningTime=\"2025-03-21 03:13:32.960772137 +0000 UTC m=+1047.299625742\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.000873    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-8ccv8\" podStartSLOduration=-9223372018.853914 podStartE2EDuration=\"18.000860804s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.738375976 +0000 UTC m=+1031.077229581\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:32.984558555 +0000 UTC m=+1047.323412260\" watchObservedRunningTime=\"2025-03-21 03:13:33.000860804 +0000 UTC m=+1047.339714409\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.002071    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-7gbtx\" podStartSLOduration=-9223372018.852713 podStartE2EDuration=\"18.002062815s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.284209447 +0000 UTC m=+1030.623063052\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.001411709 +0000 UTC m=+1047.340265314\" watchObservedRunningTime=\"2025-03-21 03:13:33.002062815 +0000 UTC m=+1047.340916520\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.021074    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-f5vfn\" podStartSLOduration=-9223372018.833714 podStartE2EDuration=\"18.021061788s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.096799602 +0000 UTC m=+1030.435653307\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.020722285 +0000 UTC m=+1047.359575890\" watchObservedRunningTime=\"2025-03-21 03:13:33.021061788 +0000 UTC m=+1047.359915493\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.041505    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-7fflq\" podStartSLOduration=-9223372018.813286 podStartE2EDuration=\"18.041490675s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.738438576 +0000 UTC m=+1031.077292181\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.040171363 +0000 UTC m=+1047.379025068\" watchObservedRunningTime=\"2025-03-21 03:13:33.041490675 +0000 UTC m=+1047.380344380\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.067260    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-4n5jw\" podStartSLOduration=-9223372018.78753 podStartE2EDuration=\"18.067245411s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.550062022 +0000 UTC m=+1030.888915727\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.064382585 +0000 UTC m=+1047.403236290\" watchObservedRunningTime=\"2025-03-21 03:13:33.067245411 +0000 UTC m=+1047.406099016\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.113932    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-4wzvx\" podStartSLOduration=-9223372018.74086 podStartE2EDuration=\"18.113915637s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.797781117 +0000 UTC m=+1030.136634822\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.095027165 +0000 UTC m=+1047.433880870\" watchObservedRunningTime=\"2025-03-21 03:13:33.113915637 +0000 UTC m=+1047.452769242\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.114175    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-q27k7\" podStartSLOduration=-9223372018.740606 podStartE2EDuration=\"18.11416994s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.727564975 +0000 UTC m=+1031.066418580\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.11310713 +0000 UTC m=+1047.451960835\" watchObservedRunningTime=\"2025-03-21 03:13:33.11416994 +0000 UTC m=+1047.453023645\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.129667    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-g567k\" podStartSLOduration=-9223372018.725124 podStartE2EDuration=\"18.129651781s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.218154732 +0000 UTC m=+1030.557008337\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.128065567 +0000 UTC m=+1047.466919272\" watchObservedRunningTime=\"2025-03-21 03:13:33.129651781 +0000 UTC m=+1047.468505386\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.147916    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-t94cg\" podStartSLOduration=-9223372018.706875 podStartE2EDuration=\"18.147900148s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:15.798969528 +0000 UTC m=+1030.137823133\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.147198442 +0000 UTC m=+1047.486052047\" watchObservedRunningTime=\"2025-03-21 03:13:33.147900148 +0000 UTC m=+1047.486753753\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.170297    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-dqm59\" podStartSLOduration=-9223372018.684494 podStartE2EDuration=\"18.170282453s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.096743801 +0000 UTC m=+1030.435597406\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.16998595 +0000 UTC m=+1047.508839655\" watchObservedRunningTime=\"2025-03-21 03:13:33.170282453 +0000 UTC m=+1047.509136158\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.221446    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-zzmbs\" podStartSLOduration=-9223372018.633348 podStartE2EDuration=\"18.221428021s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.193080298 +0000 UTC m=+1030.531933903\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.197713204 +0000 UTC m=+1047.536566909\" watchObservedRunningTime=\"2025-03-21 03:13:33.221428021 +0000 UTC m=+1047.560281626\"\nMar 21 03:13:33 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:13:33.222630    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-5d695b7459-l4wcb\" podStartSLOduration=-9223372018.632154 podStartE2EDuration=\"18.222620532s\" podCreationTimestamp=\"2025-03-21 03:13:15 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:13:16.098106114 +0000 UTC m=+1030.436959719\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\" observedRunningTime=\"2025-03-21 03:13:33.220730714 +0000 UTC m=+1047.559584319\" watchObservedRunningTime=\"2025-03-21 03:13:33.222620532 +0000 UTC m=+1047.561474237\"\nMar 21 03:14:05 aks-userpool0-13101832-vmss000000 kubelet[3277]: I0321 03:14:05.976677    3277 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/nsenter-qp4ayb\" podStartSLOduration=0.679895828 podStartE2EDuration=\"2.976662035s\" podCreationTimestamp=\"2025-03-21 03:14:03 +0000 UTC\" firstStartedPulling=\"2025-03-21 03:14:03.528909062 +0000 UTC m=+1077.867762667\" lastFinishedPulling=\"2025-03-21 03:14:05.825675269 +0000 UTC m=+1080.164528874\" observedRunningTime=\"2025-03-21 03:14:05.974919418 +0000 UTC m=+1080.313773023\" watchObservedRunningTime=\"2025-03-21 03:14:05.976662035 +0000 UTC m=+1080.315515740\"\n```\n\nFull log is here: \n\n[kubelet.log](https://github.com/user-attachments/files/19380521/kubelet.log)\n\nThanks @alyssa1303 for providing additional information. Continuing investigation.\n\nReproduced issue starting a local up cluster with parallel image pulls enabled ( `serializeImagePulls` field set to false ) .\n\nI0321 14:29:35.653481  364481 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"default/resource-consume-8665c9fdf7-wmxz8\" **podStartSLOduration=-9223372020.201311** podStartE2EDuration=\"16.653464473s\" podCreationTimestamp=\"2025-03-21 14:29:19 +0100 CET\" **firstStartedPulling=\"2025-03-21 14:29:20.051045925 +0100 CET m=+45.327360658\" lastFinishedPulling=\"0001-01-01 00:00:00 +0000 UTC\"** observedRunningTime=\"2025-03-21 14:29:35.651967605 +0100 CET m=+60.928282339\" watchObservedRunningTime=\"2025-03-21 14:29:35.653464473 +0100 CET m=+60.929779200\"\n\nWith parallel image pulls disabled ( `serializeImagePulls` field set to true ) , didn't manage to reproduce issue. \n\n/retitle pod_startup_latency_tracker : podStartSLOduration returns negative value with serializeImagePulls set to false\n\nContinuing investigation to find root cause"
  },
  {
    "id": "gen_nat_061",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: Bug: `React.use` inside `React.lazy`-ed component returns other `React.use` value on SSR\n\nA user reported the following issue titled 'Bug: `React.use` inside `React.lazy`-ed component returns other `React.use` value on SSR' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version: 19.1.0, 19.2.0-canary-dffacc7b-20250717\n\n## Steps To Reproduce\n\nRun a following code:\n\n```js\n// [Component Tree]\n// \n// Component1 -> use(promise1)\n//   Component2Lazy\n//     COmponent2 -> use(promise2)\n\nimport React from \"react\";\nimport ReactDOMServer from \"react-dom/server.edge\";\n\nconst promise1 = Promise.resolve(\"value1\");\nconst promise2 = Promise.resolve(\"value2\");\n\nfunction Component1() {\n  const data = React.use(promise1);\n  console.log(\"[Component1] React.use(promise1) =\", data);\n  return React.createElement(\n    \"div\",\n    null,\n    `Component1: ${data}`,\n    React.createElement(Component2Lazy),\n  );\n}\n\nfunction Component2() {\n  const data = React.use(promise2);\n  console.log(\"[Component2] React.use(promise2) =\", data);\n  return React.createElement(\"div\", null, `Component2: ${data}`);\n}\n\nconst Component2Lazy = React.lazy(async () => ({ default: Component2 }));\n\nfunction App() {\n  return React.createElement(\"div\", null, React.createElement(Component1));\n}\n\nasync function main() {\n  console.log(\"react\", React.version);\n  console.log(\"react-dom\", ReactDOMServer.version);\n  try {\n    const stream = await ReactDOMServer.renderToReadableStream(React.createElement(App));\n    let html = \"\";\n    await stream.pipeThrough(new TextDecoderStream()).pipeTo(\n      new WritableStream({\n        write(chunk) {\n          html += chunk;\n        },\n      }),\n    );\n    console.log(\"HTML output:\", html);\n  } catch (error) {\n    console.error(\"Error:\", error);\n  }\n}\n\nmain();\n```\n\n\nLink to code example:\n- https://github.com/hi-ogawa/reproductions/tree/main/waku-1496-react-use-mixed-up\n- https://stackblitz.com/github/hi-ogawa/reproductions/tree/main/waku-1496-react-use-mixed-up?file=index.js (same code on stackblitz)\n\n\n## The current behavior\n\nI also checked 19.2.0-canary-dffacc7b-20250717 showing the same behavior.\n\n```sh\n$ node index.js\nreact 19.1.0\nreact-dom 19.1.0\n[Component1] React.use(promise1) = value1\n[Component2] React.use(promise2) = value1    \ud83d\udc48\ud83d\udc48\ud83d\udc48 `value2` is expected\nHTML output: <div><div>Component1: value1<div>Component2: value1</div></div></div>\n```\n\n## The expected behavior\n\n```sh\n$ node index.js\nreact 19.1.0\nreact-dom 19.1.0\n[Component1] React.use(promise1) = value1\n[Component2] React.use(promise2) = value2 \nHTML output: <div><div>Component1: value1<div>Component2: value2</div></div></div>\n```\n\n## (context)\n\nWe found a issue where multiple `React.use` calls in different components are mixed up on Waku https://github.com/wakujs/waku/issues/1496. I haven't fully tied two issues, but I suspect client reference becoming implicitly being lazy component during ssr is causing a similar behavior.\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'himself65' said:\n---\nJust found the similar issue:\n\nhttps://github.com/facebook/react/issues/27731\n---\n\nUser 'hi-ogawa' said:\n---\nHere are two workaround I'm exploring on user land. As in the reproduction, the component tree of a broken case looks like:\n\n```\n\u274c Comp with use -> lazy -> Comp with use \n```\n\nBy replacing top level `use` (at least the ones used inside framework) with legacy \"throw thenable\" use implementation, it avoids breaking `use` usage in end-user's component. cf. https://github.com/wakujs/waku/pull/1542\n\n```\n\u2705 Comp with legacy-use -> lazy -> Comp with use\n   ^^^^^^^^^^^^^^^^^^^^\nframework's internal component\n                                   ^^^^^^^^^^^^^^\n                                   framework end-user's component\n```\n\nAnother approach is to somehow wrap each \"framework end-user's component\" with additional functional component since it seems to avoid React SSR thenable state bug by nesting function component at least twice. Depending on framework's convention, this might be possible.  cf. https://github.com/wakujs/waku/pull/1545\n\n```\n\u2705 Comp with use -> Comp -> lazy -> Comp with use\n\u2705 Comp with use -> lazy -> Comp -> Comp with use\n```\n---\n\nUser 'sairamarava' said:\n---\nIs the issue closed? I want to work on this one. Could anyone please tell?\n---\n\nUser 'himself65' said:\n---\n> Is the issue closed? I want to work on this one. Could anyone please tell?\n\nthere is already the PR, see https://github.com/facebook/react/pull/33941\n---\n\nUser 'sairamarava' said:\n---\n> > Is the issue closed? I want to work on this one. Could anyone please tell?\n> \n> there is already the PR, see [#33941](https://github.com/facebook/react/pull/33941)\n\nYeah sorry about that.\n---",
    "question": "I'm getting an error: `React.use` inside `React.lazy`-ed component returns other `React.use` value on SSR. Is there a known workaround?",
    "ideal_answer": "Just found the similar issue:\n\nhttps://github.com/facebook/react/issues/27731\n\nHere are two workaround I'm exploring on user land. As in the reproduction, the component tree of a broken case looks like:\n\n```\n\u274c Comp with use -> lazy -> Comp with use \n```\n\nBy replacing top level `use` (at least the ones used inside framework) with legacy \"throw thenable\" use implementation, it avoids breaking `use` usage in end-user's component. cf. https://github.com/wakujs/waku/pull/1542\n\n```\n\u2705 Comp with legacy-use -> lazy -> Comp with use\n   ^^^^^^^^^^^^^^^^^^^^\nframework's internal component\n                                   ^^^^^^^^^^^^^^\n                                   framework end-user's component\n```\n\nAnother approach is to somehow wrap each \"framework end-user's component\" with additional functional component since it seems to avoid React SSR thenable state bug by nesting function component at least twice. Depending on framework's convention, this might be possible.  cf. https://github.com/wakujs/waku/pull/1545\n\n```\n\u2705 Comp with use -> Comp -> lazy -> Comp with use\n\u2705 Comp with use -> lazy -> Comp -> Comp with use\n```\n\nIs the issue closed? I want to work on this one. Could anyone please tell?\n\n> Is the issue closed? I want to work on this one. Could anyone please tell?\n\nthere is already the PR, see https://github.com/facebook/react/pull/33941\n\n> > Is the issue closed? I want to work on this one. Could anyone please tell?\n> \n> there is already the PR, see [#33941](https://github.com/facebook/react/pull/33941)\n\nYeah sorry about that."
  },
  {
    "id": "gen_nat_062",
    "category": "troubleshooting",
    "source_file": "microsoft_vscode_issue.json",
    "context": "Repository: microsoft/vscode\nTitle: VS Code OAuth2 Flow Violates RFC 8707: Missing Resource Parameter in Token Exchange Request\n\nA user reported the following issue titled 'VS Code OAuth2 Flow Violates RFC 8707: Missing Resource Parameter in Token Exchange Request' in the 'microsoft/vscode' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! bug_report_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- \ud83d\udd6e Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- \ud83d\udd0e Search existing issues to avoid creating duplicates. -->\n<!-- \ud83e\uddea Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- \ud83d\udca1 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- \ud83d\udd27 Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: Yes/No\n\n<!-- \ud83e\ude93 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- \ud83d\udce3 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version:  Version: 1.104.0-insider (Universal)\n- OS Version:  15.6 (24G84) MAC\n\n**Problem**\nVS Code's external authorization code request includes a resource indicator which allows AuthZ server to construct the token narrowly targeted for the protected resource (MCP server). However, VSCode doesn't include the same resource indicator into subsequent token request (exchange code for tokens) as mandated by rfc8707 (https://datatracker.ietf.org/doc/html/rfc8707#token-endpoint-example-ac). When resource indicator is present on the authorize request but is missing from the token request, some implementations of IDPs will fail to create an audience claim on the resulting token resulting in token validation failures on the protected resource (MCP server).\n\n**Root Cause**\nMissing check for the resource indicator leading to a missing resource indicator parameter in the exchange code for token request.\n\n**Impact**\nToken validation failures on the protected resource due to missing audience.\n\n**Code Reference**\nThis code perhaps may be the cause\nhttps://github.com/microsoft/vscode/blob/0e00a15ef0c59d39aff227c63fb3c55b1b25f9ac/src/vs/workbench/api/common/extHostAuthentication.ts#L599\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'bluedog13' said:\n---\nThis missing \"resource\" parameter violates the MCP Authorization specification requirements. \n\nAccording to the https://modelcontextprotocol.io/specification/draft/basic/authorization#access-token-privilege-restriction, \nMCP clients MUST implement and use   the resource parameter as defined in https://www.rfc-editor.org/rfc/rfc8707.html to explicitly specify the target resource for which the token is being requested.\n---\n\nUser 'bluedog13' said:\n---\nI have submitted a PR for the same\nhttps://github.com/microsoft/vscode/pull/261815\n---",
    "question": "Could you explain how to resolve 'VS Code OAuth2 Flow Violates RFC 8707: Missing Resource Parameter in Token Exchange Request'?",
    "ideal_answer": "This missing \"resource\" parameter violates the MCP Authorization specification requirements. \n\nAccording to the https://modelcontextprotocol.io/specification/draft/basic/authorization#access-token-privilege-restriction, \nMCP clients MUST implement and use   the resource parameter as defined in https://www.rfc-editor.org/rfc/rfc8707.html to explicitly specify the target resource for which the token is being requested.\n\nI have submitted a PR for the same\nhttps://github.com/microsoft/vscode/pull/261815"
  },
  {
    "id": "gen_nat_063",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: API Server returns panic message to caller when Creating CronJob with Some Invalid Schedule Format\n\nA user reported the following issue titled 'API Server returns panic message to caller when Creating CronJob with Some Invalid Schedule Format' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nAPI Server panics when creating a CronJob with an invalid schedule format like `\"TZ=0\"`. This causes a slice bounds out of range error in the robfig/cron parser.\n\n\n### What did you expect to happen?\n\nAPI Server should return a proper validation error instead of panic.\n\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n\n1. Create a CronJob YAML with invalid schedule:\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: daily-backup\nspec:\n  schedule: \"TZ=0\"  # Invalid: missing space and cron expression\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: backup\n            image: alpine\n            command: [\"/bin/sh\"]\n            args: [\"-c\", \"echo 'Starting backup...' && date\"]\n          restartPolicy: OnFailure\n```\n\n2. Apply the YAML:\n```bash\nkubectl apply -f test-cron-job.yaml\n```\n\n**Actual behavior:**\nThe API server panics with:\n```\nerror: error when creating \"test-cron-job.yaml\": Post \"https://127.0.0.1:56870/apis/batch/v1/namespaces/default/cronjobs?fieldManager=kubectl-client-side-apply&fieldValidation=Strict\": stream error: stream ID 7; INTERNAL_ERROR; received from peer\n```\n\n\n**Log:**\n```\nE0928 08:07:51.393094       1 timeout.go:121] \"Observed a panic\" panic=<\n\truntime error: slice bounds out of range [:-1]\n\tgoroutine 70144 [running]:\n\tk8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest.func1.1()\n\t\tk8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:105 +0x98\n\tpanic({0x2907bc0?, 0x4009b9d110?})\n\t\truntime/panic.go:792 +0x124\n\tgithub.com/robfig/cron/v3.Parser.Parse({0x4011dd3701?}, {0x4011df4e30, 0x4})\n\t\tgithub.com/robfig/cron/v3@v3.0.1/parser.go:99 +0x5cc\n\tgithub.com/robfig/cron/v3.ParseStandard(...)\n\t\tgithub.com/robfig/cron/v3@v3.0.1/parser.go:230\n\tk8s.io/kubernetes/pkg/apis/batch/validation.validateScheduleFormat({0x4011df4e30, 0x4}, 0x0, 0x0, 0x400d0bf590)\n\t\tk8s.io/kubernetes/pkg/apis/batch/validation/validation.go:793 +0x48\n\tk8s.io/kubernetes/pkg/apis/batch/validation.validateCronJobSpec(0x400325a710, 0x0, 0x400d0bf560, {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, ...})\n\t\tk8s.io/kubernetes/pkg/apis/batch/validation/validation.go:750 +0x9ec\n\tk8s.io/kubernetes/pkg/apis/batch/validation.ValidateCronJobCreate(0x400325a608, {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x1, 0x1, ...})\n\t\tk8s.io/kubernetes/pkg/apis/batch/validation/validation.go:718 +0xc4\n\tk8s.io/kubernetes/pkg/registry/batch/cronjob.cronJobStrategy.Validate({{0xf94b7cd42c38?, 0x2b6c3f2?}, {0x4011dd3a38?, 0x13e9944?}}, {0x4011dd3a38?, 0x13e98a4?}, {0x31c1360?, 0x400325a608})\n\t\tk8s.io/kubernetes/pkg/registry/batch/cronjob/strategy.go:115 +0x74\n\tk8s.io/apiserver/pkg/registry/rest.BeforeCreate({0x31f1768, 0x4002558ee0}, {0x31e0940, 0x400d07c9f0}, {0x31c1360, 0x400325a608})\n\t\tk8s.io/apiserver/pkg/registry/rest/create.go:122 +0x188\n\tk8s.io/apiserver/pkg/registry/generic/registry.(*Store).create(0x4000f28180, {0x31e0940, 0x400d07c9f0}, {0x31c1360, 0x400325a608}, 0x4011dcb140, 0x400335c4e0)\n\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'PersistentJZH' said:\n---\n/assign\n/sig api-machinery\n---\n\nUser 'ardaguclu' said:\n---\n/sig apps\n---\n\nUser 'artschur' said:\n---\n/assign\n---\n\nUser 'artschur' said:\n---\nHI @PersistentJZH, im a new contributor and assigned myself to the issue you were already assigned without knowing it would affect you.\n\nSorry for the unassignment! Ill remove my assignement here.\n---\n\nUser 'artschur' said:\n---\n/unassign\n---\n\nUser 'PersistentJZH' said:\n---\n/assign\n---\n\nUser 'soltysh' said:\n---\nLet's see if we can get https://github.com/robfig/cron/pull/555 landed, given that there hasn't been much activity in that library for a while. If not, I'm fine addressing this problem on our side.\n---\n\nUser 'soltysh' said:\n---\n/triage accepted\n---\n\nUser 'liggitt' said:\n---\nJust for clarity, the API server doesn't *exit* on the panic, right? (it is recovered and handled for the particular request)\n---\n\nUser 'PersistentJZH' said:\n---\n> Just for clarity, the API server doesn't _exit_ on the panic, right? (it is recovered and handled for the particular request)\n\nYes, apiserver will not exit on the panic, and will be recovered through runtime.HandleCrash.\n---",
    "question": "In kubernetes/kubernetes, API Server returns panic message to caller when Creating CronJob with Some Invalid Schedule Format. Any ideas why?",
    "ideal_answer": "HI @PersistentJZH, im a new contributor and assigned myself to the issue you were already assigned without knowing it would affect you.\n\nSorry for the unassignment! Ill remove my assignement here.\n\n/unassign\n\nLet's see if we can get https://github.com/robfig/cron/pull/555 landed, given that there hasn't been much activity in that library for a while. If not, I'm fine addressing this problem on our side.\n\n/triage accepted\n\nJust for clarity, the API server doesn't *exit* on the panic, right? (it is recovered and handled for the particular request)\n\n> Just for clarity, the API server doesn't _exit_ on the panic, right? (it is recovered and handled for the particular request)\n\nYes, apiserver will not exit on the panic, and will be recovered through runtime.HandleCrash."
  },
  {
    "id": "gen_nat_064",
    "category": "troubleshooting",
    "source_file": "microsoft_vscode_issue.json",
    "context": "Repository: microsoft/vscode\nTitle: Unable to clear API key or specify BYOK model in Insiders build\n\nA user reported the following issue titled 'Unable to clear API key or specify BYOK model in Insiders build' in the 'microsoft/vscode' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n## Bug Description\nIn the latest Insiders build, I'm unable to clear out the API key for Gemini, and there is no apparent way to specify which Gemini model to use. This functionality works as expected in stable, but in Insiders, the following issues occur:\n- The API key cannot be cleared or reset. Hitting Enter with the field empty dismisses the command pallette, but appears to do nothing.\n- There is no option to select or specify a particular Gemini model.\n\n### Steps to Reproduce\n1. Open VS Code Insiders.\n2. Attempt to clear the Gemini API key by pressing enter in the empty field\n3. Try to specify a Gemini model to use for AI features.\n\n### Expected Behavior\n- The Gemini API key should be clearable/resettable in Insiders, just like in stable.\n- I should be able to add a model that doesn't exist with the \"plus\" icon.\n\n### Actual Behavior\n- The API key is stuck and cannot be cleared.\n- No + option is available.\n\n### Additional Information\nRelevant code related to secret state management and API key handling:\n\nVersion: 1.103.0-insider (user setup)\nCommit: 6977c2a503c0fefc0f9290111c4154482c63b856\nDate: 2025-08-01T03:48:00.876Z\nElectron: 37.2.3\nElectronBuildId: 12035395\nChromium: 138.0.7204.100\nNode.js: 22.17.0\nV8: 13.8.500258-electron.0\nOS: Windows_NT x64 10.0.26100\n\nCopilot Chat Version: 0.30.2025073102\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'lramos15' said:\n---\nThis should all be fixed.\n\nNote the `+` was replaced with the OpenAI Compatible model provider entry instead\n---\n\nUser 'TylerLeonhardt' said:\n---\nVerified but opened https://github.com/microsoft/vscode/issues/265048\n---",
    "question": "Has anyone seen 'Unable to clear API key or specify BYOK model in Insiders build' in microsoft/vscode? I can't figure out the cause.",
    "ideal_answer": "This should all be fixed.\n\nNote the `+` was replaced with the OpenAI Compatible model provider entry instead\n\nVerified but opened https://github.com/microsoft/vscode/issues/265048"
  },
  {
    "id": "gen_nat_065",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [DevTools Bug] Cannot remove node \"25\" because no matching node was found in the Store.\n\nA user reported the following issue titled '[DevTools Bug] Cannot remove node \"25\" because no matching node was found in the Store.' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### Website or app\n\nhttps://vinaykumardku2zrjsivsfcbc.drops.nxtwave.tech/\n\n### Repro steps\n\n.\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n6.1.2-4d6cf75921\n\n### Error message (automated)\n\nCannot remove node \"25\" because no matching node was found in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1228673\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1195122)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1196729\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1607819)\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'zhg163' said:\n---\nThank you for reporting this issue. Could you please provide more information about your React and React DevTools versions? This would help in investigating the problem.\n---\n\nUser 'skylarmb' said:\n---\nI am seeing this intermittently as well. \n\n- Firefox 138.0.4\n- React 19.1.0\n- React Developer Tools 6.0.0\n\nSeems to happen if I have react devtools open when I load / reload the page. If I open them after loading the page it seems to work?\n---\n\nUser 'janvorwerk' said:\n---\nSeems very much related to #32852  I get these errors all the time => the profiler is pretty much unusable \ud83d\ude22\n---\n\nUser 'Hardanish-Singh' said:\n---\nLooks like its related to https://github.com/facebook/react/issues/32852\n---\n\nUser 'AstralHunt' said:\n---\nThese \"Cannot Remove node ... found in Store\" issues are prevalent...\n---\n\nUser 'Ahmard' said:\n---\n> I am seeing this intermittently as well.\n> \n>     * Firefox 138.0.4\n> \n>     * React 19.1.0\n> \n>     * React Developer Tools 6.0.0\n> \n> \n> Seems to happen if I have react devtools open when I load / reload the page. If I open them after loading the page it seems to work?\n\nHaving same behavior here as well, it only happens when you open the devtool and reload the page.\n---\n\nUser 'eps1lon' said:\n---\nWe fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue.\n---",
    "question": "Has anyone seen 'Cannot remove node \"25\" because no matching node was found in the Store.' in facebook/react? I can't figure out the cause.",
    "ideal_answer": "Thank you for reporting this issue. Could you please provide more information about your React and React DevTools versions? This would help in investigating the problem.\n\nI am seeing this intermittently as well. \n\n- Firefox 138.0.4\n- React 19.1.0\n- React Developer Tools 6.0.0\n\nSeems to happen if I have react devtools open when I load / reload the page. If I open them after loading the page it seems to work?\n\nSeems very much related to #32852  I get these errors all the time => the profiler is pretty much unusable \ud83d\ude22\n\nLooks like its related to https://github.com/facebook/react/issues/32852\n\nThese \"Cannot Remove node ... found in Store\" issues are prevalent...\n\n> I am seeing this intermittently as well.\n> \n>     * Firefox 138.0.4\n> \n>     * React 19.1.0\n> \n>     * React Developer Tools 6.0.0\n> \n> \n> Seems to happen if I have react devtools open when I load / reload the page. If I open them after loading the page it seems to work?\n\nHaving same behavior here as well, it only happens when you open the devtool and reload the page.\n\nWe fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."
  },
  {
    "id": "gen_nat_066",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: Bug: react-hooks/immutability wrong detect bug\n\nA user reported the following issue titled 'Bug: react-hooks/immutability wrong detect bug' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version:\n\n## Steps To Reproduce\n\n1. const name is useTestHooks\n2. Set inside useMemo\n\n\nLink to code example:\n(https://playground.react.dev/#N4Igzg9grgTgxgUxALhAcimBACOAbASwQDsAXNAHWKoIFsAHCGU7YKi0zBAYQEM88AI15wA1gBpsXALIJaEKgF9sAMxgRa2CiBgIRpbe2JwIxMCy4AVBOYASECKLDYAvNgAUASlcA+Vu1ITMxYAE15SXlcpLFl5dy9fDzZibFTsUgQAD1JkbAADPKpUxU9JAG0AXU8jDiDzdJtSADEoYyiuPgFhMXjvFz9kjiGDUjCIgDoAN34oHBcA4eHpvFnx3Xo8EQR3BZHF0gB6MoBBAFoALwqDgHM6cV39jgKHvdJq6lfFe4-hspeOMa8F4VXbvAK6TgwFKDEbLAhjBABRRKIxZRjMbAhBAqXhQPAWLDWOwOJxUEDiEBBFQEa4oEB0dEsUgAT3oOGA2AACitbsQAPL0UgEUzOZRqDTYNDCQQIPCnDZQXmnXT6U4mBgEPAIGAHEIEcxoADcRncyVSBwO6vomvCwuI0ggWNy2n4eEMxGUYFtYGpNi5PII-MFdrAnkN5PAAAsIAB3ACSZG1xH4YBQOLwWEUQA)\n\n## The current behavior\n\n<img width=\"866\" height=\"320\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f9628aa4-ae60-4bed-8c58-c588b7b0870e\" />\n\n## The expected behavior\n\nAn ESLint error occurs when the constant name contains \u201cuse\u201d, but it works fine when \u201cuse\u201d is not included in the name.\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'josephsavona' said:\n---\nThanks for posting. The compiler is correctly flagging this as a violation of the Rules of React: hook return values are not safe to modify. Mutating the value without recreating it means anything that depends on this value won\u2019t re-render, causing bugs. \n\nDoes that make sense? Note that if you only use this value in the callback, you can create it there. Or copy it (`{\u2026data}`) and modify the copy.\n---\n\nUser 'demonguyj' said:\n---\nI understood.\nBut, \nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAcimBACOAbASwQDsAXNAHWKoIFsAHCGU7YKi0zBAYQEM88AI15wA1gBpsXALIJaEKgF9sAMxgRa2CiBgIRpbe2JwIxMC1IJzACQgRRYbAF5sACgCUzgHyt2pE2YsACa8pLzOUliy8q4e3m5sxNjJ2JYAHqTI2AAG2VTJiu6SANoAuu5GHAHmqVakAGJQxhFcfALCYrGeTj6JHP0GpCFhAHQAbvxQOE5+AwMTeFMjuvR4Igius4NzpAD0xQCCALQAXqW7AOZ04ls7HLm326QV1E+KN68DxY8cw7yPpS2Lz8uk4MCSfUGCwIwwQfkUSiMCDSjGY2CCCBUvCgeAsdVs9jAVBA4hAARUBAuKBAdFRFgAnvQcMBsAAFRZXYgAeXopAIpkcyjUGmwaGEggQeCOqygnKOun0RxMDAIeAQMF2QQI5jQAG4jK5EsldrtlfRVaF+cRpBAMVltPw8IZiMowJawBSrGyOQRubyrWB3LqSeAABYQADuAEkyOriPwwCgsXgsIogA\n\nThis case doesn't return error.\n\nThis case change name only.\n---\n\nUser 'josephsavona' said:\n---\nNote that your second example isn't a hook (per the function name). I'm gonna close since this is working as expected but feel free to continue discussion here.\n---",
    "question": "Help needed with facebook/react. react-hooks/immutability wrong detect bug.",
    "ideal_answer": "Thanks for posting. The compiler is correctly flagging this as a violation of the Rules of React: hook return values are not safe to modify. Mutating the value without recreating it means anything that depends on this value won\u2019t re-render, causing bugs. \n\nDoes that make sense? Note that if you only use this value in the callback, you can create it there. Or copy it (`{\u2026data}`) and modify the copy.\n\nI understood.\nBut, \nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAcimBACOAbASwQDsAXNAHWKoIFsAHCGU7YKi0zBAYQEM88AI15wA1gBpsXALIJaEKgF9sAMxgRa2CiBgIRpbe2JwIxMC1IJzACQgRRYbAF5sACgCUzgHyt2pE2YsACa8pLzOUliy8q4e3m5sxNjJ2JYAHqTI2AAG2VTJiu6SANoAuu5GHAHmqVakAGJQxhFcfALCYrGeTj6JHP0GpCFhAHQAbvxQOE5+AwMTeFMjuvR4Igius4NzpAD0xQCCALQAXqW7AOZ04ls7HLm326QV1E+KN68DxY8cw7yPpS2Lz8uk4MCSfUGCwIwwQfkUSiMCDSjGY2CCCBUvCgeAsdVs9jAVBA4hAARUBAuKBAdFRFgAnvQcMBsAAFRZXYgAeXopAIpkcyjUGmwaGEggQeCOqygnKOun0RxMDAIeAQMF2QQI5jQAG4jK5EsldrtlfRVaF+cRpBAMVltPw8IZiMowJawBSrGyOQRubyrWB3LqSeAABYQADuAEkyOriPwwCgsXgsIogA\n\nThis case doesn't return error.\n\nThis case change name only.\n\nNote that your second example isn't a hook (per the function name). I'm gonna close since this is working as expected but feel free to continue discussion here."
  },
  {
    "id": "gen_nat_067",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [Compiler Bug]: Compiler fails to memoize hooks with no hook calls\n\nA user reported the following issue titled '[Compiler Bug]: Compiler fails to memoize hooks with no hook calls' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [X] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://github.com/billyjanitsch/react-compiler-hook-detection-bug\n\n### Repro steps\n\nGiven the following three custom hooks:\r\n\r\n```tsx\r\nimport {useDebugValue} from 'react'\r\n\r\nfunction useFoo() {\r\n  return () => 'foo'\r\n}\r\n\r\nfunction useBar() {\r\n  useDebugValue('bar')\r\n  return () => 'bar'\r\n}\r\n\r\nfunction useBaz() {\r\n  return useCallback(() => 'baz', [])\r\n}\r\n```\r\n\r\nI'd expect the compiler to memoize all of them, but it only memoizes `useBar` and `useBaz`:\r\n\r\n```tsx\r\nimport { useCallback, useDebugValue } from \"react\";\r\nfunction useFoo() {\r\n  return () => \"foo\";\r\n}\r\nfunction useBar() {\r\n  useDebugValue(\"bar\");\r\n  return _temp;\r\n}\r\nfunction _temp() {\r\n  return \"bar\";\r\n}\r\nfunction useBaz() {\r\n  return _temp2;\r\n}\r\nfunction _temp2() {\r\n  return \"baz\";\r\n}\r\n```\r\n\r\nI'm guessing that it's because the compiler's hook detection logic looks for at least one hook call in the function body. I understand that it generally doesn't make sense to write a custom hook that doesn't use any other hooks, but the exception is when the custom hook would have used only `useMemo` and/or `useCallback`, such as `useBaz`. I expect the compiler to let me remove those hooks without losing memoization.\r\n\r\nThis doesn't reproduce in the playground. I'm not sure why.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.0.0\n\n### What version of React Compiler are you using?\n\n19.0.0-beta-37ed2a7-20241206\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'Donhv' said:\n---\ni want handle memo & callback by my self\n---\n\nUser 'mofeiZ' said:\n---\nThanks for reporting @billyjanitsch.\r\n\r\nAs you observed, currently the compiler only optimizes functions that look to contain react code (such as hook calls or JSX)\r\n<img width=\"500\" alt=\"image\" src=\"https://github.com/user-attachments/assets/1e5e35b7-e858-427c-9264-58e71b22fe93\" />\r\n\r\nWe know that this isn't a perfect heuristic --  internally, we use [Flow's component and hook syntax](https://flow.org/en/docs/react/component-syntax/) which unambiguously indicates which functions are eligible for optimization. \r\n\r\nMost React users outside of Meta don't use Flow, so this is an area that we know the compiler needs improvement on. We'd love to hear more details about your use case, could you share a bit about:\r\n- what app logic your hook contains, and why it needs to be a custom hook (instead of a non-hook function)\r\n- why memoization optimizations need to be applied to the hook\n---\n\nUser 'billyjanitsch' said:\n---\n@mofeiZ Sure! Building off of the general case that I mentioned:\r\n\r\n>I understand that it generally doesn't make sense to write a custom hook that doesn't use any other hooks, but the exception is when the custom hook would have used only useMemo and/or useCallback, such as useBaz. I expect the compiler to let me remove those hooks without losing memoization.\r\n\r\nHere are some simplified concrete examples of hooks that only use `useCallback` and/or `useMemo`:\r\n\r\n```tsx\r\nfunction useMergedRefs(refA, refB) {\r\n  return useCallback((node) => {\r\n    // (simplified implementation only for ref callbacks)\r\n    refA(node)\r\n    refB(node)\r\n  }, [refA, refB])\r\n}\r\n```\r\n\r\n```tsx\r\nfunction useProcessedData(data) {\r\n  return useMemo(() => someExpensiveTransform(data), [data])\r\n}\r\n```\r\n\r\nThe crux of the issue is that I expect the React compiler to let me remove `useCallback` and `useMemo` from these hooks because it can memoize them automatically. But when I do so, the hooks lose memoization because the compiler no longer considers them hooks without those calls.\r\n\r\nDoes that clear up why these need to be hooks rather than non-hook functions, and why memoization optimizations are required?\r\n\r\n(As a workaround, I can leave the manual `useCallback` and `useMemo` calls in place. Is this the intended long-term solution?)\r\n\r\n>We know that this isn't a perfect heuristic\r\n\r\nI assume the current heuristic is designed to avoid false positives. Maybe the compiler could loosen the heuristic to compile all `^use[A-Z]` functions if the current filename has a `.jsx`/`.tsx` extension, on the assumption that if a file has explicitly opted into React syntax, it's unlikely to include non-React code that uses React naming conventions? Still imperfect ofc.\n---\n\nUser 'mofeiZ' said:\n---\nI see, you're moving code out of your component / custom hook body, and you'd like React Compiler to apply the same granular memoization to it.\r\n\r\nThe main issue with loosening the heuristic is that false positives currently error at runtime (producing an error like reported in #31802). There are two approaches you could take here:\r\n1. Rename your custom hook-named functions to non-hook names. This will let the compiler memoize your code at the function callsite (within the function body of a hook / component). It also lets you be more flexible with how your function gets called (e.g. it can be dynamically composed, used in a loop, etc).\r\n```js\r\nfunction processData(data) {\r\n  return someExpensiveTransform(data);\r\n}\r\n// caller of processData\r\nfunction MyComponent(props) {\r\n  // React Compiler will memoize this line\r\n  const data = processData(props.data)\r\n```\r\n\r\n2. If you really need to ensure that these functions are memoized, you can annotate them with a `use memo` directive.\r\n```js\r\nfunction useProcessedData(data) {\r\n  'use memo';\r\n  return someExpensiveTransform(data);\r\n}\r\n```\r\n\r\nA major semantic difference between these two approaches is that (2) means that your function is subject to the [rules of hooks and must be pure](https://react.dev/reference/rules#components-and-hooks-must-be-pure).\n---\n\nUser 'anukaal' said:\n---\nThe code for the createFastHash function currently uses MD5. To replace MD5 with SHA-256, we can modify the function to use the sha256 algorithm instead. Here's the updated code:\r\n\r\nimport {createHash} from 'crypto';\r\n\r\n// Other imports and code...\r\n\r\nexport function createFastHash(input: string): string {\r\n  const hash = createHash('sha256');\r\n  hash.update(input);\r\n  return hash.digest('hex');\r\n}\r\n\r\n// Rest of the code...``\n---\n\nUser 'AR-Army' said:\n---\nApk\n---\n\nUser 'AR-Army' said:\n---\nUploading 2024-12-15-193920105.mp4\u2026\n---\n\nUser 'tusharsnx' said:\n---\n~@mofeiZ I think it wasn't mentioned here (or in the docs) that hooks need to be in a `.tsx` file to be memoizable?~\n\n~I'm seeing an issue where the compiler does not memoize hooks defined in `.ts` files.~\n\nnvm, [it was nextjs](https://github.com/vercel/next.js/issues/78867).\n---\n\nUser 'josephsavona' said:\n---\n@tusharsnx the compiler should be targeting .ts files as well. Can you file a separate issue with more details on your setup?\n---\n\nUser 'tusharsnx' said:\n---\n@josephsavona [It was nextjs](https://github.com/vercel/next.js/issues/78867). Resolved after downgrading to `next@15.2`. Feel free to mark this 'off-topic'.\n---\n\nUser 'josephsavona' said:\n---\nFollowing up here, applying optimizations to all functions that start with \"use\" could break code that isn't meant to be a hook. The existing heuristic is the best compromise we've found. I'm going to close for now.\n---",
    "question": "Could you explain how to resolve ': Compiler fails to memoize hooks with no hook calls'?",
    "ideal_answer": "i want handle memo & callback by my self\n\nThanks for reporting @billyjanitsch.\r\n\nAs you observed, currently the compiler only optimizes functions that look to contain react code (such as hook calls or JSX)\r\n<img width=\"500\" alt=\"image\" src=\"https://github.com/user-attachments/assets/1e5e35b7-e858-427c-9264-58e71b22fe93\" />\r\n\nWe know that this isn't a perfect heuristic --  internally, we use [Flow's component and hook syntax](https://flow.org/en/docs/react/component-syntax/) which unambiguously indicates which functions are eligible for optimization. \r\n\nMost React users outside of Meta don't use Flow, so this is an area that we know the compiler needs improvement on. We'd love to hear more details about your use case, could you share a bit about:\r\n- what app logic your hook contains, and why it needs to be a custom hook (instead of a non-hook function)\r\n- why memoization optimizations need to be applied to the hook\n\n@mofeiZ Sure! Building off of the general case that I mentioned:\r\n\n>I understand that it generally doesn't make sense to write a custom hook that doesn't use any other hooks, but the exception is when the custom hook would have used only useMemo and/or useCallback, such as useBaz. I expect the compiler to let me remove those hooks without losing memoization.\r\n\nHere are some simplified concrete examples of hooks that only use `useCallback` and/or `useMemo`:\r\n\n```tsx\r\nfunction useMergedRefs(refA, refB) {\r\n  return useCallback((node) => {\r\n    // (simplified implementation only for ref callbacks)\r\n    refA(node)\r\n    refB(node)\r\n  }, [refA, refB])\r\n}\r\n```\r\n\n```tsx\r\nfunction useProcessedData(data) {\r\n  return useMemo(() => someExpensiveTransform(data), [data])\r\n}\r\n```\r\n\nThe crux of the issue is that I expect the React compiler to let me remove `useCallback` and `useMemo` from these hooks because it can memoize them automatically. But when I do so, the hooks lose memoization because the compiler no longer considers them hooks without those calls.\r\n\nDoes that clear up why these need to be hooks rather than non-hook functions, and why memoization optimizations are required?\r\n\n(As a workaround, I can leave the manual `useCallback` and `useMemo` calls in place. Is this the intended long-term solution?)\r\n\n>We know that this isn't a perfect heuristic\r\n\nI assume the current heuristic is designed to avoid false positives. Maybe the compiler could loosen the heuristic to compile all `^use[A-Z]` functions if the current filename has a `.jsx`/`.tsx` extension, on the assumption that if a file has explicitly opted into React syntax, it's unlikely to include non-React code that uses React naming conventions? Still imperfect ofc.\n\nI see, you're moving code out of your component / custom hook body, and you'd like React Compiler to apply the same granular memoization to it.\r\n\nThe main issue with loosening the heuristic is that false positives currently error at runtime (producing an error like reported in #31802). There are two approaches you could take here:\r\n1. Rename your custom hook-named functions to non-hook names. This will let the compiler memoize your code at the function callsite (within the function body of a hook / component). It also lets you be more flexible with how your function gets called (e.g. it can be dynamically composed, used in a loop, etc).\r\n```js\r\nfunction processData(data) {\r\n  return someExpensiveTransform(data);\r\n}\r\n// caller of processData\r\nfunction MyComponent(props) {\r\n  // React Compiler will memoize this line\r\n  const data = processData(props.data)\r\n```\r\n\n2. If you really need to ensure that these functions are memoized, you can annotate them with a `use memo` directive.\r\n```js\r\nfunction useProcessedData(data) {\r\n  'use memo';\r\n  return someExpensiveTransform(data);\r\n}\r\n```\r\n\nA major semantic difference between these two approaches is that (2) means that your function is subject to the [rules of hooks and must be pure](https://react.dev/reference/rules#components-and-hooks-must-be-pure).\n\nThe code for the createFastHash function currently uses MD5. To replace MD5 with SHA-256, we can modify the function to use the sha256 algorithm instead. Here's the updated code:\r\n\nimport {createHash} from 'crypto';\r\n\n// Other imports and code...\r\n\nexport function createFastHash(input: string): string {\r\n  const hash = createHash('sha256');\r\n  hash.update(input);\r\n  return hash.digest('hex');\r\n}\r\n\n// Rest of the code...``\n\nApk\n\nUploading 2024-12-15-193920105.mp4\u2026\n\n~@mofeiZ I think it wasn't mentioned here (or in the docs) that hooks need to be in a `.tsx` file to be memoizable?~\n\n~I'm seeing an issue where the compiler does not memoize hooks defined in `.ts` files.~\n\nnvm, [it was nextjs](https://github.com/vercel/next.js/issues/78867).\n\n@tusharsnx the compiler should be targeting .ts files as well. Can you file a separate issue with more details on your setup?\n\n@josephsavona [It was nextjs](https://github.com/vercel/next.js/issues/78867). Resolved after downgrading to `next@15.2`. Feel free to mark this 'off-topic'.\n\nFollowing up here, applying optimizations to all functions that start with \"use\" could break code that isn't meant to be a hook. The existing heuristic is the best compromise we've found. I'm going to close for now."
  },
  {
    "id": "gen_nat_068",
    "category": "troubleshooting",
    "source_file": "microsoft_vscode_issue.json",
    "context": "Repository: microsoft/vscode\nTitle: CLI hangs indefinitely when updating extensions on IPv6 Network\n\nA user reported the following issue titled 'CLI hangs indefinitely when updating extensions on IPv6 Network' in the 'microsoft/vscode' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n<!-- \u26a0\ufe0f\u26a0\ufe0f Do Not Delete This! bug_report_template \u26a0\ufe0f\u26a0\ufe0f -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- \ud83d\udd6e Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- \ud83d\udd0e Search existing issues to avoid creating duplicates. -->\n<!-- \ud83e\uddea Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- \ud83d\udca1 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- \ud83d\udd27 Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: No\n\n<!-- \ud83e\ude93 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- \ud83d\udce3 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version: 1.103.2\n- OS Version: Windows 10 22H2\n\nSteps to Reproduce:\n\n1. have a working IPv6 connection\n2. run `code --update-extensions` with at least one extension out of date\n\nFrom what I can tell, on the CDN side this is the same issue as in winget (https://github.com/microsoft/winget-cli/issues/5693). Both `cdn.winget.microsoft.com` and `main.vscode-cdn.net` use the same CDN (`star-azurefd-prod.trafficmanager.net`).\n\nIn the VS Code UI, there seems to be a timeout and automatic fallback to IPv4, but the same is not happening with the CLI. It just hangs when trying to get `https://main.vscode-cdn.net/extensions/marketplace.json`.\n\nOn the CDN side, an incident should already be opened. But it would probably be good to have the same fallback logic in the CLI as exists in the UI.\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'Sanel0101' said:\n---\nso i search a parner\r\n\r\nGesendet von Outlook f\u00fcr iOS<https://aka.ms/o0ukef>\r\n________________________________\r\nVon: SteveCoding125 ***@***.***>\r\nGesendet: Saturday, August 30, 2025 9:35:13 AM\r\nAn: microsoft/vscode ***@***.***>\r\nCc: Subscribed ***@***.***>\r\nBetreff: [microsoft/vscode] CLI hangs indefinitely when updating extensions on IPv6 Network (Issue #264136)\r\n\r\n[https://avatars.githubusercontent.com/u/43781517?s=20&v=4]SteveCoding125 created an issue (microsoft/vscode#264136)<https://github.com/microsoft/vscode/issues/264136>\r\n\r\nDoes this issue occur when all extensions are disabled?: No\r\n\r\n  *   VS Code Version: 1.103.2\r\n  *   OS Version: Windows 10 22H2\r\n\r\nSteps to Reproduce:\r\n\r\n  1.  have a working IPv6 connection\r\n  2.  run code --update-extensions with at least one extension out of date\r\n\r\nFrom what I can tell, on the CDN side this is the same issue as in winget (microsoft/winget-cli#5693<https://github.com/microsoft/winget-cli/issues/5693>). Both cdn.winget.microsoft.com and main.vscode-cdn.net use the same CDN (star-azurefd-prod.trafficmanager.net).\r\n\r\nIn the VS Code UI, there seems to be a timeout and automatic fallback to IPv4, but the same is not happening with the CLI. It just hangs when trying to get https://main.vscode-cdn.net/extensions/marketplace.json.\r\n\r\nOn the CDN side, an incident should already be opened. But it would probably be good to have the same fallback logic in the CLI as exists in the UI.\r\n\r\n\u2014\r\nReply to this email directly, view it on GitHub<https://github.com/microsoft/vscode/issues/264136>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/BJWPSBO74DPG2PGULDLGOAL3QFH3DAVCNFSM6AAAAACFGKU6L6VHI2DSMVQWIX3LMV43ASLTON2WKOZTGM3DQOBVGMZDEOA>.\r\nYou are receiving this because you are subscribed to this thread.Message ID: ***@***.***>\n---\n\nUser 'sandy081' said:\n---\nCan you please run with `--verbose` option and share the logs\n---\n\nUser 'SteveCoding125' said:\n---\n[cli.log](https://github.com/user-attachments/files/22214617/cli.log)\n\nThis is what I get with IPv6 enabled and at least one update available. After this, the CLI just hangs, until I terminate it manually.\nIf no updates are available or IPv6 is disabled, the CLI behaves as expected.\n\nWhen I run updates through the UI with IPv6 enabled, everything works as expected.\n---\n\nUser 'sandy081' said:\n---\nI would like to compare the logs when running through the UI. \n\n- Start VS Code in verbose mode from CLI `code --verbose`\n- Reproduce the issue\n- Share the log - F1 > Open View... > Shared\n---\n\nUser 'SteveCoding125' said:\n---\n[Shared.log](https://github.com/user-attachments/files/22274792/Shared.log)\n\nThis is with IPv6 enabled, updating the same extension, and the update goes through.\n---\n\nUser 'sandy081' said:\n---\nI do not see network requests in this log. Have you followed my steps to get the log?\n---\n\nUser 'SteveCoding125' said:\n---\nI think so:\n- I opened vscode using the `code --verbose` command while having IPv6 enabled and one extension to update\n- I clicked on the update button on the integration\n- after the update was completed, I pressed `F1` -> `View: Open View` -> `Shared` -> three dots in the top right of the Output panel -> `Save Output As...`\nThat produced the `Shared.log`.\n\nI have now tried removing the `AppData\\Roaming\\Code\\CachedExtensionVSIXs` folder to force vscode to download these files.\n\n[Shared.log](https://github.com/user-attachments/files/22294261/Shared.log)\n---\n\nUser 'sandy081' said:\n---\n> F1 -> View: Open View -> Shared\n\nCan you see network requests in that log?\n---\n\nUser 'SteveCoding125' said:\n---\ndo you mean this?\n\n```\n2025-09-12 11:06:27.653 [trace] [Network] #1: https://vscode-sync.trafficmanager.net/v1/manifest - begin GET\n```\n---\n\nUser 'sandy081' said:\n---\nOk, I see now. It looks like you have issues accessing following url from CLI - `https://main.vscode-cdn.net/extensions/marketplace.json` - Can you please try accessing it and let me know what you get?\n---\n\nUser 'SteveCoding125' said:\n---\nWith IPv6 enable, it get `main.vscode-cdn.net took too long to respond.`\nWith  IPv4 the JSON loads correctly.\n---\n\nUser 'sandy081' said:\n---\n> With IPv6 enable, it get main.vscode-cdn.net took too long to respond.\n\nI am not a network expert. Looks like it is related to your device configuration issue. I would suggest to check in internet for the answer.\n---\n\nUser 'SteveCoding125' said:\n---\nThe same issue happens for other people on winget since it and vscode share a CDN provider (`star-azurefd-prod.trafficmanager.net`).\nIn the winget thread (https://github.com/microsoft/winget-cli/issues/5693) you can see a very detailed report for this issue.\n[In this comment](https://github.com/microsoft/winget-cli/issues/5693#issuecomment-3268066065) you can see that the CDN answers with malformed packets.\n\nThat means the issue is not just my network configuration.\n\nI was hoping, we can add something to the CLI to be able to update the extensions when something like this happens. The UI already has something in place, since updates work perfectly fine with IPv6 enabled.\n---\n\nUser 'sandy081' said:\n---\n> The UI already has something in place, since updates work perfectly fine with IPv6 enabled.\n\nActually there is a bug in UI because of which it is working. Fixing this bug makes it consistent with CLI - https://github.com/microsoft/vscode/issues/266346\n\nBut we have a timeout of 10s for network requests. I am not sure why this is not happening on CLI. \n\n@deepak1556 Can you please check and let me know why timeout is not working in CLI?\n---\n\nUser 'deepak1556' said:\n---\n@SteveCoding125 can you confirm the following,\n\n1) The output of `curl -6 https://main.vscode-cdn.net/extensions/marketplace.json -v` and `curl https://main.vscode-cdn.net/extensions/marketplace.json -v`\n2) Runtime network log for ipv4 and ipv6 using the following steps,\n   a) Start with `code --update-extensions --log-net-log=<path-to>/netlog.json`, you should replace `<path-to>` with some absolute directory path\n   b) Attach the generated network log files to this issue or you can send them to Deepak.Mohan@microsoft.com\n---\n\nUser 'SteveCoding125' said:\n---\nI have run `curl -6 ...`, `curl -4 ...` and `curl ...`:\n- [curl-6.log](https://github.com/user-attachments/files/22309814/curl-6.log)\n- [curl-4.log](https://github.com/user-attachments/files/22309816/curl-4.log)\n- [curl-default.log](https://github.com/user-attachments/files/22309818/curl-default.log)\n\nWhen I try running `code --update-extensions --log-net-log=<path-to>/netlog.json` I get no `netlog.json` for both IPv4 and IPv6. Do I need a specific version of vscode for this option?\n\nWith verbose output the CLI gets stuck at\n```\nTRACE #120: https://main.vscode-cdn.net/extensions/marketplace.json - begin GET Im { b: {}, a: [Object: null prototype] {} }\n```\n---\n\nUser 'deepak1556' said:\n---\n@sandy081 do we use Node.js network stack for the `--update-extensions` requests from cli or does it use the same chromium network stack as the extensions UI  ?\n---\n\nUser 'sandy081' said:\n---\nits CLI so I suppose it uses Node.js network stack\n---\n\nUser 'deepak1556' said:\n---\nAh that would explain the difference in results\n\n@SteveCoding125 can you try the following with node.js >= v22 and share the output\n\n```\nNODE_DEBUG=\"net\" node /Applications/Visual\\ Studio\\ Code.app/Contents/Resources/app/out/cli.js --update-extensions\n```\n---\n\nUser 'SteveCoding125' said:\n---\nI have run this with node.js v24.9.0, and I got the following output. The last part keeps repeating until I stop the process.\n\n[output.txt](https://github.com/user-attachments/files/22685976/output.txt)\n---\n\nUser 'deepak1556' said:\n---\nThanks for the log, couple of things\n\n1) the current timeout to autoselect between families is 250ms\n2) the default result order is ipv6 and then ipv4 \n\nSo I believe either of these should address the issue, can you confirm the request completes\n\n```\nNODE_DEBUG=\"net\" node --network-family-autoselection-attempt-timeout=10 /Applications/Visual\\ Studio\\ Code.app/Contents/Resources/app/out/cli.js --update-extensions\n```\n\nor\n\n```\nNODE_DEBUG=\"net\" node --dns-result-order=ipv4first /Applications/Visual\\ Studio\\ Code.app/Contents/Resources/app/out/cli.js --update-extensions\n```\n---\n\nUser 'SteveCoding125' said:\n---\nThis was a bit weird:\n\n`--dns-result-order=ipv4first` worked immediately with ipv6 is enabled: [output-fixed.txt](https://github.com/user-attachments/files/22687646/output-fixed.txt)\n\n `--network-family-autoselection-attempt-timeout=10` produced the same output as no flags. I wanted to take a look at the timing of the log output because I noticed, that I wouldn't get any more logs after about 30s (even when running for 5+ minutes). After running with this flag 5-10 times it suddenly worked with ipv6 enabled.\nI have tried running this a few more times and found this:\nNormally I get this output\n```\nNET 36764: connect/multiple: attempting to connect to 2620:1ec:29:1::67:443 (addressType: 6)\nNET 36764: connect/multiple: setting the attempt timeout to 10 ms\nNET 36764: connect/multiple: connection attempt to 2620:1ec:29:1::67:443 completed with status 0\nNET 36764: afterConnect\nNET 36764: _read - n 16384 isConnecting? false hasHandle? true\nNET 36764: Socket._handle.readStart\nNET 36764: _onTimeout\nNET 36764: destroy\nNET 36764: close\nNET 36764: close handle\nNET 36764: _onTimeout\n...\n```\nBut *sometimes* when I run with this flag I get the following:\n```\nNET 31152: connect/multiple: attempting to connect to 2620:1ec:bdf::44:443 (addressType: 6)\nNET 31152: connect/multiple: setting the attempt timeout to 10 ms\nNET 31152: connect/multiple: connection to 2620:1ec:bdf::44:443 timed out\nNET 31152: connect/multiple: attempting to connect to 13.107.246.44:443 (addressType: 4)\nNET 31152: connect/multiple: setting the attempt timeout to 10 ms\nNET 31152: connect/multiple: connection attempt to 13.107.246.44:443 completed with status 0\nNET 31152: afterConnect\nNET 31152: _read - n 16384 isConnecting? false hasHandle? true\n...\n```\nWith this output, the updates works.\n\nThis looks like, there is a check to see whether IPv6 works, this comes back as 'all good', but when trying to download the necessary files, the connection times out and won't switch to IPv4. But sometimes this first check fails, a fallback to IPv4 is done and everything works.\n---\n\nUser 'asnas-achu' said:\n---\n[]()\n---\n\nUser 'deepak1556' said:\n---\n> This looks like, there is a check to see whether IPv6 works, this comes back as 'all good', but when trying to download the necessary files, the connection times out and won't switch to IPv4.\n\nAh yes, the fallback only happens when the initial connection attempt fails. If that succeeds then a timeout down in the socket will not retry.\n\nRunning with `--dns-result-order=ipv4first` for the cli should be a good first change for this issue, we already do that for Node.js network usages in the extension host.\n---",
    "question": "I'm getting an error: CLI hangs indefinitely when updating extensions on IPv6 Network. Is there a known workaround?",
    "ideal_answer": "so i search a parner\r\n\nGesendet von Outlook f\u00fcr iOS<https://aka.ms/o0ukef>\r\n________________________________\r\nVon: SteveCoding125 ***@***.***>\r\nGesendet: Saturday, August 30, 2025 9:35:13 AM\r\nAn: microsoft/vscode ***@***.***>\r\nCc: Subscribed ***@***.***>\r\nBetreff: [microsoft/vscode] CLI hangs indefinitely when updating extensions on IPv6 Network (Issue #264136)\r\n\n[https://avatars.githubusercontent.com/u/43781517?s=20&v=4]SteveCoding125 created an issue (microsoft/vscode#264136)<https://github.com/microsoft/vscode/issues/264136>\r\n\nDoes this issue occur when all extensions are disabled?: No\r\n\n  *   VS Code Version: 1.103.2\r\n  *   OS Version: Windows 10 22H2\r\n\nSteps to Reproduce:\r\n\n  1.  have a working IPv6 connection\r\n  2.  run code --update-extensions with at least one extension out of date\r\n\nFrom what I can tell, on the CDN side this is the same issue as in winget (microsoft/winget-cli#5693<https://github.com/microsoft/winget-cli/issues/5693>). Both cdn.winget.microsoft.com and main.vscode-cdn.net use the same CDN (star-azurefd-prod.trafficmanager.net).\r\n\nIn the VS Code UI, there seems to be a timeout and automatic fallback to IPv4, but the same is not happening with the CLI. It just hangs when trying to get https://main.vscode-cdn.net/extensions/marketplace.json.\r\n\nOn the CDN side, an incident should already be opened. But it would probably be good to have the same fallback logic in the CLI as exists in the UI.\r\n\n\u2014\r\nReply to this email directly, view it on GitHub<https://github.com/microsoft/vscode/issues/264136>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/BJWPSBO74DPG2PGULDLGOAL3QFH3DAVCNFSM6AAAAACFGKU6L6VHI2DSMVQWIX3LMV43ASLTON2WKOZTGM3DQOBVGMZDEOA>.\r\nYou are receiving this because you are subscribed to this thread.Message ID: ***@***.***>\n\nCan you please run with `--verbose` option and share the logs\n\n[cli.log](https://github.com/user-attachments/files/22214617/cli.log)\n\nThis is what I get with IPv6 enabled and at least one update available. After this, the CLI just hangs, until I terminate it manually.\nIf no updates are available or IPv6 is disabled, the CLI behaves as expected.\n\nWhen I run updates through the UI with IPv6 enabled, everything works as expected.\n\nI would like to compare the logs when running through the UI. \n\n- Start VS Code in verbose mode from CLI `code --verbose`\n- Reproduce the issue\n- Share the log - F1 > Open View... > Shared\n\n[Shared.log](https://github.com/user-attachments/files/22274792/Shared.log)\n\nThis is with IPv6 enabled, updating the same extension, and the update goes through.\n\nI do not see network requests in this log. Have you followed my steps to get the log?\n\nI think so:\n- I opened vscode using the `code --verbose` command while having IPv6 enabled and one extension to update\n- I clicked on the update button on the integration\n- after the update was completed, I pressed `F1` -> `View: Open View` -> `Shared` -> three dots in the top right of the Output panel -> `Save Output As...`\nThat produced the `Shared.log`.\n\nI have now tried removing the `AppData\\Roaming\\Code\\CachedExtensionVSIXs` folder to force vscode to download these files.\n\n[Shared.log](https://github.com/user-attachments/files/22294261/Shared.log)\n\n> F1 -> View: Open View -> Shared\n\nCan you see network requests in that log?\n\ndo you mean this?\n\n```\n2025-09-12 11:06:27.653 [trace] [Network] #1: https://vscode-sync.trafficmanager.net/v1/manifest - begin GET\n```\n\nOk, I see now. It looks like you have issues accessing following url from CLI - `https://main.vscode-cdn.net/extensions/marketplace.json` - Can you please try accessing it and let me know what you get?\n\nWith IPv6 enable, it get `main.vscode-cdn.net took too long to respond.`\nWith  IPv4 the JSON loads correctly.\n\n> With IPv6 enable, it get main.vscode-cdn.net took too long to respond.\n\nI am not a network expert. Looks like it is related to your device configuration issue. I would suggest to check in internet for the answer.\n\nThe same issue happens for other people on winget since it and vscode share a CDN provider (`star-azurefd-prod.trafficmanager.net`).\nIn the winget thread (https://github.com/microsoft/winget-cli/issues/5693) you can see a very detailed report for this issue.\n[In this comment](https://github.com/microsoft/winget-cli/issues/5693#issuecomment-3268066065) you can see that the CDN answers with malformed packets.\n\nThat means the issue is not just my network configuration.\n\nI was hoping, we can add something to the CLI to be able to update the extensions when something like this happens. The UI already has something in place, since updates work perfectly fine with IPv6 enabled.\n\n> The UI already has something in place, since updates work perfectly fine with IPv6 enabled.\n\nActually there is a bug in UI because of which it is working. Fixing this bug makes it consistent with CLI - https://github.com/microsoft/vscode/issues/266346\n\nBut we have a timeout of 10s for network requests. I am not sure why this is not happening on CLI. \n\n@deepak1556 Can you please check and let me know why timeout is not working in CLI?\n\n@SteveCoding125 can you confirm the following,\n\n1) The output of `curl -6 https://main.vscode-cdn.net/extensions/marketplace.json -v` and `curl https://main.vscode-cdn.net/extensions/marketplace.json -v`\n2) Runtime network log for ipv4 and ipv6 using the following steps,\n   a) Start with `code --update-extensions --log-net-log=<path-to>/netlog.json`, you should replace `<path-to>` with some absolute directory path\n   b) Attach the generated network log files to this issue or you can send them to Deepak.Mohan@microsoft.com\n\nI have run `curl -6 ...`, `curl -4 ...` and `curl ...`:\n- [curl-6.log](https://github.com/user-attachments/files/22309814/curl-6.log)\n- [curl-4.log](https://github.com/user-attachments/files/22309816/curl-4.log)\n- [curl-default.log](https://github.com/user-attachments/files/22309818/curl-default.log)\n\nWhen I try running `code --update-extensions --log-net-log=<path-to>/netlog.json` I get no `netlog.json` for both IPv4 and IPv6. Do I need a specific version of vscode for this option?\n\nWith verbose output the CLI gets stuck at\n```\nTRACE #120: https://main.vscode-cdn.net/extensions/marketplace.json - begin GET Im { b: {}, a: [Object: null prototype] {} }\n```\n\n@sandy081 do we use Node.js network stack for the `--update-extensions` requests from cli or does it use the same chromium network stack as the extensions UI  ?\n\nits CLI so I suppose it uses Node.js network stack\n\nAh that would explain the difference in results\n\n@SteveCoding125 can you try the following with node.js >= v22 and share the output\n\n```\nNODE_DEBUG=\"net\" node /Applications/Visual\\ Studio\\ Code.app/Contents/Resources/app/out/cli.js --update-extensions\n```\n\nI have run this with node.js v24.9.0, and I got the following output. The last part keeps repeating until I stop the process.\n\n[output.txt](https://github.com/user-attachments/files/22685976/output.txt)\n\nThanks for the log, couple of things\n\n1) the current timeout to autoselect between families is 250ms\n2) the default result order is ipv6 and then ipv4 \n\nSo I believe either of these should address the issue, can you confirm the request completes\n\n```\nNODE_DEBUG=\"net\" node --network-family-autoselection-attempt-timeout=10 /Applications/Visual\\ Studio\\ Code.app/Contents/Resources/app/out/cli.js --update-extensions\n```\n\nor\n\n```\nNODE_DEBUG=\"net\" node --dns-result-order=ipv4first /Applications/Visual\\ Studio\\ Code.app/Contents/Resources/app/out/cli.js --update-extensions\n```\n\nThis was a bit weird:\n\n`--dns-result-order=ipv4first` worked immediately with ipv6 is enabled: [output-fixed.txt](https://github.com/user-attachments/files/22687646/output-fixed.txt)\n\n `--network-family-autoselection-attempt-timeout=10` produced the same output as no flags. I wanted to take a look at the timing of the log output because I noticed, that I wouldn't get any more logs after about 30s (even when running for 5+ minutes). After running with this flag 5-10 times it suddenly worked with ipv6 enabled.\nI have tried running this a few more times and found this:\nNormally I get this output\n```\nNET 36764: connect/multiple: attempting to connect to 2620:1ec:29:1::67:443 (addressType: 6)\nNET 36764: connect/multiple: setting the attempt timeout to 10 ms\nNET 36764: connect/multiple: connection attempt to 2620:1ec:29:1::67:443 completed with status 0\nNET 36764: afterConnect\nNET 36764: _read - n 16384 isConnecting? false hasHandle? true\nNET 36764: Socket._handle.readStart\nNET 36764: _onTimeout\nNET 36764: destroy\nNET 36764: close\nNET 36764: close handle\nNET 36764: _onTimeout\n...\n```\nBut *sometimes* when I run with this flag I get the following:\n```\nNET 31152: connect/multiple: attempting to connect to 2620:1ec:bdf::44:443 (addressType: 6)\nNET 31152: connect/multiple: setting the attempt timeout to 10 ms\nNET 31152: connect/multiple: connection to 2620:1ec:bdf::44:443 timed out\nNET 31152: connect/multiple: attempting to connect to 13.107.246.44:443 (addressType: 4)\nNET 31152: connect/multiple: setting the attempt timeout to 10 ms\nNET 31152: connect/multiple: connection attempt to 13.107.246.44:443 completed with status 0\nNET 31152: afterConnect\nNET 31152: _read - n 16384 isConnecting? false hasHandle? true\n...\n```\nWith this output, the updates works.\n\nThis looks like, there is a check to see whether IPv6 works, this comes back as 'all good', but when trying to download the necessary files, the connection times out and won't switch to IPv4. But sometimes this first check fails, a fallback to IPv4 is done and everything works.\n\n[]()\n\n> This looks like, there is a check to see whether IPv6 works, this comes back as 'all good', but when trying to download the necessary files, the connection times out and won't switch to IPv4.\n\nAh yes, the fallback only happens when the initial connection attempt fails. If that succeeds then a timeout down in the socket will not retry.\n\nRunning with `--dns-result-order=ipv4first` for the cli should be a good first change for this issue, we already do that for Node.js network usages in the extension host."
  },
  {
    "id": "gen_nat_069",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: CoreDNS HNS Endpoint IP is not updated on pod restart\n\nA user reported the following issue titled 'CoreDNS HNS Endpoint IP is not updated on pod restart' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nWhen CoreDNS pod on Linux node restarts and receives a new cluster IP, endpoints/endpointslices in Kubertenes are updated with new pod IP, but on Windows worker HNS Endpoint still holds the old IP address. Kubernetes restart on Windows doesn't help, need to delete all stuff and re-join node to cluster again.\n\n### What did you expect to happen?\n\n HNS Endpoint would be updated with new IP address after CoreDNS pod restart\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n\n1.    Deploy a cluster with 3 Linux master nodes.\n2.    Join 1 Windows worker node.\n3.    Start sample workload on Windows node (used mcr.microsoft.com/dotnet/framework/samples:aspnetapp)\n4.    Exec into Windows pod with cmd, type nslookup, observe working output\n```\nDefault Server:  kube-dns.kube-system.svc.cluster.local\nAddress:  10.43.0.10\n```\n5.    Exit Windows pod, get pods list from kube-system namespace with -o wide, note CoreDNS pod ip.\n6.    Delete CoreDNS pod, wait for new to start, check its IP address.\n7.    Exec again into Windows pod, type nslookup, observe timeout errors\n```\nDNS request timed out.\n    timeout was 2 seconds.\nDefault Server:  UnKnown\nAddress:  10.43.0.10\n```\n8.    Type server <new_coredns_pod_ip>, and after timeout try to resolve any dns name (for example c.cc), you would see that connectivity is working.\n9.    RDP into Windows server, start Powershell, run\n```\nhnsdiag list all | findstr 10.43.0.10\n```\nyou will see the output like this\n```\nce87ca71-adfd-4334-a5ce-760eae297bef |  10.43.0.10      | 6f995c07-0b4a-4cfb-ba6e-c62dd48a554d\n302522f6-f540-4120-881c-b7a2eeecf0f6 |  10.43.0.10      | 6f995c07-0b4a-4cfb-ba6e-c62dd48a554d\nddd666f5-c1e9-4fe1-af6b-da97be5cdb29 |  10.43.0.10      | 6f995c07-0b4a-4cfb-ba6e-c62dd48a554d\n```\n10.    Run\n```\nget-hnsendpoint 6f995c07-0b4a-4cfb-ba6e-c62dd48a554d\n```\nand in output IPAddress would be old CoreDNS pod IP address, not the new one.\n\n### Anything else we need to know?\n\nThe deployment is done with k3s all in one binary, which could make some influence here, but the issue looks like in Kubernetes itself.\nThe one thing I didn't test is what would happen if I would update HNS Endpoint with the new pod IP, but unfortunately I didn't find a way to perform such update. Any ideas? :)\n\n### Kubernetes version\n\n1.28 - 1.32\n\n### Cloud provider\n\nk3s baremetal deployment\n\n\n### OS version\n\n<details>\n```\nPRETTY_NAME=\"Ubuntu 22.04.5 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"22.04\"\nVERSION=\"22.04.5 LTS (Jammy Jellyfish)\"\nVERSION_CODENAME=jammy\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=jammy\n```\n$ uname -a\nLinux test01 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\nBuildNu\n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'k8s-ci-robot' said:\n---\nThere are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'k8s-ci-robot' said:\n---\nThis issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'ValeriiVozniuk' said:\n---\nhttps://github.com/kubernetes-sigs/sig-windows-tools/issues/380 - Initial report on sig-windows-tools\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-triage-robot' said:\n---\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n---\n\nUser 'k8s-ci-robot' said:\n---\n@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/131387#issuecomment-3302266939):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---",
    "question": "I'm seeing 'CoreDNS HNS Endpoint IP is not updated on pod restart'. Is this a known bug in kubernetes/kubernetes?",
    "ideal_answer": "There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\nhttps://github.com/kubernetes-sigs/sig-windows-tools/issues/380 - Initial report on sig-windows-tools\n\nThe Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\nThe Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n@k8s-triage-robot: Closing this issue, marking it as \"Not Planned\"."
  },
  {
    "id": "gen_nat_070",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: Constant kube-system pod restard due to SandboxChanged under ubuntu Jammy\n\nA user reported the following issue titled 'Constant kube-system pod restard due to SandboxChanged under ubuntu Jammy' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What happened?\n\nWhen installing a master node under Ubuntu jammy, the base kube-system pods (etcd, kube-apiserver, kube-proxy...) restarts every few minutes.\r\nThe etcd pod restarts because of a `SandboxChanged`\r\n```\r\n  Normal   SandboxChanged  57s                    kubelet  Pod sandbox changed, it will be killed and re-created.\r\n```\r\nThe other pods restarts because of the `SandboxChanged` event and fail because of unreachable etcd / api server (as a consequence)\r\n\r\nI haven't found the origin of the `SandboxChanged` event\n\n### What did you expect to happen?\n\nThe kube-system pods shouldn't restart constently because of the `SandboxChanged` under ubuntu Jammy\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n* Setup an ubuntu Jammy VM with 2 network interfaces\r\n* Setup the kuberntes repository\r\n* Install containerd kubeadm & kubelet\r\n* modprobe br_netfilter, enable routing & iptable for bridge\r\n* run kubeadm init\r\n\r\n```\r\napt-get --yes install containerd kubelet kubectl kubeadm\r\nmodprobe br_netfilter\r\nsysctl -w net.ipv4.ip_forward=1\r\nsysctl -w net.bridge.bridge-nf-call-ip6tables=1\r\nsysctl -w net.bridge.bridge-nf-call-iptables=1\r\nkubeadm config images pull\r\nkubeadm init --cri-socket unix:///run/containerd/containerd.sock --apiserver-advertise-address 10.88.3.60 --pod-network-cidr=10.88.3.60/16\r\n```\r\n\r\nResult:\r\n<details>\r\n```\r\nroot@master0:~# crictl -r unix:///run/containerd/containerd.sock ps -a\r\nCONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID\r\n0426a01c0d933       aebe758cef4cd       17 seconds ago       Running             etcd                      12                  3081199357710\r\na46307ecd7db9       529072250ccc6       39 seconds ago       Exited              kube-apiserver            11                  bd9d95f33a01c\r\n75facd3e2c943       77b49675beae1       47 seconds ago       Running             kube-proxy                13                  87642cfed70e1\r\n508f16a359c66       e3ed7dee73e93       About a minute ago   Exited              kube-scheduler            15                  80d6114fb09b4\r\nfefd165b18546       88784fb4ac2f6       3 minutes ago        Running             kube-controller-manager   12                  ebaca3ae71ac8\r\n679b2f042eab1       aebe758cef4cd       6 minutes ago        Exited              etcd                      11                  da070257e5ea7\r\n4e6452b46dce2       77b49675beae1       7 minutes ago        Exited              kube-proxy                12                  417f51b4372b9\r\n6dde3dee7c82d       88784fb4ac2f6       8 minutes ago        Exited              kube-controller-manager   11                  ca85f97a018ed\r\n```\r\n\r\n```\r\nroot@master0:~# kubectl describe pod -n kube-system etcd-master0\r\nName:                 etcd-master0\r\nNamespace:            kube-system\r\nPriority:             2000001000\r\nPriority Class Name:  system-node-critical\r\nNode:                 master0/10.88.3.60\r\nStart Time:       \n... (truncated)\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'k8s-ci-robot' said:\n---\n@Congelli501: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'pacoxu' said:\n---\n/kind support\r\nWould you provide more kubelet logs to convince us that it is a Kubernetes bug or your node/configuration problem?\n---\n\nUser 'Congelli501' said:\n---\nIt seems all the logs are caused by the flaky apiserver that constantly restart and the fact that no CNI is initialized yet.\r\n\r\nFrom what I got from `kubelet describe`, the pod restart seems to be linked to the `SandboxChanged` event, which is fired on a `SandboxID` change on the container.\r\nhttps://github.com/kubernetes/kubernetes/blob/520b991347b9c77635c0e4555f1703a86dcdd4ff/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L721\r\nI can't find how sandboxID is built nor why it would change\r\n\r\nHere is the first 300 lines of the kubelet log after a reboot:\r\n<details>\r\n\r\n```\r\nroot@master0:~# journalctl -u kubelet.service -b | head -n300\r\nMay 24 18:19:46 master0 systemd[1]: Started kubelet: The Kubernetes Node Agent.\r\nMay 24 18:19:45 master0 kubelet[518]: Flag --container-runtime has been deprecated, will be removed in 1.27 as the only valid value is 'remote'\r\nMay 24 18:19:45 master0 kubelet[518]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.707378     518 server.go:193] \"--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime\"\r\nMay 24 18:19:45 master0 kubelet[518]: Flag --container-runtime has been deprecated, will be removed in 1.27 as the only valid value is 'remote'\r\nMay 24 18:19:45 master0 kubelet[518]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.737035     518 server.go:399] \"Kubelet version\" kubeletVersion=\"v1.24.0\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.737070     518 server.go:401] \"Golang settings\" GOGC=\"\" GOMAXPROCS=\"\" GOTRACEBACK=\"\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.737986     518 server.go:813] \"Client rotation is on, will bootstrap in background\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.740669     518 certificate_store.go:130] Loading cert/key pair from \"/var/lib/kubelet/pki/kubelet-client-current.pem\".\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.746970     518 dynamic_cafile_content.go:157] \"Starting controller\" name=\"client-ca-bundle::/etc/kubernetes/pki/ca.crt\"\r\nMay 24 18:19:45 master0 kubelet[518]: W0524 18:19:45.749027     518 manager.go:159] Cannot detect current cgroup on cgroup v2\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.754857     518 server.go:648] \"--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.755491     518 container_manager_linux.go:262] \"Container manager verified user specified cgroup-root exists\" cgroupRoot=[]\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.755547     518 container_manager_linux.go:267] \"Creating Container Manager object based on Node Config\" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>} {Signal:imagefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.15} GracePeriod:0s MinReclaim:<nil>} {Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>}]} QOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerPolicyOptions:map[] ExperimentalTopologyManagerScope:container ExperimentalCPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none}\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.755566     518 topology_manager.go:133] \"Creating topology manager with policy per scope\" topologyPolicyName=\"none\" topologyScopeName=\"container\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.755573     518 container_manager_linux.go:302] \"Creating device plugin manager\" devicePluginEnabled=true\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.755601     518 state_mem.go:36] \"Initialized new in-memory state store\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.760709     518 kubelet.go:376] \"Attempting to sync node with API server\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.760867     518 kubelet.go:267] \"Adding static pod path\" path=\"/etc/kubernetes/manifests\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.761186     518 kubelet.go:278] \"Adding apiserver pod source\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.761339     518 apiserver.go:42] \"Waiting for node sync before watching apiserver pods\"\r\nMay 24 18:19:45 master0 kubelet[518]: W0524 18:19:45.762544     518 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: Get \"https://10.88.3.60:6443/api/v1/nodes?fieldSelector=metadata.name%3Dmaster0&limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.762596     518 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://10.88.3.60:6443/api/v1/nodes?fieldSelector=metadata.name%3Dmaster0&limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:45 master0 kubelet[518]: W0524 18:19:45.765190     518 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: Get \"https://10.88.3.60:6443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.765475     518 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://10.88.3.60:6443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.770934     518 kuberuntime_manager.go:239] \"Container runtime initialized\" containerRuntime=\"containerd\" version=\"1.5.9-0ubuntu3\" apiVersion=\"v1alpha2\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.773106     518 server.go:1181] \"Started kubelet\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.773625     518 server.go:150] \"Starting to listen\" address=\"0.0.0.0\" port=10250\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.774679     518 server.go:410] \"Adding debug handlers to kubelet server\"\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.775043     518 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"master0.16f21d7153d06a40\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"master0\", UID:\"master0\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}, Reason:\"Starting\", Message:\"Starting kubelet.\", Source:v1.EventSource{Component:\"kubelet\", Host:\"master0\"}, FirstTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), LastTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), Count:1, Type:\"Normal\", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:\"\", Related:(*v1.ObjectReference)(nil), ReportingController:\"\", ReportingInstance:\"\"}': 'Post \"https://10.88.3.60:6443/api/v1/namespaces/default/events\": dial tcp 10.88.3.60:6443: connect: connection refused'(may retry after sleeping)\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.775996     518 cri_stats_provider.go:455] \"Failed to get the info of the filesystem with mountpoint\" err=\"unable to find data in memory cache\" mountpoint=\"/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs\"\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.776037     518 kubelet.go:1298] \"Image garbage collection failed once. Stats initialization may not have completed yet\" err=\"invalid capacity 0 on image filesystem\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.777272     518 scope.go:110] \"RemoveContainer\" containerID=\"bbd5cf52c256a5dade01dc92cd71fe58aea818dc051747e39c922ef40fe6ce4b\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.777437     518 fs_resource_analyzer.go:67] \"Starting FS ResourceAnalyzer\"\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.779457     518 controller.go:144] failed to ensure lease exists, will retry in 200ms, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.779239     518 volume_manager.go:289] \"Starting Kubelet Volume Manager\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.780837     518 desired_state_of_world_populator.go:145] \"Desired state populator starts to run\"\r\nMay 24 18:19:45 master0 kubelet[518]: W0524 18:19:45.780928     518 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: Get \"https://10.88.3.60:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.780964     518 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://10.88.3.60:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.781063     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.793747     518 cpu_manager.go:213] \"Starting CPU manager\" policy=\"none\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.793758     518 cpu_manager.go:214] \"Reconciling\" reconcilePeriod=\"10s\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.793784     518 state_mem.go:36] \"Initialized new in-memory state store\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.855402     518 state_mem.go:88] \"Updated default CPUSet\" cpuSet=\"\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.855487     518 state_mem.go:96] \"Updated CPUSet assignments\" assignments=map[]\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.855504     518 policy_none.go:49] \"None policy: Start\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.857594     518 memory_manager.go:168] \"Starting memorymanager\" policy=\"None\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.857787     518 state_mem.go:35] \"Initializing new in-memory state store\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.858634     518 state_mem.go:75] \"Updated machine memory state\"\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.880225     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.881191     518 kubelet_node_status.go:70] \"Attempting to register node\" node=\"master0\"\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.881726     518 kubelet_node_status.go:92] \"Unable to register node with API server\" err=\"Post \\\"https://10.88.3.60:6443/api/v1/nodes\\\": dial tcp 10.88.3.60:6443: connect: connection refused\" node=\"master0\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.915105     518 manager.go:610] \"Failed to read data from checkpoint\" checkpoint=\"kubelet_internal_checkpoint\" err=\"checkpoint is not found\"\r\nMay 24 18:19:45 master0 kubelet[518]: I0524 18:19:45.915476     518 plugin_manager.go:114] \"Starting Kubelet Plugin Manager\"\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.916231     518 eviction_manager.go:254] \"Eviction manager: failed to get summary stats\" err=\"failed to get node info: node \\\"master0\\\" not found\"\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.979873     518 controller.go:144] failed to ensure lease exists, will retry in 400ms, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:45 master0 kubelet[518]: E0524 18:19:45.980915     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.023533     518 kubelet_network_linux.go:76] \"Initialized protocol iptables rules.\" protocol=IPv4\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.060331     518 kubelet_network_linux.go:76] \"Initialized protocol iptables rules.\" protocol=IPv6\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.060522     518 status_manager.go:161] \"Starting to sync pod status with apiserver\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.060631     518 kubelet.go:1974] \"Starting kubelet main sync loop\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.060803     518 kubelet.go:1998] \"Skipping pod synchronization\" err=\"PLEG is not healthy: pleg has yet to be successful\"\r\nMay 24 18:19:46 master0 kubelet[518]: W0524 18:19:46.063595     518 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.RuntimeClass: Get \"https://10.88.3.60:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.063783     518 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://10.88.3.60:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.081026     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.083034     518 kubelet_node_status.go:70] \"Attempting to register node\" node=\"master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.083499     518 kubelet_node_status.go:92] \"Unable to register node with API server\" err=\"Post \\\"https://10.88.3.60:6443/api/v1/nodes\\\": dial tcp 10.88.3.60:6443: connect: connection refused\" node=\"master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.161087     518 topology_manager.go:200] \"Topology Admit Handler\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.162565     518 topology_manager.go:200] \"Topology Admit Handler\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.163696     518 topology_manager.go:200] \"Topology Admit Handler\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.164747     518 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.165093     518 topology_manager.go:200] \"Topology Admit Handler\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.166291     518 status_manager.go:664] \"Failed to get status for pod\" podUID=7b1a28f140e1f4144f7bd4fae347b484 pod=\"kube-system/kube-apiserver-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167198     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167441     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"9b22268671b35f95a232195864ff89c05352da0583b343581e6de3e612cbdd0e\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167469     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"e1695308f8981a20c9b9f7873e05c9af1c6ea802aa9f4438bd73849807561301\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167478     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"e6f427af5098b9dc994317e7768222f272b8be208c1b90e6cb6bd2b780a3f9b3\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167484     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"0a6956ea481436282b52087017fd160d105b0d6a1a0da1e47e7de61860691e1a\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167493     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"54e668f4f86e0ed0703f4919a00e8c61e742d5144df963490b34b1f08217b5d2\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167500     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"ffa91f087f9c051c22d8dded4c0a5017e1713493ecbf81e91d030e052b6d3a89\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167507     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"6a85776a547957cc1180daae554f97e16bf0f35e66c0296dcb55f5f7b2555652\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167515     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"f765d736bafce8cec9c265620a729b9d4be11909ed1211a36adefc0634de61ef\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.167521     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"9c2f9378fd2b7e7b6d389757dabb85447bb0ba7e69080e0dda5482a2a8123872\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.168901     518 status_manager.go:664] \"Failed to get status for pod\" podUID=fa768acb94c6cd1f77a2619331162053 pod=\"kube-system/kube-scheduler-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.181192     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.281656     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.282171     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"usr-share-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/7b1a28f140e1f4144f7bd4fae347b484-usr-share-ca-certificates\\\") pod \\\"kube-apiserver-master0\\\" (UID: \\\"7b1a28f140e1f4144f7bd4fae347b484\\\") \" pod=\"kube-system/kube-apiserver-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.282223     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"ca-certs\\\" (UniqueName: \\\"kubernetes.io/host-path/07dc1079c410123fdbdd120d096c4f3e-ca-certs\\\") pod \\\"kube-controller-manager-master0\\\" (UID: \\\"07dc1079c410123fdbdd120d096c4f3e\\\") \" pod=\"kube-system/kube-controller-manager-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.282421     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"etc-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/07dc1079c410123fdbdd120d096c4f3e-etc-ca-certificates\\\") pod \\\"kube-controller-manager-master0\\\" (UID: \\\"07dc1079c410123fdbdd120d096c4f3e\\\") \" pod=\"kube-system/kube-controller-manager-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.282948     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"usr-local-share-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/07dc1079c410123fdbdd120d096c4f3e-usr-local-share-ca-certificates\\\") pod \\\"kube-controller-manager-master0\\\" (UID: \\\"07dc1079c410123fdbdd120d096c4f3e\\\") \" pod=\"kube-system/kube-controller-manager-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283072     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"k8s-certs\\\" (UniqueName: \\\"kubernetes.io/host-path/7b1a28f140e1f4144f7bd4fae347b484-k8s-certs\\\") pod \\\"kube-apiserver-master0\\\" (UID: \\\"7b1a28f140e1f4144f7bd4fae347b484\\\") \" pod=\"kube-system/kube-apiserver-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283119     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"kubeconfig\\\" (UniqueName: \\\"kubernetes.io/host-path/07dc1079c410123fdbdd120d096c4f3e-kubeconfig\\\") pod \\\"kube-controller-manager-master0\\\" (UID: \\\"07dc1079c410123fdbdd120d096c4f3e\\\") \" pod=\"kube-system/kube-controller-manager-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283300     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"usr-share-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/07dc1079c410123fdbdd120d096c4f3e-usr-share-ca-certificates\\\") pod \\\"kube-controller-manager-master0\\\" (UID: \\\"07dc1079c410123fdbdd120d096c4f3e\\\") \" pod=\"kube-system/kube-controller-manager-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283451     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"etcd-certs\\\" (UniqueName: \\\"kubernetes.io/host-path/3da0b3b3f5b168e56c71dd2c6212a28e-etcd-certs\\\") pod \\\"etcd-master0\\\" (UID: \\\"3da0b3b3f5b168e56c71dd2c6212a28e\\\") \" pod=\"kube-system/etcd-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283484     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"ca-certs\\\" (UniqueName: \\\"kubernetes.io/host-path/7b1a28f140e1f4144f7bd4fae347b484-ca-certs\\\") pod \\\"kube-apiserver-master0\\\" (UID: \\\"7b1a28f140e1f4144f7bd4fae347b484\\\") \" pod=\"kube-system/kube-apiserver-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283511     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"etc-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/7b1a28f140e1f4144f7bd4fae347b484-etc-ca-certificates\\\") pod \\\"kube-apiserver-master0\\\" (UID: \\\"7b1a28f140e1f4144f7bd4fae347b484\\\") \" pod=\"kube-system/kube-apiserver-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283760     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"usr-local-share-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/7b1a28f140e1f4144f7bd4fae347b484-usr-local-share-ca-certificates\\\") pod \\\"kube-apiserver-master0\\\" (UID: \\\"7b1a28f140e1f4144f7bd4fae347b484\\\") \" pod=\"kube-system/kube-apiserver-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283802     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"k8s-certs\\\" (UniqueName: \\\"kubernetes.io/host-path/07dc1079c410123fdbdd120d096c4f3e-k8s-certs\\\") pod \\\"kube-controller-manager-master0\\\" (UID: \\\"07dc1079c410123fdbdd120d096c4f3e\\\") \" pod=\"kube-system/kube-controller-manager-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.283981     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"kubeconfig\\\" (UniqueName: \\\"kubernetes.io/host-path/fa768acb94c6cd1f77a2619331162053-kubeconfig\\\") pod \\\"kube-scheduler-master0\\\" (UID: \\\"fa768acb94c6cd1f77a2619331162053\\\") \" pod=\"kube-system/kube-scheduler-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.284037     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"flexvolume-dir\\\" (UniqueName: \\\"kubernetes.io/host-path/07dc1079c410123fdbdd120d096c4f3e-flexvolume-dir\\\") pod \\\"kube-controller-manager-master0\\\" (UID: \\\"07dc1079c410123fdbdd120d096c4f3e\\\") \" pod=\"kube-system/kube-controller-manager-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.284202     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"etcd-data\\\" (UniqueName: \\\"kubernetes.io/host-path/3da0b3b3f5b168e56c71dd2c6212a28e-etcd-data\\\") pod \\\"etcd-master0\\\" (UID: \\\"3da0b3b3f5b168e56c71dd2c6212a28e\\\") \" pod=\"kube-system/etcd-master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.380742     518 controller.go:144] failed to ensure lease exists, will retry in 800ms, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.382733     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.483379     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: I0524 18:19:46.485356     518 kubelet_node_status.go:70] \"Attempting to register node\" node=\"master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.485793     518 kubelet_node_status.go:92] \"Unable to register node with API server\" err=\"Post \\\"https://10.88.3.60:6443/api/v1/nodes\\\": dial tcp 10.88.3.60:6443: connect: connection refused\" node=\"master0\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.584143     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.684873     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.785677     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: W0524 18:19:46.846517     518 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: Get \"https://10.88.3.60:6443/api/v1/nodes?fieldSelector=metadata.name%3Dmaster0&limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.846584     518 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://10.88.3.60:6443/api/v1/nodes?fieldSelector=metadata.name%3Dmaster0&limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.886003     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:46 master0 kubelet[518]: E0524 18:19:46.986112     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.086585     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: W0524 18:19:47.132629     518 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: Get \"https://10.88.3.60:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.132768     518 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://10.88.3.60:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.181456     518 controller.go:144] failed to ensure lease exists, will retry in 1.6s, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.187554     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.288236     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: I0524 18:19:47.289140     518 kubelet_node_status.go:70] \"Attempting to register node\" node=\"master0\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.289925     518 kubelet_node_status.go:92] \"Unable to register node with API server\" err=\"Post \\\"https://10.88.3.60:6443/api/v1/nodes\\\": dial tcp 10.88.3.60:6443: connect: connection refused\" node=\"master0\"\r\nMay 24 18:19:47 master0 kubelet[518]: W0524 18:19:47.309588     518 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: Get \"https://10.88.3.60:6443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.309656     518 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://10.88.3.60:6443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:47 master0 kubelet[518]: W0524 18:19:47.375910     518 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.RuntimeClass: Get \"https://10.88.3.60:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.376016     518 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://10.88.3.60:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.388524     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.489581     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.590040     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.690475     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.790763     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.891512     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:47 master0 kubelet[518]: E0524 18:19:47.992058     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.092128     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.192577     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.293485     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.394008     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.494931     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.595720     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.696317     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.797343     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: I0524 18:19:48.891199     518 kubelet_node_status.go:70] \"Attempting to register node\" node=\"master0\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.897739     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:48 master0 kubelet[518]: E0524 18:19:48.998530     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.098940     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.199369     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.300471     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.400543     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.501110     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.601767     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.702216     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.803257     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:49 master0 kubelet[518]: E0524 18:19:49.903944     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.004555     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.105001     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.205489     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.306277     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.406917     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.507528     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.607706     518 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.708103     518 kuberuntime_manager.go:1095] \"Updating runtime config through cri with podcidr\" CIDR=\"10.88.0.0/24\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.709271     518 kubelet_network.go:60] \"Updating Pod CIDR\" originalPodCIDR=\"\" newPodCIDR=\"10.88.0.0/24\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.709814     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.718221     518 kubelet_node_status.go:108] \"Node was previously registered\" node=\"master0\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.718297     518 kubelet_node_status.go:73] \"Successfully registered node\" node=\"master0\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.765836     518 apiserver.go:52] \"Watching apiserver\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.769323     518 topology_manager.go:200] \"Topology Admit Handler\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.808894     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"kube-proxy\\\" (UniqueName: \\\"kubernetes.io/configmap/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-proxy\\\") pod \\\"kube-proxy-qzlw9\\\" (UID: \\\"f4e454d0-9316-4b44-92e9-498d8668a6fb\\\") \" pod=\"kube-system/kube-proxy-qzlw9\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.808940     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"kube-api-access-tncz8\\\" (UniqueName: \\\"kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8\\\") pod \\\"kube-proxy-qzlw9\\\" (UID: \\\"f4e454d0-9316-4b44-92e9-498d8668a6fb\\\") \" pod=\"kube-system/kube-proxy-qzlw9\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.809112     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"xtables-lock\\\" (UniqueName: \\\"kubernetes.io/host-path/f4e454d0-9316-4b44-92e9-498d8668a6fb-xtables-lock\\\") pod \\\"kube-proxy-qzlw9\\\" (UID: \\\"f4e454d0-9316-4b44-92e9-498d8668a6fb\\\") \" pod=\"kube-system/kube-proxy-qzlw9\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.809147     518 reconciler.go:270] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"lib-modules\\\" (UniqueName: \\\"kubernetes.io/host-path/f4e454d0-9316-4b44-92e9-498d8668a6fb-lib-modules\\\") pod \\\"kube-proxy-qzlw9\\\" (UID: \\\"f4e454d0-9316-4b44-92e9-498d8668a6fb\\\") \" pod=\"kube-system/kube-proxy-qzlw9\"\r\nMay 24 18:19:50 master0 kubelet[518]: I0524 18:19:50.809220     518 reconciler.go:157] \"Reconciler: start to sync state\"\r\nMay 24 18:19:50 master0 kubelet[518]: E0524 18:19:50.916407     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:19:51 master0 kubelet[518]: E0524 18:19:51.163832     518 projected.go:192] Error preparing data for projected volume kube-api-access-tncz8 for pod kube-system/kube-proxy-qzlw9: failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:51 master0 kubelet[518]: E0524 18:19:51.164023     518 nestedpendingoperations.go:335] Operation for \"{volumeName:kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8 podName:f4e454d0-9316-4b44-92e9-498d8668a6fb nodeName:}\" failed. No retries permitted until 2022-05-24 18:19:51.663945996 +0000 UTC m=+6.183344588 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume \"kube-api-access-tncz8\" (UniqueName: \"kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8\") pod \"kube-proxy-qzlw9\" (UID: \"f4e454d0-9316-4b44-92e9-498d8668a6fb\") : failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:51 master0 kubelet[518]: I0524 18:19:51.362581     518 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:51 master0 kubelet[518]: I0524 18:19:51.563091     518 status_manager.go:664] \"Failed to get status for pod\" podUID=f4e454d0-9316-4b44-92e9-498d8668a6fb pod=\"kube-system/kube-proxy-qzlw9\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-proxy-qzlw9\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:51 master0 kubelet[518]: I0524 18:19:51.762947     518 status_manager.go:664] \"Failed to get status for pod\" podUID=fa768acb94c6cd1f77a2619331162053 pod=\"kube-system/kube-scheduler-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:51 master0 kubelet[518]: E0524 18:19:51.963286     518 projected.go:192] Error preparing data for projected volume kube-api-access-tncz8 for pod kube-system/kube-proxy-qzlw9: failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:51 master0 kubelet[518]: E0524 18:19:51.963416     518 nestedpendingoperations.go:335] Operation for \"{volumeName:kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8 podName:f4e454d0-9316-4b44-92e9-498d8668a6fb nodeName:}\" failed. No retries permitted until 2022-05-24 18:19:52.963383045 +0000 UTC m=+7.482781627 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume \"kube-api-access-tncz8\" (UniqueName: \"kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8\") pod \"kube-proxy-qzlw9\" (UID: \"f4e454d0-9316-4b44-92e9-498d8668a6fb\") : failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:52 master0 kubelet[518]: I0524 18:19:52.162933     518 status_manager.go:664] \"Failed to get status for pod\" podUID=7b1a28f140e1f4144f7bd4fae347b484 pod=\"kube-system/kube-apiserver-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:52 master0 kubelet[518]: E0524 18:19:52.828164     518 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"master0.16f21d7153d06a40\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"master0\", UID:\"master0\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}, Reason:\"Starting\", Message:\"Starting kubelet.\", Source:v1.EventSource{Component:\"kubelet\", Host:\"master0\"}, FirstTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), LastTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), Count:1, Type:\"Normal\", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:\"\", Related:(*v1.ObjectReference)(nil), ReportingController:\"\", ReportingInstance:\"\"}': 'Post \"https://10.88.3.60:6443/api/v1/namespaces/default/events\": dial tcp 10.88.3.60:6443: connect: connection refused'(may retry after sleeping)\r\nMay 24 18:19:53 master0 kubelet[518]: E0524 18:19:53.026725     518 projected.go:192] Error preparing data for projected volume kube-api-access-tncz8 for pod kube-system/kube-proxy-qzlw9: failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:53 master0 kubelet[518]: E0524 18:19:53.026791     518 nestedpendingoperations.go:335] Operation for \"{volumeName:kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8 podName:f4e454d0-9316-4b44-92e9-498d8668a6fb nodeName:}\" failed. No retries permitted until 2022-05-24 18:19:55.026772871 +0000 UTC m=+9.546171453 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume \"kube-api-access-tncz8\" (UniqueName: \"kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8\") pod \"kube-proxy-qzlw9\" (UID: \"f4e454d0-9316-4b44-92e9-498d8668a6fb\") : failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:53 master0 kubelet[518]: I0524 18:19:53.099505     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"4f6c16dd740a75bb31e4309147708c7be8601054718f52f1111534a9888b4bae\"\r\nMay 24 18:19:53 master0 kubelet[518]: I0524 18:19:53.099560     518 scope.go:110] \"RemoveContainer\" containerID=\"a6341035f36b2480c86c469b6457050a36e64f395c5db9b9730443073541e1db\"\r\nMay 24 18:19:53 master0 kubelet[518]: I0524 18:19:53.100197     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:53 master0 kubelet[518]: E0524 18:19:53.191973     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-controller-manager\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-master0_kube-system(07dc1079c410123fdbdd120d096c4f3e)\\\"\" pod=\"kube-system/kube-controller-manager-master0\" podUID=07dc1079c410123fdbdd120d096c4f3e\r\nMay 24 18:19:54 master0 kubelet[518]: I0524 18:19:54.102584     518 scope.go:110] \"RemoveContainer\" containerID=\"17244f7547b1952d013c6d9b6df964d09db000e185b935070d14c9add39d2896\"\r\nMay 24 18:19:54 master0 kubelet[518]: I0524 18:19:54.103196     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:54 master0 kubelet[518]: E0524 18:19:54.103775     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-controller-manager\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-master0_kube-system(07dc1079c410123fdbdd120d096c4f3e)\\\"\" pod=\"kube-system/kube-controller-manager-master0\" podUID=07dc1079c410123fdbdd120d096c4f3e\r\nMay 24 18:19:55 master0 kubelet[518]: E0524 18:19:55.041267     518 projected.go:192] Error preparing data for projected volume kube-api-access-tncz8 for pod kube-system/kube-proxy-qzlw9: failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:55 master0 kubelet[518]: E0524 18:19:55.041428     518 nestedpendingoperations.go:335] Operation for \"{volumeName:kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8 podName:f4e454d0-9316-4b44-92e9-498d8668a6fb nodeName:}\" failed. No retries permitted until 2022-05-24 18:19:59.041337549 +0000 UTC m=+13.560736131 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume \"kube-api-access-tncz8\" (UniqueName: \"kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8\") pod \"kube-proxy-qzlw9\" (UID: \"f4e454d0-9316-4b44-92e9-498d8668a6fb\") : failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:55 master0 kubelet[518]: I0524 18:19:55.104234     518 scope.go:110] \"RemoveContainer\" containerID=\"17244f7547b1952d013c6d9b6df964d09db000e185b935070d14c9add39d2896\"\r\nMay 24 18:19:55 master0 kubelet[518]: E0524 18:19:55.104693     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-controller-manager\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-master0_kube-system(07dc1079c410123fdbdd120d096c4f3e)\\\"\" pod=\"kube-system/kube-controller-manager-master0\" podUID=07dc1079c410123fdbdd120d096c4f3e\r\nMay 24 18:19:55 master0 kubelet[518]: E0524 18:19:55.918927     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:19:56 master0 kubelet[518]: I0524 18:19:56.061912     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:56 master0 kubelet[518]: I0524 18:19:56.062241     518 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:56 master0 kubelet[518]: I0524 18:19:56.062549     518 status_manager.go:664] \"Failed to get status for pod\" podUID=f4e454d0-9316-4b44-92e9-498d8668a6fb pod=\"kube-system/kube-proxy-qzlw9\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-proxy-qzlw9\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:56 master0 kubelet[518]: I0524 18:19:56.062880     518 status_manager.go:664] \"Failed to get status for pod\" podUID=fa768acb94c6cd1f77a2619331162053 pod=\"kube-system/kube-scheduler-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:56 master0 kubelet[518]: I0524 18:19:56.063185     518 status_manager.go:664] \"Failed to get status for pod\" podUID=7b1a28f140e1f4144f7bd4fae347b484 pod=\"kube-system/kube-apiserver-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:19:59 master0 kubelet[518]: E0524 18:19:59.066404     518 projected.go:192] Error preparing data for projected volume kube-api-access-tncz8 for pod kube-system/kube-proxy-qzlw9: failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:19:59 master0 kubelet[518]: E0524 18:19:59.066471     518 nestedpendingoperations.go:335] Operation for \"{volumeName:kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8 podName:f4e454d0-9316-4b44-92e9-498d8668a6fb nodeName:}\" failed. No retries permitted until 2022-05-24 18:20:07.066453006 +0000 UTC m=+21.585851588 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume \"kube-api-access-tncz8\" (UniqueName: \"kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8\") pod \"kube-proxy-qzlw9\" (UID: \"f4e454d0-9316-4b44-92e9-498d8668a6fb\") : failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:00 master0 kubelet[518]: I0524 18:20:00.842537     518 scope.go:110] \"RemoveContainer\" containerID=\"17244f7547b1952d013c6d9b6df964d09db000e185b935070d14c9add39d2896\"\r\nMay 24 18:20:00 master0 kubelet[518]: E0524 18:20:00.843076     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-controller-manager\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-master0_kube-system(07dc1079c410123fdbdd120d096c4f3e)\\\"\" pod=\"kube-system/kube-controller-manager-master0\" podUID=07dc1079c410123fdbdd120d096c4f3e\r\nMay 24 18:20:00 master0 kubelet[518]: E0524 18:20:00.920492     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:20:00 master0 kubelet[518]: E0524 18:20:00.958448     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?resourceVersion=0&timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:00 master0 kubelet[518]: E0524 18:20:00.958761     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:00 master0 kubelet[518]: E0524 18:20:00.958921     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:00 master0 kubelet[518]: E0524 18:20:00.959041     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:00 master0 kubelet[518]: E0524 18:20:00.959146     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:00 master0 kubelet[518]: E0524 18:20:00.959156     518 kubelet_node_status.go:447] \"Unable to update node status\" err=\"update node status exceeds retry count\"\r\nMay 24 18:20:01 master0 kubelet[518]: E0524 18:20:01.040488     518 controller.go:187] failed to update lease, error: Put \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:01 master0 kubelet[518]: E0524 18:20:01.040832     518 controller.go:187] failed to update lease, error: Put \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:01 master0 kubelet[518]: E0524 18:20:01.041261     518 controller.go:187] failed to update lease, error: Put \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:01 master0 kubelet[518]: E0524 18:20:01.041562     518 controller.go:187] failed to update lease, error: Put \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:01 master0 kubelet[518]: E0524 18:20:01.041780     518 controller.go:187] failed to update lease, error: Put \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:01 master0 kubelet[518]: I0524 18:20:01.041810     518 controller.go:114] failed to update lease using latest lease, fallback to ensure lease, err: failed 5 attempts to update lease\r\nMay 24 18:20:01 master0 kubelet[518]: E0524 18:20:01.041974     518 controller.go:144] failed to ensure lease exists, will retry in 200ms, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:01 master0 kubelet[518]: E0524 18:20:01.243262     518 controller.go:144] failed to ensure lease exists, will retry in 400ms, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:01 master0 kubelet[518]: E0524 18:20:01.644411     518 controller.go:144] failed to ensure lease exists, will retry in 800ms, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:02 master0 kubelet[518]: I0524 18:20:02.337215     518 scope.go:110] \"RemoveContainer\" containerID=\"17244f7547b1952d013c6d9b6df964d09db000e185b935070d14c9add39d2896\"\r\nMay 24 18:20:02 master0 kubelet[518]: E0524 18:20:02.338881     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-controller-manager\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-master0_kube-system(07dc1079c410123fdbdd120d096c4f3e)\\\"\" pod=\"kube-system/kube-controller-manager-master0\" podUID=07dc1079c410123fdbdd120d096c4f3e\r\nMay 24 18:20:02 master0 kubelet[518]: E0524 18:20:02.446424     518 controller.go:144] failed to ensure lease exists, will retry in 1.6s, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:02 master0 kubelet[518]: E0524 18:20:02.829207     518 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"master0.16f21d7153d06a40\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"master0\", UID:\"master0\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}, Reason:\"Starting\", Message:\"Starting kubelet.\", Source:v1.EventSource{Component:\"kubelet\", Host:\"master0\"}, FirstTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), LastTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), Count:1, Type:\"Normal\", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:\"\", Related:(*v1.ObjectReference)(nil), ReportingController:\"\", ReportingInstance:\"\"}': 'Post \"https://10.88.3.60:6443/api/v1/namespaces/default/events\": dial tcp 10.88.3.60:6443: connect: connection refused'(may retry after sleeping)\r\nMay 24 18:20:03 master0 kubelet[518]: I0524 18:20:03.368074     518 status_manager.go:664] \"Failed to get status for pod\" podUID=fa768acb94c6cd1f77a2619331162053 pod=\"kube-system/kube-scheduler-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:03 master0 kubelet[518]: I0524 18:20:03.368244     518 status_manager.go:664] \"Failed to get status for pod\" podUID=fa768acb94c6cd1f77a2619331162053 pod=\"kube-system/kube-scheduler-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:04 master0 kubelet[518]: E0524 18:20:04.047681     518 controller.go:144] failed to ensure lease exists, will retry in 3.2s, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:05 master0 kubelet[518]: E0524 18:20:05.922159     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:20:06 master0 kubelet[518]: I0524 18:20:06.061821     518 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:06 master0 kubelet[518]: I0524 18:20:06.062131     518 status_manager.go:664] \"Failed to get status for pod\" podUID=f4e454d0-9316-4b44-92e9-498d8668a6fb pod=\"kube-system/kube-proxy-qzlw9\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-proxy-qzlw9\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:06 master0 kubelet[518]: I0524 18:20:06.062476     518 status_manager.go:664] \"Failed to get status for pod\" podUID=fa768acb94c6cd1f77a2619331162053 pod=\"kube-system/kube-scheduler-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:06 master0 kubelet[518]: I0524 18:20:06.062807     518 status_manager.go:664] \"Failed to get status for pod\" podUID=7b1a28f140e1f4144f7bd4fae347b484 pod=\"kube-system/kube-apiserver-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:06 master0 kubelet[518]: I0524 18:20:06.063093     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:06 master0 kubelet[518]: I0524 18:20:06.539101     518 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:06 master0 kubelet[518]: I0524 18:20:06.539402     518 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:07 master0 kubelet[518]: E0524 18:20:07.127461     518 projected.go:192] Error preparing data for projected volume kube-api-access-tncz8 for pod kube-system/kube-proxy-qzlw9: failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:07 master0 kubelet[518]: E0524 18:20:07.127575     518 nestedpendingoperations.go:335] Operation for \"{volumeName:kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8 podName:f4e454d0-9316-4b44-92e9-498d8668a6fb nodeName:}\" failed. No retries permitted until 2022-05-24 18:20:23.127540562 +0000 UTC m=+37.646939154 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume \"kube-api-access-tncz8\" (UniqueName: \"kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8\") pod \"kube-proxy-qzlw9\" (UID: \"f4e454d0-9316-4b44-92e9-498d8668a6fb\") : failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:07 master0 kubelet[518]: E0524 18:20:07.248463     518 controller.go:144] failed to ensure lease exists, will retry in 6.4s, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:10 master0 kubelet[518]: E0524 18:20:10.923449     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:20:11 master0 kubelet[518]: E0524 18:20:11.287927     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?resourceVersion=0&timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:11 master0 kubelet[518]: E0524 18:20:11.288355     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:11 master0 kubelet[518]: E0524 18:20:11.288597     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:11 master0 kubelet[518]: E0524 18:20:11.288792     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:11 master0 kubelet[518]: E0524 18:20:11.289071     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:11 master0 kubelet[518]: E0524 18:20:11.289098     518 kubelet_node_status.go:447] \"Unable to update node status\" err=\"update node status exceeds retry count\"\r\nMay 24 18:20:12 master0 kubelet[518]: E0524 18:20:12.830457     518 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"master0.16f21d7153d06a40\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"master0\", UID:\"master0\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}, Reason:\"Starting\", Message:\"Starting kubelet.\", Source:v1.EventSource{Component:\"kubelet\", Host:\"master0\"}, FirstTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), LastTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), Count:1, Type:\"Normal\", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:\"\", Related:(*v1.ObjectReference)(nil), ReportingController:\"\", ReportingInstance:\"\"}': 'Post \"https://10.88.3.60:6443/api/v1/namespaces/default/events\": dial tcp 10.88.3.60:6443: connect: connection refused'(may retry after sleeping)\r\nMay 24 18:20:13 master0 kubelet[518]: E0524 18:20:13.649732     518 controller.go:144] failed to ensure lease exists, will retry in 7s, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:15 master0 kubelet[518]: I0524 18:20:15.061686     518 scope.go:110] \"RemoveContainer\" containerID=\"17244f7547b1952d013c6d9b6df964d09db000e185b935070d14c9add39d2896\"\r\nMay 24 18:20:15 master0 kubelet[518]: E0524 18:20:15.924563     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:20:16 master0 kubelet[518]: I0524 18:20:16.061596     518 status_manager.go:664] \"Failed to get status for pod\" podUID=fa768acb94c6cd1f77a2619331162053 pod=\"kube-system/kube-scheduler-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:16 master0 kubelet[518]: I0524 18:20:16.061869     518 status_manager.go:664] \"Failed to get status for pod\" podUID=7b1a28f140e1f4144f7bd4fae347b484 pod=\"kube-system/kube-apiserver-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:16 master0 kubelet[518]: I0524 18:20:16.062172     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:16 master0 kubelet[518]: I0524 18:20:16.062502     518 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:16 master0 kubelet[518]: I0524 18:20:16.062732     518 status_manager.go:664] \"Failed to get status for pod\" podUID=f4e454d0-9316-4b44-92e9-498d8668a6fb pod=\"kube-system/kube-proxy-qzlw9\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-proxy-qzlw9\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:16 master0 kubelet[518]: I0524 18:20:16.143016     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:20 master0 kubelet[518]: E0524 18:20:20.650275     518 controller.go:144] failed to ensure lease exists, will retry in 7s, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:20 master0 kubelet[518]: E0524 18:20:20.926102     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:20:21 master0 kubelet[518]: I0524 18:20:21.154132     518 scope.go:110] \"RemoveContainer\" containerID=\"2859411c702c538d5016d1fc8231ba326afc4aa8f2ee895b0cffb0c92856866c\"\r\nMay 24 18:20:21 master0 kubelet[518]: I0524 18:20:21.154491     518 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"ef31a505d60061b78cf18bf44c88d05afe4610b2b9428fc84322ed0c294fae59\"\r\nMay 24 18:20:21 master0 kubelet[518]: I0524 18:20:21.154549     518 status_manager.go:664] \"Failed to get status for pod\" podUID=7b1a28f140e1f4144f7bd4fae347b484 pod=\"kube-system/kube-apiserver-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:21 master0 kubelet[518]: E0524 18:20:21.230091     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-apiserver\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-apiserver pod=kube-apiserver-master0_kube-system(7b1a28f140e1f4144f7bd4fae347b484)\\\"\" pod=\"kube-system/kube-apiserver-master0\" podUID=7b1a28f140e1f4144f7bd4fae347b484\r\nMay 24 18:20:21 master0 kubelet[518]: E0524 18:20:21.610662     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?resourceVersion=0&timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:21 master0 kubelet[518]: E0524 18:20:21.611039     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:21 master0 kubelet[518]: E0524 18:20:21.611436     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:21 master0 kubelet[518]: E0524 18:20:21.611692     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:21 master0 kubelet[518]: E0524 18:20:21.611953     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:21 master0 kubelet[518]: E0524 18:20:21.611975     518 kubelet_node_status.go:447] \"Unable to update node status\" err=\"update node status exceeds retry count\"\r\nMay 24 18:20:22 master0 kubelet[518]: I0524 18:20:22.157884     518 scope.go:110] \"RemoveContainer\" containerID=\"f7229ae9be4f1419b961cb8ae3996c8bae34fbf205ac4a6498f3a60a21257421\"\r\nMay 24 18:20:22 master0 kubelet[518]: I0524 18:20:22.158145     518 status_manager.go:664] \"Failed to get status for pod\" podUID=7b1a28f140e1f4144f7bd4fae347b484 pod=\"kube-system/kube-apiserver-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:22 master0 kubelet[518]: E0524 18:20:22.158564     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-apiserver\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-apiserver pod=kube-apiserver-master0_kube-system(7b1a28f140e1f4144f7bd4fae347b484)\\\"\" pod=\"kube-system/kube-apiserver-master0\" podUID=7b1a28f140e1f4144f7bd4fae347b484\r\nMay 24 18:20:22 master0 kubelet[518]: E0524 18:20:22.831792     518 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"master0.16f21d7153d06a40\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"master0\", UID:\"master0\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}, Reason:\"Starting\", Message:\"Starting kubelet.\", Source:v1.EventSource{Component:\"kubelet\", Host:\"master0\"}, FirstTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), LastTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), Count:1, Type:\"Normal\", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:\"\", Related:(*v1.ObjectReference)(nil), ReportingController:\"\", ReportingInstance:\"\"}': 'Post \"https://10.88.3.60:6443/api/v1/namespaces/default/events\": dial tcp 10.88.3.60:6443: connect: connection refused'(may retry after sleeping)\r\nMay 24 18:20:23 master0 kubelet[518]: I0524 18:20:23.159329     518 scope.go:110] \"RemoveContainer\" containerID=\"f7229ae9be4f1419b961cb8ae3996c8bae34fbf205ac4a6498f3a60a21257421\"\r\nMay 24 18:20:23 master0 kubelet[518]: E0524 18:20:23.159960     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-apiserver\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-apiserver pod=kube-apiserver-master0_kube-system(7b1a28f140e1f4144f7bd4fae347b484)\\\"\" pod=\"kube-system/kube-apiserver-master0\" podUID=7b1a28f140e1f4144f7bd4fae347b484\r\nMay 24 18:20:23 master0 kubelet[518]: E0524 18:20:23.224982     518 projected.go:192] Error preparing data for projected volume kube-api-access-tncz8 for pod kube-system/kube-proxy-qzlw9: failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:23 master0 kubelet[518]: E0524 18:20:23.225053     518 nestedpendingoperations.go:335] Operation for \"{volumeName:kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8 podName:f4e454d0-9316-4b44-92e9-498d8668a6fb nodeName:}\" failed. No retries permitted until 2022-05-24 18:20:55.225034048 +0000 UTC m=+69.744432630 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume \"kube-api-access-tncz8\" (UniqueName: \"kubernetes.io/projected/f4e454d0-9316-4b44-92e9-498d8668a6fb-kube-api-access-tncz8\") pod \"kube-proxy-qzlw9\" (UID: \"f4e454d0-9316-4b44-92e9-498d8668a6fb\") : failed to fetch token: Post \"https://10.88.3.60:6443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:24 master0 kubelet[518]: I0524 18:20:24.160916     518 scope.go:110] \"RemoveContainer\" containerID=\"f7229ae9be4f1419b961cb8ae3996c8bae34fbf205ac4a6498f3a60a21257421\"\r\nMay 24 18:20:24 master0 kubelet[518]: E0524 18:20:24.161649     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-apiserver\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-apiserver pod=kube-apiserver-master0_kube-system(7b1a28f140e1f4144f7bd4fae347b484)\\\"\" pod=\"kube-system/kube-apiserver-master0\" podUID=7b1a28f140e1f4144f7bd4fae347b484\r\nMay 24 18:20:25 master0 kubelet[518]: E0524 18:20:25.927579     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:20:26 master0 kubelet[518]: I0524 18:20:26.062780     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:26 master0 kubelet[518]: I0524 18:20:26.067179     518 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:26 master0 kubelet[518]: I0524 18:20:26.068141     518 status_manager.go:664] \"Failed to get status for pod\" podUID=f4e454d0-9316-4b44-92e9-498d8668a6fb pod=\"kube-system/kube-proxy-qzlw9\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-proxy-qzlw9\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:26 master0 kubelet[518]: I0524 18:20:26.068425     518 status_manager.go:664] \"Failed to get status for pod\" podUID=fa768acb94c6cd1f77a2619331162053 pod=\"kube-system/kube-scheduler-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:26 master0 kubelet[518]: I0524 18:20:26.068696     518 status_manager.go:664] \"Failed to get status for pod\" podUID=7b1a28f140e1f4144f7bd4fae347b484 pod=\"kube-system/kube-apiserver-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:26 master0 kubelet[518]: I0524 18:20:26.964016     518 scope.go:110] \"RemoveContainer\" containerID=\"f7229ae9be4f1419b961cb8ae3996c8bae34fbf205ac4a6498f3a60a21257421\"\r\nMay 24 18:20:26 master0 kubelet[518]: E0524 18:20:26.964838     518 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-apiserver\\\" with CrashLoopBackOff: \\\"back-off 10s restarting failed container=kube-apiserver pod=kube-apiserver-master0_kube-system(7b1a28f140e1f4144f7bd4fae347b484)\\\"\" pod=\"kube-system/kube-apiserver-master0\" podUID=7b1a28f140e1f4144f7bd4fae347b484\r\nMay 24 18:20:27 master0 kubelet[518]: E0524 18:20:27.651236     518 controller.go:144] failed to ensure lease exists, will retry in 7s, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:30 master0 kubelet[518]: E0524 18:20:30.928371     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:20:31 master0 kubelet[518]: E0524 18:20:31.623305     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?resourceVersion=0&timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:31 master0 kubelet[518]: E0524 18:20:31.623641     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:31 master0 kubelet[518]: E0524 18:20:31.623854     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:31 master0 kubelet[518]: E0524 18:20:31.624135     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:31 master0 kubelet[518]: E0524 18:20:31.624360     518 kubelet_node_status.go:460] \"Error updating node status, will retry\" err=\"error getting node \\\"master0\\\": Get \\\"https://10.88.3.60:6443/api/v1/nodes/master0?timeout=10s\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:31 master0 kubelet[518]: E0524 18:20:31.624382     518 kubelet_node_status.go:447] \"Unable to update node status\" err=\"update node status exceeds retry count\"\r\nMay 24 18:20:32 master0 kubelet[518]: I0524 18:20:32.342034     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:32 master0 kubelet[518]: I0524 18:20:32.342285     518 status_manager.go:664] \"Failed to get status for pod\" podUID=07dc1079c410123fdbdd120d096c4f3e pod=\"kube-system/kube-controller-manager-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 24 18:20:32 master0 kubelet[518]: E0524 18:20:32.833244     518 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"master0.16f21d7153d06a40\", GenerateName:\"\", Namespace:\"default\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:\"\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:\"Node\", Namespace:\"\", Name:\"master0\", UID:\"master0\", APIVersion:\"\", ResourceVersion:\"\", FieldPath:\"\"}, Reason:\"Starting\", Message:\"Starting kubelet.\", Source:v1.EventSource{Component:\"kubelet\", Host:\"master0\"}, FirstTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), LastTimestamp:time.Date(2022, time.May, 24, 18, 19, 45, 773070912, time.Local), Count:1, Type:\"Normal\", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:\"\", Related:(*v1.ObjectReference)(nil), ReportingController:\"\", ReportingInstance:\"\"}': 'Post \"https://10.88.3.60:6443/api/v1/namespaces/default/events\": dial tcp 10.88.3.60:6443: connect: connection refused'(may retry after sleeping)\r\nMay 24 18:20:34 master0 kubelet[518]: E0524 18:20:34.652280     518 controller.go:144] failed to ensure lease exists, will retry in 7s, error: Get \"https://10.88.3.60:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master0?timeout=10s\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 24 18:20:35 master0 kubelet[518]: E0524 18:20:35.929997     518 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 24 18:20:36 master0 kubelet[518]: I0524 18:20:36.060924     518 status_manager.go:664] \"Failed to get status for pod\" podUID=f4e454d0-9316-4b44-92e9-498d8668a6fb pod=\"kube-system/kube-proxy-qzlw9\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/kube-proxy-qzlw9\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\n```\r\n\r\n</details>\n---\n\nUser 'pacoxu' said:\n---\nYou cannot connect the apiserver from the node. There are two network interfaces.\r\n```\r\nsysctl -a | grep net.ipv4.conf.all.arp_filter\r\n```\r\n/sig network\r\nWould you check the route and arp_filter setting?\n---\n\nUser 'Congelli501' said:\n---\nHere are the arp_filter setting & routes:\r\n```\r\nroot@master0:~# sysctl -a | grep net.ipv4.conf.all.arp_filter\r\nnet.ipv4.conf.all.arp_filter = 0\r\n\r\nroot@master0:~# ip a\r\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\r\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\r\n    inet 127.0.0.1/8 scope host lo\r\n       valid_lft forever preferred_lft forever\r\n    inet6 ::1/128 scope host \r\n       valid_lft forever preferred_lft forever\r\n2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000\r\n    link/ether 52:54:00:8c:85:a9 brd ff:ff:ff:ff:ff:ff\r\n    inet 10.10.42.85/16 metric 100 brd 10.10.255.255 scope global dynamic enp1s0\r\n       valid_lft 39350sec preferred_lft 39350sec\r\n    inet6 fe80::5054:ff:fe8c:85a9/64 scope link \r\n       valid_lft forever preferred_lft forever\r\n3: enp2s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000\r\n    link/ether 52:54:00:c4:0c:36 brd ff:ff:ff:ff:ff:ff\r\n    inet 10.88.3.60/16 brd 10.88.255.255 scope global enp2s0\r\n       valid_lft forever preferred_lft forever\r\n    inet6 fe80::5054:ff:fec4:c36/64 scope link \r\n       valid_lft forever preferred_lft forever\r\n\r\nroot@master0:~# ip r\r\ndefault via 10.10.0.3 dev enp1s0 proto dhcp src 10.10.42.85 metric 100 \r\n10.10.0.0/16 dev enp1s0 proto kernel scope link src 10.10.42.85 metric 100 \r\n10.10.0.1 dev enp1s0 proto dhcp scope link src 10.10.42.85 metric 100 \r\n10.10.0.3 dev enp1s0 proto dhcp scope link src 10.10.42.85 metric 100 \r\n10.88.0.0/16 dev enp2s0 proto kernel scope link src 10.88.3.60 \r\n```\r\n\r\nThe api server is accessible when it is UP, it just doesn't stay up for long.\r\n```\r\nroot@master0:~# while ! kubectl get pods -n kube-system; do sleep 5; done\r\nThe connection to the server 10.88.3.60:6443 was refused - did you specify the right host or port?\r\nThe connection to the server 10.88.3.60:6443 was refused - did you specify the right host or port?\r\nNAME                              READY   STATUS             RESTARTS          AGE\r\ncoredns-6d4b75cb6d-jfp4t          0/1     Pending            0                 47h\r\ncoredns-6d4b75cb6d-nmcmb          0/1     Pending            0                 47h\r\netcd-master0                      1/1     Running            559 (10m ago)     47h\r\nkube-apiserver-master0            0/1     Running            534 (24s ago)     47h\r\nkube-controller-manager-master0   0/1     Running            552 (3m23s ago)   47h\r\nkube-proxy-qzlw9                  0/1     CrashLoopBackOff   466 (5m28s ago)   47h\r\nkube-scheduler-master0            1/1     Running            555 (10m ago)     47h\r\n```\r\n\r\nAnd kubelet manage to talk to the api server as it tries to start the coredns pods (they are pending because of the lack of CNI, but their definition is only found in etcd).\r\n\r\nSame thing for `https://10.96.0.1:443`, `https://localhost:6443` and `https://10.88.3.60:6443`:\r\n```\r\nroot@master0:~# while ! curl -k 'https://10.96.0.1'; do sleep 5; done\r\ncurl: (7) Failed to connect to 10.96.0.1 port 443 after 16 ms: Connection refused\r\n...\r\ncurl: (7) Failed to connect to 10.96.0.1 port 443 after 17 ms: Connection refused\r\n{\r\n  \"kind\": \"Status\",\r\n  \"apiVersion\": \"v1\",\r\n  \"metadata\": {},\r\n  \"status\": \"Failure\",\r\n  \"message\": \"forbidden: User \\\"system:anonymous\\\" cannot get path \\\"/\\\"\",\r\n  \"reason\": \"Forbidden\",\r\n  \"details\": {},\r\n  \"code\": 403\r\n}\r\n```\r\n\r\n```\r\nroot@master0:~# while ! curl -k 'https://localhost:6443'; do sleep 1; done\r\ncurl: (7) Failed to connect to localhost port 6443 after 0 ms: Connection refused\r\n...\r\ncurl: (7) Failed to connect to localhost port 6443 after 0 ms: Connection refused\r\n{\r\n  \"kind\": \"Status\",\r\n  \"apiVersion\": \"v1\",\r\n  \"metadata\": {},\r\n  \"status\": \"Failure\",\r\n  \"message\": \"forbidden: User \\\"system:anonymous\\\" cannot get path \\\"/\\\"\",\r\n  \"reason\": \"Forbidden\",\r\n  \"details\": {},\r\n  \"code\": 403\r\n}\r\n```\n---\n\nUser 'aojea' said:\n---\n/remove sig-network\r\n\r\nremoving the label until is clear it belongs to sig-network, but the description says\r\n\r\n> The same install works without any problem under ubuntu focal (20.04)\r\n> The install is 100% automated (the only change is the ubuntu version)\r\n\r\nI don't know , but it sounds more like an environment problem ... check the logs of the apiserver to understand why is he exiting, maybe there is a hint there\n---\n\nUser 'Congelli501' said:\n---\nThe api-server is killed by because it can't connect to etcd when it is down (so it fails its health check with a 500 error) and I can also see the `SandboxChanged` event too for the api server.\r\n\r\nThe etcd server doesn't crash, it is just constantly stopped and restarted. It seems to be caused by a `SandboxChanged` event, and it doesn't fail its health check.\r\n\r\n```\r\n  Normal   Created         115m (x163 over 16h)     kubelet  Created container etcd\r\n  Normal   Started         115m (x163 over 16h)     kubelet  Started container etcd\r\n  Warning  BackOff         34m (x2935 over 16h)     kubelet  Back-off restarting failed container\r\n  Normal   Killing         22m (x184 over 16h)      kubelet  Stopping container etcd\r\n  Normal   SandboxChanged  11m (x186 over 16h)      kubelet  Pod sandbox changed, it will be killed and re-created.\r\n  Normal   Pulled          11m (x183 over 16h)      kubelet  Container image \"k8s.gcr.io/etcd:3.5.3-0\" already present on machine\r\n```\r\n\r\nIn its logs, we can see that the etcd server was running fine for 3 minutes before beeing killed (it was up at `2022-05-26T10:41:46.837Z` and received a kill signal at `2022-05-26T10:44:35.558Z`).\r\n<details>\r\n```\r\nroot@master0:~# crictl -r unix:///run/containerd/containerd.sock logs 4fab77304c9b5\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:41:45.711Z\",\"caller\":\"etcdmain/etcd.go:73\",\"msg\":\"Running: \",\"args\":[\"etcd\",\"--advertise-client-urls=https://10.88.3.60:2379\",\"--cert-file=/etc/kubernetes/pki/etcd/server.crt\",\"--client-cert-auth=true\",\"--data-dir=/var/lib/etcd\",\"--experimental-initial-corrupt-check=true\",\"--initial-advertise-peer-urls=https://10.88.3.60:2380\",\"--initial-cluster=master0=https://10.88.3.60:2380\",\"--key-file=/etc/kubernetes/pki/etcd/server.key\",\"--listen-client-urls=https://127.0.0.1:2379,https://10.88.3.60:2379\",\"--listen-metrics-urls=http://127.0.0.1:2381\",\"--listen-peer-urls=https://10.88.3.60:2380\",\"--name=master0\",\"--peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt\",\"--peer-client-cert-auth=true\",\"--peer-key-file=/etc/kubernetes/pki/etcd/peer.key\",\"--peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\",\"--snapshot-count=10000\",\"--trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\"]}\r\n...\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:41:46.835Z\",\"caller\":\"etcdserver/server.go:2042\",\"msg\":\"published local member to cluster through raft\",\"local-member-id\":\"9d88ba3537208382\",\"local-member-attributes\":\"{Name:master0 ClientURLs:[https://10.88.3.60:2379]}\",\"request-path\":\"/0/members/9d88ba3537208382/attributes\",\"cluster-id\":\"92a615471ffeccdb\",\"publish-timeout\":\"7s\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:41:46.835Z\",\"caller\":\"embed/serve.go:98\",\"msg\":\"ready to serve client requests\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:41:46.835Z\",\"caller\":\"embed/serve.go:98\",\"msg\":\"ready to serve client requests\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:41:46.835Z\",\"caller\":\"etcdmain/main.go:44\",\"msg\":\"notifying init daemon\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:41:46.835Z\",\"caller\":\"etcdmain/main.go:50\",\"msg\":\"successfully notified init daemon\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:41:46.837Z\",\"caller\":\"embed/serve.go:188\",\"msg\":\"serving client traffic securely\",\"address\":\"10.88.3.60:2379\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:41:46.837Z\",\"caller\":\"embed/serve.go:188\",\"msg\":\"serving client traffic securely\",\"address\":\"127.0.0.1:2379\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:44:35.558Z\",\"caller\":\"osutil/interrupt_unix.go:64\",\"msg\":\"received signal; shutting down\",\"signal\":\"terminated\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:44:35.558Z\",\"caller\":\"embed/etcd.go:368\",\"msg\":\"closing etcd server\",\"name\":\"master0\",\"data-dir\":\"/var/lib/etcd\",\"advertise-peer-urls\":[\"https://10.88.3.60:2380\"],\"advertise-client-urls\":[\"https://10.88.3.60:2379\"]}\r\nWARNING: 2022/05/26 10:44:35 [core] grpc: addrConn.createTransport failed to connect to {10.88.3.60:2379 10.88.3.60:2379 <nil> 0 <nil>}. Err: connection error: desc = \"transport: Error while dialing dial tcp 10.88.3.60:2379: connect: connection refused\". Reconnecting...\r\nWARNING: 2022/05/26 10:44:35 [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1:2379 <nil> 0 <nil>}. Err: connection error: desc = \"transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused\". Reconnecting...\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:44:35.561Z\",\"caller\":\"etcdserver/server.go:1453\",\"msg\":\"skipped leadership transfer for single voting member cluster\",\"local-member-id\":\"9d88ba3537208382\",\"current-leader-member-id\":\"9d88ba3537208382\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:44:35.571Z\",\"caller\":\"embed/etcd.go:563\",\"msg\":\"stopping serving peer traffic\",\"address\":\"10.88.3.60:2380\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:44:35.572Z\",\"caller\":\"embed/etcd.go:568\",\"msg\":\"stopped serving peer traffic\",\"address\":\"10.88.3.60:2380\"}\r\n{\"level\":\"info\",\"ts\":\"2022-05-26T10:44:35.572Z\",\"caller\":\"embed/etcd.go:370\",\"msg\":\"closed etcd server\",\"name\":\"master0\",\"data-dir\":\"/var/lib/etcd\",\"advertise-peer-urls\":[\"https://10.88.3.60:2380\"],\"advertise-client-urls\":[\"https://10.88.3.60:2379\"]}\r\n```\r\n</details>\r\n\r\nThe kube-scheduler-master0 pod is crashing because it can't connect to the api server most of the time and is also getting the `SandboxChanged` event.\r\n\r\nI might be completely wrong about it, but it seems to me the root cause of all this is the `SandboxChanged` event that get fired constantly, making the pods restart. Every other errors (like the api server can't connect to etcd when it's down) just seem to be a consequence of it.\n---\n\nUser 'dcbw' said:\n---\n@Congelli501 Kubelet logs sandbox change reasons at V(2) so if you can attach kubelet logs we should be able to figure out why it's killing/re-creating the sandbox.\n---\n\nUser 'dcbw' said:\n---\nWe should see at least one of these messages in kubelet logs:\r\n\r\n```\r\nklog.V(2).InfoS(\"No sandbox for pod can be found. Need to start a new one\", \"pod\", klog.KObj(pod))\r\nklog.V(2).InfoS(\"Multiple sandboxes are ready for Pod. Need to reconcile them\", \"pod\", klog.KObj(pod))\r\nklog.V(2).InfoS(\"No ready sandbox for pod can be found. Need to start a new one\", \"pod\", klog.KObj(pod))\r\nklog.V(2).InfoS(\"Sandbox for pod has changed. Need to start a new one\", \"pod\", klog.KObj(pod))\r\nklog.V(2).InfoS(\"Sandbox for pod has no IP address. Need to start a new one\", \"pod\", klog.KObj(pod))\r\n```\n---\n\nUser 'Congelli501' said:\n---\nMy kubelet logs with `-v2` are indeed full of `No ready sandbox for pod can be found. Need to start a new one` messages:\r\n<details>\r\n\r\n```\r\n...\r\nMay 26 16:43:58 master0 kubelet[485]: I0526 16:43:58.743876     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/etcd-master0\"\r\nMay 26 16:44:06 master0 kubelet[485]: I0526 16:44:06.769150     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-proxy-qzlw9\"\r\nMay 26 16:44:28 master0 kubelet[485]: I0526 16:44:28.806304     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-scheduler-master0\"\r\nMay 26 16:45:54 master0 kubelet[485]: I0526 16:45:54.961485     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-proxy-qzlw9\"\r\nMay 26 16:45:59 master0 kubelet[485]: I0526 16:45:59.981402     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-scheduler-master0\"\r\nMay 26 16:46:14 master0 kubelet[485]: I0526 16:46:14.015592     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-controller-manager-master0\"\r\nMay 26 16:46:27 master0 kubelet[485]: I0526 16:46:27.044067     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-apiserver-master0\"\r\nMay 26 16:48:07 master0 kubelet[485]: I0526 16:48:07.223428     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-proxy-qzlw9\"\r\nMay 26 16:48:59 master0 kubelet[485]: I0526 16:48:59.318984     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-scheduler-master0\"\r\nMay 26 16:50:34 master0 kubelet[485]: I0526 16:50:34.480272     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-proxy-qzlw9\"\r\nMay 26 16:50:57 master0 kubelet[485]: I0526 16:50:57.527808     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/etcd-master0\"\r\nMay 26 16:51:49 master0 kubelet[485]: I0526 16:51:49.629923     485 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/kube-apiserver-master0\"\r\n```\r\n</details>\r\n\r\nTo limit the logs, I removed all but the etcd definition from `/etc/kubernetes/manifests`, and I see logs like these ones in the `containerd` logs when \"No ready sandbox for pod can be found. Need to start a new one\" is logged in kubelet:\r\n\r\n```\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.004773741Z\" level=info msg=\"StopContainer for \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" with timeout 30 (s)\"\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.006107608Z\" level=info msg=\"Stop container \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" with signal terminated\"\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.060508850Z\" level=info msg=\"shim disconnected\" id=93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.060559516Z\" level=warning msg=\"cleaning up after shim disconnected\" id=93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591 namespace=k8s.io\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.072275065Z\" level=info msg=\"StopContainer for \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" returns successfully\"\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.072829105Z\" level=info msg=\"Container to stop \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" must be in running or unknown state, current state \\\"CONTAINER_EXITED\\\"\"\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.939190231Z\" level=info msg=\"Container to stop \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" must be in running or unknown state, current state \\\"CONTAINER_EXITED\\\"\"\r\nMay 26 17:43:34 master0 containerd[402]: time=\"2022-05-26T17:43:34.136292727Z\" level=info msg=\"Container to stop \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" must be in running or unknown state, current state \\\"CONTAINER_EXITED\\\"\"\r\nMay 26 17:43:34 master0 containerd[402]: time=\"2022-05-26T17:43:34.550325593Z\" level=info msg=\"RemoveContainer for \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\"\"\r\nMay 26 17:43:34 master0 containerd[402]: time=\"2022-05-26T17:43:34.550903234Z\" level=info msg=\"Container to stop \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" must be in running or unknown state, current state \\\"CONTAINER_EXITED\\\"\"\r\nMay 26 17:43:34 master0 containerd[402]: time=\"2022-05-26T17:43:34.563524011Z\" level=info msg=\"RemoveContainer for \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" returns successfully\"\r\n```\r\n\r\n\r\nExtract of kubelet logs:\r\n<details>\r\n\r\n```\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.036935     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.137615     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: I0526 17:36:45.157868     483 kubelet_node_status.go:352] \"Setting node annotation to enable volume controller attach/detach\"\r\nMay 26 17:36:45 master0 kubelet[483]: I0526 17:36:45.158963     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasSufficientMemory\"\r\nMay 26 17:36:45 master0 kubelet[483]: I0526 17:36:45.159222     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasNoDiskPressure\"\r\nMay 26 17:36:45 master0 kubelet[483]: I0526 17:36:45.159488     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasSufficientPID\"\r\nMay 26 17:36:45 master0 kubelet[483]: I0526 17:36:45.159751     483 kubelet_node_status.go:70] \"Attempting to register node\" node=\"master0\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.160429     483 kubelet_node_status.go:92] \"Unable to register node with API server\" err=\"Post \\\"https://10.88.3.60:6443/api/v1/nodes\\\": dial tcp 10.88.3.60:6443: connect: connection refused\" node=\"master0\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.238120     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.339013     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.439231     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.540013     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.640659     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.741337     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: I0526 17:36:45.810357     483 csi_plugin.go:1063] Failed to contact API server when waiting for CSINode publishing: Get \"https://10.88.3.60:6443/apis/storage.k8s.io/v1/csinodes/master0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 26 17:36:45 master0 kubelet[483]: W0526 17:36:45.821036     483 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: Get \"https://10.88.3.60:6443/api/v1/nodes?fieldSelector=metadata.name%3Dmaster0&limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.821117     483 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://10.88.3.60:6443/api/v1/nodes?fieldSelector=metadata.name%3Dmaster0&limit=500&resourceVersion=0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.842186     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:45 master0 kubelet[483]: E0526 17:36:45.942992     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.001778     483 kubelet_node_status.go:352] \"Setting node annotation to enable volume controller attach/detach\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.002620     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasSufficientMemory\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.002909     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasNoDiskPressure\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.003163     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasSufficientPID\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.003424     483 eviction_manager.go:254] \"Eviction manager: failed to get summary stats\" err=\"failed to get node info: node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.004224     483 kuberuntime_container.go:722] \"Killing container with a grace period\" pod=\"kube-system/etcd-master0\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e containerName=\"etcd\" containerID=\"containerd://93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\" gracePeriod=30\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.043243     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.143849     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.244834     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.320749     483 kubelet.go:2344] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.345973     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.446711     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.547463     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.648269     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.749076     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.810584     483 csi_plugin.go:1063] Failed to contact API server when waiting for CSINode publishing: Get \"https://10.88.3.60:6443/apis/storage.k8s.io/v1/csinodes/master0\": dial tcp 10.88.3.60:6443: connect: connection refused\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.850061     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.936354     483 generic.go:296] \"Generic (PLEG): container finished\" podID=3da0b3b3f5b168e56c71dd2c6212a28e containerID=\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\" exitCode=0\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.936438     483 kubelet.go:2098] \"SyncLoop (PLEG): event for pod\" pod=\"kube-system/etcd-master0\" event=&{ID:3da0b3b3f5b168e56c71dd2c6212a28e Type:ContainerDied Data:93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591}\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.936485     483 kubelet.go:2098] \"SyncLoop (PLEG): event for pod\" pod=\"kube-system/etcd-master0\" event=&{ID:3da0b3b3f5b168e56c71dd2c6212a28e Type:ContainerDied Data:a6b4d0f34f4860749e9213893878900a1af4bebd06705a424a831660e6bfd94a}\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.936513     483 pod_container_deletor.go:79] \"Container not found in pod's containers\" containerID=\"a6b4d0f34f4860749e9213893878900a1af4bebd06705a424a831660e6bfd94a\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.936564     483 scope.go:110] \"RemoveContainer\" containerID=\"b1bafb0b8208aedda8d8720c7db6bee631326ae478c37f82f0f0b39b0f2e7746\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.936919     483 kubelet_node_status.go:352] \"Setting node annotation to enable volume controller attach/detach\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.938462     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasSufficientMemory\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.938545     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasNoDiskPressure\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.938566     483 kubelet_node_status.go:563] \"Recording event message for node\" node=\"master0\" event=\"NodeHasSufficientPID\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.938897     483 kuberuntime_manager.go:488] \"No ready sandbox for pod can be found. Need to start a new one\" pod=\"kube-system/etcd-master0\"\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.939290     483 status_manager.go:664] \"Failed to get status for pod\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e pod=\"kube-system/etcd-master0\" err=\"Get \\\"https://10.88.3.60:6443/api/v1/namespaces/kube-system/pods/etcd-master0\\\": dial tcp 10.88.3.60:6443: connect: connection refused\"\r\nMay 26 17:36:46 master0 kubelet[483]: E0526 17:36:46.950842     483 kubelet.go:2419] \"Error getting node\" err=\"node \\\"master0\\\" not found\"\r\n```\r\n</details>\r\n\r\nSo kubelet seems to decide to kill the etcd pod, but I don't know why:\r\n```\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.004224     483 kuberuntime_container.go:722] \"Killing container with a grace period\" pod=\"kube-system/etcd-master0\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e containerName=\"etcd\" containerID=\"containerd://93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\" gracePeriod=30\r\n```\r\n\r\nI attached the full `containerd` and `kubelet` logs:\r\n[full_kubelet_logs.txt](https://github.com/kubernetes/kubernetes/files/8781269/full_kubelet_logs.txt)\r\n[full_containerd_logs.txt](https://github.com/kubernetes/kubernetes/files/8781273/full_containerd_logs.txt)\n---\n\nUser 'vsxen' said:\n---\nsame question in Fedora \r\nhttps://github.com/containerd/containerd/issues/6704\n---\n\nUser 'Congelli501' said:\n---\nI tried to create a pod with `crictl` manually, and it stayed up:\r\n\r\npod-config.json\r\n``` json\r\n{\r\n    \"metadata\": {\r\n        \"name\": \"busybox\",\r\n        \"namespace\": \"default\",\r\n        \"attempt\": 1\r\n    },\r\n    \"log_directory\": \"/tmp\",\r\n    \"linux\": {\r\n      \"security_context\": {\"namespace_options\": {\"network\": 2}}\r\n    }\r\n}\r\n```\r\n\r\ncontainer-config.json\r\n``` json\r\n{\r\n  \"metadata\": {\r\n      \"name\": \"busybox\"\r\n  },\r\n  \"image\":{\r\n      \"image\": \"busybox\"\r\n  },\r\n  \"command\": [\r\n      \"top\"\r\n  ],\r\n  \"log_path\":\"busybox.0.log\",\r\n  \"linux\": {\r\n  }\r\n}\r\n```\r\n\r\n``` bash\r\ncrictl -r unix:///run/containerd/containerd.sock run container-config.json pod-config.json\r\n```\r\n\r\n```\r\nCONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID\r\n2a5c235a70220       aebe758cef4cd       6 minutes ago       Exited              etcd                1059                2586cdecbc207\r\n34b48303aaab1       busybox             7 minutes ago       Running             busybox             0                   add650e8059e4\r\n```\n---\n\nUser 'Congelli501' said:\n---\nStopping `kubelet` on the machine will prevent further restart of etcd, so `containerd` on its own doesn't restart the pod.\r\n\r\n```\r\nroot@master0:~# systemctl stop kubelet\r\n\r\nroot@master0:~# crictl -r unix:///run/containerd/containerd.sock ps -a\r\nCONTAINER           IMAGE               CREATED              STATE               NAME                ATTEMPT             POD ID\r\nfef6b1e164527       aebe758cef4cd       About a minute ago   Running             etcd                1060                fc02addc9a093\r\n34b48303aaab1       busybox             9 minutes ago        Running             busybox             0                   add650e8059e4\r\n```\r\n\r\n```\r\nroot@master0:~# crictl -r unix:///run/containerd/containerd.sock ps -a\r\nCONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID\r\nfef6b1e164527       aebe758cef4cd       29 minutes ago      Running             etcd                1060                fc02addc9a093\r\n34b48303aaab1       busybox             37 minutes ago      Running             busybox             0                   add650e8059e4\r\n```\n---\n\nUser 'vsxen' said:\n---\nhttps://twitter.com/ctrahey/status/1530386324749950976\r\nping @medyagh\n---\n\nUser 'htech7x' said:\n---\nHave the same issue.\r\nIs there a solution at the moment ?\n---\n\nUser 'dcbw' said:\n---\nFrom the logs, kubelet seems to think there are two containers in the etcd pod:\r\n\r\n```\r\nMay 26 17:05:17 master0 kubelet[483]: I0526 17:05:17.009874     483 kubelet.go:2098] \"SyncLoop (PLEG): event for pod\" pod=\"kube-system/etcd-master0\" event=&{ID:3da0b3b3f5b168e56c71dd2c6212a28e Type:ContainerStarted Data:16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1}\r\nMay 26 17:05:17 master0 kubelet[483]: I0526 17:05:17.009925     483 kubelet.go:2098] \"SyncLoop (PLEG): event for pod\" pod=\"kube-system/etcd-master0\" event=&{ID:3da0b3b3f5b168e56c71dd2c6212a28e Type:ContainerStarted Data:7fcce86b2abf8d029abe5322624280211ccdc8257c6fb3cdb7ae1ba3f535b31d}\r\n```\r\n\r\nBut containerd thinks there's only one:\r\n\r\n```\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.455252775Z\" level=info msg=\"RunPodsandbox for &PodSandboxMetadata{Name:etcd-master0,Uid:3da0b3b3f5b168e56c71dd2c6212a28e,Namespace:kube-system,A\r\nttempt:845,}\"\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.499404103Z\" level=info msg=\"starting signal loop\" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/7fcce86b2abf8d029ab\r\ne5322624280211ccdc8257c6fb3cdb7ae1ba3f535b31d pid=574\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.581129864Z\" level=info msg=\"RunPodSandbox for &PodSandboxMetadata{Name:etcd-master0,Uid:3da0b3b3f5b168e56c71dd2c6212a28e,Namespace:kube-system,A\r\nttempt:845,} returns sandbox id \\\"7fcce86b2abf8d029abe5322624280211ccdc8257c6fb3cdb7ae1ba3f535b31d\\\"\"\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.589599770Z\" level=info msg=\"CreateContainer within sandbox \\\"7fcce86b2abf8d029abe5322624280211ccdc8257c6fb3cdb7ae1ba3f535b31d\\\" for container &C\r\nontainerMetadata{Name:etcd,Attempt:842,}\"\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.636894203Z\" level=info msg=\"CreateContainer within sandbox \\\"7fcce86b2abf8d029abe5322624280211ccdc8257c6fb3cdb7ae1ba3f535b31d\\\" for &ContainerMetadata{Name:etcd,Attempt:842,} returns container id \\\"16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\\\"\"\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.637804180Z\" level=info msg=\"StartContainer for \\\"16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\\\"\"\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.705326261Z\" level=info msg=\"StartContainer for \\\"16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\\\" returns successfully\"\r\nMay 26 17:05:17 master0 containerd[402]: time=\"2022-05-26T17:05:17.012040611Z\" level=info msg=\"StopContainer for \\\"16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\\\" with timeout 30 (s)\"\r\nMay 26 17:05:17 master0 containerd[402]: time=\"2022-05-26T17:05:17.012363507Z\" level=info msg=\"Stop container \\\"16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\\\" with signal terminated\"\r\n```\r\n\r\ncould be an issue, but maybe I forget how containerd works. Seems odd though.\r\n\r\nBut really, we need more logging from kubelet to figure out why this happens:\r\n\r\n```\r\nMay 26 17:05:17 master0 kubelet[483]: I0526 17:05:17.011799     483 kuberuntime_container.go:722] \"Killing container with a grace period\" pod=\"kube-system/etcd-master0\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e containerName=\"etcd\" containerID=\"containerd://16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\" gracePeriod=30\r\n```\r\n\r\nAny chance you can bump kubelet logging to V=5?\n---\n\nUser 'elfadel' said:\n---\n+1\r\nHaving the same issue on Ubuntu 22.04.\r\n\r\n~ Update\r\nAnd also confirm that the issue does not appear on Ubuntu 20.04. I keep the same k8s installation process in both Ubuntu versions.\n---\n\nUser 'fabi200123' said:\n---\n**Found the solution:**\r\nThe problem with this is that if you install containerd by using `sudo apt install containerd` you will install the version v1.5.9 by default which has the option of `SystemdCgroup = false` (which worked for Ubuntu 20.04 but it does not work for Ubuntu 22.04). \r\nThe solution for this problem is either of those: \r\n- you install containerd v1.6.2 which has the `SystemdCgroup = true`\r\n(there is a tutorial here: https://www.itzgeek.com/how-tos/linux/ubuntu-how-tos/install-containerd-on-ubuntu-22-04.html )\r\n- or you change the config file of containerd v1.5.9 so it has this option set to true\r\nIn the config file of containerd v1.5.9 you will find something like this:\r\n```\r\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\r\n            BinaryName = \"\"\r\n            CriuImagePath = \"\"\r\n            CriuPath = \"\"\r\n            CriuWorkPath = \"\"\r\n            IoGid = 0\r\n            IoUid = 0\r\n            NoNewKeyring = false\r\n            NoPivotRoot = false\r\n            Root = \"\"\r\n            ShimCgroup = \"\"\r\n            SystemdCgroup = false\r\n```\r\nyou need to change it to be like this\r\n```\r\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\r\n            BinaryName = \"\"\r\n            CriuImagePath = \"\"\r\n            CriuPath = \"\"\r\n            CriuWorkPath = \"\"\r\n            IoGid = 0\r\n            IoUid = 0\r\n            NoNewKeyring = false\r\n            NoPivotRoot = false\r\n            Root = \"\"\r\n            ShimCgroup = \"\"\r\n            SystemdCgroup = true\r\n```\r\n\r\nIn case you can`t find the config for it, just create the file:\r\n\r\n```\r\nsudo mkdir /etc/containerd\r\nnano /etc/containerd/config.toml\r\n```\r\n\r\nand paste the content of the file attached here.\r\n[correct_config.txt](https://github.com/kubernetes/kubernetes/files/8948623/correct_config.txt)\r\nand then restart the containerd:\r\n`systemctl restart containerd`\n---\n\nUser 'JavadHosseini' said:\n---\nI have containerd version 1.6.6 and I still have this issue on Ubuntu 22.04.\n---\n\nUser 'sahil-kcx' said:\n---\nSame issue for me when using Ubuntu 22.04 and containerd version is 1.6.6\n---\n\nUser 'mpartel' said:\n---\nSame on Debian 11 with containerd 1.6.6.\r\n[Starting with the default containerd config](https://github.com/containerd/containerd/issues/6964#issuecomment-1132580240) with `containerd config default > /etc/containerd/config.toml` and editing `SystemdCgroup` to `true` seems to have fixed it.\n---\n\nUser 'Congelli501' said:\n---\nIs it possible to add a kubeadm check for this case, and even make kubelet fail on startup with an explicit log ?\r\n\r\nI suppose I'm not the best at diagnosing those kind of problem, but adding those checks would save days of debugging for other people / prevent people from giving up a kubernetes cluster setup.\n---\n\nUser 'EricoCartmanez' said:\n---\nYou have to tweak containerd as shown [here](https://stackoverflow.com/a/73743910/8679627) \r\n\r\n```\r\nsudo mkdir -p /etc/containerd/\r\n\r\ncontainerd config default | sudo tee /etc/containerd/config.toml\r\n\r\nsudo sed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml\r\n```\r\n\r\nThen reboot the server\n---\n\nUser 'Um4r-Arafath' said:\n---\nits 2023 December and i still had this issue on my Ubuntu 22.04 and I think I solved it after 3 days of troubleshooting\r\nI noticed something like this when setting `kubeadm init`\r\n\r\n`detected that the sandbox image \"registry.k8s.io/pause:3.6\" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using \"registry.k8s.io/pause:3.9\" as the CRI sandbox image.`\r\n\r\nso I set contained to default config using\r\n\r\n`containerd config default | sudo tee /etc/containerd/config.toml`\r\n\r\nand updated and set `SystemdCgroup = true` & `sandbox_image = \"registry.k8s.io/pause:3.9\"`\r\n\r\nAfter rebooting the server, everything should be up and running perfectly fine.\n---\n\nUser 'd123456temp' said:\n---\nIt's devastating that it is 2024.02.04 and I got hit by the same issue. 22.04 is pretty popular... Lost 3 days pulling my hairs out. The Kubernetes project is in a strange shape these days - was a heavy on premises user from early releases but starting to look for alternatives, this doesn't look like safe and stable choice any more.\r\n\r\nFellow users, thank you for investigating this.\r\n\r\nThis was not that straightforward to google up, so adding keyword phrases for future victims:\r\n- kubernetes k8s cluster stops responding to api calls several minutes after cluster bootstrap\r\n- The connection to the server :6443 was refused - did you specify the right host or port?\r\n- 22.04 kubernetes cannot connect to kubernetes api\r\n- jammy cannot connect to kubernetes api\r\n- kube-system pods keep restarting after cluster bootstrap\r\n- kube-apiserver stops responding after cluster bootstrap\n---\n\nUser 'fuch1m' said:\n---\nCame also accross this. The `SystemdCgroup` was not present at all after running `containerd config default | sudo tee /etc/containerd/config.toml`. But after I inserted `SystemdCgroup = true` it was running and pods didn't crash anymore.\n---\n\nUser 'henriqueccapozzi' said:\n---\n@Um4r-Arafath\nThank you very much for this solution \ud83d\ude4f\n---\n\nUser 'mikersays' said:\n---\nI wasted almost my entire weekend trying to troubleshoot this, but my deepest gratitude to @Um4r-Arafath for the excellent solution!\n---\n\nUser 'asdfsx' said:\n---\n@Um4r-Arafath @fabi200123\nIt works for me!\nThank you all very much!\n---",
    "question": "I'm getting an error: Constant kube-system pod restard due to SandboxChanged under ubuntu Jammy. Is there a known workaround?",
    "ideal_answer": "@Congelli501: \nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n/kind support\r\nWould you provide more kubelet logs to convince us that it is a Kubernetes bug or your node/configuration problem?\n\nIt seems all the logs are caused by the flaky apiserver that constantly restart and the fact that no CNI is initialized yet.\r\n\nFrom what I got from `kubelet describe`, the pod restart seems to be linked to the `SandboxChanged` event, which is fired on a `SandboxID` change on the container.\r\nhttps://github.com/kubernetes/kubernetes/blob/520b991347b9c77635c0e4555f1703a86dcdd4ff/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L721\r\nI can't find how sandboxID is built nor why it would change\r\n\nHere is the first 300 lines of the kubelet log after a reboot:\r\n\nYou cannot connect the apiserver from the node. There are two network interfaces.\r\n```\r\nsysctl -a | grep net.ipv4.conf.all.arp_filter\r\n```\r\nWould you check the route and arp_filter setting?\n\nHere are the arp_filter setting & routes:\r\n```\r\nroot@master0:~# sysctl -a | grep net.ipv4.conf.all.arp_filter\r\nnet.ipv4.conf.all.arp_filter = 0\r\n\nroot@master0:~# ip a\r\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\r\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\r\n    inet 127.0.0.1/8 scope host lo\r\n       valid_lft forever preferred_lft forever\r\n    inet6 ::1/128 scope host \r\n       valid_lft forever preferred_lft forever\r\n2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000\r\n    link/ether 52:54:00:8c:85:a9 brd ff:ff:ff:ff:ff:ff\r\n    inet 10.10.42.85/16 metric 100 brd 10.10.255.255 scope global dynamic enp1s0\r\n       valid_lft 39350sec preferred_lft 39350sec\r\n    inet6 fe80::5054:ff:fe8c:85a9/64 scope link \r\n       valid_lft forever preferred_lft forever\r\n3: enp2s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000\r\n    link/ether 52:54:00:c4:0c:36 brd ff:ff:ff:ff:ff:ff\r\n    inet 10.88.3.60/16 brd 10.88.255.255 scope global enp2s0\r\n       valid_lft forever preferred_lft forever\r\n    inet6 fe80::5054:ff:fec4:c36/64 scope link \r\n       valid_lft forever preferred_lft forever\r\n\nroot@master0:~# ip r\r\ndefault via 10.10.0.3 dev enp1s0 proto dhcp src 10.10.42.85 metric 100 \r\n10.10.0.0/16 dev enp1s0 proto kernel scope link src 10.10.42.85 metric 100 \r\n10.10.0.1 dev enp1s0 proto dhcp scope link src 10.10.42.85 metric 100 \r\n10.10.0.3 dev enp1s0 proto dhcp scope link src 10.10.42.85 metric 100 \r\n10.88.0.0/16 dev enp2s0 proto kernel scope link src 10.88.3.60 \r\n```\r\n\nThe api server is accessible when it is UP, it just doesn't stay up for long.\r\n```\r\nroot@master0:~# while ! kubectl get pods -n kube-system; do sleep 5; done\r\nThe connection to the server 10.88.3.60:6443 was refused - did you specify the right host or port?\r\nThe connection to the server 10.88.3.60:6443 was refused - did you specify the right host or port?\r\nNAME                              READY   STATUS             RESTARTS          AGE\r\ncoredns-6d4b75cb6d-jfp4t          0/1     Pending            0                 47h\r\ncoredns-6d4b75cb6d-nmcmb          0/1     Pending            0                 47h\r\netcd-master0                      1/1     Running            559 (10m ago)     47h\r\nkube-apiserver-master0            0/1     Running            534 (24s ago)     47h\r\nkube-controller-manager-master0   0/1     Running            552 (3m23s ago)   47h\r\nkube-proxy-qzlw9                  0/1     CrashLoopBackOff   466 (5m28s ago)   47h\r\nkube-scheduler-master0            1/1     Running            555 (10m ago)     47h\r\n```\r\n\nAnd kubelet manage to talk to the api server as it tries to start the coredns pods (they are pending because of the lack of CNI, but their definition is only found in etcd).\r\n\nSame thing for `https://10.96.0.1:443`, `https://localhost:6443` and `https://10.88.3.60:6443`:\r\n```\r\nroot@master0:~# while ! curl -k 'https://10.96.0.1'; do sleep 5; done\r\ncurl: (7) Failed to connect to 10.96.0.1 port 443 after 16 ms: Connection refused\r\n...\r\ncurl: (7) Failed to connect to 10.96.0.1 port 443 after 17 ms: Connection refused\r\n{\r\n  \"kind\": \"Status\",\r\n  \"apiVersion\": \"v1\",\r\n  \"metadata\": {},\r\n  \"status\": \"Failure\",\r\n  \"message\": \"forbidden: User \\\"system:anonymous\\\" cannot get path \\\"/\\\"\",\r\n  \"reason\": \"Forbidden\",\r\n  \"details\": {},\r\n  \"code\": 403\r\n}\r\n```\r\n\n```\r\nroot@master0:~# while ! curl -k 'https://localhost:6443'; do sleep 1; done\r\ncurl: (7) Failed to connect to localhost port 6443 after 0 ms: Connection refused\r\n...\r\ncurl: (7) Failed to connect to localhost port 6443 after 0 ms: Connection refused\r\n{\r\n  \"kind\": \"Status\",\r\n  \"apiVersion\": \"v1\",\r\n  \"metadata\": {},\r\n  \"status\": \"Failure\",\r\n  \"message\": \"forbidden: User \\\"system:anonymous\\\" cannot get path \\\"/\\\"\",\r\n  \"reason\": \"Forbidden\",\r\n  \"details\": {},\r\n  \"code\": 403\r\n}\r\n```\n\n/remove sig-network\r\n\nremoving the label until is clear it belongs to sig-network, but the description says\r\n\n> The same install works without any problem under ubuntu focal (20.04)\r\n> The install is 100% automated (the only change is the ubuntu version)\r\n\nI don't know , but it sounds more like an environment problem ... check the logs of the apiserver to understand why is he exiting, maybe there is a hint there\n\nThe api-server is killed by because it can't connect to etcd when it is down (so it fails its health check with a 500 error) and I can also see the `SandboxChanged` event too for the api server.\r\n\nThe etcd server doesn't crash, it is just constantly stopped and restarted. It seems to be caused by a `SandboxChanged` event, and it doesn't fail its health check.\r\n\n```\r\n  Normal   Created         115m (x163 over 16h)     kubelet  Created container etcd\r\n  Normal   Started         115m (x163 over 16h)     kubelet  Started container etcd\r\n  Warning  BackOff         34m (x2935 over 16h)     kubelet  Back-off restarting failed container\r\n  Normal   Killing         22m (x184 over 16h)      kubelet  Stopping container etcd\r\n  Normal   SandboxChanged  11m (x186 over 16h)      kubelet  Pod sandbox changed, it will be killed and re-created.\r\n  Normal   Pulled          11m (x183 over 16h)      kubelet  Container image \"k8s.gcr.io/etcd:3.5.3-0\" already present on machine\r\n```\r\n\nIn its logs, we can see that the etcd server was running fine for 3 minutes before beeing killed (it was up at `2022-05-26T10:41:46.837Z` and received a kill signal at `2022-05-26T10:44:35.558Z`).\r\n\nThe kube-scheduler-master0 pod is crashing because it can't connect to the api server most of the time and is also getting the `SandboxChanged` event.\r\n\nI might be completely wrong about it, but it seems to me the root cause of all this is the `SandboxChanged` event that get fired constantly, making the pods restart. Every other errors (like the api server can't connect to etcd when it's down) just seem to be a consequence of it.\n\n@Congelli501 Kubelet logs sandbox change reasons at V(2) so if you can attach kubelet logs we should be able to figure out why it's killing/re-creating the sandbox.\n\nWe should see at least one of these messages in kubelet logs:\r\n\n```\r\nklog.V(2).InfoS(\"No sandbox for pod can be found. Need to start a new one\", \"pod\", klog.KObj(pod))\r\nklog.V(2).InfoS(\"Multiple sandboxes are ready for Pod. Need to reconcile them\", \"pod\", klog.KObj(pod))\r\nklog.V(2).InfoS(\"No ready sandbox for pod can be found. Need to start a new one\", \"pod\", klog.KObj(pod))\r\nklog.V(2).InfoS(\"Sandbox for pod has changed. Need to start a new one\", \"pod\", klog.KObj(pod))\r\nklog.V(2).InfoS(\"Sandbox for pod has no IP address. Need to start a new one\", \"pod\", klog.KObj(pod))\r\n```\n\nMy kubelet logs with `-v2` are indeed full of `No ready sandbox for pod can be found. Need to start a new one` messages:\r\n\nTo limit the logs, I removed all but the etcd definition from `/etc/kubernetes/manifests`, and I see logs like these ones in the `containerd` logs when \"No ready sandbox for pod can be found. Need to start a new one\" is logged in kubelet:\r\n\n```\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.004773741Z\" level=info msg=\"StopContainer for \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" with timeout 30 (s)\"\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.006107608Z\" level=info msg=\"Stop container \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" with signal terminated\"\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.060508850Z\" level=info msg=\"shim disconnected\" id=93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.060559516Z\" level=warning msg=\"cleaning up after shim disconnected\" id=93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591 namespace=k8s.io\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.072275065Z\" level=info msg=\"StopContainer for \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" returns successfully\"\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.072829105Z\" level=info msg=\"Container to stop \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" must be in running or unknown state, current state \\\"CONTAINER_EXITED\\\"\"\r\nMay 26 17:36:46 master0 containerd[402]: time=\"2022-05-26T17:36:46.939190231Z\" level=info msg=\"Container to stop \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" must be in running or unknown state, current state \\\"CONTAINER_EXITED\\\"\"\r\nMay 26 17:43:34 master0 containerd[402]: time=\"2022-05-26T17:43:34.136292727Z\" level=info msg=\"Container to stop \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" must be in running or unknown state, current state \\\"CONTAINER_EXITED\\\"\"\r\nMay 26 17:43:34 master0 containerd[402]: time=\"2022-05-26T17:43:34.550325593Z\" level=info msg=\"RemoveContainer for \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\"\"\r\nMay 26 17:43:34 master0 containerd[402]: time=\"2022-05-26T17:43:34.550903234Z\" level=info msg=\"Container to stop \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" must be in running or unknown state, current state \\\"CONTAINER_EXITED\\\"\"\r\nMay 26 17:43:34 master0 containerd[402]: time=\"2022-05-26T17:43:34.563524011Z\" level=info msg=\"RemoveContainer for \\\"93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\\\" returns successfully\"\r\n```\r\n\nExtract of kubelet logs:\r\n\nSo kubelet seems to decide to kill the etcd pod, but I don't know why:\r\n```\r\nMay 26 17:36:46 master0 kubelet[483]: I0526 17:36:46.004224     483 kuberuntime_container.go:722] \"Killing container with a grace period\" pod=\"kube-system/etcd-master0\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e containerName=\"etcd\" containerID=\"containerd://93d2df37ce2e6a98fc9cc5773a486f60675eca29dc4a859752ec0b735c185591\" gracePeriod=30\r\n```\r\n\nI attached the full `containerd` and `kubelet` logs:\r\n[full_kubelet_logs.txt](https://github.com/kubernetes/kubernetes/files/8781269/full_kubelet_logs.txt)\r\n[full_containerd_logs.txt](https://github.com/kubernetes/kubernetes/files/8781273/full_containerd_logs.txt)\n\nsame question in Fedora \r\nhttps://github.com/containerd/containerd/issues/6704\n\nI tried to create a pod with `crictl` manually, and it stayed up:\r\n\npod-config.json\r\n``` json\r\n{\r\n    \"metadata\": {\r\n        \"name\": \"busybox\",\r\n        \"namespace\": \"default\",\r\n        \"attempt\": 1\r\n    },\r\n    \"log_directory\": \"/tmp\",\r\n    \"linux\": {\r\n      \"security_context\": {\"namespace_options\": {\"network\": 2}}\r\n    }\r\n}\r\n```\r\n\ncontainer-config.json\r\n``` json\r\n{\r\n  \"metadata\": {\r\n      \"name\": \"busybox\"\r\n  },\r\n  \"image\":{\r\n      \"image\": \"busybox\"\r\n  },\r\n  \"command\": [\r\n      \"top\"\r\n  ],\r\n  \"log_path\":\"busybox.0.log\",\r\n  \"linux\": {\r\n  }\r\n}\r\n```\r\n\n``` bash\r\ncrictl -r unix:///run/containerd/containerd.sock run container-config.json pod-config.json\r\n```\r\n\n```\r\nCONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID\r\n2a5c235a70220       aebe758cef4cd       6 minutes ago       Exited              etcd                1059                2586cdecbc207\r\n34b48303aaab1       busybox             7 minutes ago       Running             busybox             0                   add650e8059e4\r\n```\n\nStopping `kubelet` on the machine will prevent further restart of etcd, so `containerd` on its own doesn't restart the pod.\r\n\n```\r\nroot@master0:~# systemctl stop kubelet\r\n\nroot@master0:~# crictl -r unix:///run/containerd/containerd.sock ps -a\r\nCONTAINER           IMAGE               CREATED              STATE               NAME                ATTEMPT             POD ID\r\nfef6b1e164527       aebe758cef4cd       About a minute ago   Running             etcd                1060                fc02addc9a093\r\n34b48303aaab1       busybox             9 minutes ago        Running             busybox             0                   add650e8059e4\r\n```\r\n\n```\r\nroot@master0:~# crictl -r unix:///run/containerd/containerd.sock ps -a\r\nCONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID\r\nfef6b1e164527       aebe758cef4cd       29 minutes ago      Running             etcd                1060                fc02addc9a093\r\n34b48303aaab1       busybox             37 minutes ago      Running             busybox             0                   add650e8059e4\r\n```\n\nhttps://twitter.com/ctrahey/status/1530386324749950976\r\nping @medyagh\n\nHave the same issue.\r\nIs there a solution at the moment ?\n\nFrom the logs, kubelet seems to think there are two containers in the etcd pod:\r\n\n```\r\nMay 26 17:05:17 master0 kubelet[483]: I0526 17:05:17.009874     483 kubelet.go:2098] \"SyncLoop (PLEG): event for pod\" pod=\"kube-system/etcd-master0\" event=&{ID:3da0b3b3f5b168e56c71dd2c6212a28e Type:ContainerStarted Data:16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1}\r\nMay 26 17:05:17 master0 kubelet[483]: I0526 17:05:17.009925     483 kubelet.go:2098] \"SyncLoop (PLEG): event for pod\" pod=\"kube-system/etcd-master0\" event=&{ID:3da0b3b3f5b168e56c71dd2c6212a28e Type:ContainerStarted Data:7fcce86b2abf8d029abe5322624280211ccdc8257c6fb3cdb7ae1ba3f535b31d}\r\n```\r\n\nBut containerd thinks there's only one:\r\n\n```\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.455252775Z\" level=info msg=\"RunPodsandbox for &PodSandboxMetadata{Name:etcd-master0,Uid:3da0b3b3f5b168e56c71dd2c6212a28e,Namespace:kube-system,A\r\nttempt:845,}\"\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.499404103Z\" level=info msg=\"starting signal loop\" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/7fcce86b2abf8d029ab\r\ne5322624280211ccdc8257c6fb3cdb7ae1ba3f535b31d pid=574\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.581129864Z\" level=info msg=\"RunPodSandbox for &PodSandboxMetadata{Name:etcd-master0,Uid:3da0b3b3f5b168e56c71dd2c6212a28e,Namespace:kube-system,A\r\nttempt:845,} returns sandbox id \\\"7fcce86b2abf8d029abe5322624280211ccdc8257c6fb3cdb7ae1ba3f535b31d\\\"\"\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.589599770Z\" level=info msg=\"CreateContainer within sandbox \\\"7fcce86b2abf8d029abe5322624280211ccdc8257c6fb3cdb7ae1ba3f535b31d\\\" for container &C\r\nontainerMetadata{Name:etcd,Attempt:842,}\"\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.636894203Z\" level=info msg=\"CreateContainer within sandbox \\\"7fcce86b2abf8d029abe5322624280211ccdc8257c6fb3cdb7ae1ba3f535b31d\\\" for &ContainerMetadata{Name:etcd,Attempt:842,} returns container id \\\"16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\\\"\"\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.637804180Z\" level=info msg=\"StartContainer for \\\"16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\\\"\"\r\nMay 26 17:05:16 master0 containerd[402]: time=\"2022-05-26T17:05:16.705326261Z\" level=info msg=\"StartContainer for \\\"16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\\\" returns successfully\"\r\nMay 26 17:05:17 master0 containerd[402]: time=\"2022-05-26T17:05:17.012040611Z\" level=info msg=\"StopContainer for \\\"16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\\\" with timeout 30 (s)\"\r\nMay 26 17:05:17 master0 containerd[402]: time=\"2022-05-26T17:05:17.012363507Z\" level=info msg=\"Stop container \\\"16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\\\" with signal terminated\"\r\n```\r\n\ncould be an issue, but maybe I forget how containerd works. Seems odd though.\r\n\nBut really, we need more logging from kubelet to figure out why this happens:\r\n\n```\r\nMay 26 17:05:17 master0 kubelet[483]: I0526 17:05:17.011799     483 kuberuntime_container.go:722] \"Killing container with a grace period\" pod=\"kube-system/etcd-master0\" podUID=3da0b3b3f5b168e56c71dd2c6212a28e containerName=\"etcd\" containerID=\"containerd://16f475cf6fd166345aacaa921ac44adb9a31079fe40d2cfed787393012bc01b1\" gracePeriod=30\r\n```\r\n\nAny chance you can bump kubelet logging to V=5?\n\n+1\r\nHaving the same issue on Ubuntu 22.04.\r\n\n~ Update\r\nAnd also confirm that the issue does not appear on Ubuntu 20.04. I keep the same k8s installation process in both Ubuntu versions.\n\n**Found the solution:**\r\nThe problem with this is that if you install containerd by using `sudo apt install containerd` you will install the version v1.5.9 by default which has the option of `SystemdCgroup = false` (which worked for Ubuntu 20.04 but it does not work for Ubuntu 22.04). \r\nThe solution for this problem is either of those: \r\n- you install containerd v1.6.2 which has the `SystemdCgroup = true`\r\n(there is a tutorial here: https://www.itzgeek.com/how-tos/linux/ubuntu-how-tos/install-containerd-on-ubuntu-22-04.html )\r\n- or you change the config file of containerd v1.5.9 so it has this option set to true\r\nIn the config file of containerd v1.5.9 you will find something like this:\r\n```\r\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\r\n            BinaryName = \"\"\r\n            CriuImagePath = \"\"\r\n            CriuPath = \"\"\r\n            CriuWorkPath = \"\"\r\n            IoGid = 0\r\n            IoUid = 0\r\n            NoNewKeyring = false\r\n            NoPivotRoot = false\r\n            Root = \"\"\r\n            ShimCgroup = \"\"\r\n            SystemdCgroup = false\r\n```\r\nyou need to change it to be like this\r\n```\r\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\r\n            BinaryName = \"\"\r\n            CriuImagePath = \"\"\r\n            CriuPath = \"\"\r\n            CriuWorkPath = \"\"\r\n            IoGid = 0\r\n            IoUid = 0\r\n            NoNewKeyring = false\r\n            NoPivotRoot = false\r\n            Root = \"\"\r\n            ShimCgroup = \"\"\r\n            SystemdCgroup = true\r\n```\r\n\nIn case you can`t find the config for it, just create the file:\r\n\n```\r\nsudo mkdir /etc/containerd\r\nnano /etc/containerd/config.toml\r\n```\r\n\nand paste the content of the file attached here.\r\n[correct_config.txt](https://github.com/kubernetes/kubernetes/files/8948623/correct_config.txt)\r\nand then restart the containerd:\r\n`systemctl restart containerd`\n\nI have containerd version 1.6.6 and I still have this issue on Ubuntu 22.04.\n\nSame issue for me when using Ubuntu 22.04 and containerd version is 1.6.6\n\nSame on Debian 11 with containerd 1.6.6.\r\n[Starting with the default containerd config](https://github.com/containerd/containerd/issues/6964#issuecomment-1132580240) with `containerd config default > /etc/containerd/config.toml` and editing `SystemdCgroup` to `true` seems to have fixed it.\n\nIs it possible to add a kubeadm check for this case, and even make kubelet fail on startup with an explicit log ?\r\n\nI suppose I'm not the best at diagnosing those kind of problem, but adding those checks would save days of debugging for other people / prevent people from giving up a kubernetes cluster setup.\n\nYou have to tweak containerd as shown [here](https://stackoverflow.com/a/73743910/8679627) \r\n\n```\r\nsudo mkdir -p /etc/containerd/\r\n\ncontainerd config default | sudo tee /etc/containerd/config.toml\r\n\nsudo sed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml\r\n```\r\n\nThen reboot the server\n\nits 2023 December and i still had this issue on my Ubuntu 22.04 and I think I solved it after 3 days of troubleshooting\r\nI noticed something like this when setting `kubeadm init`\r\n\n`detected that the sandbox image \"registry.k8s.io/pause:3.6\" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using \"registry.k8s.io/pause:3.9\" as the CRI sandbox image.`\r\n\nso I set contained to default config using\r\n\n`containerd config default | sudo tee /etc/containerd/config.toml`\r\n\nand updated and set `SystemdCgroup = true` & `sandbox_image = \"registry.k8s.io/pause:3.9\"`\r\n\nAfter rebooting the server, everything should be up and running perfectly fine.\n\nIt's devastating that it is 2024.02.04 and I got hit by the same issue. 22.04 is pretty popular... Lost 3 days pulling my hairs out. The Kubernetes project is in a strange shape these days - was a heavy on premises user from early releases but starting to look for alternatives, this doesn't look like safe and stable choice any more.\r\n\nFellow users, thank you for investigating this.\r\n\nThis was not that straightforward to google up, so adding keyword phrases for future victims:\r\n- kubernetes k8s cluster stops responding to api calls several minutes after cluster bootstrap\r\n- The connection to the server :6443 was refused - did you specify the right host or port?\r\n- 22.04 kubernetes cannot connect to kubernetes api\r\n- jammy cannot connect to kubernetes api\r\n- kube-system pods keep restarting after cluster bootstrap\r\n- kube-apiserver stops responding after cluster bootstrap\n\nCame also accross this. The `SystemdCgroup` was not present at all after running `containerd config default | sudo tee /etc/containerd/config.toml`. But after I inserted `SystemdCgroup = true` it was running and pods didn't crash anymore.\n\n@Um4r-Arafath\nThank you very much for this solution \ud83d\ude4f\n\nI wasted almost my entire weekend trying to troubleshoot this, but my deepest gratitude to @Um4r-Arafath for the excellent solution!\n\n@Um4r-Arafath @fabi200123\nIt works for me!\nThank you all very much!"
  },
  {
    "id": "gen_nat_071",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [Compiler Bug]: eslint-plugin-react-compiler has incorrect type definitions\n\nA user reported the following issue titled '[Compiler Bug]: eslint-plugin-react-compiler has incorrect type definitions' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://stackblitz.com/edit/vitejs-vite-c52sauuy?file=eslint.config.js\n\n### Repro steps\n\n* Install eslint-plugin-react-compiler version 19.0.0-beta-bafa41b-20250307\n* Add `reactCompiler.configs.recommended` to eslint.config.js (with `@ts-check` enabled)\n* `npx tsc -b`\n\nError:\n```\neslint.config.js:11:3 - error TS2345: Argument of type '{ plugins: { 'react-compiler': { rules: { 'react-compiler': RuleModule; }; }; }; rules: { 'react-compiler/react-compiler': string; }; }' is not assignable to parameter of type 'InfiniteDepthConfigWithExtends'.\n  Type '{ plugins: { 'react-compiler': { rules: { 'react-compiler': RuleModule; }; }; }; rules: { 'react-compiler/react-compiler': string; }; }' is not assignable to type 'ConfigWithExtends'.\n    Types of property 'rules' are incompatible.\n      Type '{ 'react-compiler/react-compiler': string; }' is not assignable to type 'Partial<Record<string, RuleEntry>>'.\n        Property ''react-compiler/react-compiler'' is incompatible with index signature.\n          Type 'string' is not assignable to type 'RuleEntry | undefined'.\n\n11   reactCompiler.configs.recommended,\n     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nFound 1 error.\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.0.0\n\n### What version of React Compiler are you using?\n\n19.0.0-beta-bafa41b-20250307\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'mofeiZ' said:\n---\nThanks for the report! Looks like this should be fixed if we exported `declare const configs = { recommended: {... rules: 'error' | 'off' | 'warn'}}` instead of `rules: string`. e.g. something like adding `'error' as const` to [this line](https://github.com/facebook/react/blob/443b7ff2a8437f7736491ae7136c21d75d5a2019/compiler/packages/eslint-plugin-react-compiler/src/index.ts#L24)\n\n@poteto Do you know how the released `index.d.ts` is generated? It doesn't look to be in our [build script](https://github.com/facebook/react/blob/443b7ff2a8437f7736491ae7136c21d75d5a2019/compiler/packages/eslint-plugin-react-compiler/scripts/build.js)\n---\n\nUser 'printfn' said:\n---\nThis seems to be fixed as of `19.0.0-beta-aeaed83-20250323`. Thanks!\n---",
    "question": "Could you explain how to resolve ': eslint-plugin-react-compiler has incorrect type definitions'?",
    "ideal_answer": "Thanks for the report! Looks like this should be fixed if we exported `declare const configs = { recommended: {... rules: 'error' | 'off' | 'warn'}}` instead of `rules: string`. e.g. something like adding `'error' as const` to [this line](https://github.com/facebook/react/blob/443b7ff2a8437f7736491ae7136c21d75d5a2019/compiler/packages/eslint-plugin-react-compiler/src/index.ts#L24)\n\n@poteto Do you know how the released `index.d.ts` is generated? It doesn't look to be in our [build script](https://github.com/facebook/react/blob/443b7ff2a8437f7736491ae7136c21d75d5a2019/compiler/packages/eslint-plugin-react-compiler/scripts/build.js)\n\nThis seems to be fixed as of `19.0.0-beta-aeaed83-20250323`. Thanks!"
  },
  {
    "id": "gen_nat_072",
    "category": "troubleshooting",
    "source_file": "kubernetes_kubernetes_issue.json",
    "context": "Repository: kubernetes/kubernetes\nTitle: `kubectl port-forward` initially fails with `query parameter \"port\" is required` before succeeding on fallback\n\nA user reported the following issue titled '`kubectl port-forward` initially fails with `query parameter \"port\" is required` before succeeding on fallback' in the 'kubernetes/kubernetes' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### Description\nWhen running kubectl port-forward, the initial upgrade attempt fails with a `400 Bad Request`:\n```\nHTTP/1.1 400 Bad Request\nContent-Length: 34\nContent-Type: text/plain; charset=utf-8\nDate: Thu, 04 Sep 2025 19:55:42 GMT\nquery parameter \"port\" is required\n```\n\nAfter this failure, kubectl falls back to a secondary dialer and successfully establishes the port-forward tunnel. \n\n### Versioning\nClient Version: v1.34.0 (darwin/arm64)\nKustomize Version: v5.7.1\nServer Version: v1.30.8\nOS: macOS Sequoia 15.6.1\n\n### Steps to Reproduce\nRun `kubectl port-forward` with `--v=8`:\n```\nI0905 15:02:08.184600   19436 tunneling_dialer.go:75] Before WebSocket Upgrade Connection...\nI0905 15:02:08.184634   19436 round_trippers.go:527] \"Request\" verb=\"GET\" url=\"https://<REDACTED>/portforward\" headers=<\n    Sec-Websocket-Protocol: SPDY/3.1+portforward.k8s.io\n    User-Agent: kubectl/v1.34.0 (darwin/arm64) kubernetes/f28b4c9\n>\nI0905 15:02:08.335009   19436 round_trippers.go:632] \"Response\" status=\"\" headers=\"\" milliseconds=150\nI0905 15:02:08.335078   19436 fallback_dialer.go:53] fallback to secondary dialer from primary dialer err: unable to upgrade streaming request: websocket: bad handshake (400 Bad Request): query parameter \"port\" is required\nI0905 15:02:08.335156   19436 round_trippers.go:527] \"Request\" verb=\"POST\" url=\"https://<REDACTED>/portforward\" headers=<\n    User-Agent: kubectl/v1.34.0 (darwin/arm64) kubernetes/f28b4c9\n    X-Stream-Protocol-Version: portforward.k8s.io\n>\nI0905 15:02:08.511107   19436 round_trippers.go:632] \"Response\" status=\"101 Switching Protocols\" headers=<\n    Connection: Upgrade\n    Date: Fri, 05 Sep 2025 19:02:08 GMT\n    Upgrade: SPDY/3.1\n    X-Stream-Protocol-Version: portforward.k8s.io\n> milliseconds=175\nForwarding from 127.0.0.1:7000 -> 80\nForwarding from [::1]:7000 -> 80\n```\n\nAs you can see in the logs above, the POST request does not include any query parameters.\n\n### Expected Behavior\n`kubectl port-forward` should negotiate the correct upgrade protocol without an initial 400 error.\n\n### Expected Fix\n`kubectl port-forward` should send the port information as a query parameter OR the error check should be updated.\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'k8s-ci-robot' said:\n---\nThis issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'ded-jajmera' said:\n---\n/kind bug\n---\n\nUser 'k8s-ci-robot' said:\n---\n@ded-jajmera: The label(s) `kind//sig, kind/cli, kind//sig, kind/api-machinery` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133913#issuecomment-3259459455):\n\n>/kind bug /sig cli /sig api-machinery\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---\n\nUser 'ded-jajmera' said:\n---\n/sig cli\n/sig api-machinery\n---\n\nUser 'aojea' said:\n---\n> Server Version: v1.30.8\n\n\nportforward is enabled by default in 1.31 https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/4006-transition-spdy-to-websockets so the server \"does not understand\" the client and falls back\n---\n\nUser 'itzPranshul' said:\n---\nIt might be my lack of understanding, but kubectl v1.31+ uses webSocket by default for streaming and SPDY on fallback and server 1.30 only supports the SPDY protocol for streaming, so you see the error when trying to use webSocket initially and succeeds with the SPDY. \n\nPossible fix - we can update kubectl logic to detect older API servers and skip WebSocket attempt, going straight to SPDY.This avoids the initial failed request and extra latency.\n---\n\nUser 'aojea' said:\n---\nthis is working as intended, the error is harmless, in case the error message worries you can switch  kubectl to use the same version as the server\n---\n\nUser 'ded-jajmera' said:\n---\nSure, that makes sense, thanks @aojea @itzPranshul. I like @itzPranshul's suggestion of skipping the initial WebSocket attempt with older API servers\n---\n\nUser 'aojea' said:\n---\n> Sure, that makes sense, thanks [@aojea](https://github.com/aojea) [@itzPranshul](https://github.com/itzPranshul). I like [@itzPranshul](https://github.com/itzPranshul)'s suggestion of skipping the initial WebSocket attempt with older API servers\n\n1.30 is EOL https://kubernetes.io/releases/\n---\n\nUser 'mpuckett159' said:\n---\n/close\nworking as expected\n---\n\nUser 'k8s-ci-robot' said:\n---\n@mpuckett159: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/133913#issuecomment-3275761642):\n\n>/close\n>working as expected\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>\n---",
    "question": "I'm running into an issue with kubernetes/kubernetes: `kubectl port-forward` initially fails with `query parameter \"port\" is required` before succeeding on fallback. How can I fix this?",
    "ideal_answer": "If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n/kind bug\n\n@ded-jajmera: The label(s) `kind//sig, kind/cli, kind//sig, kind/api-machinery` cannot be applied, because the repository doesn't have them.\n\n> Server Version: v1.30.8\n\nportforward is enabled by default in 1.31 https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/4006-transition-spdy-to-websockets so the server \"does not understand\" the client and falls back\n\nIt might be my lack of understanding, but kubectl v1.31+ uses webSocket by default for streaming and SPDY on fallback and server 1.30 only supports the SPDY protocol for streaming, so you see the error when trying to use webSocket initially and succeeds with the SPDY. \n\nPossible fix - we can update kubectl logic to detect older API servers and skip WebSocket attempt, going straight to SPDY.This avoids the initial failed request and extra latency.\n\nthis is working as intended, the error is harmless, in case the error message worries you can switch  kubectl to use the same version as the server\n\nSure, that makes sense, thanks @aojea @itzPranshul. I like @itzPranshul's suggestion of skipping the initial WebSocket attempt with older API servers\n\n> Sure, that makes sense, thanks [@aojea](https://github.com/aojea) [@itzPranshul](https://github.com/itzPranshul). I like [@itzPranshul](https://github.com/itzPranshul)'s suggestion of skipping the initial WebSocket attempt with older API servers\n\n1.30 is EOL https://kubernetes.io/releases/\n\nworking as expected\n\n@mpuckett159: Closing this issue."
  },
  {
    "id": "gen_nat_073",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [DevTools Bug] Cannot add node \"1\" because a node with that id is already in the Store.\n\nA user reported the following issue titled '[DevTools Bug] Cannot add node \"1\" because a node with that id is already in the Store.' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### Website or app\n\nlocal repo\n\n### Repro steps\n\njust loaded the local  react app\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n5.3.1-ccb20cb88b\n\n### Error message (automated)\n\nCannot add node \"1\" because a node with that id is already in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1172435\r\n    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1141877)\r\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1143565\r\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1551564)\n```\n\n\n### Error component stack (automated)\n\n_No response_\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'Marvinrose' said:\n---\nHello,\r\nI\u2019m interested in working on this issue. Before I do so, I wanted to check if anyone is currently working on it or if there are any ongoing discussions that I should be aware of.\r\nIf it's available, I\u2019d love to take a look and contribute. Please let me know!\r\nThanks!\n---\n\nUser 'eric-gitta-moore' said:\n---\n+1\r\nI'm followed https://zh-hans.react.dev/learn/react-developer-tools#safari-and-other-browsers\r\n\r\n```\r\nUncaught Error: \r\nCannot add node \"1\" because a node with that id is already in the Store.\r\n```\r\n\r\n```\r\nThe error was thrown \r\nat /Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1043404\r\n    at f.emit (/Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:989541)\r\n    at /Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:991083\r\n    at /Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1392030\r\n    at Array.forEach (<anonymous>)\r\n    at e.onmessage (/Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1392013)\r\n    at H.s (/Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:404466)\r\n    at H.emit (node:events:513:28)\r\n    at e.exports.B (/Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:438611)\r\n    at e.exports.emit (node:events:513:28)\r\n\r\n```\n---\n\nUser 'stealth90' said:\n---\n+1\n---\n\nUser 'ramansaini14' said:\n---\n+1\n---\n\nUser 'Not-James-Bond' said:\n---\n+1 .Facing this with Firefox.\r\n\r\n\r\n`\r\nUncaught Error: Cannot add node \"1\" because a node with that id is already in the Store.\r\n`\r\n\r\n`\r\nThe error was thrown emit@moz-extension://35d01427-9a13-4135-8eff-8ba691c5a227/build/main.js:1:1141877\r\nv/this._wallUnlisten<@moz-extension://35d01427-9a13-4135-8eff-8ba691c5a227/build/main.js:1:1143565\r\nbridgeListener@moz-extension://35d01427-9a13-4135-8eff-8ba691c5a227/build/main.js:1:1551393\r\n`\n---\n\nUser 'jasperfernandez' said:\n---\n+1 Facing this with Microsoft Edge\n---\n\nUser 'konoufo' said:\n---\n+1\n---\n\nUser 'KunalSukhija-Fareye' said:\n---\n+1 facing in Google Chrome\n---\n\nUser 'RPSingh0' said:\n---\n+1 Facing this issue in Microsoft Edge\n---\n\nUser 'kayquealmeida' said:\n---\n+1 facing this issue in Edge\n---\n\nUser 'GLEF1X' said:\n---\n+1 facing this issue in Google Chrome Version 128.0.6613.138 (Official Build) (arm64)\n---\n\nUser 'ZandercraftGames' said:\n---\n+1 Facing this issue in Brave browser\r\n```\r\n1.69.168 Chromium: 128.0.6613.138 (Official Build) (64-bit)\r\nWindows 11 Version 23H2 (Build 22631.4169)\r\n```\n---\n\nUser 'eps1lon' said:\n---\nWe fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue.\n---",
    "question": "Help needed with facebook/react. Cannot add node \"1\" because a node with that id is already in the Store..",
    "ideal_answer": "Hello,\r\nI\u2019m interested in working on this issue. Before I do so, I wanted to check if anyone is currently working on it or if there are any ongoing discussions that I should be aware of.\r\nIf it's available, I\u2019d love to take a look and contribute. Please let me know!\r\nThanks!\n\n+1\r\nI'm followed https://zh-hans.react.dev/learn/react-developer-tools#safari-and-other-browsers\r\n\n```\r\nUncaught Error: \r\nCannot add node \"1\" because a node with that id is already in the Store.\r\n```\r\n\n```\r\nThe error was thrown \r\nat /Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1043404\r\n    at f.emit (/Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:989541)\r\n    at /Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:991083\r\n    at /Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1392030\r\n    at Array.forEach (<anonymous>)\r\n    at e.onmessage (/Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:1392013)\r\n    at H.s (/Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:404466)\r\n    at H.emit (node:events:513:28)\r\n    at e.exports.B (/Users/admin/.nvm/versions/node/v20.14.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:2:438611)\r\n    at e.exports.emit (node:events:513:28)\r\n\n```\n\n+1\n\n+1\n\n+1 .Facing this with Firefox.\r\n\n`\r\nUncaught Error: Cannot add node \"1\" because a node with that id is already in the Store.\r\n`\r\n\n`\r\nThe error was thrown emit@moz-extension://35d01427-9a13-4135-8eff-8ba691c5a227/build/main.js:1:1141877\r\nv/this._wallUnlisten<@moz-extension://35d01427-9a13-4135-8eff-8ba691c5a227/build/main.js:1:1143565\r\nbridgeListener@moz-extension://35d01427-9a13-4135-8eff-8ba691c5a227/build/main.js:1:1551393\r\n`\n\n+1 Facing this with Microsoft Edge\n\n+1\n\n+1 facing in Google Chrome\n\n+1 Facing this issue in Microsoft Edge\n\n+1 facing this issue in Edge\n\n+1 facing this issue in Google Chrome Version 128.0.6613.138 (Official Build) (arm64)\n\n+1 Facing this issue in Brave browser\r\n```\r\n1.69.168 Chromium: 128.0.6613.138 (Official Build) (64-bit)\r\nWindows 11 Version 23H2 (Build 22631.4169)\r\n```\n\nWe fixed some scenarios where this could occur in React DevTools 7.0. If this issue still persists, please open a new issue."
  },
  {
    "id": "gen_nat_074",
    "category": "troubleshooting",
    "source_file": "facebook_react_issue.json",
    "context": "Repository: facebook/react\nTitle: [Compiler Bug]: Make memoization more granular\n\nA user reported the following issue titled '[Compiler Bug]: Make memoization more granular' in the 'facebook/react' repository. Please provide a summary of the discussion that led to its resolution.\n\nISSUE DESCRIPTION:\n### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBYPjAIy8ABb4AvvgC8+KAoDKilUPH5J02UTK4y6rToUARK2WN0TZujPwBbMnwSETtZ2hM74APyW1gB0vgJMuAjeWgB8+AlJwsj4ANoAuuLuUp40TNQIMAFBNtokZYkw1fxxmFXOADTyhnDKgoVuEpW4sGwAPABKCHTEjWGh1prAdeWVgc4aAPQp4mriIGpAA\n\n### Repro steps\n\nExample is provided in the [playground link](https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBYPjAIy8ABb4AvvgC8+KAoDKilUPH5J02UTK4y6rToUARK2WN0TZujPwBbMnwSETtZ2hM74APyW1gB0vgJMuAjeWgB8+AlJwsj4ANoAuuLuUp40TNQIMAFBNtokZYkw1fxxmFXOADTyhnDKgoVuEpW4sGwAPABKCHTEjWGh1prAdeWVgc4aAPQp4mriIGpAA)\n\nIn this example I would expect react compiler to memoize `mappedData` separately from the `filteredData` as I don't want to re-do the mapping each time when search string changes. \nWhat I would expect is for `filteredData` to have it's own memoization block with the `search` and `mappedData` as dependencies.\n\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19\n\n### What version of React Compiler are you using?\n\nlatest\n\nResolution/Discussion:\nThe issue was addressed with the following discussion:\n\nUser 'oleksii-kononykhin' said:\n---\nAs a workaround I can try to make some of the functions \"hooks\" with explicit `'use memo'` directives, but I don't think this is the best way to go\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBYPjAIy8ABb4AvvgC8+KAoDKilUPH5J02UTK4y6rToUARK2WN0TZujPwBbMnwSETtZ2hM74APyW1gB0vgJMuAjeWgB8+AlJwsj4ANoAuuLuUp40TNQIMAFBNtq6CABiZYmVgc78cZhVzgA08oZwyoKFbhKVuLBsADwASgh0xDDVUWSawCRNFV3WGgD0KeJqw+K0DMys9g0bLdXtfp2t1r0KSgPCYhIA5HU+SRAf7mMJvh1uVFm0OlsyE9+oMDiA1EA\n---\n\nUser 'NeutronDisk' said:\n---\nIt is using react-stick controller to render react-compass view in tic tac toe board model  \n```\ndigital_board: [1, 2, 3, 4, 5, 6, 7, 8, 9];\nbyte_board: [x/o, x/o, x/o, x/o, x/o, x/o, x/o, x/o, x/o];\nrun_stick: 4*byte_board - digital_board;\nroute('/');\n```\n![Image](https://github.com/user-attachments/assets/073db9be-7f2c-4117-bedf-770dcb71cf1f)\n---\n\nUser 'ogbukosichukwu63' said:\n---\n> ### What kind of issue is this?\n> * [x]  React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)[ ]  babel-plugin-react-compiler (build issue installing or using the Babel plugin)[ ]  eslint-plugin-react-compiler (build issue installing or using the eslint plugin)[ ]  react-compiler-healthcheck (build issue installing or using the healthcheck script)\n> \n> ### Link to repro\n> https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBYPjAIy8ABb4AvvgC8+KAoDKilUPH5J02UTK4y6rToUARK2WN0TZujPwBbMnwSETtZ2hM74APyW1gB0vgJMuAjeWgB8+AlJwsj4ANoAuuLuUp40TNQIMAFBNtokZYkw1fxxmFXOADTyhnDKgoVuEpW4sGwAPABKCHTEjWGh1prAdeWVgc4aAPQp4mriIGpAA\n> \n> ### Repro steps\n> Example is provided in the [playground link](https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBYPjAIy8ABb4AvvgC8+KAoDKilUPH5J02UTK4y6rToUARK2WN0TZujPwBbMnwSETtZ2hM74APyW1gB0vgJMuAjeWgB8+AlJwsj4ANoAuuLuUp40TNQIMAFBNtokZYkw1fxxmFXOADTyhnDKgoVuEpW4sGwAPABKCHTEjWGh1prAdeWVgc4aAPQp4mriIGpAA)\n> \n> In this example I would expect react compiler to memoize `mappedData` separately from the `filteredData` as I don't want to re-do the mapping each time when search string changes. What I would expect is for `filteredData` to have it's own memoization block with the `search` and `mappedData` as dependencies.\n> \n> ### How often does this bug happen?\n> Every time\n> \n> ### What version of React are you using?\n> 19\n> \n> ### What version of React Compiler are you using?\n> latest\n\nogbukosiw88p3v5\n---\n\nUser 'mofeiZ' said:\n---\nThanks for the report! This isn't a bug as much as React Compiler not having access to function parameter types + effects. Right now, there's no way for us to understand that `filterData` does not write to its arguments. Replacing your filterData call with something that writes to its arguments, we see that it's invalid to memoize `mappedData` and `filteredData` separately.\n\n```js\nfunction appendIds(data, makeId) {\n  for (const entry of data) {\n    entry.id += makeId(entry);\n  }\n  return data;\n}\nexport default function MyApp() {\n  const { makeId } = useMakeId()\n  const { data } = useData()\n\n  const mappedData = data ? data.map(item => item) : []\n  const resultData = appendIds(mappedData, makeId)\n\n  return <RenderData data={resultData} />\n}\n```\nIf React Compiler produced the memoization you requested, the compiled output might look something like this.\n```\nexport default function MyApp() {\n  const { makeId } = useMakeId()\n  const { data } = useData()\n\n  let mappedData;\n  if (/* data changed */\n    mappedData = data ? data.map(item => item) : []\n  } else { /* reuse */ }\n  \n  let resultData;\n  if (/* mappedData or makeId changed */) {\n    resultData = appendIds(mappedData, makeId)\n  } else { /* reuse */ }\n  ...\n}\n```\nNow let's walk through what happens when we re-render with a changed `makeId` but the same `data`.\n1. We enter the else block of the first memo block, reusing the previous calculated `mappedData`. This already has IDs appended (from the previous render)\n2. We enter the if block of the second memo block, appending IDs (*again*) to mappedData. This is invalid.\n\n---\nDifferentiating between functions that change their arguments and ones that only read is difficult because neither typescript nor flow has deep read-only / mutable types. If you're a library author or working with widely reused functions, feel free to play around with the experimental / unstable `moduleTypeProvider` option, which should let you specify function effects\n---\n\nUser 'oleksii-kononykhin' said:\n---\n@mofeiZ Thank you for taking you time to answer! Your explanation makes perfect sense and thanks for a hint at `moduleTypeProvider`. Also if possible, would be nice to make such compiler behaviors more obvious.\n---\n\nUser 'NeutronDisk' said:\n---\nDepth = 104*7 = 91*8 = 9*9*9-1 = x728\n---",
    "question": "In facebook/react, : Make memoization more granular. Any ideas why?",
    "ideal_answer": "As a workaround I can try to make some of the functions \"hooks\" with explicit `'use memo'` directives, but I don't think this is the best way to go\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBYPjAIy8ABb4AvvgC8+KAoDKilUPH5J02UTK4y6rToUARK2WN0TZujPwBbMnwSETtZ2hM74APyW1gB0vgJMuAjeWgB8+AlJwsj4ANoAuuLuUp40TNQIMAFBNtq6CABiZYmVgc78cZhVzgA08oZwyoKFbhKVuLBsADwASgh0xDDVUWSawCRNFV3WGgD0KeJqw+K0DMys9g0bLdXtfp2t1r0KSgPCYhIA5HU+SRAf7mMJvh1uVFm0OlsyE9+oMDiA1EA\n\nIt is using react-stick controller to render react-compass view in tic tac toe board model  \n```\ndigital_board: [1, 2, 3, 4, 5, 6, 7, 8, 9];\nbyte_board: [x/o, x/o, x/o, x/o, x/o, x/o, x/o, x/o, x/o];\nrun_stick: 4*byte_board - digital_board;\nroute('/');\n```\n![Image](https://github.com/user-attachments/assets/073db9be-7f2c-4117-bedf-770dcb71cf1f)\n\n> ### What kind of issue is this?\n> * [x]  React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)[ ]  babel-plugin-react-compiler (build issue installing or using the Babel plugin)[ ]  eslint-plugin-react-compiler (build issue installing or using the eslint plugin)[ ]  react-compiler-healthcheck (build issue installing or using the healthcheck script)\n> \n> ### Link to repro\n> https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBYPjAIy8ABb4AvvgC8+KAoDKilUPH5J02UTK4y6rToUARK2WN0TZujPwBbMnwSETtZ2hM74APyW1gB0vgJMuAjeWgB8+AlJwsj4ANoAuuLuUp40TNQIMAFBNtokZYkw1fxxmFXOADTyhnDKgoVuEpW4sGwAPABKCHTEjWGh1prAdeWVgc4aAPQp4mriIGpAA\n> \n> ### Repro steps\n> Example is provided in the [playground link](https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBYPjAIy8ABb4AvvgC8+KAoDKilUPH5J02UTK4y6rToUARK2WN0TZujPwBbMnwSETtZ2hM74APyW1gB0vgJMuAjeWgB8+AlJwsj4ANoAuuLuUp40TNQIMAFBNtokZYkw1fxxmFXOADTyhnDKgoVuEpW4sGwAPABKCHTEjWGh1prAdeWVgc4aAPQp4mriIGpAA)\n> \n> In this example I would expect react compiler to memoize `mappedData` separately from the `filteredData` as I don't want to re-do the mapping each time when search string changes. What I would expect is for `filteredData` to have it's own memoization block with the `search` and `mappedData` as dependencies.\n> \n> ### How often does this bug happen?\n> Every time\n> \n> ### What version of React are you using?\n> 19\n> \n> ### What version of React Compiler are you using?\n> latest\n\nogbukosiw88p3v5\n\nThanks for the report! This isn't a bug as much as React Compiler not having access to function parameter types + effects. Right now, there's no way for us to understand that `filterData` does not write to its arguments. Replacing your filterData call with something that writes to its arguments, we see that it's invalid to memoize `mappedData` and `filteredData` separately.\n\n```js\nfunction appendIds(data, makeId) {\n  for (const entry of data) {\n    entry.id += makeId(entry);\n  }\n  return data;\n}\nexport default function MyApp() {\n  const { makeId } = useMakeId()\n  const { data } = useData()\n\n  const mappedData = data ? data.map(item => item) : []\n  const resultData = appendIds(mappedData, makeId)\n\n  return <RenderData data={resultData} />\n}\n```\nIf React Compiler produced the memoization you requested, the compiled output might look something like this.\n```\nexport default function MyApp() {\n  const { makeId } = useMakeId()\n  const { data } = useData()\n\n  let mappedData;\n  if (/* data changed */\n    mappedData = data ? data.map(item => item) : []\n  } else { /* reuse */ }\n\n  let resultData;\n  if (/* mappedData or makeId changed */) {\n    resultData = appendIds(mappedData, makeId)\n  } else { /* reuse */ }\n  ...\n}\n```\nNow let's walk through what happens when we re-render with a changed `makeId` but the same `data`.\n1. We enter the else block of the first memo block, reusing the previous calculated `mappedData`. This already has IDs appended (from the previous render)\n2. We enter the if block of the second memo block, appending IDs (*again*) to mappedData. This is invalid.\n\nDifferentiating between functions that change their arguments and ones that only read is difficult because neither typescript nor flow has deep read-only / mutable types. If you're a library author or working with widely reused functions, feel free to play around with the experimental / unstable `moduleTypeProvider` option, which should let you specify function effects\n\n@mofeiZ Thank you for taking you time to answer! Your explanation makes perfect sense and thanks for a hint at `moduleTypeProvider`. Also if possible, would be nice to make such compiler behaviors more obvious.\n\nDepth = 104*7 = 91*8 = 9*9*9-1 = x728"
  }
]